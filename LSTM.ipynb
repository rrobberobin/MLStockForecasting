{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "# from tensorflow.python.keras.models import Sequential\n",
    "# from tensorflow.python.keras.layers import Activation, Dense, Flatten\n",
    "# import tensorflow.python.keras.optimizers\n",
    "# import tensorflow.python.keras.metrics\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten\n",
    "import tensorflow.keras.optimizers\n",
    "import tensorflow.keras.metrics\n",
    "\n",
    "#%%capture\n",
    "data_file_path = os.path.join(os.getcwd(), \"Data\\\\returnsData.csv\")\n",
    "returnsData = pd.read_csv(data_file_path)\n",
    "#returnsData.info(verbose=True)\n",
    "longReturns = returnsData[[\"YearQuarter\", \"gvkey\", \"quarterlyReturns\"]] #\"quarterlyVolatility\",\n",
    "#longReturns.info(verbose=True)\n",
    "wideReturns = pd.pivot(longReturns, index=\"YearQuarter\", columns=\"gvkey\", values=\"quarterlyReturns\")\n",
    "#wideReturns.info(verbose=True)\n",
    "#print(wideReturns.iloc[0:10,0:10])\n",
    "\n",
    "#wideReturns[\"YearQuarter\"] = wideReturns.index\n",
    "#wideReturns[\"YQ\"] = wideReturns.index\n",
    "\n",
    "trainSize = math.floor(len(wideReturns.index) * 0.5)\n",
    "validationSize = math.floor(len(wideReturns.index) * 0.75)\n",
    "testSize = len(wideReturns.index)\n",
    "# trainSize = len(wideReturns[\"YearQuarter\"]) * 0.5\n",
    "# validationSize = len(wideReturns[\"YearQuarter\"]) * 0.75\n",
    "#wideReturns[:,\"Split\"] = \n",
    "\n",
    "class PreprocessData:\n",
    "\n",
    "  def __init__(self, sample_length=2):\n",
    "\n",
    "    dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "        data=wideReturns, targets=None,\n",
    "        sequence_length=sample_length, sequence_stride=1, sampling_rate=1,\n",
    "        batch_size=128,  shuffle=False, \n",
    "        start_index=None, end_index=None)\n",
    "\n",
    "    for batch in dataset:\n",
    "      data = batch\n",
    "      print(data.shape)\n",
    "      break\n",
    "\n",
    "    trainSize = math.floor(data.shape[0] * 0.5)\n",
    "    validationSize = math.floor(data.shape[0] * 0.75)\n",
    "    testSize = data.shape[0]\n",
    "\n",
    "    train = data[:trainSize, :, :]\n",
    "    validation = data[trainSize:validationSize, :, :]\n",
    "    test = data[validationSize:, :, :]\n",
    "\n",
    "    def reshapeData(rawData):\n",
    "      #[lags of YearQuarter, YearQuarter, gvkey] => [Batch, YearQuarter, None]\n",
    "      reshapedData = np.transpose(a=rawData, axes=(2, 0, 1))\n",
    "      reshapedData = np.reshape(reshapedData, (-1, rawData.shape[1]))\n",
    "      reshapedData = reshapedData[~np.isnan(reshapedData).any(axis=1),:]\n",
    "      reshapedData = np.expand_dims(reshapedData, axis=2)\n",
    "      return reshapedData\n",
    "\n",
    "    train = reshapeData(train)\n",
    "    validation = reshapeData(validation)\n",
    "    test = reshapeData(test)\n",
    "\n",
    "    xTrain, yTrain = train[:, :-1, :], train[:, -1, :]\n",
    "    xVal, yVal = validation[:, :-1, :], validation[:, -1, :]\n",
    "    xTest, yTest = test[:, :-1, :], test[:, -1, :]\n",
    "\n",
    "    # train = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "    #     data=xTrainReshaped, targets=yTrainReshaped,\n",
    "    #     sequence_length=sample_length, sequence_stride=1, sampling_rate=1,\n",
    "    #     batch_size=batch_size,  shuffle=False)\n",
    "    # validation = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "    #     data=xValReshaped, targets=yValReshaped,\n",
    "    #     sequence_length=sample_length, sequence_stride=1, sampling_rate=1,\n",
    "    #     batch_size=batch_size,  shuffle=False)\n",
    "    # test = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "    #     data=xTestReshaped, targets=yTestReshaped,\n",
    "    #     sequence_length=sample_length, sequence_stride=1, sampling_rate=1,\n",
    "    #     batch_size=batch_size,  shuffle=False)\n",
    "\n",
    "\n",
    "    normlayer = tf.keras.layers.Normalization(axis=None)\n",
    "    normlayer.adapt(xTrain) #Calculates some returns multiple times\n",
    "    self.xTrain, self.yTrain, self.xVal, self.yVal, self.xTest, self.yTest, self.normlayer = xTrain, yTrain, xVal, yVal, xTest, yTest, normlayer\n",
    "    #return xTrain, yTrain, xVal, yVal, xTest, yTest, normlayer\n",
    "\n",
    "#normlayer.adapt(xTrain[:,0,:])\n",
    "#normlayer.adapt(xTrain[:,[0, -1],:])\n",
    "# normlayer.adapt(wideReturns[trainSize, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(normlayer)\n",
    "# model.add(tf.keras.layers.LSTM(units=5)) #, input_shape=(,sample_length-1)   #mask\n",
    "# model.add(Dense(units=1))\n",
    "\n",
    "# model.compile(optimizer=\"adam\", #tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#               loss=\"MeanAbsoluteError\",\n",
    "#               metrics=[tf.keras.losses.MeanSquaredError(), tf.keras.losses.MeanAbsoluteError()])\n",
    "\n",
    "# #model.fit(x=dataset, batch_size=8)#, validation_data=(xVal, yVal))\n",
    "# model.fit(x=xTrain, y=yTrain, batch_size=32, validation_data=(xVal, yVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 2, 9668)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[  5.79090556]],\n",
       "\n",
       "       [[ 23.80602498]],\n",
       "\n",
       "       [[ 16.08308605]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-29.10334347]],\n",
       "\n",
       "       [[-32.11861379]],\n",
       "\n",
       "       [[ 52.63157895]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processedData = PreprocessData(2)\n",
    "processedData.xTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 01m 16s]\n",
      "val_mean_absolute_error: 14.042917251586914\n",
      "\n",
      "Best val_mean_absolute_error So Far: 14.042917251586914\n",
      "Total elapsed time: 00h 01m 16s\n",
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "1                 |1                 |units\n",
      "0.7               |0.5               |dropout\n",
      "0.0001            |0.01              |learning_rate\n",
      "10                |2                 |lags\n",
      "32                |16                |batch_size\n",
      "\n",
      "(44, 10, 9668)\n",
      "1595/4465 [=========>....................] - ETA: 26s - loss: 15.2778 - mean_squared_error: 674.2974 - mean_absolute_error: 15.2778"
     ]
    }
   ],
   "source": [
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        #model.add(normlayer)\n",
    "        model.add(tf.keras.layers.LSTM(\n",
    "            units=hp.Int(\"units\", min_value=1, max_value=10, step=1),\n",
    "            #input_shape=(sample_length-1,),\n",
    "            dropout = hp.Float(\"dropout\", min_value=0, max_value=0.9, step=0.1)\n",
    "\n",
    "        ))\n",
    "        model.add(Dense(units=1))\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Choice(\"learning_rate\", [0.01, 0.001, 0.0001]))\n",
    "        model.compile(optimizer = optimizer,\n",
    "                  loss=\"MeanAbsoluteError\",\n",
    "                  metrics=[tf.keras.losses.MeanSquaredError(), tf.keras.losses.MeanAbsoluteError()])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        \n",
    "        lag_length = hp.Int(\"lags\", min_value=2, max_value=15, step=1)\n",
    "        dataForFitting = PreprocessData(lag_length)\n",
    "        \n",
    "        return model.fit(\n",
    "            x=dataForFitting.xTrain, y=dataForFitting.yTrain,\n",
    "            batch_size=hp.Choice(\"batch_size\", [16, 32]),\n",
    "            validation_data=(dataForFitting.xVal, dataForFitting.yVal)\n",
    "        )\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    MyHyperModel(),\n",
    "    objective=\"val_mean_absolute_error\",\n",
    "    max_trials=3,\n",
    "    overwrite=True,\n",
    "    directory=\"LSTM_KerasTuner_ResultsDir\",\n",
    "    project_name=\"tune_hypermodel\",\n",
    ")\n",
    "\n",
    "tuner.search()\n",
    "\n",
    "bestModel = tuner.get_best_models(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6646144ec7618d86cefdfa307b5e5ba5f5893ee78adb19daf28ba13bcfecfb8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
