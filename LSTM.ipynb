{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <keras.optimizers.optimizer_v2.adam.Adam object at 0x000001142660A0B0>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [24], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m model\u001b[39m.\u001b[39madd(tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mLSTM(units\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, input_shape\u001b[39m=\u001b[39m(sample_length\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,)))    \u001b[39m#mask\u001b[39;00m\n\u001b[0;32m     62\u001b[0m model\u001b[39m.\u001b[39madd(Dense(units\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m---> 64\u001b[0m model\u001b[39m.\u001b[39;49mcompile(optimizer\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49moptimizers\u001b[39m.\u001b[39;49mAdam(learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m),\n\u001b[0;32m     65\u001b[0m               loss\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mMeanAbsoluteError\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     66\u001b[0m               metrics\u001b[39m=\u001b[39;49m[tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlosses\u001b[39m.\u001b[39;49mMeanSquaredError(), tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlosses\u001b[39m.\u001b[39;49mMeanAbsoluteError()])\n\u001b[0;32m     68\u001b[0m model\u001b[39m.\u001b[39mfit(x\u001b[39m=\u001b[39mxTrain, y\u001b[39m=\u001b[39myTrain, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[39m# class MyHyperModel(kt.HyperModel):\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m#     def build(self, hp):\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39m#         model = Sequential()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39m#     project_name=\"tune_hypermodel\",\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39m# )\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RobinForMLThesis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:568\u001b[0m, in \u001b[0;36mModel.compile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_compile(optimizer, metrics, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    566\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_eagerly \u001b[39m=\u001b[39m run_eagerly\n\u001b[1;32m--> 568\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_optimizer(optimizer)\n\u001b[0;32m    569\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiled_loss \u001b[39m=\u001b[39m compile_utils\u001b[39m.\u001b[39mLossesContainer(\n\u001b[0;32m    570\u001b[0m     loss, loss_weights, output_names\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_names)\n\u001b[0;32m    571\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiled_metrics \u001b[39m=\u001b[39m compile_utils\u001b[39m.\u001b[39mMetricsContainer(\n\u001b[0;32m    572\u001b[0m     metrics, weighted_metrics, output_names\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_names,\n\u001b[0;32m    573\u001b[0m     from_serialized\u001b[39m=\u001b[39mfrom_serialized)\n",
      "File \u001b[1;32mc:\\Users\\RobinForMLThesis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:606\u001b[0m, in \u001b[0;36mModel._get_optimizer\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m    603\u001b[0m       opt \u001b[39m=\u001b[39m lso\u001b[39m.\u001b[39mLossScaleOptimizerV1(opt, loss_scale)\n\u001b[0;32m    604\u001b[0m   \u001b[39mreturn\u001b[39;00m opt\n\u001b[1;32m--> 606\u001b[0m \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39;49mmap_structure(_get_single_optimizer, optimizer)\n",
      "File \u001b[1;32mc:\\Users\\RobinForMLThesis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\RobinForMLThesis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\RobinForMLThesis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:597\u001b[0m, in \u001b[0;36mModel._get_optimizer.<locals>._get_single_optimizer\u001b[1;34m(opt)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_single_optimizer\u001b[39m(opt):\n\u001b[1;32m--> 597\u001b[0m   opt \u001b[39m=\u001b[39m optimizers\u001b[39m.\u001b[39;49mget(opt)\n\u001b[0;32m    598\u001b[0m   \u001b[39mif\u001b[39;00m (loss_scale \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    599\u001b[0m       \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(opt, lso\u001b[39m.\u001b[39mLossScaleOptimizer)):\n\u001b[0;32m    600\u001b[0m     \u001b[39mif\u001b[39;00m loss_scale \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdynamic\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\RobinForMLThesis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py:131\u001b[0m, in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    129\u001b[0m   \u001b[39mreturn\u001b[39;00m deserialize(config)\n\u001b[0;32m    130\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    132\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mCould not interpret optimizer identifier: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(identifier))\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret optimizer identifier: <keras.optimizers.optimizer_v2.adam.Adam object at 0x000001142660A0B0>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Activation, Dense, Flatten\n",
    "import tensorflow.python.keras.optimizers\n",
    "import tensorflow.python.keras.metrics\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "#%%capture\n",
    "data_file_path = os.path.join(os.getcwd(), \"Data\\\\returnsData.csv\")\n",
    "returnsData = pd.read_csv(data_file_path)\n",
    "#returnsData.info(verbose=True)\n",
    "longReturns = returnsData[[\"YearQuarter\", \"gvkey\", \"quarterlyReturns\"]] #\"quarterlyVolatility\",\n",
    "#longReturns.info(verbose=True)\n",
    "wideReturns = pd.pivot(longReturns, index=\"YearQuarter\", columns=\"gvkey\", values=\"quarterlyReturns\")\n",
    "#wideReturns.info(verbose=True)\n",
    "#print(wideReturns.iloc[0:10,0:10])\n",
    "\n",
    "sample_length = 10\n",
    "dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "    data=wideReturns, targets=None,\n",
    "    sequence_length=sample_length, sequence_stride=1, sampling_rate=1,\n",
    "    batch_size=128,  shuffle=False)\n",
    "\n",
    "for batch in dataset:\n",
    "  data = batch\n",
    "  #assert np.array_equal(inputs[0], wideReturns[:sample_length])\n",
    "  # second sample equals output timestamps 20-40\n",
    "  #assert np.array_equal(targets[1], wideReturns[sample_length:2*sample_length])\n",
    "  break\n",
    "\n",
    "# targets = data[:,-1,:]\n",
    "# inputs = data[:,:-1,:]\n",
    "\n",
    "# a = inputs[..., 0:20]\n",
    "#print(inputs[..., 0:10])\n",
    "#print(targets.iloc[0:10,0:10])\n",
    "\n",
    "# #[lags of YearQuarter, YearQuarter, gvkey] => [Batch, YearQuarter, None]\n",
    "# finalData = np.transpose(a=inputs, axes = (2,0,1))\n",
    "# finalData = np.reshape(finalData, (-1, sample_length-1))\n",
    "# #b = finalData[0:50,:]\n",
    "\n",
    "\n",
    "\n",
    "finalData = np.transpose(a=data, axes = (2,0,1))\n",
    "finalData = np.reshape(finalData, (-1, sample_length))\n",
    "# input = finalData[:,:-1]\n",
    "# targets = finalData[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "xTrain, xVal, xTest = finalData[:,:-1], 0, 0\n",
    "yTrain, yVal, yTest = finalData[:,-1], 0, 0\n",
    "\n",
    "model = Sequential()\n",
    "model.add(tf.keras.layers.LSTM(units=5, input_shape=(sample_length-1,)))    #mask\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=\"MeanAbsoluteError\",\n",
    "              metrics=[tf.keras.losses.MeanSquaredError(), tf.keras.losses.MeanAbsoluteError()])\n",
    "\n",
    "model.fit(x=xTrain, y=yTrain, batch_size=32)\n",
    "\n",
    "# class MyHyperModel(kt.HyperModel):\n",
    "#     def build(self, hp):\n",
    "#         model = Sequential()\n",
    "#         model.add(tf.keras.layers.LSTM(units=hp.Int(\"units\", min_value=32, max_value=512, step=32), input_shape=(sample_length-1,)))\n",
    "#         model.add(Dense(units=1))\n",
    "\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Int(\"learning_rate\", [16, 32])),\n",
    "#                   loss=\"MeanAbsoluteError\",\n",
    "#                   metrics=[tf.keras.losses.MeanSquaredError(), tf.keras.losses.MeanAbsoluteError()])\n",
    "#         return model\n",
    "\n",
    "#     def fit(self, hp, model, *args, **kwargs):\n",
    "#         return model.fit(\n",
    "#             *args,\n",
    "#             batch_size=hp.Choice(\"batch_size\", [16, 32]),\n",
    "#             **kwargs,\n",
    "#         )\n",
    "\n",
    "# tuner = kt.RandomSearch(\n",
    "#     MyHyperModel(),\n",
    "#     objective=\"val_accuracy\",\n",
    "#     max_trials=3,\n",
    "#     overwrite=True,\n",
    "#     directory=\"my_dir\",\n",
    "#     project_name=\"tune_hypermodel\",\n",
    "# )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6646144ec7618d86cefdfa307b5e5ba5f5893ee78adb19daf28ba13bcfecfb8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
