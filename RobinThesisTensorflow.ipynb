{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOC:\n",
    "* [First Bullet Header](#first-bullet)\n",
    "* [Second Bullet Header](#second-bullet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(\"Num GPUs Available: \", len(physical_devices))\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras #I think this package produces a discrepency between val_loss and val_metric (where both are MAE)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "import tensorflow.keras.optimizers\n",
    "import tensorflow.keras.metrics\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "pd.set_option(\"display.precision\", 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "largeSample = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if largeSample:\n",
    "    dataSample = \"Large\"\n",
    "    data_file_path = os.path.join(os.getcwd(), \"Data\\YXLarge.csv\")\n",
    "else:\n",
    "    dataSample = \"Ordinary\"\n",
    "    data_file_path = os.path.join(os.getcwd(), \"Data\\YX.csv\")\n",
    "\n",
    "\n",
    "data = pd.read_csv(data_file_path)\n",
    "data = data.dropna(axis=1, how='all').drop([\"compstq\"], axis=1)\n",
    "\n",
    "\n",
    "# data = dataset.map(..., num_parallel_calls=10)\n",
    "# data = dataset.prefetch(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_file_path = os.path.join(os.getcwd(), \"Data\\OriginalData\\CompustatPrices.csv\")\n",
    "# prices = pd.read_csv(data_file_path, \n",
    "#                  nrows=270000, float_precision='round_trip', dtype = {\"ajexdi\": np.float64})[269000:270000]\n",
    "\n",
    "\n",
    "# data_file_path = os.path.join(os.getcwd(), \"Data\\Other\\AjexdiZero.csv\")\n",
    "# prices = pd.read_csv(data_file_path)\n",
    "\n",
    "# data_file_path = os.path.join(os.getcwd(), \"Data\\Other\\AjexdiZeroCompressed.csv\")\n",
    "# prices = pd.read_csv(data_file_path)\n",
    "\n",
    "\n",
    "# #sum(prices[\"ajexdi\"])\n",
    "# prices.loc[prices[\"ajexdi\"]==0.0]\n",
    "\n",
    "# , dtype = {‘ajexdi’: np.float64}, ‘b’: np.int32, ‘c’: ‘Int64’}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 805843 entries, 0 to 805842\n",
      "Data columns (total 210 columns):\n",
      " #    Column                       Dtype  \n",
      "---   ------                       -----  \n",
      " 0    gvkey                        int64  \n",
      " 1    fdateYQ                      float64\n",
      " 2    returns                      float64\n",
      " 3    volatility                   float64\n",
      " 4    MktCap                       float64\n",
      " 5    Split                        object \n",
      " 6    TimeDifferenceInDays         int64  \n",
      " 7    loc                          object \n",
      " 8    city                         object \n",
      " 9    acctstdq                     object \n",
      " 10   compstq                      object \n",
      " 11   Sector                       float64\n",
      " 12   Subsector                    float64\n",
      " 13   IndustryGroup                float64\n",
      " 14   Industry                     float64\n",
      " 15   NationalIndustry             float64\n",
      " 16   accdq                        float64\n",
      " 17   acoq                         float64\n",
      " 18   acoxq                        float64\n",
      " 19   actq                         float64\n",
      " 20   ancq                         float64\n",
      " 21   aoq                          float64\n",
      " 22   apq                          float64\n",
      " 23   atq                          float64\n",
      " 24   capsq                        float64\n",
      " 25   ceqq                         float64\n",
      " 26   cheq                         float64\n",
      " 27   cogsq                        float64\n",
      " 28   cstkq                        float64\n",
      " 29   dfxaq                        float64\n",
      " 30   dlcq                         float64\n",
      " 31   dlttq                        float64\n",
      " 32   dpq                          float64\n",
      " 33   eqrtq                        float64\n",
      " 34   eroq                         float64\n",
      " 35   gpq                          float64\n",
      " 36   ibmiiq                       float64\n",
      " 37   ibq                          float64\n",
      " 38   iditq                        float64\n",
      " 39   intanq                       float64\n",
      " 40   invtq                        float64\n",
      " 41   ivaoq                        float64\n",
      " 42   lcoq                         float64\n",
      " 43   lcoxq                        float64\n",
      " 44   lctq                         float64\n",
      " 45   lltq                         float64\n",
      " 46   loq                          float64\n",
      " 47   lseq                         float64\n",
      " 48   ltmibq                       float64\n",
      " 49   ltq                          float64\n",
      " 50   nopioq                       float64\n",
      " 51   nopiq                        float64\n",
      " 52   oiadpq                       float64\n",
      " 53   oibdpq                       float64\n",
      " 54   piq                          float64\n",
      " 55   ppentq                       float64\n",
      " 56   reccoq                       float64\n",
      " 57   rectoq                       float64\n",
      " 58   rectq                        float64\n",
      " 59   rectrq                       float64\n",
      " 60   req                          float64\n",
      " 61   revtq                        float64\n",
      " 62   saleq                        float64\n",
      " 63   sctq                         float64\n",
      " 64   seqq                         float64\n",
      " 65   teqq                         float64\n",
      " 66   txtq                         float64\n",
      " 67   xintq                        float64\n",
      " 68   xoproq                       float64\n",
      " 69   xoprq                        float64\n",
      " 70   xsgaq                        float64\n",
      " 71   quarterlyReturnsLagged1Q     float64\n",
      " 72   quarterlyReturnsLagged2Q     float64\n",
      " 73   quarterlyReturnsLagged3Q     float64\n",
      " 74   quarterlyReturnsLagged4Q     float64\n",
      " 75   quarterlyVolatilityLagged1Q  float64\n",
      " 76   quarterlyVolatilityLagged2Q  float64\n",
      " 77   quarterlyVolatilityLagged3Q  float64\n",
      " 78   quarterlyVolatilityLagged4Q  float64\n",
      " 79   yearlyReturnsLagged1Y        float64\n",
      " 80   yearlyReturnsLagged2Y        float64\n",
      " 81   yearlyReturnsLagged3Y        float64\n",
      " 82   yearlyReturnsLagged4Y        float64\n",
      " 83   past2YearReturn              float64\n",
      " 84   past3YearReturn              float64\n",
      " 85   past4YearReturn              float64\n",
      " 86   past5YearReturn              float64\n",
      " 87   past2YearVolatility          float64\n",
      " 88   past3YearVolatility          float64\n",
      " 89   past4YearVolatility          float64\n",
      " 90   past5YearVolatility          float64\n",
      " 91   2Classes                     float64\n",
      " 92   4Classes                     float64\n",
      " 93   6Classes                     float64\n",
      " 94   8Classes                     float64\n",
      " 95   10Classes                    float64\n",
      " 96   Simple2Classes               float64\n",
      " 97   Simple4Classes               float64\n",
      " 98   Simple6Classes               float64\n",
      " 99   Simple8Classes               float64\n",
      " 100  Simple10Classes              float64\n",
      " 101  accoq                        float64\n",
      " 102  adpacq                       float64\n",
      " 103  amq                          float64\n",
      " 104  aotq                         float64\n",
      " 105  apoq                         float64\n",
      " 106  artfsq                       float64\n",
      " 107  autxrq                       float64\n",
      " 108  bcefq                        float64\n",
      " 109  bctq                         float64\n",
      " 110  bdiq                         float64\n",
      " 111  capcstq                      float64\n",
      " 112  capr1q                       float64\n",
      " 113  capr2q                       float64\n",
      " 114  capr3q                       float64\n",
      " 115  caq                          float64\n",
      " 116  cfbdq                        float64\n",
      " 117  cfereq                       float64\n",
      " 118  cfoq                         float64\n",
      " 119  cfpdoq                       float64\n",
      " 120  chq                          float64\n",
      " 121  chsq                         float64\n",
      " 122  cltq                         float64\n",
      " 123  dfpacq                       float64\n",
      " 124  ditq                         float64\n",
      " 125  dpactq                       float64\n",
      " 126  dptbq                        float64\n",
      " 127  dptcq                        float64\n",
      " 128  dvpdpq                       float64\n",
      " 129  dvrreq                       float64\n",
      " 130  dvtq                         float64\n",
      " 131  esubq                        float64\n",
      " 132  fcaq                         float64\n",
      " 133  feaq                         float64\n",
      " 134  felq                         float64\n",
      " 135  gdwlamq                      float64\n",
      " 136  gdwlq                        float64\n",
      " 137  iatiq                        float64\n",
      " 138  ibkiq                        float64\n",
      " 139  iireq                        float64\n",
      " 140  iitq                         float64\n",
      " 141  intcq                        float64\n",
      " 142  iobdq                        float64\n",
      " 143  ioiq                         float64\n",
      " 144  ioreq                        float64\n",
      " 145  ipq                          float64\n",
      " 146  iptiq                        float64\n",
      " 147  isgtq                        float64\n",
      " 148  istq                         float64\n",
      " 149  ivaeqq                       float64\n",
      " 150  iviq                         float64\n",
      " 151  ivptq                        float64\n",
      " 152  ivstq                        float64\n",
      " 153  ivtfsq                       float64\n",
      " 154  lcabgq                       float64\n",
      " 155  lcacuq                       float64\n",
      " 156  lsq                          float64\n",
      " 157  mibnq                        float64\n",
      " 158  mibq                         float64\n",
      " 159  mibtq                        float64\n",
      " 160  miiq                         float64\n",
      " 161  mtlq                         float64\n",
      " 162  nitq                         float64\n",
      " 163  oproq                        float64\n",
      " 164  pclq                         float64\n",
      " 165  prcq                         float64\n",
      " 166  pstkq                        float64\n",
      " 167  ptranq                       float64\n",
      " 168  pvoq                         float64\n",
      " 169  pvtq                         float64\n",
      " 170  ratiq                        float64\n",
      " 171  rawmsmq                      float64\n",
      " 172  reitq                        float64\n",
      " 173  risq                         float64\n",
      " 174  rltq                         float64\n",
      " 175  rvlrvq                       float64\n",
      " 176  rvtiq                        float64\n",
      " 177  rvutxq                       float64\n",
      " 178  saaq                         float64\n",
      " 179  salq                         float64\n",
      " 180  sbdcq                        float64\n",
      " 181  scoq                         float64\n",
      " 182  scq                          float64\n",
      " 183  spiq                         float64\n",
      " 184  ssnpq                        float64\n",
      " 185  stkchq                       float64\n",
      " 186  tdsgq                        float64\n",
      " 187  tdstq                        float64\n",
      " 188  transaq                      float64\n",
      " 189  tstkq                        float64\n",
      " 190  txdbq                        float64\n",
      " 191  unnpq                        float64\n",
      " 192  xagtq                        float64\n",
      " 193  xbdtq                        float64\n",
      " 194  xcomiq                       float64\n",
      " 195  xcomq                        float64\n",
      " 196  xdvreq                       float64\n",
      " 197  xioq                         float64\n",
      " 198  xiq                          float64\n",
      " 199  xiviq                        float64\n",
      " 200  xivreq                       float64\n",
      " 201  xobdq                        float64\n",
      " 202  xoiq                         float64\n",
      " 203  xoreq                        float64\n",
      " 204  xppq                         float64\n",
      " 205  xretq                        float64\n",
      " 206  xsq                          float64\n",
      " 207  xstoq                        float64\n",
      " 208  xstq                         float64\n",
      " 209  xtq                          float64\n",
      "dtypes: float64(203), int64(2), object(5)\n",
      "memory usage: 1.3+ GB\n"
     ]
    }
   ],
   "source": [
    "data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "naPercentage = data.isna().sum() / len(data) * 100\n",
    "naPercentage\n",
    "\n",
    "filteredData = data.loc[:, naPercentage<75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe\n",
    "# finalData = dask.dataframe.read_csv(data_file_path)\n",
    "# finalData\n",
    "#finalData.dtypes\n",
    "\n",
    "# trainY = finalData[\"returns\"][0:20].compute()\n",
    "# trainY\n",
    "# trainX = finalData.loc[:, finalData.columns != \"returns\"]\n",
    "# trainX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if largeSample:\n",
    "    data = filteredData\n",
    "\n",
    "\n",
    "descrNames = [\"gvkey\", \"fdateYQ\", \"Split\"] + [\"MktCap\"]\n",
    "dummyNames = [\"loc\", \"Sector\", \"acctstdq\"] # , \"compstq\", \"bsprq\", \"scfq\", \"staltq\",\n",
    "detailedDummyNames = [\"city\", \"Subsector\", \"IndustryGroup\", \"Industry\", \"NationalIndustry\"]\n",
    "dependentNames = [\"returns\", \"volatility\",\n",
    "                  \"2Classes\", \"4Classes\", \"6Classes\", \"8Classes\", \"10Classes\",\n",
    "                  \"Simple2Classes\", \"Simple4Classes\", \"Simple6Classes\", \"Simple8Classes\", \"Simple10Classes\"]\n",
    "\n",
    "dontCreateRatio = [#\"quarterlyReturns-1\", \"quarterlyReturns-2\", \"quarterlyReturns-3\", \"quarterlyReturns-4\",\n",
    "                   #\"quarterlyVolatility-1\",  \"quarterlyVolatility-2\", \"quarterlyVolatility-3\", \"quarterlyVolatility-4\",\n",
    "                   \"past2YearReturn\", \"past3YearReturn\", \"past4YearReturn\", \"past5YearReturn\",\n",
    "                   \"quarterlyReturnsLagged1Q\", \"quarterlyReturnsLagged2Q\", \"quarterlyReturnsLagged3Q\", \"quarterlyReturnsLagged4Q\", \n",
    "                   \"quarterlyVolatilityLagged1Q\",  \"quarterlyVolatilityLagged2Q\", \"quarterlyVolatilityLagged3Q\", \"quarterlyVolatilityLagged4Q\", \n",
    "                   \"yearlyReturnsLagged1Y\", \"yearlyReturnsLagged2Y\", \"yearlyReturnsLagged3Y\", \"yearlyReturnsLagged4Y\", \n",
    "                   \"past2YearVolatility\", \"past3YearVolatility\", \"past4YearVolatility\", \"past5YearVolatility\"]\n",
    "\n",
    "\n",
    "dontWinsorizeCols = [\"TimeDifferenceInDays\"]\n",
    "dontNormalizeCols = dependentNames + descrNames + dummyNames + detailedDummyNames\n",
    "\n",
    "ratioCols = data.columns.difference(dontCreateRatio + dontWinsorizeCols + dontNormalizeCols).to_list()\n",
    "winsCols = data.columns.difference(dontWinsorizeCols + dontNormalizeCols).to_list()\n",
    "numCols = data.columns.difference(dontNormalizeCols).to_list()\n",
    "\n",
    "\n",
    "#finalData[factorCols] = finalData[factorCols].astype(\"category\")\n",
    "#finalData[numCols] = finalData[numCols].astype('float32')\n",
    "#finalData[numCols] = finalData[numCols].astype('int32')\n",
    "\n",
    "\n",
    "#finalData[\"fdateq\"] = pd.to_datetime(finalData[\"fdateq\"], format = \"%Y%m%d\")\n",
    "#finalData[\"datadate\"] = pd.to_datetime(finalData[\"datadate\"], format = \"%Y%m%d\")\n",
    "#finalData[\"fdateYQ\"] = pd.to_datetime(finalData[\"fdateYQ\"], format = \"%Y%q\") #check quarter format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeDifferenceInDays</th>\n",
       "      <th>accdq</th>\n",
       "      <th>acoq</th>\n",
       "      <th>acoxq</th>\n",
       "      <th>actq</th>\n",
       "      <th>amq</th>\n",
       "      <th>ancq</th>\n",
       "      <th>aoq</th>\n",
       "      <th>apq</th>\n",
       "      <th>atq</th>\n",
       "      <th>...</th>\n",
       "      <th>xintq</th>\n",
       "      <th>xoproq</th>\n",
       "      <th>xoprq</th>\n",
       "      <th>xppq</th>\n",
       "      <th>xsgaq</th>\n",
       "      <th>xstq</th>\n",
       "      <th>yearlyReturnsLagged1Y</th>\n",
       "      <th>yearlyReturnsLagged2Y</th>\n",
       "      <th>yearlyReturnsLagged3Y</th>\n",
       "      <th>yearlyReturnsLagged4Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5</td>\n",
       "      <td>-1.14541e+04</td>\n",
       "      <td>-207943.0</td>\n",
       "      <td>-207943.0</td>\n",
       "      <td>-3.33000e-01</td>\n",
       "      <td>-758643.0</td>\n",
       "      <td>-1.04200e+00</td>\n",
       "      <td>-3.99115e+05</td>\n",
       "      <td>-4.13288e+03</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.62793e+06</td>\n",
       "      <td>-8.09732e+06</td>\n",
       "      <td>-8.09732e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.65893e+03</td>\n",
       "      <td>-4.76666e+02</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>2.68383e+08</td>\n",
       "      <td>42092278.0</td>\n",
       "      <td>42057082.0</td>\n",
       "      <td>2.36287e+08</td>\n",
       "      <td>4212436.0</td>\n",
       "      <td>4.43236e+09</td>\n",
       "      <td>4.43128e+09</td>\n",
       "      <td>4.26825e+07</td>\n",
       "      <td>4.51633e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.97205e+09</td>\n",
       "      <td>2.55095e+11</td>\n",
       "      <td>2.55095e+11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.72297e+07</td>\n",
       "      <td>7.70791e+06</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TimeDifferenceInDays        accdq        acoq       acoxq         actq  \\\n",
       "0                    -5 -1.14541e+04   -207943.0   -207943.0 -3.33000e-01   \n",
       "1                    99  2.68383e+08  42092278.0  42057082.0  2.36287e+08   \n",
       "\n",
       "         amq         ancq          aoq          apq          atq  ...  \\\n",
       "0  -758643.0 -1.04200e+00 -3.99115e+05 -4.13288e+03  0.00000e+00  ...   \n",
       "1  4212436.0  4.43236e+09  4.43128e+09  4.26825e+07  4.51633e+09  ...   \n",
       "\n",
       "         xintq       xoproq        xoprq  xppq        xsgaq         xstq  \\\n",
       "0 -8.62793e+06 -8.09732e+06 -8.09732e+06   NaN -3.65893e+03 -4.76666e+02   \n",
       "1  1.97205e+09  2.55095e+11  2.55095e+11   NaN  1.72297e+07  7.70791e+06   \n",
       "\n",
       "   yearlyReturnsLagged1Y  yearlyReturnsLagged2Y  yearlyReturnsLagged3Y  \\\n",
       "0                 -100.0                 -100.0                 -100.0   \n",
       "1                    inf                    inf                    inf   \n",
       "\n",
       "   yearlyReturnsLagged4Y  \n",
       "0                 -100.0  \n",
       "1                    inf  \n",
       "\n",
       "[2 rows x 89 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xLogData = finalData[numCols].apply(lambda x: np.log(x+0.001))\n",
    "data[numCols].apply(lambda x: (min(x), max(x)))\n",
    "# plt.hist(xLogData, bins=1000)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finalData.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #xLogData = xRatioData[\"revtq\"]\n",
    "# xLogData = finalData[\"revtq\"].apply(lambda x: np.log(x+0.0001))\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(xLogData, bins=1000)\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformations\n",
    "#### Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ratio data\n",
    "ratioData = filteredData.copy()\n",
    "ratioData[ratioCols] = ratioData[ratioCols].div(ratioData[\"MktCap\"].values, axis=0) #This is an inplace operation if copy() is not used\n",
    "#ratioData[ratioCols] = finalData[ratioCols].apply(lambda x: x/finalData[\"MktCap\"].values, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import math\n",
    "# #xLogData = math.log(ratioData[\"returns\"])\n",
    "# #xLogData = np.log(ratioData[\"returns\"] + 105)\n",
    "# #xLogData = np.log1p(ratioData[\"returns\"]/105)\n",
    "# #xLogData = ratioData[numCols].apply(lambda x: np.log(x), axis=1)\n",
    "# #xLogData = ratioData[\"revtq\"]\n",
    "# xLogData = ratioData[\"revtq\"].apply(lambda x: np.log(x+0.0001))\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(xLogData, bins=1000)\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Winsorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accdq\n",
      "acoq\n",
      "acoxq\n",
      "actq\n",
      "amq\n",
      "ancq\n",
      "aoq\n",
      "apq\n",
      "atq\n",
      "capsq\n",
      "ceqq\n",
      "cheq\n",
      "cogsq\n",
      "cstkq\n",
      "dfxaq\n",
      "dlcq\n",
      "dlttq\n",
      "dpactq\n",
      "dpq\n",
      "eqrtq\n",
      "eroq\n",
      "gdwlq\n",
      "gpq\n",
      "ibmiiq\n",
      "ibq\n",
      "iditq\n",
      "intanq\n",
      "invtq\n",
      "ivaeqq\n",
      "ivaoq\n",
      "lcoq\n",
      "lcoxq\n",
      "lctq\n",
      "lltq\n",
      "loq\n",
      "lseq\n",
      "ltmibq\n",
      "ltq\n",
      "mibnq\n",
      "mibtq\n",
      "miiq\n",
      "nopioq\n",
      "nopiq\n",
      "oiadpq\n",
      "oibdpq\n",
      "oproq\n",
      "past2YearReturn\n",
      "past2YearVolatility\n",
      "past3YearReturn\n",
      "past3YearVolatility\n",
      "past4YearReturn\n",
      "past4YearVolatility\n",
      "past5YearReturn\n",
      "past5YearVolatility\n",
      "piq\n",
      "ppentq\n",
      "quarterlyReturnsLagged1Q\n",
      "quarterlyReturnsLagged2Q\n",
      "quarterlyReturnsLagged3Q\n",
      "quarterlyReturnsLagged4Q\n",
      "quarterlyVolatilityLagged1Q\n",
      "quarterlyVolatilityLagged2Q\n",
      "quarterlyVolatilityLagged3Q\n",
      "quarterlyVolatilityLagged4Q\n",
      "reccoq\n",
      "rectoq\n",
      "rectq\n",
      "rectrq\n",
      "req\n",
      "revtq\n",
      "saleq\n",
      "sctq\n",
      "seqq\n",
      "spiq\n",
      "teqq\n",
      "tstkq\n",
      "txdbq\n",
      "txtq\n",
      "xintq\n",
      "xoproq\n",
      "xoprq\n",
      "xppq\n",
      "xsgaq\n",
      "xstq\n",
      "yearlyReturnsLagged1Y\n",
      "yearlyReturnsLagged2Y\n",
      "yearlyReturnsLagged3Y\n",
      "yearlyReturnsLagged4Y\n"
     ]
    }
   ],
   "source": [
    "#Winsorised data\n",
    "lowerLimit = 0.05\n",
    "upperLimit = 0.05\n",
    "\n",
    "winData=ratioData.copy()\n",
    "#winData[winsCols] = ratioData[winsCols].apply(lambda x: scipy.stats.mstats.winsorize(x, limits = (lowerLimit, upperLimit))) #This is an inplace operation if copy() is not used\n",
    "winData.loc[winData[\"Split\"]==\"Train\", winsCols] = ratioData.loc[ratioData[\"Split\"]==\"Train\", winsCols].apply(lambda x: scipy.stats.mstats.winsorize(x, limits = (lowerLimit, upperLimit))) #This is an inplace operation if copy() is not used\n",
    "#Should we winsorize and scale the dependent variable?\n",
    "\n",
    "\n",
    "for col in winData[winsCols].columns:\n",
    "    maxThreshold = max(winData.loc[winData[\"Split\"]==\"Train\", col])\n",
    "    minThreshold = min(winData.loc[winData[\"Split\"]==\"Train\", col])\n",
    "\n",
    "    valMaxCondition = (winData[\"Split\"]==\"Validation\") & (winData.loc[winData[\"Split\"]==\"Validation\", col] > maxThreshold)    \n",
    "    testMaxCondition = (winData[\"Split\"]==\"Test\") & (winData.loc[ratioData[\"Split\"]==\"Test\", col] > maxThreshold)\n",
    "\n",
    "    valMinCondition = (winData[\"Split\"]==\"Validation\") & (winData.loc[winData[\"Split\"]==\"Validation\", col] < minThreshold)\n",
    "    testMinCondition = (winData[\"Split\"]==\"Test\") & (winData.loc[winData[\"Split\"]==\"Test\", col] < minThreshold)\n",
    "\n",
    "    winData.loc[valMaxCondition, col] = maxThreshold\n",
    "    winData.loc[testMaxCondition, col] = maxThreshold\n",
    "\n",
    "    winData.loc[valMinCondition, col] = minThreshold\n",
    "    winData.loc[testMinCondition, col] = minThreshold\n",
    "    print(col)\n",
    "\n",
    "\n",
    "#winData.loc[winData[\"Split\"]==\"Train\", numCols].describe()\n",
    "#ratioData.loc[winData[\"Split\"]==\"Train\", numCols].describe()\n",
    "\n",
    "# a = ratioData.loc[ratioData[\"Split\"]==\"Train\", winsCols]\n",
    "# low_limit = 0.01\n",
    "# up_limit = 0.01\n",
    "\n",
    "# n = a.count()\n",
    "# idx = a.argsort()\n",
    "# lowidx = int(low_limit * n)\n",
    "# a[idx[:lowidx]] = a[idx[lowidx]]\n",
    "\n",
    "# upidx = n - int(n * up_limit)\n",
    "# a[idx[upidx:]] = a[idx[upidx - 1]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xLogData = winData[\"revtq\"].apply(lambda x: np.log(x)) \n",
    "# #xLogData = scaledData[\"revtq\"].apply(lambda x: np.log(x+0.005)) \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# #plt.hist(xLogData, bins=100)\n",
    "# #plt.hist(ratioData[\"revtq\"], bins=1000)\n",
    "# plt.hist(winData[\"revtq\"], bins=1000)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RobinForMLThesis\\AppData\\Local\\Temp\\ipykernel_7944\\2403715917.py:12: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n",
      "  finalData.iloc[finalData[numCols].index[np.isinf(finalData[numCols]).any(1)].union(\n",
      "C:\\Users\\RobinForMLThesis\\AppData\\Local\\Temp\\ipykernel_7944\\2403715917.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n",
      "  finalData[numCols].index[np.isinf(finalData[numCols]).any(1)] - 1), :][finalData[numCols].columns[np.isinf(finalData[numCols]).any()].union([\"returns\"])]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>past2YearReturn</th>\n",
       "      <th>past3YearReturn</th>\n",
       "      <th>past4YearReturn</th>\n",
       "      <th>quarterlyReturnsLagged1Q</th>\n",
       "      <th>quarterlyReturnsLagged2Q</th>\n",
       "      <th>quarterlyReturnsLagged3Q</th>\n",
       "      <th>returns</th>\n",
       "      <th>yearlyReturnsLagged1Y</th>\n",
       "      <th>yearlyReturnsLagged2Y</th>\n",
       "      <th>yearlyReturnsLagged3Y</th>\n",
       "      <th>yearlyReturnsLagged4Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147751</th>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>-6.66667e+01</td>\n",
       "      <td>-4.00000e+01</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>-70.58824</td>\n",
       "      <td>1.42857e+02</td>\n",
       "      <td>-5.88235e+01</td>\n",
       "      <td>-5.52632e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147752</th>\n",
       "      <td>-93.14286</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.42857e+01</td>\n",
       "      <td>1.11111e+01</td>\n",
       "      <td>-5.43478e+01</td>\n",
       "      <td>-3.66667e+01</td>\n",
       "      <td>-84.22857</td>\n",
       "      <td>inf</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>-7.05882e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147753</th>\n",
       "      <td>-78.26087</td>\n",
       "      <td>-96.57143</td>\n",
       "      <td>inf</td>\n",
       "      <td>-2.50000e+01</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>5.26316e+00</td>\n",
       "      <td>-2.33333e+01</td>\n",
       "      <td>-56.52174</td>\n",
       "      <td>-8.42286e+01</td>\n",
       "      <td>inf</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224063</th>\n",
       "      <td>-1.96217</td>\n",
       "      <td>4.39102</td>\n",
       "      <td>7.85266</td>\n",
       "      <td>-1.62137e+00</td>\n",
       "      <td>-1.11517e+01</td>\n",
       "      <td>9.39568e+00</td>\n",
       "      <td>-1.38741e+00</td>\n",
       "      <td>11.25844</td>\n",
       "      <td>6.48034e+00</td>\n",
       "      <td>3.31603e+00</td>\n",
       "      <td>2.45967e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224064</th>\n",
       "      <td>-100.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.40000e+01</td>\n",
       "      <td>2.50000e+01</td>\n",
       "      <td>-5.12195e+01</td>\n",
       "      <td>1.03226e+02</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>inf</td>\n",
       "      <td>-3.21965e-13</td>\n",
       "      <td>-7.50000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224065</th>\n",
       "      <td>-100.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.74603e+01</td>\n",
       "      <td>1.03226e+02</td>\n",
       "      <td>2.40000e+01</td>\n",
       "      <td>7.69231e+00</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>inf</td>\n",
       "      <td>-2.22222e+01</td>\n",
       "      <td>-5.71429e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224066</th>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.32143e+02</td>\n",
       "      <td>7.69231e+00</td>\n",
       "      <td>-1.74603e+01</td>\n",
       "      <td>-5.00000e+01</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>inf</td>\n",
       "      <td>-3.21965e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224067</th>\n",
       "      <td>100.00000</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.84615e+01</td>\n",
       "      <td>-5.00000e+01</td>\n",
       "      <td>1.32143e+02</td>\n",
       "      <td>-2.25000e+01</td>\n",
       "      <td>160.00000</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>inf</td>\n",
       "      <td>-2.22222e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224068</th>\n",
       "      <td>-38.70968</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-3.87097e+01</td>\n",
       "      <td>-2.25000e+01</td>\n",
       "      <td>-3.84615e+01</td>\n",
       "      <td>2.10526e+01</td>\n",
       "      <td>319.35484</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343006</th>\n",
       "      <td>-3.22915</td>\n",
       "      <td>21.93196</td>\n",
       "      <td>33.67713</td>\n",
       "      <td>2.66667e+01</td>\n",
       "      <td>7.51748e+00</td>\n",
       "      <td>5.27241e-01</td>\n",
       "      <td>5.26316e+00</td>\n",
       "      <td>-37.14293</td>\n",
       "      <td>2.60007e+01</td>\n",
       "      <td>9.63256e+00</td>\n",
       "      <td>-6.23107e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343007</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>inf</td>\n",
       "      <td>3.80952e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432109</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>-83.33333</td>\n",
       "      <td>-88.18182</td>\n",
       "      <td>-2.35294e+01</td>\n",
       "      <td>-1.50000e+01</td>\n",
       "      <td>-9.09091e+00</td>\n",
       "      <td>-5.38462e+01</td>\n",
       "      <td>84.61538</td>\n",
       "      <td>-8.33333e+01</td>\n",
       "      <td>-2.90909e+01</td>\n",
       "      <td>-5.81749e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432110</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.18182e+01</td>\n",
       "      <td>inf</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>-4.21348e+01</td>\n",
       "      <td>-84.61538</td>\n",
       "      <td>-4.58333e+01</td>\n",
       "      <td>8.46154e+01</td>\n",
       "      <td>-8.33333e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483093</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.94737e+01</td>\n",
       "      <td>-6.20000e+01</td>\n",
       "      <td>-3.24324e+01</td>\n",
       "      <td>-3.91304e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483094</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>-3.91304e+01</td>\n",
       "      <td>-3.94737e+01</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483095</th>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>-78.60465</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483096</th>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483097</th>\n",
       "      <td>-54.79167</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-5.88235e+01</td>\n",
       "      <td>-5.55556e+00</td>\n",
       "      <td>-5.26316e+00</td>\n",
       "      <td>-1.11022e-14</td>\n",
       "      <td>55.00000</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550157</th>\n",
       "      <td>-39.56954</td>\n",
       "      <td>-39.56297</td>\n",
       "      <td>20.87405</td>\n",
       "      <td>-1.58986e+01</td>\n",
       "      <td>1.87793e+00</td>\n",
       "      <td>-2.25455e+01</td>\n",
       "      <td>-9.58904e+00</td>\n",
       "      <td>-11.92053</td>\n",
       "      <td>1.08591e-02</td>\n",
       "      <td>1.00000e+02</td>\n",
       "      <td>3.48837e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550158</th>\n",
       "      <td>-49.31507</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.12821e+00</td>\n",
       "      <td>-9.30233e+00</td>\n",
       "      <td>-4.44444e+00</td>\n",
       "      <td>-4.86486e+01</td>\n",
       "      <td>-27.39726</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550159</th>\n",
       "      <td>-68.33333</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.86486e+01</td>\n",
       "      <td>-5.12821e+00</td>\n",
       "      <td>-9.30233e+00</td>\n",
       "      <td>-9.99201e-14</td>\n",
       "      <td>-25.00000</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550160</th>\n",
       "      <td>-67.24138</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-9.99201e-14</td>\n",
       "      <td>-4.86486e+01</td>\n",
       "      <td>-5.12821e+00</td>\n",
       "      <td>-5.26316e+00</td>\n",
       "      <td>-25.86207</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550161</th>\n",
       "      <td>-63.26531</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-5.26316e+00</td>\n",
       "      <td>-9.99201e-14</td>\n",
       "      <td>-4.86486e+01</td>\n",
       "      <td>-2.77778e+01</td>\n",
       "      <td>-20.40816</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550162</th>\n",
       "      <td>-75.47170</td>\n",
       "      <td>-82.19178</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-2.77778e+01</td>\n",
       "      <td>-5.26316e+00</td>\n",
       "      <td>-9.99201e-14</td>\n",
       "      <td>-4.44089e-14</td>\n",
       "      <td>-30.18868</td>\n",
       "      <td>-2.73973e+01</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550163</th>\n",
       "      <td>-71.11111</td>\n",
       "      <td>-78.33333</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-4.44089e-14</td>\n",
       "      <td>-2.77778e+01</td>\n",
       "      <td>-5.26316e+00</td>\n",
       "      <td>-2.30769e+01</td>\n",
       "      <td>-57.77778</td>\n",
       "      <td>-2.50000e+01</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608926</th>\n",
       "      <td>-85.36585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-6.66667e+01</td>\n",
       "      <td>3.84615e+01</td>\n",
       "      <td>-3.15789e+01</td>\n",
       "      <td>inf</td>\n",
       "      <td>-46.34146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608927</th>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>-6.66667e+01</td>\n",
       "      <td>3.84615e+01</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>-53.65854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608928</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>inf</td>\n",
       "      <td>-6.66667e+01</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>-62.85714</td>\n",
       "      <td>3.46154e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608929</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>-41.93548</td>\n",
       "      <td>-2.55351e-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608930</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>-8.00000e+01</td>\n",
       "      <td>inf</td>\n",
       "      <td>-5.36585e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608931</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.06329e+00</td>\n",
       "      <td>-8.00000e+01</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>-7.77778e+00</td>\n",
       "      <td>inf</td>\n",
       "      <td>-4.19355e+01</td>\n",
       "      <td>-2.55351e-13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608932</th>\n",
       "      <td>-100.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.77108e+01</td>\n",
       "      <td>-7.77778e+00</td>\n",
       "      <td>-5.06329e+00</td>\n",
       "      <td>1.62950e+00</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>inf</td>\n",
       "      <td>-5.36585e+01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608933</th>\n",
       "      <td>-100.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.62950e+00</td>\n",
       "      <td>2.77108e+01</td>\n",
       "      <td>-7.77778e+00</td>\n",
       "      <td>-5.55556e+00</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>inf</td>\n",
       "      <td>-6.28571e+01</td>\n",
       "      <td>3.46154e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608934</th>\n",
       "      <td>-100.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.55556e+00</td>\n",
       "      <td>1.62950e+00</td>\n",
       "      <td>2.77108e+01</td>\n",
       "      <td>-2.35294e+01</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>inf</td>\n",
       "      <td>-4.19355e+01</td>\n",
       "      <td>-2.55351e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608935</th>\n",
       "      <td>-100.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.35294e+01</td>\n",
       "      <td>-5.55556e+00</td>\n",
       "      <td>1.62950e+00</td>\n",
       "      <td>-2.80769e+01</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>inf</td>\n",
       "      <td>-7.27273e+01</td>\n",
       "      <td>-4.63415e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608936</th>\n",
       "      <td>-65.25463</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.00000e+00</td>\n",
       "      <td>-4.11765e+01</td>\n",
       "      <td>-2.80769e+01</td>\n",
       "      <td>-3.51945e+01</td>\n",
       "      <td>13.04714</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>inf</td>\n",
       "      <td>-4.19355e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608937</th>\n",
       "      <td>-75.58408</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.51945e+01</td>\n",
       "      <td>-5.00000e+00</td>\n",
       "      <td>-4.11765e+01</td>\n",
       "      <td>-3.22034e+01</td>\n",
       "      <td>-6.26141</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>inf</td>\n",
       "      <td>-7.27273e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608938</th>\n",
       "      <td>-86.49616</td>\n",
       "      <td>-84.73430</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-2.30769e+01</td>\n",
       "      <td>3.00000e+01</td>\n",
       "      <td>-3.22034e+01</td>\n",
       "      <td>-1.75000e+01</td>\n",
       "      <td>-69.26471</td>\n",
       "      <td>1.30471e+01</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608939</th>\n",
       "      <td>-85.43144</td>\n",
       "      <td>-86.34364</td>\n",
       "      <td>-100.00000</td>\n",
       "      <td>-1.75000e+01</td>\n",
       "      <td>-2.30769e+01</td>\n",
       "      <td>3.00000e+01</td>\n",
       "      <td>1.21212e+02</td>\n",
       "      <td>-73.95318</td>\n",
       "      <td>-6.26141e+00</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        past2YearReturn  past3YearReturn  past4YearReturn  \\\n",
       "147751       -100.00000       -100.00000       -100.00000   \n",
       "147752        -93.14286              inf              NaN   \n",
       "147753        -78.26087        -96.57143              inf   \n",
       "224063         -1.96217          4.39102          7.85266   \n",
       "224064       -100.00000              NaN              NaN   \n",
       "224065       -100.00000              NaN              NaN   \n",
       "224066       -100.00000       -100.00000              NaN   \n",
       "224067        100.00000       -100.00000              NaN   \n",
       "224068        -38.70968       -100.00000       -100.00000   \n",
       "343006         -3.22915         21.93196         33.67713   \n",
       "343007          0.00000              inf              inf   \n",
       "432109          0.00000        -83.33333        -88.18182   \n",
       "432110              NaN              NaN              NaN   \n",
       "483093              NaN              NaN              NaN   \n",
       "483094              NaN              NaN              NaN   \n",
       "483095              inf              NaN              NaN   \n",
       "483096              inf              NaN              NaN   \n",
       "483097        -54.79167       -100.00000       -100.00000   \n",
       "550157        -39.56954        -39.56297         20.87405   \n",
       "550158        -49.31507       -100.00000              NaN   \n",
       "550159        -68.33333       -100.00000              NaN   \n",
       "550160        -67.24138       -100.00000       -100.00000   \n",
       "550161        -63.26531       -100.00000       -100.00000   \n",
       "550162        -75.47170        -82.19178       -100.00000   \n",
       "550163        -71.11111        -78.33333       -100.00000   \n",
       "608926        -85.36585              NaN              NaN   \n",
       "608927              inf              NaN              NaN   \n",
       "608928              inf              inf              NaN   \n",
       "608929              inf              inf              NaN   \n",
       "608930              NaN              NaN              NaN   \n",
       "608931              NaN              NaN              NaN   \n",
       "608932       -100.00000              NaN              NaN   \n",
       "608933       -100.00000              NaN              NaN   \n",
       "608934       -100.00000              NaN              NaN   \n",
       "608935       -100.00000              NaN              NaN   \n",
       "608936        -65.25463       -100.00000              NaN   \n",
       "608937        -75.58408       -100.00000              NaN   \n",
       "608938        -86.49616        -84.73430       -100.00000   \n",
       "608939        -85.43144        -86.34364       -100.00000   \n",
       "\n",
       "        quarterlyReturnsLagged1Q  quarterlyReturnsLagged2Q  \\\n",
       "147751              -1.00000e+02              -6.66667e+01   \n",
       "147752              -1.42857e+01               1.11111e+01   \n",
       "147753              -2.50000e+01               0.00000e+00   \n",
       "224063              -1.62137e+00              -1.11517e+01   \n",
       "224064               2.40000e+01               2.50000e+01   \n",
       "224065              -1.74603e+01               1.03226e+02   \n",
       "224066               1.32143e+02               7.69231e+00   \n",
       "224067              -3.84615e+01              -5.00000e+01   \n",
       "224068              -3.87097e+01              -2.25000e+01   \n",
       "343006               2.66667e+01               7.51748e+00   \n",
       "343007               0.00000e+00               0.00000e+00   \n",
       "432109              -2.35294e+01              -1.50000e+01   \n",
       "432110               6.18182e+01                       inf   \n",
       "483093              -3.94737e+01              -6.20000e+01   \n",
       "483094                       inf              -3.91304e+01   \n",
       "483095               0.00000e+00               0.00000e+00   \n",
       "483096               0.00000e+00               0.00000e+00   \n",
       "483097              -5.88235e+01              -5.55556e+00   \n",
       "550157              -1.58986e+01               1.87793e+00   \n",
       "550158              -5.12821e+00              -9.30233e+00   \n",
       "550159              -4.86486e+01              -5.12821e+00   \n",
       "550160              -9.99201e-14              -4.86486e+01   \n",
       "550161              -5.26316e+00              -9.99201e-14   \n",
       "550162              -2.77778e+01              -5.26316e+00   \n",
       "550163              -4.44089e-14              -2.77778e+01   \n",
       "608926              -6.66667e+01               3.84615e+01   \n",
       "608927                       inf              -6.66667e+01   \n",
       "608928               0.00000e+00                       inf   \n",
       "608929               0.00000e+00               0.00000e+00   \n",
       "608930              -1.00000e+02               0.00000e+00   \n",
       "608931              -5.06329e+00              -8.00000e+01   \n",
       "608932               2.77108e+01              -7.77778e+00   \n",
       "608933               1.62950e+00               2.77108e+01   \n",
       "608934              -5.55556e+00               1.62950e+00   \n",
       "608935              -2.35294e+01              -5.55556e+00   \n",
       "608936              -5.00000e+00              -4.11765e+01   \n",
       "608937              -3.51945e+01              -5.00000e+00   \n",
       "608938              -2.30769e+01               3.00000e+01   \n",
       "608939              -1.75000e+01              -2.30769e+01   \n",
       "\n",
       "        quarterlyReturnsLagged3Q      returns  yearlyReturnsLagged1Y  \\\n",
       "147751              -4.00000e+01  0.00000e+00              -70.58824   \n",
       "147752              -5.43478e+01 -3.66667e+01              -84.22857   \n",
       "147753               5.26316e+00 -2.33333e+01              -56.52174   \n",
       "224063               9.39568e+00 -1.38741e+00               11.25844   \n",
       "224064              -5.12195e+01  1.03226e+02                0.00000   \n",
       "224065               2.40000e+01  7.69231e+00             -100.00000   \n",
       "224066              -1.74603e+01 -5.00000e+01             -100.00000   \n",
       "224067               1.32143e+02 -2.25000e+01              160.00000   \n",
       "224068              -3.84615e+01  2.10526e+01              319.35484   \n",
       "343006               5.27241e-01  5.26316e+00              -37.14293   \n",
       "343007               0.00000e+00  0.00000e+00                0.00000   \n",
       "432109              -9.09091e+00 -5.38462e+01               84.61538   \n",
       "432110              -1.00000e+02 -4.21348e+01              -84.61538   \n",
       "483093              -3.24324e+01 -3.91304e+01                    NaN   \n",
       "483094              -3.94737e+01  0.00000e+00                    NaN   \n",
       "483095                       inf  0.00000e+00              -78.60465   \n",
       "483096               0.00000e+00  0.00000e+00                    inf   \n",
       "483097              -5.26316e+00 -1.11022e-14               55.00000   \n",
       "550157              -2.25455e+01 -9.58904e+00              -11.92053   \n",
       "550158              -4.44444e+00 -4.86486e+01              -27.39726   \n",
       "550159              -9.30233e+00 -9.99201e-14              -25.00000   \n",
       "550160              -5.12821e+00 -5.26316e+00              -25.86207   \n",
       "550161              -4.86486e+01 -2.77778e+01              -20.40816   \n",
       "550162              -9.99201e-14 -4.44089e-14              -30.18868   \n",
       "550163              -5.26316e+00 -2.30769e+01              -57.77778   \n",
       "608926              -3.15789e+01          inf              -46.34146   \n",
       "608927               3.84615e+01  0.00000e+00              -53.65854   \n",
       "608928              -6.66667e+01  0.00000e+00              -62.85714   \n",
       "608929                       inf  0.00000e+00              -41.93548   \n",
       "608930               0.00000e+00 -8.00000e+01                    inf   \n",
       "608931              -1.00000e+02 -7.77778e+00                    inf   \n",
       "608932              -5.06329e+00  1.62950e+00             -100.00000   \n",
       "608933              -7.77778e+00 -5.55556e+00             -100.00000   \n",
       "608934               2.77108e+01 -2.35294e+01             -100.00000   \n",
       "608935               1.62950e+00 -2.80769e+01             -100.00000   \n",
       "608936              -2.80769e+01 -3.51945e+01               13.04714   \n",
       "608937              -4.11765e+01 -3.22034e+01               -6.26141   \n",
       "608938              -3.22034e+01 -1.75000e+01              -69.26471   \n",
       "608939               3.00000e+01  1.21212e+02              -73.95318   \n",
       "\n",
       "        yearlyReturnsLagged2Y  yearlyReturnsLagged3Y  yearlyReturnsLagged4Y  \n",
       "147751            1.42857e+02           -5.88235e+01           -5.52632e+01  \n",
       "147752                    inf           -1.00000e+02           -7.05882e+01  \n",
       "147753           -8.42286e+01                    inf           -1.00000e+02  \n",
       "224063            6.48034e+00            3.31603e+00            2.45967e+01  \n",
       "224064                    inf           -3.21965e-13           -7.50000e+01  \n",
       "224065                    inf           -2.22222e+01           -5.71429e+01  \n",
       "224066            0.00000e+00                    inf           -3.21965e-13  \n",
       "224067           -1.00000e+02                    inf           -2.22222e+01  \n",
       "224068           -1.00000e+02            0.00000e+00                    inf  \n",
       "343006            2.60007e+01            9.63256e+00           -6.23107e+00  \n",
       "343007                    inf            3.80952e+01                    NaN  \n",
       "432109           -8.33333e+01           -2.90909e+01           -5.81749e+01  \n",
       "432110           -4.58333e+01            8.46154e+01           -8.33333e+01  \n",
       "483093                    NaN                    NaN                    NaN  \n",
       "483094                    NaN                    NaN                    NaN  \n",
       "483095                    NaN                    NaN                    NaN  \n",
       "483096                    NaN                    NaN                    NaN  \n",
       "483097           -1.00000e+02            0.00000e+00                    inf  \n",
       "550157            1.08591e-02            1.00000e+02            3.48837e+01  \n",
       "550158           -1.00000e+02                    inf                    NaN  \n",
       "550159           -1.00000e+02                    inf                    NaN  \n",
       "550160           -1.00000e+02            0.00000e+00                    NaN  \n",
       "550161           -1.00000e+02            0.00000e+00                    inf  \n",
       "550162           -2.73973e+01           -1.00000e+02                    inf  \n",
       "550163           -2.50000e+01           -1.00000e+02                    inf  \n",
       "608926                    NaN                    NaN                    NaN  \n",
       "608927                    NaN                    NaN                    NaN  \n",
       "608928            3.46154e+01                    NaN                    NaN  \n",
       "608929           -2.55351e-13                    NaN                    NaN  \n",
       "608930           -5.36585e+01                    NaN                    NaN  \n",
       "608931           -4.19355e+01           -2.55351e-13                    NaN  \n",
       "608932                    inf           -5.36585e+01                    NaN  \n",
       "608933                    inf           -6.28571e+01            3.46154e+01  \n",
       "608934                    inf           -4.19355e+01           -2.55351e-13  \n",
       "608935                    inf           -7.27273e+01           -4.63415e+01  \n",
       "608936           -1.00000e+02                    inf           -4.19355e+01  \n",
       "608937           -1.00000e+02                    inf           -7.27273e+01  \n",
       "608938            1.30471e+01           -1.00000e+02                    inf  \n",
       "608939           -6.26141e+00           -1.00000e+02                    inf  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#winData[numCols].apply(lambda x: (min(x), max(x)))\n",
    "#max(winData[numCols])\n",
    "#min(winData[numCols])\n",
    "\n",
    "#np.isinf(finalData[numCols]).values.sum()\n",
    "#finalData[numCols].columns.to_series()[np.isinf(finalData[numCols]).any()]\n",
    "#finalData[numCols].index[np.isinf(finalData[numCols]).any(1)]\n",
    "\n",
    "#finalData[numCols].iloc[finalData[numCols].index[np.isinf(finalData[numCols]).any(1)], :]\n",
    "#finalData[numCols].iloc[finalData[numCols].index[np.isinf(finalData[numCols]).any(1)], :][finalData[numCols].columns.to_series()[np.isinf(finalData[numCols]).any()]]\n",
    "\n",
    "data.iloc[data[numCols].index[np.isinf(data[numCols]).any(1)].union(\n",
    "    data[numCols].index[np.isinf(data[numCols]).any(1)] - 1), :][data[numCols].columns[np.isinf(data[numCols]).any()].union([\"returns\"])]\n",
    "\n",
    "\n",
    "#finalData[numCols].columns[np.isinf(finalData[numCols]).any()].union([\"returns\"])\n",
    "\n",
    "# np.isinf(winData[numCols]).values.sum()\n",
    "# #winData[numCols].columns.to_series()[np.isinf(winData[numCols]).any()]\n",
    "# winData[numCols].index[np.isinf(winData[numCols]).any(1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#My scaling and winsorizing is biased. They should be done based on the training data, not testing and validation\n",
    "scaledData=winData.copy()\n",
    "scaledData.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#scaledData.dropna(inplace=True)\n",
    "\n",
    "scaler.fit(scaledData.loc[scaledData[\"Split\"]==\"Train\", numCols])\n",
    "scaledData[numCols] = scaler.transform(scaledData[numCols]) #This is an inplace operation if copy() is not used\n",
    "\n",
    "float64Cols = scaledData.select_dtypes(include=[np.float64]).columns\n",
    "scaledData[float64Cols] = scaledData[float64Cols].astype('float32')\n",
    "#scaledData[numCols] = scaledData[numCols].astype('float16')\n",
    "#scaledData.info(verbose=True)\n",
    "#scaledData.select_dtypes(include=[np.int64]).columns\n",
    "\n",
    "# scaledData.loc[:, scaledData.dtypes == 'float64'] = scaledData.loc[:, scaledData.dtypes == 'float64'].astype('float32')\n",
    "# scaledData.loc[:, scaledData.dtypes == 'int64'] = scaledData.loc[:, scaledData.dtypes == 'int64'].astype('int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ancq        ancq\n",
       "dlcq        dlcq\n",
       "eroq        eroq\n",
       "ivaoq      ivaoq\n",
       "lctq        lctq\n",
       "ltmibq    ltmibq\n",
       "mibnq      mibnq\n",
       "reccoq    reccoq\n",
       "rectoq    rectoq\n",
       "teqq        teqq\n",
       "dtype: object"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.isinf(scaledData[numCols]).sum()\n",
    "\n",
    "scaledData[numCols].columns.to_series()[np.isinf(scaledData[numCols]).any()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey\n"
     ]
    }
   ],
   "source": [
    "if largeSample:\n",
    "\n",
    "    #impute\n",
    "    imputedData=scaledData.copy()\n",
    "    imputedData.dropna(axis=1, how='all')\n",
    "    #imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n",
    "    #imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=False, copy=False)\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=False, copy=True, keep_empty_features=True)\n",
    "    #imp = IterativeImputer(max_iter=2, random_state=0, add_indicator=True, verbose=0)\n",
    "    imp.fit(imputedData.loc[imputedData[\"Split\"]==\"Train\", numCols])\n",
    "    #IterativeImputer(random_state=0)\n",
    "    print(\"Hey\")\n",
    "\n",
    "    #imputedData = imp.transform(imputedData[numCols])\n",
    "    imputedData[numCols] = imp.transform(imputedData[numCols])\n",
    "    # imputedData.loc[imputedData[\"Split\"]==\"Validation\", numCols] = imp.transform(imputedData.loc[imputedData[\"Split\"]==\"Validation\", numCols])\n",
    "    # imputedData.loc[imputedData[\"Split\"]==\"Test\", numCols] = imp.transform(imputedData.loc[imputedData[\"Split\"]==\"Test\", numCols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "if largeSample:\n",
    "    pd.DataFrame.to_csv(scaledData, \"Data/scaledDataMedium.csv\", index=False)\n",
    "    pd.DataFrame.to_csv(imputedData, \"Data/imputedDataMedium.csv\", index=False)\n",
    "else:\n",
    "    pd.DataFrame.to_csv(scaledData, \"Data/scaledData.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #xLogData = scaledData[\"revtq\"].apply(lambda x: np.log(x)) \n",
    "# #xLogData = scaledData[\"revtq\"].apply(lambda x: np.log(x+0.005)) \n",
    "# #xLogData = scaledData[\"revtq\"].apply(lambda x: np.log(x+0.9)) \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# #plt.hist(xLogData, bins=100)\n",
    "# plt.hist(scaledData[\"revtq\"], bins=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvkey</th>\n",
       "      <th>fdateYQ</th>\n",
       "      <th>returns</th>\n",
       "      <th>volatility</th>\n",
       "      <th>MktCap</th>\n",
       "      <th>TimeDifferenceInDays</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Subsector</th>\n",
       "      <th>IndustryGroup</th>\n",
       "      <th>Industry</th>\n",
       "      <th>...</th>\n",
       "      <th>ivaeqq</th>\n",
       "      <th>mibnq</th>\n",
       "      <th>mibtq</th>\n",
       "      <th>miiq</th>\n",
       "      <th>oproq</th>\n",
       "      <th>spiq</th>\n",
       "      <th>tstkq</th>\n",
       "      <th>txdbq</th>\n",
       "      <th>xppq</th>\n",
       "      <th>xstq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>805843.00000</td>\n",
       "      <td>805843.00000</td>\n",
       "      <td>8.05839e+05</td>\n",
       "      <td>805254.00000</td>\n",
       "      <td>8.05582e+05</td>\n",
       "      <td>805843.00000</td>\n",
       "      <td>805453.00000</td>\n",
       "      <td>805453.00000</td>\n",
       "      <td>805453.00000</td>\n",
       "      <td>805453.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>230182.00000</td>\n",
       "      <td>3.91434e+05</td>\n",
       "      <td>445831.00000</td>\n",
       "      <td>400018.00000</td>\n",
       "      <td>248363.00000</td>\n",
       "      <td>338996.00000</td>\n",
       "      <td>243662.00000</td>\n",
       "      <td>410297.00000</td>\n",
       "      <td>267837.00000</td>\n",
       "      <td>301616.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>250462.32698</td>\n",
       "      <td>2016.20544</td>\n",
       "      <td>6.24523e+00</td>\n",
       "      <td>0.47428</td>\n",
       "      <td>4.59206e+11</td>\n",
       "      <td>-0.02586</td>\n",
       "      <td>39.66792</td>\n",
       "      <td>398.41388</td>\n",
       "      <td>3588.32764</td>\n",
       "      <td>29637.75195</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00632</td>\n",
       "      <td>5.47082e+00</td>\n",
       "      <td>0.00676</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00167</td>\n",
       "      <td>-0.01352</td>\n",
       "      <td>0.00452</td>\n",
       "      <td>0.00441</td>\n",
       "      <td>-0.03651</td>\n",
       "      <td>-0.05447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>61784.94129</td>\n",
       "      <td>4.28722</td>\n",
       "      <td>2.43723e+03</td>\n",
       "      <td>17.21597</td>\n",
       "      <td>8.35808e+12</td>\n",
       "      <td>0.95944</td>\n",
       "      <td>13.28535</td>\n",
       "      <td>134.05699</td>\n",
       "      <td>1729.64331</td>\n",
       "      <td>21040.23828</td>\n",
       "      <td>...</td>\n",
       "      <td>1.40411</td>\n",
       "      <td>3.41949e+03</td>\n",
       "      <td>1.24463</td>\n",
       "      <td>0.87572</td>\n",
       "      <td>1.43388</td>\n",
       "      <td>5.59552</td>\n",
       "      <td>1.41698</td>\n",
       "      <td>1.24234</td>\n",
       "      <td>1.40332</td>\n",
       "      <td>0.98192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1166.00000</td>\n",
       "      <td>2006.50000</td>\n",
       "      <td>-1.00000e+02</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>-3.02843</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.83063</td>\n",
       "      <td>-1.61818e+02</td>\n",
       "      <td>-0.06511</td>\n",
       "      <td>-0.18745</td>\n",
       "      <td>-0.06405</td>\n",
       "      <td>-3109.87769</td>\n",
       "      <td>-0.14549</td>\n",
       "      <td>-0.09838</td>\n",
       "      <td>-0.45490</td>\n",
       "      <td>-0.30998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>212970.00000</td>\n",
       "      <td>2013.00000</td>\n",
       "      <td>-1.00000e+01</td>\n",
       "      <td>0.24591</td>\n",
       "      <td>2.78491e+08</td>\n",
       "      <td>-0.75496</td>\n",
       "      <td>32.00000</td>\n",
       "      <td>325.00000</td>\n",
       "      <td>3122.00000</td>\n",
       "      <td>4811.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.20166</td>\n",
       "      <td>-5.52546e-02</td>\n",
       "      <td>-0.06004</td>\n",
       "      <td>-0.03205</td>\n",
       "      <td>-0.06356</td>\n",
       "      <td>-0.00966</td>\n",
       "      <td>-0.13904</td>\n",
       "      <td>-0.09630</td>\n",
       "      <td>-0.30380</td>\n",
       "      <td>-0.28502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>260504.00000</td>\n",
       "      <td>2016.75000</td>\n",
       "      <td>0.00000e+00</td>\n",
       "      <td>0.35925</td>\n",
       "      <td>2.44236e+09</td>\n",
       "      <td>-0.05900</td>\n",
       "      <td>33.00000</td>\n",
       "      <td>334.00000</td>\n",
       "      <td>3343.00000</td>\n",
       "      <td>32799.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.17380</td>\n",
       "      <td>-4.98248e-02</td>\n",
       "      <td>-0.05373</td>\n",
       "      <td>-0.03035</td>\n",
       "      <td>-0.05958</td>\n",
       "      <td>-0.00939</td>\n",
       "      <td>-0.10516</td>\n",
       "      <td>-0.08160</td>\n",
       "      <td>-0.24810</td>\n",
       "      <td>-0.22719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>290815.00000</td>\n",
       "      <td>2019.75000</td>\n",
       "      <td>1.08354e+01</td>\n",
       "      <td>0.52583</td>\n",
       "      <td>1.34482e+10</td>\n",
       "      <td>0.63696</td>\n",
       "      <td>52.00000</td>\n",
       "      <td>522.00000</td>\n",
       "      <td>5151.00000</td>\n",
       "      <td>48511.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.06114</td>\n",
       "      <td>-2.49657e-02</td>\n",
       "      <td>-0.02556</td>\n",
       "      <td>-0.01708</td>\n",
       "      <td>-0.04428</td>\n",
       "      <td>-0.00925</td>\n",
       "      <td>-0.00748</td>\n",
       "      <td>-0.03206</td>\n",
       "      <td>-0.09304</td>\n",
       "      <td>-0.08567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>354340.00000</td>\n",
       "      <td>2022.50000</td>\n",
       "      <td>2.18621e+06</td>\n",
       "      <td>15320.10742</td>\n",
       "      <td>1.35863e+15</td>\n",
       "      <td>1.79690</td>\n",
       "      <td>99.00000</td>\n",
       "      <td>999.00000</td>\n",
       "      <td>9999.00000</td>\n",
       "      <td>99999.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>325.22522</td>\n",
       "      <td>2.13940e+06</td>\n",
       "      <td>379.47485</td>\n",
       "      <td>218.12157</td>\n",
       "      <td>460.43573</td>\n",
       "      <td>788.00787</td>\n",
       "      <td>396.26154</td>\n",
       "      <td>186.67596</td>\n",
       "      <td>363.44061</td>\n",
       "      <td>69.33879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              gvkey       fdateYQ      returns    volatility       MktCap  \\\n",
       "count  805843.00000  805843.00000  8.05839e+05  805254.00000  8.05582e+05   \n",
       "mean   250462.32698    2016.20544  6.24523e+00       0.47428  4.59206e+11   \n",
       "std     61784.94129       4.28722  2.43723e+03      17.21597  8.35808e+12   \n",
       "min      1166.00000    2006.50000 -1.00000e+02       0.00000  0.00000e+00   \n",
       "25%    212970.00000    2013.00000 -1.00000e+01       0.24591  2.78491e+08   \n",
       "50%    260504.00000    2016.75000  0.00000e+00       0.35925  2.44236e+09   \n",
       "75%    290815.00000    2019.75000  1.08354e+01       0.52583  1.34482e+10   \n",
       "max    354340.00000    2022.50000  2.18621e+06   15320.10742  1.35863e+15   \n",
       "\n",
       "       TimeDifferenceInDays        Sector     Subsector  IndustryGroup  \\\n",
       "count          805843.00000  805453.00000  805453.00000   805453.00000   \n",
       "mean               -0.02586      39.66792     398.41388     3588.32764   \n",
       "std                 0.95944      13.28535     134.05699     1729.64331   \n",
       "min                -3.02843      11.00000      11.00000       11.00000   \n",
       "25%                -0.75496      32.00000     325.00000     3122.00000   \n",
       "50%                -0.05900      33.00000     334.00000     3343.00000   \n",
       "75%                 0.63696      52.00000     522.00000     5151.00000   \n",
       "max                 1.79690      99.00000     999.00000     9999.00000   \n",
       "\n",
       "           Industry  ...        ivaeqq        mibnq         mibtq  \\\n",
       "count  805453.00000  ...  230182.00000  3.91434e+05  445831.00000   \n",
       "mean    29637.75195  ...      -0.00632  5.47082e+00       0.00676   \n",
       "std     21040.23828  ...       1.40411  3.41949e+03       1.24463   \n",
       "min        11.00000  ...      -3.83063 -1.61818e+02      -0.06511   \n",
       "25%      4811.00000  ...      -0.20166 -5.52546e-02      -0.06004   \n",
       "50%     32799.00000  ...      -0.17380 -4.98248e-02      -0.05373   \n",
       "75%     48511.00000  ...      -0.06114 -2.49657e-02      -0.02556   \n",
       "max     99999.00000  ...     325.22522  2.13940e+06     379.47485   \n",
       "\n",
       "               miiq         oproq          spiq         tstkq         txdbq  \\\n",
       "count  400018.00000  248363.00000  338996.00000  243662.00000  410297.00000   \n",
       "mean        0.00011       0.00167      -0.01352       0.00452       0.00441   \n",
       "std         0.87572       1.43388       5.59552       1.41698       1.24234   \n",
       "min        -0.18745      -0.06405   -3109.87769      -0.14549      -0.09838   \n",
       "25%        -0.03205      -0.06356      -0.00966      -0.13904      -0.09630   \n",
       "50%        -0.03035      -0.05958      -0.00939      -0.10516      -0.08160   \n",
       "75%        -0.01708      -0.04428      -0.00925      -0.00748      -0.03206   \n",
       "max       218.12157     460.43573     788.00787     396.26154     186.67596   \n",
       "\n",
       "               xppq          xstq  \n",
       "count  267837.00000  301616.00000  \n",
       "mean       -0.03651      -0.05447  \n",
       "std         1.40332       0.98192  \n",
       "min        -0.45490      -0.30998  \n",
       "25%        -0.30380      -0.28502  \n",
       "50%        -0.24810      -0.22719  \n",
       "75%        -0.09304      -0.08567  \n",
       "max       363.44061      69.33879  \n",
       "\n",
       "[8 rows x 109 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xScaledData\n",
    "# scaledData.describe().to_csv(\"Descr//FinalizedDesc.csv\")\n",
    "# scaledData.describe().to_html(\"Descr//FinalizedDesc.html\")\n",
    "\n",
    "descriptives = scaledData.describe()\n",
    "descriptives.to_csv(\"Descr//MediumFinalizedDesc.csv\")\n",
    "descriptives.to_html(\"Descr//MediumFinalizedDesc.html\")\n",
    "descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #tf.sparse.SparseTensor\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# np.set_printoptions(precision=3, suppress=True)\n",
    "# ###tf.enable_eager_execution()\n",
    "# #tf.keras.Model.run_eagerly\n",
    "\n",
    "# dataset = tf.data.experimental.make_csv_dataset(\n",
    "#     data_file_path, label_name=\"returns\", batch_size=2, num_epochs=1,\n",
    "#     shuffle=False, sloppy=True) # compression_type = GZIP #ZLIB\n",
    "# iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "# #dataset.zip()\n",
    "# #dataset\n",
    "# #iterator.next()\n",
    "\n",
    "# # for batch, label in dataset.take(1):\n",
    "# #   for key, value in batch.items():\n",
    "# #     print(f\"{key:20s}: {value}\")\n",
    "# #   print()\n",
    "# #   print(f\"{'label':20s}: {label}\")\n",
    "\n",
    "# #b = dataset.map(lambda x: x*2)\n",
    "\n",
    "\n",
    "# dummyLayer = tf.keras.layers.StringLookup(\n",
    "#     mask_token=None,\n",
    "#     encoding=\"utf-8\",\n",
    "#     output_mode=\"one_hot\",\n",
    "#     sparse=True\n",
    "# )\n",
    "\n",
    "# # dummyDataset = dataset.map(lambda x, y: (\n",
    "# #     {n: x[n] for n in numCols} | {f: dummyLayer.adapt(x[f]) for f in factorCols},\n",
    "# #     y))\n",
    "\n",
    "\n",
    "\n",
    "# # feature_ds = dataset.map(lambda x, y: x[name])\n",
    "# # feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "# # # Learn the set of possible string values and assign them a fixed integer index\n",
    "# # lookup.adapt(feature_ds)\n",
    "\n",
    "\n",
    "\n",
    "# #dummyDataset = dataset.map(lambda x, y: ({f: x[f] for f in factorCols}))\n",
    "# #dummyDataset = dataset.map(lambda x, y: (x[f] for f in factorCols))\n",
    "# #dummyDataset = dataset.map(lambda x, y: x[factorCols])\n",
    "# #dummyDataset = dataset.map(lambda x, y: x[\"loc\"])\n",
    "\n",
    "# #dummyDataset.batch(32)\n",
    "\n",
    "# #d = dummyLayer.adapt(dummyDataset)\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# ohe = OneHotEncoder(categories='auto', sparse=False)\n",
    "# # ohe.fit_transform(dataset[['loc']])\n",
    "\n",
    "# dummyDataset = dataset.map(lambda x, y: ({f: ohe.fit_transform(x[f]) for f in factorCols}))\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# le.fit_transform(df['col1'])\n",
    "# dictionary_length = len(le.classes_)\n",
    "\n",
    "\n",
    "\n",
    "# #from sklearn.model_selection import TimeSeriesSplit\n",
    "# #tscv = TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None)\n",
    "\n",
    "\n",
    "\n",
    "# # dummyLayer = tf.keras.layers.StringLookup(\n",
    "# #     mask_token=None,\n",
    "# #     encoding=\"utf-8\",\n",
    "# #     output_mode=\"int\"\n",
    "# # )\n",
    "\n",
    "# # dummyDataset = dataset.map(lambda x, y: (\n",
    "# #     {n: x[n] for n in numCols} | {f: dummyLayer(x[f]) for f in factorCols},\n",
    "# #     y))\n",
    "\n",
    "\n",
    "# # for batch, label in dummyDataset.take(1):\n",
    "# #   for key, value in batch.items():\n",
    "# #     print(f\"{key:20s}: {value}\")\n",
    "# #     print()\n",
    "# #     print(f\"{'label':20s}: {label}\")\n",
    "\n",
    "\n",
    "\n",
    "# # embeddedDummies = tf.keras.layers.Embedding(\n",
    "# #     input_dim=2000,\n",
    "# #     output_dim=10,\n",
    "# #     activity_regularizer=None,\n",
    "# #     mask_zero=True,\n",
    "# # )\n",
    "\n",
    "# # dummyModel = tf.keras.Sequential()\n",
    "# # dummyModel.add(embeddedDummies)\n",
    "\n",
    "# # with tf.device('cpu:0'):\n",
    "# #   embedding_layer = Embedding(...)\n",
    "# #   embedding_layer.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if largeSample:\n",
    "    finalData = imputedData.copy()\n",
    "else: \n",
    "    finalData = scaledData.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if largeSample:\n",
    "    finalData = pd.read_csv(os.path.join(os.getcwd(), \"Data\\imputedDataMedium.csv\"))\n",
    "else: \n",
    "    finalData = pd.read_csv(os.path.join(os.getcwd(), \"Data\\scaledData.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalData = finalData.dropna()\n",
    "\n",
    "dummyData = pd.get_dummies(finalData, dummy_na=False, sparse=False, drop_first=False,\n",
    "                         columns=dummyNames)#, \"bsprq\", \"scfq\", \"staltq\"])\n",
    "\n",
    "\n",
    "target = [\"6Classes\"]\n",
    "#target = [\"returns\"]\n",
    "\n",
    "if target != [\"returns\"] and target != [\"volatility\"] and target != [\"returns\", \"volatility\"]:\n",
    "    classification = True\n",
    "    Y = pd.get_dummies(dummyData[target + [\"Split\", \"fdateYQ\"]], dummy_na=False, sparse=False, drop_first=False,\n",
    "                       columns=target)\n",
    "else:\n",
    "    classification = False\n",
    "    Y = finalData[target + [\"Split\", \"fdateYQ\"]]\n",
    "\n",
    "\n",
    "yTrain = Y[Y[\"Split\"]==\"Train\"].drop([\"Split\", \"fdateYQ\"], axis=1)\n",
    "yVal = Y[Y[\"Split\"]==\"Validation\"].drop([\"Split\", \"fdateYQ\"], axis=1)\n",
    "yTest = Y[Y[\"Split\"]==\"Test\"].drop([\"Split\", \"fdateYQ\"], axis=1)\n",
    "\n",
    "X = pd.get_dummies(finalData[numCols + dummyNames + [\"Split\", \"fdateYQ\"]], dummy_na=False, sparse=False, drop_first=False,\n",
    "                         columns=dummyNames)#, \"bsprq\", \"scfq\", \"staltq\"])\n",
    "\n",
    "namesOfAllPredictorsAndDummies = list(X.columns)\n",
    "\n",
    "xTrain = X.loc[X[\"Split\"]==\"Train\"].drop([\"Split\", \"fdateYQ\"], axis=1)\n",
    "xVal = X.loc[X[\"Split\"]==\"Validation\"].drop([\"Split\", \"fdateYQ\"], axis=1)\n",
    "xTest = X.loc[X[\"Split\"]==\"Test\"].drop([\"Split\", \"fdateYQ\"], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catEncodingLayer = tf.keras.layers.CategoryEncoding(\n",
    "#     num_tokens=3, output_mode=\"one_hot\", sparse=False\n",
    "# )\n",
    "\n",
    "# catEncodingLayer(xScaledData[[\"2Classes\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intLookupLayer = tf.keras.layers.IntegerLookup(\n",
    "#     max_tokens=3,\n",
    "#     num_oov_indices=1,\n",
    "#     mask_token=None,\n",
    "#     vocabulary=[1,10],\n",
    "#     output_mode=\"one_hot\",\n",
    "#     sparse=False,\n",
    "#     pad_to_max_tokens=False\n",
    "# )\n",
    "\n",
    "# #intLookupLayer.adapt(xScaledData[[\"10Classes\"]])\n",
    "# intLookupLayer(xScaledData[[\"10Classes\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummyData.iloc[:, 15:40].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame.to_csv(dummyData, \"Data/dummiesData.csv\", index=False)\n",
    "# #dummies.to_hdf(r\"Data/dummiesData2.csv\", key='dummies', mode='w')\n",
    "\n",
    "#X.info(verbose=True)\n",
    "#dummies.info(verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# np.set_printoptions(precision=3, suppress=True)\n",
    "# ###tf.enable_eager_execution()\n",
    "# #tf.keras.Model.run_eagerly\n",
    "\n",
    "# #xScaledDataWithDummies = pd.concat([xScaledData, dummies], axis=1) \n",
    "\n",
    "\n",
    "# #numeric_dict_ds = tf.data.Dataset.from_tensor_slices((dict(numeric_features), target))\n",
    "# categorical_feature_names = factorCols\n",
    "# binary_feature_names = []\n",
    "\n",
    "# inputs = {}\n",
    "# for name, column in xScaledData.items():\n",
    "#   if type(column[0]) == str:\n",
    "#     dtype = tf.string\n",
    "#   elif (name in categorical_feature_names or\n",
    "#         name in binary_feature_names):\n",
    "#     dtype = tf.int64\n",
    "#   else:\n",
    "#     dtype = tf.float32\n",
    "\n",
    "#   inputs[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)\n",
    "\n",
    "\n",
    "# preprocessed = []\n",
    "# for name in categorical_feature_names:\n",
    "#   vocab = sorted(set(xScaledData[name]))\n",
    "#   #print(f'name: {name}')\n",
    "#   #print(f'vocab: {vocab}\\n')\n",
    "\n",
    "#   if type(vocab[0]) is str:\n",
    "#     lookup = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode='one_hot')\n",
    "#   else:\n",
    "#     lookup = tf.keras.layers.IntegerLookup(vocabulary=vocab, output_mode='one_hot')\n",
    "\n",
    "#   x = inputs[name][:, tf.newaxis]\n",
    "#   x = lookup(x)\n",
    "#   preprocessed.append(x)\n",
    "\n",
    "\n",
    "# numeric_feature_names = numCols\n",
    "\n",
    "# def stack_dict(inputs, fun=tf.stack):\n",
    "#     values = []\n",
    "#     for key in sorted(inputs.keys()):\n",
    "#       values.append(tf.cast(inputs[key], tf.float32))\n",
    "\n",
    "#     return fun(values, axis=-1)\n",
    "# #normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "# #normalizer.adapt(numeric_features)\n",
    "\n",
    "# numeric_inputs = {}\n",
    "# for name in numeric_feature_names:\n",
    "#   numeric_inputs[name]=inputs[name]\n",
    "\n",
    "# numeric_inputs = stack_dict(numeric_inputs)\n",
    "# #numeric_normalized = normalizer(numeric_inputs)\n",
    "\n",
    "# #preprocessed.append(numeric_normalized)\n",
    "# preprocessed.append(numeric_inputs)\n",
    "# #preprocessed\n",
    "\n",
    "# preprocessed_result = tf.concat(preprocessed, axis=-1)\n",
    "# #preprocessed_result\n",
    "\n",
    "# preprocessor = tf.keras.Model(inputs, preprocessed_result)\n",
    "\n",
    "# tf.keras.utils.plot_model(preprocessor, rankdir=\"LR\", show_shapes=True)\n",
    "\n",
    "# preprocessor(dict(xScaledData.iloc[:1]))\n",
    "\n",
    "# xWinTrainDict, xWinValDict, xWinTestDict = xScaledData.loc[split==\"Train\", :], xScaledData.loc[split==\"Validation\", :], xScaledData.loc[split==\"Test\", :]\n",
    "# xWinTrainDict, xWinValDict, xWinTestDict = dict(xWinTrainDict), dict(xWinValDict), dict(xWinTestDict)\n",
    "# xWinTrainDict, xWinValDict, xWinTestDict = preprocessor(xWinTrainDict), preprocessor(xWinValDict), preprocessor(xWinTestDict)\n",
    "\n",
    "\n",
    "# # dataset = tf.data.experimental.make_csv_dataset(xScaledDataWithDummies, label_name=\"returns\", batch_size=2, num_epochs=1,\n",
    "# #     shuffle=False, sloppy=True) # compression_type = GZIP #ZLIB\n",
    "# # iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "\n",
    "# # inputs = {}\n",
    "# # for name, column in df.items():\n",
    "# #   if type(column[0]) == str:\n",
    "# #     dtype = tf.string\n",
    "# #   elif (name in categorical_feature_names or\n",
    "# #         name in binary_feature_names):\n",
    "# #     dtype = tf.int64\n",
    "# #   else:\n",
    "# #     dtype = tf.float32\n",
    "\n",
    "# #   inputs[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(\"Num GPUs Available: \", len(physical_devices))\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float16, numpy=1.0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.array([\n",
    "    [0.1, 0.2, 0.9, 0],\n",
    "    [0.2, 0.9, 0.1, 0]\n",
    "])\n",
    "\n",
    "y_true = np.array([\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "y_pred2 = np.array([\n",
    "    [0.1, 0.2, 0.9, 0],\n",
    "    [0.2, 0.9, 0.1, 0]\n",
    "])\n",
    "\n",
    "y_true2 = np.array([\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 1, 0, 0]\n",
    "])\n",
    "\n",
    "\n",
    "y_pred_max = tf.argmax(y_pred, axis=1)\n",
    "y_true_max = tf.argmax(y_true, axis=1)\n",
    "\n",
    "y_pred_max = tf.cast(y_pred_max, \"float16\")\n",
    "y_true_max = tf.cast(y_true_max, \"float16\")\n",
    "\n",
    "shape = tf.constant(y_pred.shape[1]/2 - 1/2, dtype = \"float16\", shape = y_pred.shape[0])\n",
    "\n",
    "values = ((y_pred_max < shape) & (y_true_max < shape)) | ((y_pred_max > shape) & (y_true_max > shape))\n",
    "values\n",
    "\n",
    "values = tf.cast(values, \"float16\")\n",
    "values\n",
    "tf.reduce_sum(values)\n",
    "\n",
    "# values = []\n",
    "# # for n in zip(y_max, y_true_max):\n",
    "# #     if n[0] > shape and n[1] > shape:\n",
    "# #         values = values + [1]\n",
    "# #     elif n[0] < shape and n[1] < shape:\n",
    "# #         values = values + [1]\n",
    "# #     else:\n",
    "# #         values = values + [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = MeanDirectionalAccuracy()\n",
    "m.reset_state()\n",
    "m.update_state(y_true2, y_pred2)\n",
    "m.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([1., 2., 3., 1., 2., 3.], dtype=float32)>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "a = tf.concat([a, [1, 2, 3]], 0)\n",
    "a = tf.concat([a, [1, 2, 3]], 0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanDirectionalAccuracy(tf.keras.metrics.Metric):\n",
    "\n",
    "  def __init__(self, name='mean_directional_accuracy', **kwargs):\n",
    "    super().__init__(name=name, **kwargs)\n",
    "    self.mda = self.add_weight(name='mda', initializer='zeros')\n",
    "    #self.n = self.add_weight(name='n', initializer='zeros')\n",
    "    #self.da = self.add_weight(name='da', initializer='zeros')\n",
    "    #self.da = tf.variable(0.5, dtype = \"float16\", shape = y_pred.shape[0])\n",
    "    #self.da = []\n",
    "    #self.da = tf.TensorArray(dtype=self.dtype, size=0, dynamic_size=True)\n",
    "    self.da = tf.constant([])\n",
    "    #self.da = tf.zeros([0])\n",
    "    #self.da = None\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    \n",
    "    y_true_max = tf.argmax(y_true, axis=1)\n",
    "    y_pred_max = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "    #print(y_true_max, y_pred_max)\n",
    "\n",
    "    y_true_max = tf.cast(y_true_max, \"float16\")\n",
    "    y_pred_max = tf.cast(y_pred_max, \"float16\")\n",
    "    shape = tf.constant(y_pred.shape[1]/2 - 1/2, dtype = \"float16\", shape = y_pred.shape[0])\n",
    "\n",
    "    values = ((y_pred_max < shape) & (y_true_max < shape)) | ((y_pred_max > shape) & (y_true_max > shape))\n",
    "    values = tf.cast(values, self.dtype)\n",
    "    if sample_weight is not None:\n",
    "      sample_weight = tf.cast(sample_weight, self.dtype)\n",
    "      values = tf.multiply(values, sample_weight)\n",
    "    #values = tf.squeeze(values)\n",
    "    #self.mda.assign_add(tf.reduce_sum(values) / y_pred.shape[0])\n",
    "    #self.da = self.da + values\n",
    "    # print(values)\n",
    "    # print(self.da)\n",
    "    #print(values)\n",
    "    #print(\"Hey!\")\n",
    "    #values = tf.reshape(values, shape=(-1))\n",
    "    self.da = tf.concat([self.da, values], axis = 0)\n",
    "    #print(self.da)\n",
    "    self.mda.assign(tf.reduce_mean(self.da))\n",
    "    #prevMDA = self.mda * self.n/(self.n + y_pred.shape[0])\n",
    "    # try:\n",
    "    #   print(len(y_pred))\n",
    "    #   prevMDA = self.mda * tf.math.divide(self.n, self.n + y_pred.shape[0])\n",
    "    #   newMDA = tf.reduce_mean(values) * tf.math.divide(y_pred.shape[0], self.n + y_pred.shape[0])\n",
    "    #   self.mda.assign(prevMDA + newMDA)\n",
    "    #   self.n.assign(self.n + y_pred.shape[0])\n",
    "    # except Exception as e: \n",
    "    #   print(e)\n",
    "    #   self.mda.assign(0.5)\n",
    "    #   self.n.assign(1)\n",
    "\n",
    "    \n",
    "\n",
    "    #self.mda.assign(tf.reduce_mean(values))\n",
    "    #print(self.mda)\n",
    "    #self.mda = tf.reduce_sum(values) / y_pred.shape[0]\n",
    "\n",
    "  def result(self):\n",
    "    return self.mda\n",
    "\n",
    "  def reset_state(self):\n",
    "    # self.da = []\n",
    "    # self.n.assign(0)\n",
    "    self.mda.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import metrics_utils\n",
    "from keras import backend\n",
    "\n",
    "# @keras_export(\"keras.metrics.categorical_accuracy\")\n",
    "# @tf.__internal__.dispatch.add_dispatch_support\n",
    "# def categorical_accuracy(y_true, y_pred):\n",
    "\n",
    "\n",
    "#     values = ((y_pred_max < shape) & (y_true_max < shape)) | ((y_pred_max > shape) & (y_true_max > shape))\n",
    "\n",
    "#     return metrics_utils.sparse_categorical_matches(\n",
    "#         tf.math.argmax(y_true, axis=-1), y_pred\n",
    "#     )\n",
    "\n",
    "\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "from keras.metrics import base_metric\n",
    "from keras.dtensor import utils as dtensor_utils\n",
    "\n",
    "#@keras_export(\"keras.metrics.CategoricalAccuracy\")\n",
    "class MDA(base_metric.MeanMetricWrapper):\n",
    "\n",
    "    # @dtensor_utils.inject_mesh\n",
    "    # def __init__(self, name=\"categorical_accuracy\", dtype=None):\n",
    "    #     super().__init__(\n",
    "    #         lambda y_true, y_pred: metrics_utils.sparse_categorical_matches(\n",
    "    #             tf.math.argmax(y_true, axis=-1), y_pred\n",
    "    #         ),\n",
    "    #         name,\n",
    "    #         dtype=dtype,\n",
    "    #     )\n",
    "    # @dtensor_utils.inject_mesh\n",
    "    # def __init__(self, name=\"categorical_accuracy\", dtype=None):\n",
    "    #     super().__init__(\n",
    "    #         lambda y_true, y_pred: (\n",
    "    #             y_true_max = tf.argmax(y_true, axis=1)\n",
    "    #             y_pred_max = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "    #             y_true_max = tf.cast(y_true_max, \"float16\")\n",
    "    #             y_pred_max = tf.cast(y_pred_max, \"float16\")\n",
    "    #             shape = tf.constant(y_pred.shape[1]/2 - 1/2, dtype = \"float16\", shape = y_pred.shape[0])\n",
    "    #             values = ((y_pred_max < shape) & (y_true_max < shape)) | ((y_pred_max > shape) & (y_true_max > shape))\n",
    "    #             values = tf.cast(values, self.dtype)\n",
    "    #             values)\n",
    "    #         ,\n",
    "    #         name,\n",
    "    #         dtype=dtype,\n",
    "    #     )\n",
    "    @dtensor_utils.inject_mesh\n",
    "    def __init__(self, name=\"mean_directional_accuracy\", dtype=None):\n",
    "        super().__init__(\n",
    "            lambda y_true, y_pred: (\n",
    "                ((tf.argmax(y_pred, axis=1)*2 < y_pred.shape[1] - 1) & \n",
    "                (tf.argmax(y_true, axis=1)*2 < y_pred.shape[1] - 1)) | \n",
    "                ((tf.argmax(y_pred, axis=1)*2 > y_pred.shape[1] - 1) & \n",
    "                (tf.argmax(y_true, axis=1)*2 > y_pred.shape[1] - 1))\n",
    "                )\n",
    "            ,\n",
    "            name,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metrics_utils.sparse_categorical_matches(\n",
    "#                 tf.math.argmax(y_true, axis=-1), y_pred\n",
    "#                 )\n",
    "\n",
    "\n",
    "y_true_max = tf.argmax(y_true, axis=1)\n",
    "y_pred_max = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "y_true_max = tf.cast(y_true_max, \"float16\")\n",
    "y_pred_max = tf.cast(y_pred_max, \"float16\")\n",
    "shape = tf.constant(y_pred.shape[1]/2 - 1/2, dtype = \"float16\", shape = y_pred.shape[0])\n",
    "((y_pred_max < shape) & (y_true_max < shape)) | ((y_pred_max > shape) & (y_true_max > shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNfunction(modelType, layers, units, activation, dropout, L1, L2, batch_size, optimizer, learning_rate, runningDate,\n",
    "               loss=\"MeanAbsoluteError\", performCV=False, patience=1, verbose=1):\n",
    "    \n",
    "    epochs = 300\n",
    "    finalActivation = \"linear\"\n",
    "    numberOfOutputs = yTrain.shape[1]\n",
    "\n",
    "    #loss = \"MeanAbsoluteError\"   # 'tf.keras.losses.MeanSquaredError()'  #Huber #MeanAbsoluteError #MeanSquaredError \n",
    "    metrics = [\"MeanAbsoluteError\", \"MeanSquaredError\"] #, \"RootMeanSquaredError\"]) #accuracy #MeanSquaredLogarithmicError #RootMeanSquaredError #MeanAbsolutePercentageError \n",
    "    EarlyStopping_monitor = \"val_mean_absolute_error\"\n",
    "\n",
    "    if classification:\n",
    "        finalActivation = \"softmax\"\n",
    "        loss = \"categorical_crossentropy\"\n",
    "        #metrics = [\"accuracy\", \"categorical_accuracy\", MeanDirectionalAccuracy()]\n",
    "        metrics = [\"accuracy\", \"categorical_accuracy\", MDA()]\n",
    "        EarlyStopping_monitor = \"val_loss\"\n",
    "\n",
    "\n",
    "    # Adam(learning_rate=0.0001) #RMSprop #sgd\n",
    "    if optimizer == \"RMSprop\":\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "    #input1 = tf.keras.layers.Input(shape=(1,))\n",
    "    #input2 = tf.keras.layers.Input(shape=(1,))\n",
    "    #merged = tf.keras.layers.Concatenate(axis=1)([input1, input2])\n",
    "    #dense1 = tf.keras.layers.Dense(2, input_dim=2, activation=keras.activations.sigmoid, use_bias=True)(merged)\n",
    "\n",
    "    input = tf.keras.layers.Input(shape=(xTrain.shape[1],))\n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if modelType == \"LSTM\":\n",
    "        hidden = tf.keras.layers.LSTM(units=units[0])(input)\n",
    "    elif modelType == \"GRU\":\n",
    "        hidden = tf.keras.layers.GRU(units=units[0])(input)\n",
    "    elif modelType == \"SimpleRNN\":\n",
    "        hidden = tf.keras.layers.SimpleRNN(units=units[0])(input)\n",
    "    elif modelType == \"FeedForward\":\n",
    "        denseLayer = tf.keras.layers.Dense(units=units[0],\n",
    "                            activation=activation,\n",
    "                            kernel_regularizer=tf.keras.regularizers.L1L2(l1=L1, l2=L2))\n",
    "        dropOutLayer = tf.keras.layers.Dropout(rate=dropout)\n",
    "\n",
    "        try:\n",
    "            hidden = denseLayer(input)\n",
    "            hidden = dropOutLayer(hidden)\n",
    "            for n in range(1, layers):\n",
    "                hidden = tf.keras.layers.Dense(units=units[n],\n",
    "                            activation=activation,\n",
    "                            kernel_regularizer=tf.keras.regularizers.L1L2(l1=L1, l2=L2))(hidden)\n",
    "                hidden = dropOutLayer(hidden)\n",
    "                # scaler.fit_transform\n",
    "                # tf.keras.layers.BatchNormalization(\n",
    "            print(f\"NN{layers}Layer\")\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            print(\"NN1Layer_WhyDidWeEndUpHere?\")\n",
    "\n",
    "\n",
    "    outputLayer = tf.keras.layers.Dense(units=numberOfOutputs, activation=finalActivation,\n",
    "                                        kernel_regularizer=tf.keras.regularizers.L1L2(l1=L1, l2=L2))\n",
    "\n",
    "    if modelType == \"RegularizedLinear\" or modelType == \"StandardLinear\":\n",
    "        output = outputLayer(input)\n",
    "        print(\"Linear\")\n",
    "    # elif modelType == \"StandardLinear\":\n",
    "    #     output = outputLayer(input)\n",
    "    #     print(\"StandardLinear\")\n",
    "    else:\n",
    "        output = outputLayer(hidden)\n",
    "\n",
    "    #model = tf.keras.models.Model(inputs=[input1, input2], output=output)\n",
    "    model = tf.keras.models.Model(inputs=[input], outputs=output, name=f\"{modelType}_{runningDate}\")\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss=loss,\n",
    "                metrics=metrics)\n",
    "\n",
    "\n",
    "\n",
    "    if performCV:\n",
    "\n",
    "        loss_and_metrics_matrix = pd.DataFrame()\n",
    "        epochs_matrix = pd.DataFrame()\n",
    "        quarters = dummyData[dummyData[\"Split\"] == \"Validation\"][\"fdateYQ\"].unique()\n",
    "        quarters.sort()\n",
    "\n",
    "        for q in quarters:\n",
    "            # Run cross validation through model fit and evaluate. \n",
    "            # Save all MSAs and MSEs from \"loss_and_metrics\" into a dataframe and take their average to obtain the best model\n",
    "            print(\"Quarter \" + str(q))\n",
    "            xT = dummyData[dummyData[\"fdateYQ\"] < q].drop(descrNames + dependentNames + detailedDummyNames, axis=1)\n",
    "            yT = dummyData[dummyData[\"fdateYQ\"] < q][target]\n",
    "            #A rolling window might be faster. E.g.: dummies[\"fdateYQ\"] == q - 0.25\n",
    "\n",
    "            xV = dummyData[dummyData[\"fdateYQ\"] == q].drop(descrNames + dependentNames + detailedDummyNames, axis=1)\n",
    "            yV = dummyData[dummyData[\"fdateYQ\"] == q][target]\n",
    "\n",
    "            if classification:\n",
    "                yT = pd.get_dummies(yT, dummy_na=False, sparse=False, drop_first=False,\n",
    "                            columns=target)\n",
    "                yV = pd.get_dummies(yV, dummy_na=False, sparse=False, drop_first=False,\n",
    "                            columns=target)\n",
    "                \n",
    "\n",
    "            earlystopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor=EarlyStopping_monitor, patience=patience, verbose=1, restore_best_weights=True) #Can this be taken out from the loop? => Less computing\n",
    "\n",
    "            # tensorboardLogPath = f\"Results/logs/{runningDate}/{modelType}_{layers}L_{units}U_{activation}_{dropout}DO_{L1}L1_{L2}L2_{batch_size}Batch_{optimizer._name}_{learning_rate}LR_Q{q}\"\n",
    "            # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboardLogPath) #tensorboard --logdir=\"10-01-2023_1606\" #tensorboard --logdir=\"logs/\"\n",
    "            #BackupAndRestore_callback = tf.keras.callbacks.BackupAndRestore(backup_dir=\"/tmp/backup\")\n",
    "\n",
    "            history = model.fit(x=xT, y=yT,\n",
    "                                batch_size=batch_size, epochs=epochs,\n",
    "                                callbacks=[earlystopping_callback], verbose=verbose, validation_data=(xV, yV))\n",
    "\n",
    "            loss_and_metrics = model.evaluate(xV, yV, batch_size=batch_size, return_dict=True)\n",
    "            epochs_EarlyStopping = len(history.history['loss']) - patience\n",
    "\n",
    "            loss_and_metrics = pd.DataFrame(loss_and_metrics, index=[q])\n",
    "            loss_and_metrics_matrix = pd.concat((loss_and_metrics_matrix, loss_and_metrics))\n",
    "\n",
    "            epochs_EarlyStopping = pd.DataFrame({\"Epochs\": epochs_EarlyStopping}, index=loss_and_metrics.index)\n",
    "            epochs_matrix = pd.concat((epochs_matrix, epochs_EarlyStopping))\n",
    "        print(pd.concat((loss_and_metrics_matrix, epochs_matrix), ignore_index=False, axis=1), \"\\n\")\n",
    "\n",
    "        loss_and_metrics = loss_and_metrics_matrix.mean()\n",
    "        loss_and_metrics = loss_and_metrics.to_dict()\n",
    "        epochs_EarlyStopping = epochs_matrix.mean()[0]\n",
    "\n",
    "    else:\n",
    "        earlystopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor=EarlyStopping_monitor, patience=patience, verbose=1, restore_best_weights=True) #Can this be taken out from the loop? => Less computing\n",
    "\n",
    "        tensorboardLogPath = f\"Results/logs/{runningDate}/{modelType}_{layers}L_{units}U_{activation}_{dropout}DO_{L1}L1_{L2}L2_{batch_size}Batch_{optimizer._name}_{learning_rate}LR\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboardLogPath) #tensorboard --logdir=\"10-01-2023_1606\" #tensorboard --logdir=\"logs/\"\n",
    "        #BackupAndRestore_callback = tf.keras.callbacks.BackupAndRestore(backup_dir=\"/tmp/backup\")\n",
    "        history = model.fit(x=xTrain, y=yTrain,\n",
    "                batch_size=batch_size, epochs=epochs,\n",
    "                callbacks=[earlystopping_callback, tensorboard_callback], verbose=verbose, validation_data = (xVal, yVal))\n",
    "        loss_and_metrics = model.evaluate(xVal, yVal, batch_size=batch_size, return_dict=True)\n",
    "        epochs_EarlyStopping = len(history.history['loss']) - patience\n",
    "        print(loss_and_metrics, \"\\n\", epochs_EarlyStopping, \"\\n\")\n",
    "        #history.history[]\n",
    "\n",
    "    # history = model.fit(x=xWinTrainDict, y=yTrain,\n",
    "    #           batch_size=16, epochs=300,\n",
    "    #           callbacks=[callback], verbose=1, validation_data = (xWinValDict, yVal))\n",
    "\n",
    "    return loss_and_metrics, epochs_EarlyStopping, model\n",
    "\n",
    "    #model.predict(xVal, batch_size=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import chain, combinations, permutations\n",
    "# def powerset(iterable):\n",
    "#     \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "#     s = list(iterable)\n",
    "#     res = chain.from_iterable(permutations(s, r) for r in range(1, len(s)+1))\n",
    "#     return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter grid creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "#Hyper-Parameters\n",
    "NNModelType = {\"FeedForward\"} #, \"LSTM\", \"GRU\"}\n",
    "hiddenLayers = {1, 2, 3, 4, 5}\n",
    "hiddenUnits = 2**np.arange(9)\n",
    "#hiddenUnitsGrid = list(itertools.product(hiddenUnits, repeat=max(hiddenLayers)))\n",
    "#hiddenUnitsGrid = random.sample(population = hiddenUnits, k=max(hiddenLayers), counts=10)\n",
    "#hiddenUnitsGrid = random.sample(list(itertools.product(hiddenUnits, repeat=max(hiddenLayers))), k=50)\n",
    "\n",
    "activationFunc = {\"linear\", \"relu\", \"sigmoid\", \"tanh\"} #etc. https://keras.io/api/layers/activations/ , \"selu\"\n",
    "dropOut = np.arange(start=0, stop=1, step=0.1)\n",
    "#epochs = [100] #Just choose best model. => Run for 100 epochs, but if 20th epoch is the best, choose it, i.e. early stopping\n",
    "#inputSize = numberOfVariables #ncol(trainXPools[[1]])\n",
    "#other parameters: loss func, node structure, location of the regularization function\n",
    "batchSize = 2**np.arange(start=4, stop=9)\n",
    "optim = {\"Adam\", \"RMSprop\"}#, \"sgd\"}\n",
    "learningRate = {0.01, 0.001, 0.0001}#, 0.00001}\n",
    "L1L2Grid = 10**np.arange(start=2, stop=-6, step=-1.0) #3 and -10\n",
    "L1L2Grid = np.append(0, L1L2Grid)\n",
    "none = {None}\n",
    "zero = {0}\n",
    "\n",
    "StandardLinearParams = {\n",
    "    \"Model type\": {\"StandardLinear\"},\n",
    "    \"Hidden layers\": zero,\n",
    "    \"Hidden units\": zero,\n",
    "    \"Activation function\": none,\n",
    "    \"Dropout\": zero,\n",
    "    \"L1\": {0.0},\n",
    "    \"L2\": {0.0},\n",
    "    \"Batch size\": {32},\n",
    "    \"Optimizer\": {\"Adam\"},\n",
    "    \"Learning rate\": {0.001}\n",
    "}\n",
    "\n",
    "LinearParams = StandardLinearParams | {\n",
    "    \"Model type\": {\"RegularizedLinear\"},\n",
    "    \"L1\": L1L2Grid,\n",
    "    \"L2\": L1L2Grid,\n",
    "    \"Batch size\": batchSize,\n",
    "    \"Optimizer\": optim,\n",
    "    \"Learning rate\": learningRate\n",
    "}\n",
    "\n",
    "NNParams = LinearParams | {\n",
    "    \"Model type\": NNModelType,  # , \"LSTM\", \"GRU\"},\n",
    "    \"Hidden layers\": hiddenLayers,\n",
    "    \"Hidden units\": hiddenUnits, #hiddenUnitsGrid,\n",
    "    \"Activation function\": activationFunc,\n",
    "    \"Dropout\": dropOut\n",
    "}\n",
    "\n",
    "NaiveZeroParams = StandardLinearParams | {\n",
    "    \"Model type\": {\"NaiveZeroForecast\"},\n",
    "    \"Batch size\": none,\n",
    "    \"Optimizer\": none,\n",
    "    \"Learning rate\": none,\n",
    "    \"Epochs\": none,\n",
    "    \"Training Time (minutes)\": none,\n",
    "    \"ModelPointer\": none\n",
    "}\n",
    "\n",
    "NaiveMeanParams = NaiveZeroParams | {\"Model type\": {\"NaiveMeanForecast\"}}\n",
    "NaiveFirmMeanParams = NaiveZeroParams| {\"Model type\": {\"NaiveFirmMeanForecast\"}}\n",
    "NaiveFiveYearParams = NaiveZeroParams| {\"Model type\": {\"NaiveFiveYearForecast\"}}\n",
    "\n",
    "StandardLinearGrid = pd.DataFrame(itertools.product(\n",
    "    *StandardLinearParams.values()), columns=StandardLinearParams.keys())\n",
    "\n",
    "linearGrid = pd.DataFrame(itertools.product(\n",
    "    *LinearParams.values()), columns=LinearParams.keys())\n",
    "\n",
    "NaiveZeroGrid = pd.DataFrame(itertools.product(\n",
    "    *NaiveZeroParams.values()), columns=NaiveZeroParams.keys())\n",
    "\n",
    "NaiveMeanGrid = pd.DataFrame(itertools.product(\n",
    "    *NaiveMeanParams.values()), columns=NaiveMeanParams.keys())\n",
    "\n",
    "NaiveFirmMeanGrid = pd.DataFrame(itertools.product(\n",
    "    *NaiveFirmMeanParams.values()), columns=NaiveFirmMeanParams.keys())\n",
    "\n",
    "NaiveFiveYearGrid = pd.DataFrame(itertools.product(\n",
    "    *NaiveFiveYearParams.values()), columns=NaiveFiveYearParams.keys())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not classification:\n",
    "    trainReturns = yTrain[\"returns\"]\n",
    "    valReturns = yVal[\"returns\"]\n",
    "\n",
    "    #Naive Zero\n",
    "    zeroForecastMSE = sum(valReturns**2)/len(valReturns)\n",
    "    zeroForecastMAE = sum(abs(valReturns))/len(valReturns)\n",
    "\n",
    "    # This works also:\n",
    "    # tf.keras.losses.MeanSquaredError().call(y_true=yVal,\n",
    "    #                                         y_pred=np.zeros(len(yVal)))\n",
    "    # tf.keras.losses.MeanAbsoluteError().call(y_true=yVal,\n",
    "    #                                          y_pred=np.zeros(len(yVal)))\n",
    "\n",
    "    NaiveZeroGrid.loc[:,\"mean_absolute_error\"] = zeroForecastMAE\n",
    "    NaiveZeroGrid.loc[:,\"mean_squared_error\"] = zeroForecastMSE\n",
    "\n",
    "    #Naive Mean\n",
    "    naiveMean = sum(trainReturns)/len(trainReturns)\n",
    "    meanForecastMSE = sum((valReturns-naiveMean)**2)/len(valReturns)\n",
    "    meanForecastMAE = sum(abs(valReturns-naiveMean))/len(valReturns)\n",
    "\n",
    "    NaiveMeanGrid.loc[:,\"mean_absolute_error\"] = meanForecastMAE\n",
    "    NaiveMeanGrid.loc[:,\"mean_squared_error\"] = meanForecastMSE\n",
    "\n",
    "    # This works also:\n",
    "    # tf.keras.losses.MeanSquaredError().call(y_true=yVal,\n",
    "    #                                         y_pred=np.full(len(yVal), naiveMean)\n",
    "    #                                         )\n",
    "    # tf.keras.losses.MeanAbsoluteError().call(y_true=yVal,\n",
    "    #                                          y_pred=np.full(len(yVal), naiveMean)\n",
    "    #                                          )\n",
    "\n",
    "\n",
    "    #Naive FirmMean\n",
    "    naiveFirmMean = finalData.loc[finalData[\"Split\"]==\"Train\", [\"gvkey\", 'returns']].groupby(['gvkey'])['returns'].mean()\n",
    "\n",
    "    naiveFirmMeanDF = pd.merge(naiveFirmMean, finalData.loc[finalData[\"Split\"] == \"Validation\", [\"gvkey\", 'returns']],\n",
    "                            on=\"gvkey\", suffixes=('_pred', '_real'))\n",
    "\n",
    "    naiveFirmError = naiveFirmMeanDF[\"returns_pred\"] - naiveFirmMeanDF[\"returns_real\"]\n",
    "    naiveFirmError = naiveFirmError.dropna() #I should maybe instead drop all firms without historical observations. Otherwise the models are not comparable. There will be a bias\n",
    "    firmMeanForecastMSE = sum(naiveFirmError**2)/len(naiveFirmError)\n",
    "    firmMeanForecastMAE = sum(abs(naiveFirmError))/len(naiveFirmError)\n",
    "\n",
    "    NaiveFirmMeanGrid.loc[:,\"mean_absolute_error\"] = firmMeanForecastMAE\n",
    "    NaiveFirmMeanGrid.loc[:,\"mean_squared_error\"] = firmMeanForecastMSE\n",
    "\n",
    "    # #Naive FiveYearFirmMean\n",
    "    naiveFiveYearError = finalData.loc[finalData[\"Split\"]==\"Validation\"][\"past5YearReturn\"]/(5*4) - finalData.loc[finalData[\"Split\"]==\"Validation\"]['returns']\n",
    "    firmFiveYearForecastMSE = sum(naiveFiveYearError**2)/len(naiveFiveYearError)\n",
    "    firmFiveYearForecastMAE = sum(abs(naiveFiveYearError))/len(naiveFiveYearError)\n",
    "\n",
    "    NaiveFiveYearGrid.loc[:,\"mean_absolute_error\"] = firmFiveYearForecastMAE\n",
    "    NaiveFiveYearGrid.loc[:,\"mean_squared_error\"] = firmFiveYearForecastMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beep():\n",
    "  import winsound\n",
    "  import math\n",
    "  for n in range(2, 30):\n",
    "      duration = round(2000/n)+300  # milliseconds\n",
    "      freq = 400 + round(math.sin(n/2)*10*n + n*10) # Hz\n",
    "      winsound.Beep(freq, duration)\n",
    "      freq = 100 + round(math.sin(n/2)*10*n + n*10)\n",
    "      winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTrain.info(verbose=True)\n",
    "# xVal.info(verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run horse race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  1  out of  111\n",
      "Model type             StandardLinear\n",
      "Hidden layers                       0\n",
      "Hidden units                        0\n",
      "Activation function              None\n",
      "Dropout                           0.0\n",
      "L1                                0.0\n",
      "L2                                0.0\n",
      "Batch size                         32\n",
      "Optimizer                        Adam\n",
      "Learning rate                   0.001\n",
      "Name: 0, dtype: object\n",
      "Linear\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 3s 10ms/step - loss: 1.5303 - accuracy: 0.2835 - categorical_accuracy: 0.2835 - mean_directional_accuracy: 0.5100 - val_loss: 1.4659 - val_accuracy: 0.2595 - val_categorical_accuracy: 0.2595 - val_mean_directional_accuracy: 0.4871\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.4120 - accuracy: 0.3063 - categorical_accuracy: 0.3063 - mean_directional_accuracy: 0.5334 - val_loss: 1.4215 - val_accuracy: 0.2714 - val_categorical_accuracy: 0.2714 - val_mean_directional_accuracy: 0.4991\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.3763 - accuracy: 0.3238 - categorical_accuracy: 0.3238 - mean_directional_accuracy: 0.5481 - val_loss: 1.4100 - val_accuracy: 0.2767 - val_categorical_accuracy: 0.2767 - val_mean_directional_accuracy: 0.5000\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3610 - accuracy: 0.3367 - categorical_accuracy: 0.3367 - mean_directional_accuracy: 0.5523 - val_loss: 1.4009 - val_accuracy: 0.2773 - val_categorical_accuracy: 0.2773 - val_mean_directional_accuracy: 0.4941\n",
      "Epoch 5/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3525 - accuracy: 0.3407 - categorical_accuracy: 0.3407 - mean_directional_accuracy: 0.5562 - val_loss: 1.3989 - val_accuracy: 0.2756 - val_categorical_accuracy: 0.2756 - val_mean_directional_accuracy: 0.4948\n",
      "Epoch 6/300\n",
      "166/178 [==========================>...] - ETA: 0s - loss: 1.3477 - accuracy: 0.3470 - categorical_accuracy: 0.3470 - mean_directional_accuracy: 0.5570Restoring model weights from the end of the best epoch: 5.\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3471 - accuracy: 0.3470 - categorical_accuracy: 0.3470 - mean_directional_accuracy: 0.5578 - val_loss: 1.4003 - val_accuracy: 0.2710 - val_categorical_accuracy: 0.2710 - val_mean_directional_accuracy: 0.4877\n",
      "Epoch 6: early stopping\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 1.3989 - accuracy: 0.2756 - categorical_accuracy: 0.2756 - mean_directional_accuracy: 0.4948\n",
      "{'loss': 1.398918867111206, 'accuracy': 0.27557411789894104, 'categorical_accuracy': 0.27557411789894104, 'mean_directional_accuracy': 0.4947807788848877} \n",
      " 5 \n",
      "\n",
      "Model time: 0.16811462864279747 minutes\n",
      "\n",
      "Total time: 0.16811462864279747 minutes\n",
      "\n",
      "\n",
      "Model  2  out of  111\n",
      "Model type             RegularizedLinear\n",
      "Hidden layers                          0\n",
      "Hidden units                           0\n",
      "Activation function                 None\n",
      "Dropout                              0.0\n",
      "L1                                   0.1\n",
      "L2                                   0.0\n",
      "Batch size                            32\n",
      "Optimizer                           Adam\n",
      "Learning rate                       0.01\n",
      "Name: 1090, dtype: object\n",
      "Linear\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 3s 12ms/step - loss: 1.8861 - accuracy: 0.2731 - categorical_accuracy: 0.2731 - mean_directional_accuracy: 0.5184 - val_loss: 1.5586 - val_accuracy: 0.2692 - val_categorical_accuracy: 0.2692 - val_mean_directional_accuracy: 0.5252\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.5362 - accuracy: 0.2826 - categorical_accuracy: 0.2826 - mean_directional_accuracy: 0.5207 - val_loss: 1.5474 - val_accuracy: 0.2415 - val_categorical_accuracy: 0.2415 - val_mean_directional_accuracy: 0.4843\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.5340 - accuracy: 0.2685 - categorical_accuracy: 0.2685 - mean_directional_accuracy: 0.5072 - val_loss: 1.5426 - val_accuracy: 0.2419 - val_categorical_accuracy: 0.2419 - val_mean_directional_accuracy: 0.4636\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.5397 - accuracy: 0.2708 - categorical_accuracy: 0.2708 - mean_directional_accuracy: 0.5149 - val_loss: 1.5307 - val_accuracy: 0.2543 - val_categorical_accuracy: 0.2543 - val_mean_directional_accuracy: 0.4731\n",
      "Epoch 5/300\n",
      "169/178 [===========================>..] - ETA: 0s - loss: 1.5275 - accuracy: 0.2641 - categorical_accuracy: 0.2641 - mean_directional_accuracy: 0.5116Restoring model weights from the end of the best epoch: 4.\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.5277 - accuracy: 0.2640 - categorical_accuracy: 0.2640 - mean_directional_accuracy: 0.5118 - val_loss: 1.5308 - val_accuracy: 0.2580 - val_categorical_accuracy: 0.2580 - val_mean_directional_accuracy: 0.5030\n",
      "Epoch 5: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.5307 - accuracy: 0.2543 - categorical_accuracy: 0.2543 - mean_directional_accuracy: 0.4731\n",
      "{'loss': 1.5307167768478394, 'accuracy': 0.2543058395385742, 'categorical_accuracy': 0.2543058395385742, 'mean_directional_accuracy': 0.4731210768222809} \n",
      " 4 \n",
      "\n",
      "Model time: 0.15820294991135597 minutes\n",
      "\n",
      "Total time: 0.3264342434704304 minutes\n",
      "\n",
      "\n",
      "Model  3  out of  111\n",
      "Model type             RegularizedLinear\n",
      "Hidden layers                          0\n",
      "Hidden units                           0\n",
      "Activation function                 None\n",
      "Dropout                              0.0\n",
      "L1                                 100.0\n",
      "L2                                 0.001\n",
      "Batch size                            32\n",
      "Optimizer                        RMSprop\n",
      "Learning rate                      0.001\n",
      "Name: 458, dtype: object\n",
      "Linear\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 3s 8ms/step - loss: 1881.1625 - accuracy: 0.2460 - categorical_accuracy: 0.2460 - mean_directional_accuracy: 0.5012 - val_loss: 35.2909 - val_accuracy: 0.2281 - val_categorical_accuracy: 0.2281 - val_mean_directional_accuracy: 0.4571\n",
      "Epoch 2/300\n",
      "163/178 [==========================>...] - ETA: 0s - loss: 35.0022 - accuracy: 0.2653 - categorical_accuracy: 0.2653 - mean_directional_accuracy: 0.5058Restoring model weights from the end of the best epoch: 1.\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 35.0033 - accuracy: 0.2664 - categorical_accuracy: 0.2664 - mean_directional_accuracy: 0.5098 - val_loss: 35.3450 - val_accuracy: 0.2263 - val_categorical_accuracy: 0.2263 - val_mean_directional_accuracy: 0.4556\n",
      "Epoch 2: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 35.2909 - accuracy: 0.2281 - categorical_accuracy: 0.2281 - mean_directional_accuracy: 0.4571\n",
      "{'loss': 35.29091262817383, 'accuracy': 0.22807933390140533, 'categorical_accuracy': 0.22807933390140533, 'mean_directional_accuracy': 0.45707201957702637} \n",
      " 1 \n",
      "\n",
      "Model time: 0.08354964107275009 minutes\n",
      "\n",
      "Total time: 0.41008390486240387 minutes\n",
      "\n",
      "\n",
      "Model  4  out of  111\n",
      "Model type             RegularizedLinear\n",
      "Hidden layers                          0\n",
      "Hidden units                           0\n",
      "Activation function                 None\n",
      "Dropout                              0.0\n",
      "L1                                  0.01\n",
      "L2                                   0.0\n",
      "Batch size                            32\n",
      "Optimizer                           Adam\n",
      "Learning rate                     0.0001\n",
      "Name: 1359, dtype: object\n",
      "Linear\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 3s 10ms/step - loss: 2.1709 - accuracy: 0.2708 - categorical_accuracy: 0.2708 - mean_directional_accuracy: 0.5088 - val_loss: 2.0802 - val_accuracy: 0.2886 - val_categorical_accuracy: 0.2886 - val_mean_directional_accuracy: 0.5355\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 2.0777 - accuracy: 0.2785 - categorical_accuracy: 0.2785 - mean_directional_accuracy: 0.5156 - val_loss: 2.0047 - val_accuracy: 0.2895 - val_categorical_accuracy: 0.2895 - val_mean_directional_accuracy: 0.5354\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.9993 - accuracy: 0.2822 - categorical_accuracy: 0.2822 - mean_directional_accuracy: 0.5191 - val_loss: 1.9372 - val_accuracy: 0.2942 - val_categorical_accuracy: 0.2942 - val_mean_directional_accuracy: 0.5381\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.9290 - accuracy: 0.2829 - categorical_accuracy: 0.2829 - mean_directional_accuracy: 0.5200 - val_loss: 1.8765 - val_accuracy: 0.2951 - val_categorical_accuracy: 0.2951 - val_mean_directional_accuracy: 0.5397\n",
      "Epoch 5/300\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.8668 - accuracy: 0.2875 - categorical_accuracy: 0.2875 - mean_directional_accuracy: 0.5230 - val_loss: 1.8224 - val_accuracy: 0.2972 - val_categorical_accuracy: 0.2972 - val_mean_directional_accuracy: 0.5427\n",
      "Epoch 6/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.8112 - accuracy: 0.2894 - categorical_accuracy: 0.2894 - mean_directional_accuracy: 0.5230 - val_loss: 1.7742 - val_accuracy: 0.2988 - val_categorical_accuracy: 0.2988 - val_mean_directional_accuracy: 0.5425\n",
      "Epoch 7/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.7629 - accuracy: 0.2947 - categorical_accuracy: 0.2947 - mean_directional_accuracy: 0.5274 - val_loss: 1.7330 - val_accuracy: 0.2997 - val_categorical_accuracy: 0.2997 - val_mean_directional_accuracy: 0.5437\n",
      "Epoch 8/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.7214 - accuracy: 0.2987 - categorical_accuracy: 0.2987 - mean_directional_accuracy: 0.5306 - val_loss: 1.6971 - val_accuracy: 0.3011 - val_categorical_accuracy: 0.3011 - val_mean_directional_accuracy: 0.5462\n",
      "Epoch 9/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.6861 - accuracy: 0.3000 - categorical_accuracy: 0.3000 - mean_directional_accuracy: 0.5314 - val_loss: 1.6674 - val_accuracy: 0.3043 - val_categorical_accuracy: 0.3043 - val_mean_directional_accuracy: 0.5470\n",
      "Epoch 10/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.6562 - accuracy: 0.3026 - categorical_accuracy: 0.3026 - mean_directional_accuracy: 0.5349 - val_loss: 1.6426 - val_accuracy: 0.3048 - val_categorical_accuracy: 0.3048 - val_mean_directional_accuracy: 0.5504\n",
      "Epoch 11/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.6322 - accuracy: 0.3042 - categorical_accuracy: 0.3042 - mean_directional_accuracy: 0.5355 - val_loss: 1.6234 - val_accuracy: 0.3068 - val_categorical_accuracy: 0.3068 - val_mean_directional_accuracy: 0.5500\n",
      "Epoch 12/300\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.6125 - accuracy: 0.3073 - categorical_accuracy: 0.3073 - mean_directional_accuracy: 0.5397 - val_loss: 1.6073 - val_accuracy: 0.3038 - val_categorical_accuracy: 0.3038 - val_mean_directional_accuracy: 0.5459\n",
      "Epoch 13/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.5956 - accuracy: 0.3105 - categorical_accuracy: 0.3105 - mean_directional_accuracy: 0.5411 - val_loss: 1.5930 - val_accuracy: 0.3052 - val_categorical_accuracy: 0.3052 - val_mean_directional_accuracy: 0.5461\n",
      "Epoch 14/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.5803 - accuracy: 0.3130 - categorical_accuracy: 0.3130 - mean_directional_accuracy: 0.5434 - val_loss: 1.5802 - val_accuracy: 0.3043 - val_categorical_accuracy: 0.3043 - val_mean_directional_accuracy: 0.5425\n",
      "Epoch 15/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.5666 - accuracy: 0.3172 - categorical_accuracy: 0.3172 - mean_directional_accuracy: 0.5457 - val_loss: 1.5686 - val_accuracy: 0.3052 - val_categorical_accuracy: 0.3052 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 16/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.5548 - accuracy: 0.3207 - categorical_accuracy: 0.3207 - mean_directional_accuracy: 0.5462 - val_loss: 1.5587 - val_accuracy: 0.3075 - val_categorical_accuracy: 0.3075 - val_mean_directional_accuracy: 0.5448\n",
      "Epoch 17/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.5443 - accuracy: 0.3210 - categorical_accuracy: 0.3210 - mean_directional_accuracy: 0.5472 - val_loss: 1.5496 - val_accuracy: 0.3090 - val_categorical_accuracy: 0.3090 - val_mean_directional_accuracy: 0.5467\n",
      "Epoch 18/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.5350 - accuracy: 0.3230 - categorical_accuracy: 0.3230 - mean_directional_accuracy: 0.5485 - val_loss: 1.5411 - val_accuracy: 0.3081 - val_categorical_accuracy: 0.3081 - val_mean_directional_accuracy: 0.5463\n",
      "Epoch 19/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.5260 - accuracy: 0.3254 - categorical_accuracy: 0.3254 - mean_directional_accuracy: 0.5492 - val_loss: 1.5330 - val_accuracy: 0.3081 - val_categorical_accuracy: 0.3081 - val_mean_directional_accuracy: 0.5479\n",
      "Epoch 20/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.5177 - accuracy: 0.3289 - categorical_accuracy: 0.3289 - mean_directional_accuracy: 0.5506 - val_loss: 1.5257 - val_accuracy: 0.3074 - val_categorical_accuracy: 0.3074 - val_mean_directional_accuracy: 0.5476\n",
      "Epoch 21/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.5101 - accuracy: 0.3302 - categorical_accuracy: 0.3302 - mean_directional_accuracy: 0.5501 - val_loss: 1.5186 - val_accuracy: 0.3069 - val_categorical_accuracy: 0.3069 - val_mean_directional_accuracy: 0.5480\n",
      "Epoch 22/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.5027 - accuracy: 0.3335 - categorical_accuracy: 0.3335 - mean_directional_accuracy: 0.5532 - val_loss: 1.5121 - val_accuracy: 0.3086 - val_categorical_accuracy: 0.3086 - val_mean_directional_accuracy: 0.5511\n",
      "Epoch 23/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.4959 - accuracy: 0.3326 - categorical_accuracy: 0.3326 - mean_directional_accuracy: 0.5516 - val_loss: 1.5056 - val_accuracy: 0.3095 - val_categorical_accuracy: 0.3095 - val_mean_directional_accuracy: 0.5501\n",
      "Epoch 24/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.4893 - accuracy: 0.3370 - categorical_accuracy: 0.3370 - mean_directional_accuracy: 0.5537 - val_loss: 1.4995 - val_accuracy: 0.3128 - val_categorical_accuracy: 0.3128 - val_mean_directional_accuracy: 0.5511\n",
      "Epoch 25/300\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.4833 - accuracy: 0.3344 - categorical_accuracy: 0.3344 - mean_directional_accuracy: 0.5523 - val_loss: 1.4942 - val_accuracy: 0.3108 - val_categorical_accuracy: 0.3108 - val_mean_directional_accuracy: 0.5528\n",
      "Epoch 26/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.4775 - accuracy: 0.3395 - categorical_accuracy: 0.3395 - mean_directional_accuracy: 0.5562 - val_loss: 1.4884 - val_accuracy: 0.3134 - val_categorical_accuracy: 0.3134 - val_mean_directional_accuracy: 0.5531\n",
      "Epoch 27/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.4719 - accuracy: 0.3383 - categorical_accuracy: 0.3383 - mean_directional_accuracy: 0.5546 - val_loss: 1.4834 - val_accuracy: 0.3126 - val_categorical_accuracy: 0.3126 - val_mean_directional_accuracy: 0.5532\n",
      "Epoch 28/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.4665 - accuracy: 0.3432 - categorical_accuracy: 0.3432 - mean_directional_accuracy: 0.5580 - val_loss: 1.4779 - val_accuracy: 0.3154 - val_categorical_accuracy: 0.3154 - val_mean_directional_accuracy: 0.5562\n",
      "Epoch 29/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.4616 - accuracy: 0.3397 - categorical_accuracy: 0.3397 - mean_directional_accuracy: 0.5537 - val_loss: 1.4731 - val_accuracy: 0.3151 - val_categorical_accuracy: 0.3151 - val_mean_directional_accuracy: 0.5551\n",
      "Epoch 30/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.4566 - accuracy: 0.3379 - categorical_accuracy: 0.3379 - mean_directional_accuracy: 0.5532 - val_loss: 1.4689 - val_accuracy: 0.3139 - val_categorical_accuracy: 0.3139 - val_mean_directional_accuracy: 0.5562\n",
      "Epoch 31/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.4520 - accuracy: 0.3414 - categorical_accuracy: 0.3414 - mean_directional_accuracy: 0.5532 - val_loss: 1.4643 - val_accuracy: 0.3147 - val_categorical_accuracy: 0.3147 - val_mean_directional_accuracy: 0.5565\n",
      "Epoch 32/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.4474 - accuracy: 0.3435 - categorical_accuracy: 0.3435 - mean_directional_accuracy: 0.5557 - val_loss: 1.4604 - val_accuracy: 0.3156 - val_categorical_accuracy: 0.3156 - val_mean_directional_accuracy: 0.5583\n",
      "Epoch 33/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.4432 - accuracy: 0.3367 - categorical_accuracy: 0.3367 - mean_directional_accuracy: 0.5509 - val_loss: 1.4560 - val_accuracy: 0.3177 - val_categorical_accuracy: 0.3177 - val_mean_directional_accuracy: 0.5562\n",
      "Epoch 34/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.4392 - accuracy: 0.3368 - categorical_accuracy: 0.3368 - mean_directional_accuracy: 0.5497 - val_loss: 1.4523 - val_accuracy: 0.3177 - val_categorical_accuracy: 0.3177 - val_mean_directional_accuracy: 0.5591\n",
      "Epoch 35/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.4352 - accuracy: 0.3416 - categorical_accuracy: 0.3416 - mean_directional_accuracy: 0.5529 - val_loss: 1.4489 - val_accuracy: 0.3186 - val_categorical_accuracy: 0.3186 - val_mean_directional_accuracy: 0.5608\n",
      "Epoch 36/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.4317 - accuracy: 0.3409 - categorical_accuracy: 0.3409 - mean_directional_accuracy: 0.5529 - val_loss: 1.4451 - val_accuracy: 0.3186 - val_categorical_accuracy: 0.3186 - val_mean_directional_accuracy: 0.5599\n",
      "Epoch 37/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.4281 - accuracy: 0.3414 - categorical_accuracy: 0.3414 - mean_directional_accuracy: 0.5529 - val_loss: 1.4415 - val_accuracy: 0.3194 - val_categorical_accuracy: 0.3194 - val_mean_directional_accuracy: 0.5594\n",
      "Epoch 38/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.4248 - accuracy: 0.3400 - categorical_accuracy: 0.3400 - mean_directional_accuracy: 0.5555 - val_loss: 1.4381 - val_accuracy: 0.3186 - val_categorical_accuracy: 0.3186 - val_mean_directional_accuracy: 0.5579\n",
      "Epoch 39/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.4216 - accuracy: 0.3384 - categorical_accuracy: 0.3384 - mean_directional_accuracy: 0.5506 - val_loss: 1.4352 - val_accuracy: 0.3192 - val_categorical_accuracy: 0.3192 - val_mean_directional_accuracy: 0.5561\n",
      "Epoch 40/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.4186 - accuracy: 0.3414 - categorical_accuracy: 0.3414 - mean_directional_accuracy: 0.5529 - val_loss: 1.4321 - val_accuracy: 0.3201 - val_categorical_accuracy: 0.3201 - val_mean_directional_accuracy: 0.5592\n",
      "Epoch 41/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.4157 - accuracy: 0.3390 - categorical_accuracy: 0.3390 - mean_directional_accuracy: 0.5534 - val_loss: 1.4292 - val_accuracy: 0.3211 - val_categorical_accuracy: 0.3211 - val_mean_directional_accuracy: 0.5583\n",
      "Epoch 42/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.4129 - accuracy: 0.3407 - categorical_accuracy: 0.3407 - mean_directional_accuracy: 0.5513 - val_loss: 1.4267 - val_accuracy: 0.3205 - val_categorical_accuracy: 0.3205 - val_mean_directional_accuracy: 0.5586\n",
      "Epoch 43/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.4103 - accuracy: 0.3409 - categorical_accuracy: 0.3409 - mean_directional_accuracy: 0.5522 - val_loss: 1.4241 - val_accuracy: 0.3203 - val_categorical_accuracy: 0.3203 - val_mean_directional_accuracy: 0.5602\n",
      "Epoch 44/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.4078 - accuracy: 0.3414 - categorical_accuracy: 0.3414 - mean_directional_accuracy: 0.5536 - val_loss: 1.4213 - val_accuracy: 0.3222 - val_categorical_accuracy: 0.3222 - val_mean_directional_accuracy: 0.5616\n",
      "Epoch 45/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.4055 - accuracy: 0.3432 - categorical_accuracy: 0.3432 - mean_directional_accuracy: 0.5534 - val_loss: 1.4192 - val_accuracy: 0.3203 - val_categorical_accuracy: 0.3203 - val_mean_directional_accuracy: 0.5598\n",
      "Epoch 46/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.4033 - accuracy: 0.3423 - categorical_accuracy: 0.3423 - mean_directional_accuracy: 0.5513 - val_loss: 1.4167 - val_accuracy: 0.3219 - val_categorical_accuracy: 0.3219 - val_mean_directional_accuracy: 0.5603\n",
      "Epoch 47/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.4011 - accuracy: 0.3414 - categorical_accuracy: 0.3414 - mean_directional_accuracy: 0.5530 - val_loss: 1.4148 - val_accuracy: 0.3216 - val_categorical_accuracy: 0.3216 - val_mean_directional_accuracy: 0.5605\n",
      "Epoch 48/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3992 - accuracy: 0.3433 - categorical_accuracy: 0.3433 - mean_directional_accuracy: 0.5527 - val_loss: 1.4134 - val_accuracy: 0.3206 - val_categorical_accuracy: 0.3206 - val_mean_directional_accuracy: 0.5586\n",
      "Epoch 49/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3974 - accuracy: 0.3418 - categorical_accuracy: 0.3418 - mean_directional_accuracy: 0.5532 - val_loss: 1.4110 - val_accuracy: 0.3207 - val_categorical_accuracy: 0.3207 - val_mean_directional_accuracy: 0.5609\n",
      "Epoch 50/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3957 - accuracy: 0.3400 - categorical_accuracy: 0.3400 - mean_directional_accuracy: 0.5536 - val_loss: 1.4093 - val_accuracy: 0.3203 - val_categorical_accuracy: 0.3203 - val_mean_directional_accuracy: 0.5598\n",
      "Epoch 51/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3941 - accuracy: 0.3409 - categorical_accuracy: 0.3409 - mean_directional_accuracy: 0.5508 - val_loss: 1.4077 - val_accuracy: 0.3203 - val_categorical_accuracy: 0.3203 - val_mean_directional_accuracy: 0.5609\n",
      "Epoch 52/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3925 - accuracy: 0.3400 - categorical_accuracy: 0.3400 - mean_directional_accuracy: 0.5546 - val_loss: 1.4061 - val_accuracy: 0.3218 - val_categorical_accuracy: 0.3218 - val_mean_directional_accuracy: 0.5609\n",
      "Epoch 53/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.3909 - accuracy: 0.3400 - categorical_accuracy: 0.3400 - mean_directional_accuracy: 0.5550 - val_loss: 1.4045 - val_accuracy: 0.3228 - val_categorical_accuracy: 0.3228 - val_mean_directional_accuracy: 0.5611\n",
      "Epoch 54/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3896 - accuracy: 0.3367 - categorical_accuracy: 0.3367 - mean_directional_accuracy: 0.5483 - val_loss: 1.4033 - val_accuracy: 0.3207 - val_categorical_accuracy: 0.3207 - val_mean_directional_accuracy: 0.5603\n",
      "Epoch 55/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3882 - accuracy: 0.3404 - categorical_accuracy: 0.3404 - mean_directional_accuracy: 0.5562 - val_loss: 1.4022 - val_accuracy: 0.3219 - val_categorical_accuracy: 0.3219 - val_mean_directional_accuracy: 0.5605\n",
      "Epoch 56/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3869 - accuracy: 0.3407 - categorical_accuracy: 0.3407 - mean_directional_accuracy: 0.5558 - val_loss: 1.4006 - val_accuracy: 0.3232 - val_categorical_accuracy: 0.3232 - val_mean_directional_accuracy: 0.5622\n",
      "Epoch 57/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3857 - accuracy: 0.3383 - categorical_accuracy: 0.3383 - mean_directional_accuracy: 0.5546 - val_loss: 1.3992 - val_accuracy: 0.3224 - val_categorical_accuracy: 0.3224 - val_mean_directional_accuracy: 0.5599\n",
      "Epoch 58/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3845 - accuracy: 0.3367 - categorical_accuracy: 0.3367 - mean_directional_accuracy: 0.5518 - val_loss: 1.3980 - val_accuracy: 0.3229 - val_categorical_accuracy: 0.3229 - val_mean_directional_accuracy: 0.5609\n",
      "Epoch 59/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3832 - accuracy: 0.3374 - categorical_accuracy: 0.3374 - mean_directional_accuracy: 0.5520 - val_loss: 1.3971 - val_accuracy: 0.3224 - val_categorical_accuracy: 0.3224 - val_mean_directional_accuracy: 0.5612\n",
      "Epoch 60/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3820 - accuracy: 0.3381 - categorical_accuracy: 0.3381 - mean_directional_accuracy: 0.5541 - val_loss: 1.3956 - val_accuracy: 0.3211 - val_categorical_accuracy: 0.3211 - val_mean_directional_accuracy: 0.5603\n",
      "Epoch 61/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3812 - accuracy: 0.3390 - categorical_accuracy: 0.3390 - mean_directional_accuracy: 0.5525 - val_loss: 1.3948 - val_accuracy: 0.3211 - val_categorical_accuracy: 0.3211 - val_mean_directional_accuracy: 0.5583\n",
      "Epoch 62/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3800 - accuracy: 0.3404 - categorical_accuracy: 0.3404 - mean_directional_accuracy: 0.5574 - val_loss: 1.3936 - val_accuracy: 0.3205 - val_categorical_accuracy: 0.3205 - val_mean_directional_accuracy: 0.5581\n",
      "Epoch 63/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3791 - accuracy: 0.3409 - categorical_accuracy: 0.3409 - mean_directional_accuracy: 0.5566 - val_loss: 1.3926 - val_accuracy: 0.3207 - val_categorical_accuracy: 0.3207 - val_mean_directional_accuracy: 0.5575\n",
      "Epoch 64/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3784 - accuracy: 0.3414 - categorical_accuracy: 0.3414 - mean_directional_accuracy: 0.5585 - val_loss: 1.3916 - val_accuracy: 0.3199 - val_categorical_accuracy: 0.3199 - val_mean_directional_accuracy: 0.5568\n",
      "Epoch 65/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3774 - accuracy: 0.3421 - categorical_accuracy: 0.3421 - mean_directional_accuracy: 0.5567 - val_loss: 1.3914 - val_accuracy: 0.3199 - val_categorical_accuracy: 0.3199 - val_mean_directional_accuracy: 0.5565\n",
      "Epoch 66/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3768 - accuracy: 0.3426 - categorical_accuracy: 0.3426 - mean_directional_accuracy: 0.5548 - val_loss: 1.3906 - val_accuracy: 0.3193 - val_categorical_accuracy: 0.3193 - val_mean_directional_accuracy: 0.5568\n",
      "Epoch 67/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3762 - accuracy: 0.3435 - categorical_accuracy: 0.3435 - mean_directional_accuracy: 0.5613 - val_loss: 1.3892 - val_accuracy: 0.3211 - val_categorical_accuracy: 0.3211 - val_mean_directional_accuracy: 0.5569\n",
      "Epoch 68/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3757 - accuracy: 0.3421 - categorical_accuracy: 0.3421 - mean_directional_accuracy: 0.5587 - val_loss: 1.3887 - val_accuracy: 0.3205 - val_categorical_accuracy: 0.3205 - val_mean_directional_accuracy: 0.5564\n",
      "Epoch 69/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3749 - accuracy: 0.3428 - categorical_accuracy: 0.3428 - mean_directional_accuracy: 0.5578 - val_loss: 1.3882 - val_accuracy: 0.3209 - val_categorical_accuracy: 0.3209 - val_mean_directional_accuracy: 0.5582\n",
      "Epoch 70/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3744 - accuracy: 0.3409 - categorical_accuracy: 0.3409 - mean_directional_accuracy: 0.5557 - val_loss: 1.3871 - val_accuracy: 0.3198 - val_categorical_accuracy: 0.3198 - val_mean_directional_accuracy: 0.5566\n",
      "Epoch 71/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3740 - accuracy: 0.3425 - categorical_accuracy: 0.3425 - mean_directional_accuracy: 0.5578 - val_loss: 1.3866 - val_accuracy: 0.3211 - val_categorical_accuracy: 0.3211 - val_mean_directional_accuracy: 0.5573\n",
      "Epoch 72/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3735 - accuracy: 0.3411 - categorical_accuracy: 0.3411 - mean_directional_accuracy: 0.5548 - val_loss: 1.3862 - val_accuracy: 0.3211 - val_categorical_accuracy: 0.3211 - val_mean_directional_accuracy: 0.5583\n",
      "Epoch 73/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3730 - accuracy: 0.3439 - categorical_accuracy: 0.3439 - mean_directional_accuracy: 0.5550 - val_loss: 1.3854 - val_accuracy: 0.3220 - val_categorical_accuracy: 0.3220 - val_mean_directional_accuracy: 0.5577\n",
      "Epoch 74/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3726 - accuracy: 0.3418 - categorical_accuracy: 0.3418 - mean_directional_accuracy: 0.5537 - val_loss: 1.3852 - val_accuracy: 0.3154 - val_categorical_accuracy: 0.3154 - val_mean_directional_accuracy: 0.5525\n",
      "Epoch 75/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3721 - accuracy: 0.3402 - categorical_accuracy: 0.3402 - mean_directional_accuracy: 0.5546 - val_loss: 1.3847 - val_accuracy: 0.3216 - val_categorical_accuracy: 0.3216 - val_mean_directional_accuracy: 0.5585\n",
      "Epoch 76/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3719 - accuracy: 0.3365 - categorical_accuracy: 0.3365 - mean_directional_accuracy: 0.5539 - val_loss: 1.3843 - val_accuracy: 0.3202 - val_categorical_accuracy: 0.3202 - val_mean_directional_accuracy: 0.5586\n",
      "Epoch 77/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.3716 - accuracy: 0.3419 - categorical_accuracy: 0.3419 - mean_directional_accuracy: 0.5580 - val_loss: 1.3836 - val_accuracy: 0.3206 - val_categorical_accuracy: 0.3206 - val_mean_directional_accuracy: 0.5579\n",
      "Epoch 78/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3712 - accuracy: 0.3405 - categorical_accuracy: 0.3405 - mean_directional_accuracy: 0.5550 - val_loss: 1.3831 - val_accuracy: 0.3231 - val_categorical_accuracy: 0.3231 - val_mean_directional_accuracy: 0.5588\n",
      "Epoch 79/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3711 - accuracy: 0.3430 - categorical_accuracy: 0.3430 - mean_directional_accuracy: 0.5581 - val_loss: 1.3830 - val_accuracy: 0.3210 - val_categorical_accuracy: 0.3210 - val_mean_directional_accuracy: 0.5573\n",
      "Epoch 80/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.3707 - accuracy: 0.3426 - categorical_accuracy: 0.3426 - mean_directional_accuracy: 0.5569 - val_loss: 1.3827 - val_accuracy: 0.3228 - val_categorical_accuracy: 0.3228 - val_mean_directional_accuracy: 0.5583\n",
      "Epoch 81/300\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.3706 - accuracy: 0.3430 - categorical_accuracy: 0.3430 - mean_directional_accuracy: 0.5558 - val_loss: 1.3824 - val_accuracy: 0.3206 - val_categorical_accuracy: 0.3206 - val_mean_directional_accuracy: 0.5579\n",
      "Epoch 82/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3704 - accuracy: 0.3439 - categorical_accuracy: 0.3439 - mean_directional_accuracy: 0.5574 - val_loss: 1.3818 - val_accuracy: 0.3232 - val_categorical_accuracy: 0.3232 - val_mean_directional_accuracy: 0.5582\n",
      "Epoch 83/300\n",
      "176/178 [============================>.] - ETA: 0s - loss: 1.3701 - accuracy: 0.3418 - categorical_accuracy: 0.3418 - mean_directional_accuracy: 0.5552Restoring model weights from the end of the best epoch: 82.\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.3700 - accuracy: 0.3423 - categorical_accuracy: 0.3423 - mean_directional_accuracy: 0.5566 - val_loss: 1.3818 - val_accuracy: 0.3232 - val_categorical_accuracy: 0.3232 - val_mean_directional_accuracy: 0.5591\n",
      "Epoch 83: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.3818 - accuracy: 0.3232 - categorical_accuracy: 0.3232 - mean_directional_accuracy: 0.5582\n",
      "{'loss': 1.381811261177063, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} \n",
      " 82 \n",
      "\n",
      "Model time: 1.6200884580612183 minutes\n",
      "\n",
      "Total time: 2.030251383781433 minutes\n",
      "\n",
      "\n",
      "Model  5  out of  111\n",
      "Model type             RegularizedLinear\n",
      "Hidden layers                          0\n",
      "Hidden units                           0\n",
      "Activation function                 None\n",
      "Dropout                              0.0\n",
      "L1                                   0.1\n",
      "L2                                  0.01\n",
      "Batch size                           128\n",
      "Optimizer                           Adam\n",
      "Learning rate                      0.001\n",
      "Name: 1253, dtype: object\n",
      "Linear\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 2s 22ms/step - loss: 6.5944 - accuracy: 0.2431 - categorical_accuracy: 0.2431 - mean_directional_accuracy: 0.5054 - val_loss: 5.1976 - val_accuracy: 0.2587 - val_categorical_accuracy: 0.2587 - val_mean_directional_accuracy: 0.5230\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 4.2246 - accuracy: 0.2457 - categorical_accuracy: 0.2457 - mean_directional_accuracy: 0.5007 - val_loss: 3.2670 - val_accuracy: 0.2576 - val_categorical_accuracy: 0.2576 - val_mean_directional_accuracy: 0.5218\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.6308 - accuracy: 0.2483 - categorical_accuracy: 0.2483 - mean_directional_accuracy: 0.5028 - val_loss: 2.0435 - val_accuracy: 0.2636 - val_categorical_accuracy: 0.2636 - val_mean_directional_accuracy: 0.5205\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.7280 - accuracy: 0.2517 - categorical_accuracy: 0.2517 - mean_directional_accuracy: 0.4995 - val_loss: 1.4934 - val_accuracy: 0.2777 - val_categorical_accuracy: 0.2777 - val_mean_directional_accuracy: 0.5300\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4291 - accuracy: 0.2752 - categorical_accuracy: 0.2752 - mean_directional_accuracy: 0.5177 - val_loss: 1.4023 - val_accuracy: 0.2300 - val_categorical_accuracy: 0.2300 - val_mean_directional_accuracy: 0.4586\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3961 - accuracy: 0.2689 - categorical_accuracy: 0.2689 - mean_directional_accuracy: 0.5123 - val_loss: 1.3977 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "44/45 [============================>.] - ETA: 0s - loss: 1.3957 - accuracy: 0.2685 - categorical_accuracy: 0.2685 - mean_directional_accuracy: 0.5119Restoring model weights from the end of the best epoch: 6.\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3957 - accuracy: 0.2682 - categorical_accuracy: 0.2682 - mean_directional_accuracy: 0.5112 - val_loss: 1.3990 - val_accuracy: 0.2289 - val_categorical_accuracy: 0.2289 - val_mean_directional_accuracy: 0.4589\n",
      "Epoch 7: early stopping\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 1.3977 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.397694706916809, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 6 \n",
      "\n",
      "Model time: 0.08373637869954109 minutes\n",
      "\n",
      "Total time: 2.1140877716243267 minutes\n",
      "\n",
      "\n",
      "Model  6  out of  111\n",
      "Model type             RegularizedLinear\n",
      "Hidden layers                          0\n",
      "Hidden units                           0\n",
      "Activation function                 None\n",
      "Dropout                              0.0\n",
      "L1                                 0.001\n",
      "L2                               0.00001\n",
      "Batch size                            64\n",
      "Optimizer                        RMSprop\n",
      "Learning rate                       0.01\n",
      "Name: 1873, dtype: object\n",
      "Linear\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 2s 15ms/step - loss: 1.4881 - accuracy: 0.2980 - categorical_accuracy: 0.2980 - mean_directional_accuracy: 0.5211 - val_loss: 1.4564 - val_accuracy: 0.2809 - val_categorical_accuracy: 0.2809 - val_mean_directional_accuracy: 0.4993\n",
      "Epoch 2/300\n",
      "86/89 [===========================>..] - ETA: 0s - loss: 1.4304 - accuracy: 0.3207 - categorical_accuracy: 0.3207 - mean_directional_accuracy: 0.5409Restoring model weights from the end of the best epoch: 1.\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4302 - accuracy: 0.3216 - categorical_accuracy: 0.3216 - mean_directional_accuracy: 0.5420 - val_loss: 1.4589 - val_accuracy: 0.2507 - val_categorical_accuracy: 0.2507 - val_mean_directional_accuracy: 0.4592\n",
      "Epoch 2: early stopping\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.4564 - accuracy: 0.2809 - categorical_accuracy: 0.2809 - mean_directional_accuracy: 0.4993\n",
      "{'loss': 1.456417202949524, 'accuracy': 0.2809238135814667, 'categorical_accuracy': 0.2809238135814667, 'mean_directional_accuracy': 0.49934759736061096} \n",
      " 1 \n",
      "\n",
      "Model time: 0.06459591910243034 minutes\n",
      "\n",
      "Total time: 2.1787503324449062 minutes\n",
      "\n",
      "\n",
      "Model  7  out of  111\n",
      "Model type             RegularizedLinear\n",
      "Hidden layers                          0\n",
      "Hidden units                           0\n",
      "Activation function                 None\n",
      "Dropout                              0.0\n",
      "L1                                0.0001\n",
      "L2                               0.00001\n",
      "Batch size                            32\n",
      "Optimizer                           Adam\n",
      "Learning rate                      0.001\n",
      "Name: 2141, dtype: object\n",
      "Linear\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 3s 9ms/step - loss: 1.5023 - accuracy: 0.2870 - categorical_accuracy: 0.2870 - mean_directional_accuracy: 0.5155 - val_loss: 1.4306 - val_accuracy: 0.2860 - val_categorical_accuracy: 0.2860 - val_mean_directional_accuracy: 0.5275\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.4019 - accuracy: 0.3058 - categorical_accuracy: 0.3058 - mean_directional_accuracy: 0.5314 - val_loss: 1.4049 - val_accuracy: 0.2848 - val_categorical_accuracy: 0.2848 - val_mean_directional_accuracy: 0.5111\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3714 - accuracy: 0.3179 - categorical_accuracy: 0.3179 - mean_directional_accuracy: 0.5429 - val_loss: 1.3990 - val_accuracy: 0.2851 - val_categorical_accuracy: 0.2851 - val_mean_directional_accuracy: 0.5070\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.3583 - accuracy: 0.3349 - categorical_accuracy: 0.3349 - mean_directional_accuracy: 0.5546 - val_loss: 1.3961 - val_accuracy: 0.2880 - val_categorical_accuracy: 0.2880 - val_mean_directional_accuracy: 0.5048\n",
      "Epoch 5/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3531 - accuracy: 0.3442 - categorical_accuracy: 0.3442 - mean_directional_accuracy: 0.5585 - val_loss: 1.3961 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.4931\n",
      "Epoch 6/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3474 - accuracy: 0.3479 - categorical_accuracy: 0.3479 - mean_directional_accuracy: 0.5685 - val_loss: 1.3931 - val_accuracy: 0.2855 - val_categorical_accuracy: 0.2855 - val_mean_directional_accuracy: 0.4997\n",
      "Epoch 7/300\n",
      "173/178 [============================>.] - ETA: 0s - loss: 1.3439 - accuracy: 0.3557 - categorical_accuracy: 0.3557 - mean_directional_accuracy: 0.5735Restoring model weights from the end of the best epoch: 6.\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3437 - accuracy: 0.3567 - categorical_accuracy: 0.3567 - mean_directional_accuracy: 0.5732 - val_loss: 1.3999 - val_accuracy: 0.2787 - val_categorical_accuracy: 0.2787 - val_mean_directional_accuracy: 0.4961\n",
      "Epoch 7: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.3931 - accuracy: 0.2855 - categorical_accuracy: 0.2855 - mean_directional_accuracy: 0.4997\n",
      "{'loss': 1.393134355545044, 'accuracy': 0.28549060225486755, 'categorical_accuracy': 0.28549060225486755, 'mean_directional_accuracy': 0.49973905086517334} \n",
      " 6 \n",
      "\n",
      "Model time: 0.17526886984705925 minutes\n",
      "\n",
      "Total time: 2.35415218770504 minutes\n",
      "\n",
      "\n",
      "Model  8  out of  111\n",
      "Model type             RegularizedLinear\n",
      "Hidden layers                          0\n",
      "Hidden units                           0\n",
      "Activation function                 None\n",
      "Dropout                              0.0\n",
      "L1                                  10.0\n",
      "L2                                  10.0\n",
      "Batch size                            16\n",
      "Optimizer                           Adam\n",
      "Learning rate                       0.01\n",
      "Name: 604, dtype: object\n",
      "Linear\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 3s 6ms/step - loss: 27.2014 - accuracy: 0.2589 - categorical_accuracy: 0.2589 - mean_directional_accuracy: 0.5044 - val_loss: 9.3989 - val_accuracy: 0.2298 - val_categorical_accuracy: 0.2298 - val_mean_directional_accuracy: 0.4597\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - ETA: 0s - loss: 9.3602 - accuracy: 0.2655 - categorical_accuracy: 0.2655 - mean_directional_accuracy: 0.5046Restoring model weights from the end of the best epoch: 1.\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 9.3602 - accuracy: 0.2655 - categorical_accuracy: 0.2655 - mean_directional_accuracy: 0.5046 - val_loss: 9.6370 - val_accuracy: 0.2276 - val_categorical_accuracy: 0.2276 - val_mean_directional_accuracy: 0.4601\n",
      "Epoch 2: early stopping\n",
      "479/479 [==============================] - 2s 3ms/step - loss: 9.3989 - accuracy: 0.2298 - categorical_accuracy: 0.2298 - mean_directional_accuracy: 0.4597\n",
      "{'loss': 9.398895263671875, 'accuracy': 0.2297755777835846, 'categorical_accuracy': 0.2297755777835846, 'mean_directional_accuracy': 0.4596816301345825} \n",
      " 1 \n",
      "\n",
      "Model time: 0.12274662032723427 minutes\n",
      "\n",
      "Total time: 2.477027527987957 minutes\n",
      "\n",
      "\n",
      "Model  9  out of  111\n",
      "Model type             RegularizedLinear\n",
      "Hidden layers                          0\n",
      "Hidden units                           0\n",
      "Activation function                 None\n",
      "Dropout                              0.0\n",
      "L1                               0.00001\n",
      "L2                                  10.0\n",
      "Batch size                            16\n",
      "Optimizer                           Adam\n",
      "Learning rate                       0.01\n",
      "Name: 2224, dtype: object\n",
      "Linear\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 4s 8ms/step - loss: 2.6298 - accuracy: 0.2720 - categorical_accuracy: 0.2720 - mean_directional_accuracy: 0.5083 - val_loss: 1.4343 - val_accuracy: 0.3031 - val_categorical_accuracy: 0.3031 - val_mean_directional_accuracy: 0.5552\n",
      "Epoch 2/300\n",
      "350/356 [============================>.] - ETA: 0s - loss: 1.4464 - accuracy: 0.2623 - categorical_accuracy: 0.2623 - mean_directional_accuracy: 0.5050Restoring model weights from the end of the best epoch: 1.\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4463 - accuracy: 0.2620 - categorical_accuracy: 0.2620 - mean_directional_accuracy: 0.5049 - val_loss: 1.4487 - val_accuracy: 0.2526 - val_categorical_accuracy: 0.2526 - val_mean_directional_accuracy: 0.4982\n",
      "Epoch 2: early stopping\n",
      "479/479 [==============================] - 1s 3ms/step - loss: 1.4343 - accuracy: 0.3031 - categorical_accuracy: 0.3031 - mean_directional_accuracy: 0.5552\n",
      "{'loss': 1.4342583417892456, 'accuracy': 0.3031054139137268, 'categorical_accuracy': 0.3031054139137268, 'mean_directional_accuracy': 0.5551931262016296} \n",
      " 1 \n",
      "\n",
      "Model time: 0.13027311861515045 minutes\n",
      "\n",
      "Total time: 2.607366666197777 minutes\n",
      "\n",
      "\n",
      "Model  10  out of  111\n",
      "Model type             RegularizedLinear\n",
      "Hidden layers                          0\n",
      "Hidden units                           0\n",
      "Activation function                 None\n",
      "Dropout                              0.0\n",
      "L1                                0.0001\n",
      "L2                               0.00001\n",
      "Batch size                            32\n",
      "Optimizer                        RMSprop\n",
      "Learning rate                       0.01\n",
      "Name: 2137, dtype: object\n",
      "Linear\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 3s 9ms/step - loss: 1.4583 - accuracy: 0.3096 - categorical_accuracy: 0.3096 - mean_directional_accuracy: 0.5406 - val_loss: 1.4659 - val_accuracy: 0.2807 - val_categorical_accuracy: 0.2807 - val_mean_directional_accuracy: 0.5013\n",
      "Epoch 2/300\n",
      "158/178 [=========================>....] - ETA: 0s - loss: 1.4221 - accuracy: 0.3333 - categorical_accuracy: 0.3333 - mean_directional_accuracy: 0.5589Restoring model weights from the end of the best epoch: 1.\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.4253 - accuracy: 0.3293 - categorical_accuracy: 0.3293 - mean_directional_accuracy: 0.5555 - val_loss: 1.4736 - val_accuracy: 0.2762 - val_categorical_accuracy: 0.2762 - val_mean_directional_accuracy: 0.4935\n",
      "Epoch 2: early stopping\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 1.4659 - accuracy: 0.2807 - categorical_accuracy: 0.2807 - mean_directional_accuracy: 0.5013\n",
      "{'loss': 1.4658995866775513, 'accuracy': 0.2806628346443176, 'categorical_accuracy': 0.2806628346443176, 'mean_directional_accuracy': 0.5013048052787781} \n",
      " 1 \n",
      "\n",
      "Model time: 0.07899585738778114 minutes\n",
      "\n",
      "Total time: 2.6864458583295345 minutes\n",
      "\n",
      "\n",
      "Model  11  out of  111\n",
      "Model type             RegularizedLinear\n",
      "Hidden layers                          0\n",
      "Hidden units                           0\n",
      "Activation function                 None\n",
      "Dropout                              0.0\n",
      "L1                                   0.0\n",
      "L2                                   0.1\n",
      "Batch size                            16\n",
      "Optimizer                           Adam\n",
      "Learning rate                     0.0001\n",
      "Name: 123, dtype: object\n",
      "Linear\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 4s 7ms/step - loss: 2.2349 - accuracy: 0.2633 - categorical_accuracy: 0.2633 - mean_directional_accuracy: 0.5111 - val_loss: 2.0597 - val_accuracy: 0.2557 - val_categorical_accuracy: 0.2557 - val_mean_directional_accuracy: 0.5027\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.9789 - accuracy: 0.2701 - categorical_accuracy: 0.2701 - mean_directional_accuracy: 0.5063 - val_loss: 1.8759 - val_accuracy: 0.2666 - val_categorical_accuracy: 0.2666 - val_mean_directional_accuracy: 0.5112\n",
      "Epoch 3/300\n",
      "356/356 [==============================] - 2s 5ms/step - loss: 1.8224 - accuracy: 0.2763 - categorical_accuracy: 0.2763 - mean_directional_accuracy: 0.5072 - val_loss: 1.7564 - val_accuracy: 0.2697 - val_categorical_accuracy: 0.2697 - val_mean_directional_accuracy: 0.5125\n",
      "Epoch 4/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.7158 - accuracy: 0.2828 - categorical_accuracy: 0.2828 - mean_directional_accuracy: 0.5102 - val_loss: 1.6734 - val_accuracy: 0.2756 - val_categorical_accuracy: 0.2756 - val_mean_directional_accuracy: 0.5163\n",
      "Epoch 5/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.6408 - accuracy: 0.2919 - categorical_accuracy: 0.2919 - mean_directional_accuracy: 0.5123 - val_loss: 1.6147 - val_accuracy: 0.2777 - val_categorical_accuracy: 0.2777 - val_mean_directional_accuracy: 0.5149\n",
      "Epoch 6/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.5868 - accuracy: 0.2949 - categorical_accuracy: 0.2949 - mean_directional_accuracy: 0.5133 - val_loss: 1.5715 - val_accuracy: 0.2775 - val_categorical_accuracy: 0.2775 - val_mean_directional_accuracy: 0.5144\n",
      "Epoch 7/300\n",
      "356/356 [==============================] - 2s 5ms/step - loss: 1.5462 - accuracy: 0.2984 - categorical_accuracy: 0.2984 - mean_directional_accuracy: 0.5169 - val_loss: 1.5385 - val_accuracy: 0.2784 - val_categorical_accuracy: 0.2784 - val_mean_directional_accuracy: 0.5164\n",
      "Epoch 8/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.5150 - accuracy: 0.3024 - categorical_accuracy: 0.3024 - mean_directional_accuracy: 0.5202 - val_loss: 1.5131 - val_accuracy: 0.2835 - val_categorical_accuracy: 0.2835 - val_mean_directional_accuracy: 0.5196\n",
      "Epoch 9/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4904 - accuracy: 0.3094 - categorical_accuracy: 0.3094 - mean_directional_accuracy: 0.5251 - val_loss: 1.4923 - val_accuracy: 0.2893 - val_categorical_accuracy: 0.2893 - val_mean_directional_accuracy: 0.5224\n",
      "Epoch 10/300\n",
      "356/356 [==============================] - 2s 5ms/step - loss: 1.4704 - accuracy: 0.3149 - categorical_accuracy: 0.3149 - mean_directional_accuracy: 0.5292 - val_loss: 1.4758 - val_accuracy: 0.2890 - val_categorical_accuracy: 0.2890 - val_mean_directional_accuracy: 0.5254\n",
      "Epoch 11/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4537 - accuracy: 0.3196 - categorical_accuracy: 0.3196 - mean_directional_accuracy: 0.5351 - val_loss: 1.4612 - val_accuracy: 0.2953 - val_categorical_accuracy: 0.2953 - val_mean_directional_accuracy: 0.5305\n",
      "Epoch 12/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4398 - accuracy: 0.3258 - categorical_accuracy: 0.3258 - mean_directional_accuracy: 0.5362 - val_loss: 1.4493 - val_accuracy: 0.2953 - val_categorical_accuracy: 0.2953 - val_mean_directional_accuracy: 0.5309\n",
      "Epoch 13/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4281 - accuracy: 0.3277 - categorical_accuracy: 0.3277 - mean_directional_accuracy: 0.5392 - val_loss: 1.4391 - val_accuracy: 0.2968 - val_categorical_accuracy: 0.2968 - val_mean_directional_accuracy: 0.5329\n",
      "Epoch 14/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4182 - accuracy: 0.3363 - categorical_accuracy: 0.3363 - mean_directional_accuracy: 0.5478 - val_loss: 1.4308 - val_accuracy: 0.2997 - val_categorical_accuracy: 0.2997 - val_mean_directional_accuracy: 0.5339\n",
      "Epoch 15/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4096 - accuracy: 0.3455 - categorical_accuracy: 0.3455 - mean_directional_accuracy: 0.5566 - val_loss: 1.4233 - val_accuracy: 0.3027 - val_categorical_accuracy: 0.3027 - val_mean_directional_accuracy: 0.5389\n",
      "Epoch 16/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4024 - accuracy: 0.3430 - categorical_accuracy: 0.3430 - mean_directional_accuracy: 0.5557 - val_loss: 1.4170 - val_accuracy: 0.3062 - val_categorical_accuracy: 0.3062 - val_mean_directional_accuracy: 0.5420\n",
      "Epoch 17/300\n",
      "356/356 [==============================] - 2s 5ms/step - loss: 1.3962 - accuracy: 0.3498 - categorical_accuracy: 0.3498 - mean_directional_accuracy: 0.5615 - val_loss: 1.4118 - val_accuracy: 0.3034 - val_categorical_accuracy: 0.3034 - val_mean_directional_accuracy: 0.5374\n",
      "Epoch 18/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3906 - accuracy: 0.3462 - categorical_accuracy: 0.3462 - mean_directional_accuracy: 0.5601 - val_loss: 1.4072 - val_accuracy: 0.3018 - val_categorical_accuracy: 0.3018 - val_mean_directional_accuracy: 0.5343\n",
      "Epoch 19/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3862 - accuracy: 0.3518 - categorical_accuracy: 0.3518 - mean_directional_accuracy: 0.5636 - val_loss: 1.4035 - val_accuracy: 0.3013 - val_categorical_accuracy: 0.3013 - val_mean_directional_accuracy: 0.5344\n",
      "Epoch 20/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3822 - accuracy: 0.3493 - categorical_accuracy: 0.3493 - mean_directional_accuracy: 0.5627 - val_loss: 1.4000 - val_accuracy: 0.3040 - val_categorical_accuracy: 0.3040 - val_mean_directional_accuracy: 0.5371\n",
      "Epoch 21/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3785 - accuracy: 0.3481 - categorical_accuracy: 0.3481 - mean_directional_accuracy: 0.5638 - val_loss: 1.3972 - val_accuracy: 0.3081 - val_categorical_accuracy: 0.3081 - val_mean_directional_accuracy: 0.5412\n",
      "Epoch 22/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3757 - accuracy: 0.3507 - categorical_accuracy: 0.3507 - mean_directional_accuracy: 0.5646 - val_loss: 1.3940 - val_accuracy: 0.3044 - val_categorical_accuracy: 0.3044 - val_mean_directional_accuracy: 0.5368\n",
      "Epoch 23/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3728 - accuracy: 0.3512 - categorical_accuracy: 0.3512 - mean_directional_accuracy: 0.5650 - val_loss: 1.3920 - val_accuracy: 0.3052 - val_categorical_accuracy: 0.3052 - val_mean_directional_accuracy: 0.5361\n",
      "Epoch 24/300\n",
      "356/356 [==============================] - 2s 5ms/step - loss: 1.3704 - accuracy: 0.3518 - categorical_accuracy: 0.3518 - mean_directional_accuracy: 0.5650 - val_loss: 1.3903 - val_accuracy: 0.3057 - val_categorical_accuracy: 0.3057 - val_mean_directional_accuracy: 0.5388\n",
      "Epoch 25/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3686 - accuracy: 0.3534 - categorical_accuracy: 0.3534 - mean_directional_accuracy: 0.5671 - val_loss: 1.3876 - val_accuracy: 0.3065 - val_categorical_accuracy: 0.3065 - val_mean_directional_accuracy: 0.5394\n",
      "Epoch 26/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3667 - accuracy: 0.3527 - categorical_accuracy: 0.3527 - mean_directional_accuracy: 0.5650 - val_loss: 1.3865 - val_accuracy: 0.3068 - val_categorical_accuracy: 0.3068 - val_mean_directional_accuracy: 0.5376\n",
      "Epoch 27/300\n",
      "356/356 [==============================] - 2s 5ms/step - loss: 1.3651 - accuracy: 0.3544 - categorical_accuracy: 0.3544 - mean_directional_accuracy: 0.5671 - val_loss: 1.3850 - val_accuracy: 0.3073 - val_categorical_accuracy: 0.3073 - val_mean_directional_accuracy: 0.5378\n",
      "Epoch 28/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3638 - accuracy: 0.3500 - categorical_accuracy: 0.3500 - mean_directional_accuracy: 0.5615 - val_loss: 1.3837 - val_accuracy: 0.3030 - val_categorical_accuracy: 0.3030 - val_mean_directional_accuracy: 0.5347\n",
      "Epoch 29/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3623 - accuracy: 0.3497 - categorical_accuracy: 0.3497 - mean_directional_accuracy: 0.5613 - val_loss: 1.3823 - val_accuracy: 0.3069 - val_categorical_accuracy: 0.3069 - val_mean_directional_accuracy: 0.5373\n",
      "Epoch 30/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3615 - accuracy: 0.3469 - categorical_accuracy: 0.3469 - mean_directional_accuracy: 0.5613 - val_loss: 1.3815 - val_accuracy: 0.3025 - val_categorical_accuracy: 0.3025 - val_mean_directional_accuracy: 0.5352\n",
      "Epoch 31/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3605 - accuracy: 0.3516 - categorical_accuracy: 0.3516 - mean_directional_accuracy: 0.5667 - val_loss: 1.3814 - val_accuracy: 0.3045 - val_categorical_accuracy: 0.3045 - val_mean_directional_accuracy: 0.5354\n",
      "Epoch 32/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3596 - accuracy: 0.3514 - categorical_accuracy: 0.3514 - mean_directional_accuracy: 0.5620 - val_loss: 1.3801 - val_accuracy: 0.3045 - val_categorical_accuracy: 0.3045 - val_mean_directional_accuracy: 0.5356\n",
      "Epoch 33/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3589 - accuracy: 0.3497 - categorical_accuracy: 0.3497 - mean_directional_accuracy: 0.5595 - val_loss: 1.3795 - val_accuracy: 0.3044 - val_categorical_accuracy: 0.3044 - val_mean_directional_accuracy: 0.5351\n",
      "Epoch 34/300\n",
      "345/356 [============================>.] - ETA: 0s - loss: 1.3586 - accuracy: 0.3480 - categorical_accuracy: 0.3480 - mean_directional_accuracy: 0.5618Restoring model weights from the end of the best epoch: 33.\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3581 - accuracy: 0.3500 - categorical_accuracy: 0.3500 - mean_directional_accuracy: 0.5623 - val_loss: 1.3795 - val_accuracy: 0.3051 - val_categorical_accuracy: 0.3051 - val_mean_directional_accuracy: 0.5350\n",
      "Epoch 34: early stopping\n",
      "479/479 [==============================] - 2s 3ms/step - loss: 1.3795 - accuracy: 0.3044 - categorical_accuracy: 0.3044 - mean_directional_accuracy: 0.5351\n",
      "{'loss': 1.379472255706787, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} \n",
      " 33 \n",
      "\n",
      "Model time: 1.2787935361266136 minutes\n",
      "\n",
      "Total time: 3.96530606970191 minutes\n",
      "\n",
      "\n",
      "Model  12  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    2\n",
      "Hidden units               [8, 32]\n",
      "Activation function           tanh\n",
      "Dropout                        0.2\n",
      "L1                            0.01\n",
      "L2                           0.001\n",
      "Batch size                      64\n",
      "Optimizer                  RMSprop\n",
      "Learning rate               0.0001\n",
      "Name: 0, dtype: object\n",
      "NN2Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 3s 14ms/step - loss: 3.3763 - accuracy: 0.2473 - categorical_accuracy: 0.2473 - mean_directional_accuracy: 0.5137 - val_loss: 3.2946 - val_accuracy: 0.2443 - val_categorical_accuracy: 0.2443 - val_mean_directional_accuracy: 0.5227\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 3.2471 - accuracy: 0.2510 - categorical_accuracy: 0.2510 - mean_directional_accuracy: 0.5148 - val_loss: 3.1793 - val_accuracy: 0.2495 - val_categorical_accuracy: 0.2495 - val_mean_directional_accuracy: 0.5265\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 3.1370 - accuracy: 0.2545 - categorical_accuracy: 0.2545 - mean_directional_accuracy: 0.5137 - val_loss: 3.0701 - val_accuracy: 0.2530 - val_categorical_accuracy: 0.2530 - val_mean_directional_accuracy: 0.5279\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 3.0217 - accuracy: 0.2692 - categorical_accuracy: 0.2692 - mean_directional_accuracy: 0.5132 - val_loss: 2.9647 - val_accuracy: 0.2572 - val_categorical_accuracy: 0.2572 - val_mean_directional_accuracy: 0.5275\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 2.9155 - accuracy: 0.2785 - categorical_accuracy: 0.2785 - mean_directional_accuracy: 0.5167 - val_loss: 2.8622 - val_accuracy: 0.2645 - val_categorical_accuracy: 0.2645 - val_mean_directional_accuracy: 0.5281\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 2.8161 - accuracy: 0.2792 - categorical_accuracy: 0.2792 - mean_directional_accuracy: 0.5223 - val_loss: 2.7632 - val_accuracy: 0.2721 - val_categorical_accuracy: 0.2721 - val_mean_directional_accuracy: 0.5329\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.7179 - accuracy: 0.2734 - categorical_accuracy: 0.2734 - mean_directional_accuracy: 0.5137 - val_loss: 2.6682 - val_accuracy: 0.2778 - val_categorical_accuracy: 0.2778 - val_mean_directional_accuracy: 0.5344\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 2.6224 - accuracy: 0.2813 - categorical_accuracy: 0.2813 - mean_directional_accuracy: 0.5176 - val_loss: 2.5757 - val_accuracy: 0.2842 - val_categorical_accuracy: 0.2842 - val_mean_directional_accuracy: 0.5363\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 2.5297 - accuracy: 0.2884 - categorical_accuracy: 0.2884 - mean_directional_accuracy: 0.5144 - val_loss: 2.4888 - val_accuracy: 0.2880 - val_categorical_accuracy: 0.2880 - val_mean_directional_accuracy: 0.5363\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 2.4454 - accuracy: 0.2952 - categorical_accuracy: 0.2952 - mean_directional_accuracy: 0.5293 - val_loss: 2.4058 - val_accuracy: 0.2914 - val_categorical_accuracy: 0.2914 - val_mean_directional_accuracy: 0.5354\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 2.3641 - accuracy: 0.2893 - categorical_accuracy: 0.2893 - mean_directional_accuracy: 0.5186 - val_loss: 2.3266 - val_accuracy: 0.2950 - val_categorical_accuracy: 0.2950 - val_mean_directional_accuracy: 0.5378\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.2865 - accuracy: 0.2936 - categorical_accuracy: 0.2936 - mean_directional_accuracy: 0.5228 - val_loss: 2.2514 - val_accuracy: 0.2957 - val_categorical_accuracy: 0.2957 - val_mean_directional_accuracy: 0.5360\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 2.2130 - accuracy: 0.2936 - categorical_accuracy: 0.2936 - mean_directional_accuracy: 0.5209 - val_loss: 2.1803 - val_accuracy: 0.2950 - val_categorical_accuracy: 0.2950 - val_mean_directional_accuracy: 0.5343\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 2.1422 - accuracy: 0.2991 - categorical_accuracy: 0.2991 - mean_directional_accuracy: 0.5302 - val_loss: 2.1132 - val_accuracy: 0.2954 - val_categorical_accuracy: 0.2954 - val_mean_directional_accuracy: 0.5308\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.0763 - accuracy: 0.3102 - categorical_accuracy: 0.3102 - mean_directional_accuracy: 0.5328 - val_loss: 2.0504 - val_accuracy: 0.2940 - val_categorical_accuracy: 0.2940 - val_mean_directional_accuracy: 0.5290\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 2.0162 - accuracy: 0.3061 - categorical_accuracy: 0.3061 - mean_directional_accuracy: 0.5265 - val_loss: 1.9924 - val_accuracy: 0.2966 - val_categorical_accuracy: 0.2966 - val_mean_directional_accuracy: 0.5291\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9594 - accuracy: 0.3063 - categorical_accuracy: 0.3063 - mean_directional_accuracy: 0.5307 - val_loss: 1.9375 - val_accuracy: 0.2968 - val_categorical_accuracy: 0.2968 - val_mean_directional_accuracy: 0.5273\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9063 - accuracy: 0.3096 - categorical_accuracy: 0.3096 - mean_directional_accuracy: 0.5327 - val_loss: 1.8867 - val_accuracy: 0.2976 - val_categorical_accuracy: 0.2976 - val_mean_directional_accuracy: 0.5270\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.8561 - accuracy: 0.3151 - categorical_accuracy: 0.3151 - mean_directional_accuracy: 0.5378 - val_loss: 1.8401 - val_accuracy: 0.2946 - val_categorical_accuracy: 0.2946 - val_mean_directional_accuracy: 0.5244\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.8132 - accuracy: 0.3102 - categorical_accuracy: 0.3102 - mean_directional_accuracy: 0.5337 - val_loss: 1.7978 - val_accuracy: 0.2941 - val_categorical_accuracy: 0.2941 - val_mean_directional_accuracy: 0.5217\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7724 - accuracy: 0.3117 - categorical_accuracy: 0.3117 - mean_directional_accuracy: 0.5269 - val_loss: 1.7604 - val_accuracy: 0.2929 - val_categorical_accuracy: 0.2929 - val_mean_directional_accuracy: 0.5183\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.7367 - accuracy: 0.3084 - categorical_accuracy: 0.3084 - mean_directional_accuracy: 0.5348 - val_loss: 1.7259 - val_accuracy: 0.2898 - val_categorical_accuracy: 0.2898 - val_mean_directional_accuracy: 0.5151\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.7038 - accuracy: 0.3056 - categorical_accuracy: 0.3056 - mean_directional_accuracy: 0.5328 - val_loss: 1.6938 - val_accuracy: 0.2837 - val_categorical_accuracy: 0.2837 - val_mean_directional_accuracy: 0.5094\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.6738 - accuracy: 0.3035 - categorical_accuracy: 0.3035 - mean_directional_accuracy: 0.5299 - val_loss: 1.6640 - val_accuracy: 0.2835 - val_categorical_accuracy: 0.2835 - val_mean_directional_accuracy: 0.5110\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.6456 - accuracy: 0.3051 - categorical_accuracy: 0.3051 - mean_directional_accuracy: 0.5369 - val_loss: 1.6365 - val_accuracy: 0.2774 - val_categorical_accuracy: 0.2774 - val_mean_directional_accuracy: 0.5050\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.6190 - accuracy: 0.3014 - categorical_accuracy: 0.3014 - mean_directional_accuracy: 0.5339 - val_loss: 1.6113 - val_accuracy: 0.2655 - val_categorical_accuracy: 0.2655 - val_mean_directional_accuracy: 0.4953\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.5955 - accuracy: 0.2975 - categorical_accuracy: 0.2975 - mean_directional_accuracy: 0.5337 - val_loss: 1.5881 - val_accuracy: 0.2507 - val_categorical_accuracy: 0.2507 - val_mean_directional_accuracy: 0.4840\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.5736 - accuracy: 0.2914 - categorical_accuracy: 0.2914 - mean_directional_accuracy: 0.5334 - val_loss: 1.5669 - val_accuracy: 0.2478 - val_categorical_accuracy: 0.2478 - val_mean_directional_accuracy: 0.4811\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.5539 - accuracy: 0.2842 - categorical_accuracy: 0.2842 - mean_directional_accuracy: 0.5255 - val_loss: 1.5477 - val_accuracy: 0.2436 - val_categorical_accuracy: 0.2436 - val_mean_directional_accuracy: 0.4744\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.5355 - accuracy: 0.2833 - categorical_accuracy: 0.2833 - mean_directional_accuracy: 0.5285 - val_loss: 1.5297 - val_accuracy: 0.2377 - val_categorical_accuracy: 0.2377 - val_mean_directional_accuracy: 0.4678\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.5187 - accuracy: 0.2754 - categorical_accuracy: 0.2754 - mean_directional_accuracy: 0.5191 - val_loss: 1.5132 - val_accuracy: 0.2341 - val_categorical_accuracy: 0.2341 - val_mean_directional_accuracy: 0.4639\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.5030 - accuracy: 0.2736 - categorical_accuracy: 0.2736 - mean_directional_accuracy: 0.5183 - val_loss: 1.4979 - val_accuracy: 0.2330 - val_categorical_accuracy: 0.2330 - val_mean_directional_accuracy: 0.4629\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4884 - accuracy: 0.2736 - categorical_accuracy: 0.2736 - mean_directional_accuracy: 0.5167 - val_loss: 1.4840 - val_accuracy: 0.2313 - val_categorical_accuracy: 0.2313 - val_mean_directional_accuracy: 0.4609\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4753 - accuracy: 0.2724 - categorical_accuracy: 0.2724 - mean_directional_accuracy: 0.5167 - val_loss: 1.4712 - val_accuracy: 0.2294 - val_categorical_accuracy: 0.2294 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4633 - accuracy: 0.2692 - categorical_accuracy: 0.2692 - mean_directional_accuracy: 0.5125 - val_loss: 1.4596 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4523 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5107 - val_loss: 1.4492 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4428 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4404 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.4344 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4324 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4269 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4253 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.4201 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4189 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4140 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4134 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4089 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4085 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4043 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4044 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4005 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4009 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3971 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3978 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3944 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3954 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3921 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3932 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3902 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3917 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3888 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3906 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3879 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3898 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3872 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3893 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3868 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3891 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.3866 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3890 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3865 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3889 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3864 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3888 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.3863 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3888 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 57/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3862 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3887 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 58/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3861 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3887 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.3860 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3886 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "85/89 [===========================>..] - ETA: 0s - loss: 1.3860 - accuracy: 0.2682 - categorical_accuracy: 0.2682 - mean_directional_accuracy: 0.5121Restoring model weights from the end of the best epoch: 59.\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3860 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3887 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60: early stopping\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 1.3886 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3886444568634033, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 59 \n",
      "\n",
      "Model time: 0.8116603754460812 minutes\n",
      "\n",
      "Total time: 4.7770672887563705 minutes\n",
      "\n",
      "\n",
      "Model  13  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                 [256]\n",
      "Activation function           relu\n",
      "Dropout                        0.4\n",
      "L1                         0.00001\n",
      "L2                          0.0001\n",
      "Batch size                      64\n",
      "Optimizer                  RMSprop\n",
      "Learning rate                0.001\n",
      "Name: 1, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 3s 19ms/step - loss: 1.5109 - accuracy: 0.2952 - categorical_accuracy: 0.2952 - mean_directional_accuracy: 0.5151 - val_loss: 1.4298 - val_accuracy: 0.2881 - val_categorical_accuracy: 0.2881 - val_mean_directional_accuracy: 0.5164\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 1.4205 - accuracy: 0.3279 - categorical_accuracy: 0.3279 - mean_directional_accuracy: 0.5527 - val_loss: 1.4189 - val_accuracy: 0.3018 - val_categorical_accuracy: 0.3018 - val_mean_directional_accuracy: 0.5313\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.3863 - accuracy: 0.3541 - categorical_accuracy: 0.3541 - mean_directional_accuracy: 0.5648Restoring model weights from the end of the best epoch: 2.\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.3863 - accuracy: 0.3541 - categorical_accuracy: 0.3541 - mean_directional_accuracy: 0.5648 - val_loss: 1.4205 - val_accuracy: 0.3018 - val_categorical_accuracy: 0.3018 - val_mean_directional_accuracy: 0.5277\n",
      "Epoch 3: early stopping\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.4189 - accuracy: 0.3018 - categorical_accuracy: 0.3018 - mean_directional_accuracy: 0.5313\n",
      "{'loss': 1.4189153909683228, 'accuracy': 0.3018006384372711, 'categorical_accuracy': 0.3018006384372711, 'mean_directional_accuracy': 0.531315267086029} \n",
      " 2 \n",
      "\n",
      "Model time: 0.12449329346418381 minutes\n",
      "\n",
      "Total time: 4.901643928140402 minutes\n",
      "\n",
      "\n",
      "Model  14  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                 [128]\n",
      "Activation function         linear\n",
      "Dropout                        0.4\n",
      "L1                             0.1\n",
      "L2                          0.0001\n",
      "Batch size                     128\n",
      "Optimizer                     Adam\n",
      "Learning rate               0.0001\n",
      "Name: 2, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 2s 23ms/step - loss: 156.0277 - accuracy: 0.2281 - categorical_accuracy: 0.2281 - mean_directional_accuracy: 0.5023 - val_loss: 150.6716 - val_accuracy: 0.2266 - val_categorical_accuracy: 0.2266 - val_mean_directional_accuracy: 0.4806\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 146.3814 - accuracy: 0.2251 - categorical_accuracy: 0.2251 - mean_directional_accuracy: 0.4882 - val_loss: 141.2238 - val_accuracy: 0.2277 - val_categorical_accuracy: 0.2277 - val_mean_directional_accuracy: 0.4806\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 137.0531 - accuracy: 0.2225 - categorical_accuracy: 0.2225 - mean_directional_accuracy: 0.4919 - val_loss: 132.1007 - val_accuracy: 0.2294 - val_categorical_accuracy: 0.2294 - val_mean_directional_accuracy: 0.4802\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 128.0435 - accuracy: 0.2345 - categorical_accuracy: 0.2345 - mean_directional_accuracy: 0.5040 - val_loss: 123.2995 - val_accuracy: 0.2289 - val_categorical_accuracy: 0.2289 - val_mean_directional_accuracy: 0.4815\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 119.3851 - accuracy: 0.2227 - categorical_accuracy: 0.2227 - mean_directional_accuracy: 0.4921 - val_loss: 114.8049 - val_accuracy: 0.2285 - val_categorical_accuracy: 0.2285 - val_mean_directional_accuracy: 0.4836\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 111.0097 - accuracy: 0.2269 - categorical_accuracy: 0.2269 - mean_directional_accuracy: 0.4979 - val_loss: 106.6231 - val_accuracy: 0.2294 - val_categorical_accuracy: 0.2294 - val_mean_directional_accuracy: 0.4841\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 102.9469 - accuracy: 0.2353 - categorical_accuracy: 0.2353 - mean_directional_accuracy: 0.5025 - val_loss: 98.7538 - val_accuracy: 0.2282 - val_categorical_accuracy: 0.2282 - val_mean_directional_accuracy: 0.4813\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 95.2114 - accuracy: 0.2315 - categorical_accuracy: 0.2315 - mean_directional_accuracy: 0.4900 - val_loss: 91.1959 - val_accuracy: 0.2299 - val_categorical_accuracy: 0.2299 - val_mean_directional_accuracy: 0.4812\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 87.7738 - accuracy: 0.2306 - categorical_accuracy: 0.2306 - mean_directional_accuracy: 0.4933 - val_loss: 83.9378 - val_accuracy: 0.2304 - val_categorical_accuracy: 0.2304 - val_mean_directional_accuracy: 0.4806\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 80.6661 - accuracy: 0.2306 - categorical_accuracy: 0.2306 - mean_directional_accuracy: 0.5111 - val_loss: 76.9708 - val_accuracy: 0.2306 - val_categorical_accuracy: 0.2306 - val_mean_directional_accuracy: 0.4806\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 73.8231 - accuracy: 0.2297 - categorical_accuracy: 0.2297 - mean_directional_accuracy: 0.5084 - val_loss: 70.3055 - val_accuracy: 0.2312 - val_categorical_accuracy: 0.2312 - val_mean_directional_accuracy: 0.4799\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 67.3114 - accuracy: 0.2260 - categorical_accuracy: 0.2260 - mean_directional_accuracy: 0.4895 - val_loss: 63.9565 - val_accuracy: 0.2313 - val_categorical_accuracy: 0.2313 - val_mean_directional_accuracy: 0.4796\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 61.1035 - accuracy: 0.2255 - categorical_accuracy: 0.2255 - mean_directional_accuracy: 0.4902 - val_loss: 57.9247 - val_accuracy: 0.2308 - val_categorical_accuracy: 0.2308 - val_mean_directional_accuracy: 0.4779\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 55.2032 - accuracy: 0.2359 - categorical_accuracy: 0.2359 - mean_directional_accuracy: 0.5042 - val_loss: 52.1980 - val_accuracy: 0.2306 - val_categorical_accuracy: 0.2306 - val_mean_directional_accuracy: 0.4766\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 49.6278 - accuracy: 0.2360 - categorical_accuracy: 0.2360 - mean_directional_accuracy: 0.5039 - val_loss: 46.8001 - val_accuracy: 0.2319 - val_categorical_accuracy: 0.2319 - val_mean_directional_accuracy: 0.4768\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 44.3671 - accuracy: 0.2295 - categorical_accuracy: 0.2295 - mean_directional_accuracy: 0.4923 - val_loss: 41.7086 - val_accuracy: 0.2323 - val_categorical_accuracy: 0.2323 - val_mean_directional_accuracy: 0.4755\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 39.4209 - accuracy: 0.2301 - categorical_accuracy: 0.2301 - mean_directional_accuracy: 0.4968 - val_loss: 36.9296 - val_accuracy: 0.2315 - val_categorical_accuracy: 0.2315 - val_mean_directional_accuracy: 0.4729\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 34.7910 - accuracy: 0.2297 - categorical_accuracy: 0.2297 - mean_directional_accuracy: 0.4963 - val_loss: 32.4601 - val_accuracy: 0.2302 - val_categorical_accuracy: 0.2302 - val_mean_directional_accuracy: 0.4712\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 30.4577 - accuracy: 0.2292 - categorical_accuracy: 0.2292 - mean_directional_accuracy: 0.4974 - val_loss: 28.2876 - val_accuracy: 0.2296 - val_categorical_accuracy: 0.2296 - val_mean_directional_accuracy: 0.4706\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 26.4276 - accuracy: 0.2334 - categorical_accuracy: 0.2334 - mean_directional_accuracy: 0.4924 - val_loss: 24.4201 - val_accuracy: 0.2291 - val_categorical_accuracy: 0.2291 - val_mean_directional_accuracy: 0.4706\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 22.7026 - accuracy: 0.2346 - categorical_accuracy: 0.2346 - mean_directional_accuracy: 0.5004 - val_loss: 20.8540 - val_accuracy: 0.2277 - val_categorical_accuracy: 0.2277 - val_mean_directional_accuracy: 0.4679\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 19.2769 - accuracy: 0.2348 - categorical_accuracy: 0.2348 - mean_directional_accuracy: 0.4995 - val_loss: 17.5951 - val_accuracy: 0.2252 - val_categorical_accuracy: 0.2252 - val_mean_directional_accuracy: 0.4648\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 16.1690 - accuracy: 0.2306 - categorical_accuracy: 0.2306 - mean_directional_accuracy: 0.4917 - val_loss: 14.6568 - val_accuracy: 0.2272 - val_categorical_accuracy: 0.2272 - val_mean_directional_accuracy: 0.4661\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 13.3804 - accuracy: 0.2401 - categorical_accuracy: 0.2401 - mean_directional_accuracy: 0.5081 - val_loss: 12.0405 - val_accuracy: 0.2264 - val_categorical_accuracy: 0.2264 - val_mean_directional_accuracy: 0.4601\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 10.9137 - accuracy: 0.2360 - categorical_accuracy: 0.2360 - mean_directional_accuracy: 0.5023 - val_loss: 9.7406 - val_accuracy: 0.2263 - val_categorical_accuracy: 0.2263 - val_mean_directional_accuracy: 0.4623\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 8.7704 - accuracy: 0.2366 - categorical_accuracy: 0.2366 - mean_directional_accuracy: 0.5021 - val_loss: 7.7663 - val_accuracy: 0.2268 - val_categorical_accuracy: 0.2268 - val_mean_directional_accuracy: 0.4597\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 6.9453 - accuracy: 0.2378 - categorical_accuracy: 0.2378 - mean_directional_accuracy: 0.5039 - val_loss: 6.1071 - val_accuracy: 0.2248 - val_categorical_accuracy: 0.2248 - val_mean_directional_accuracy: 0.4567\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 5.4317 - accuracy: 0.2424 - categorical_accuracy: 0.2424 - mean_directional_accuracy: 0.5058 - val_loss: 4.7493 - val_accuracy: 0.2265 - val_categorical_accuracy: 0.2265 - val_mean_directional_accuracy: 0.4581\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 4.2169 - accuracy: 0.2459 - categorical_accuracy: 0.2459 - mean_directional_accuracy: 0.5086 - val_loss: 3.6937 - val_accuracy: 0.2257 - val_categorical_accuracy: 0.2257 - val_mean_directional_accuracy: 0.4589\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 3.3072 - accuracy: 0.2513 - categorical_accuracy: 0.2513 - mean_directional_accuracy: 0.5088 - val_loss: 2.9457 - val_accuracy: 0.2246 - val_categorical_accuracy: 0.2246 - val_mean_directional_accuracy: 0.4582\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 2.7033 - accuracy: 0.2566 - categorical_accuracy: 0.2566 - mean_directional_accuracy: 0.5102 - val_loss: 2.4934 - val_accuracy: 0.2246 - val_categorical_accuracy: 0.2246 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 2.3791 - accuracy: 0.2612 - categorical_accuracy: 0.2612 - mean_directional_accuracy: 0.5102 - val_loss: 2.2887 - val_accuracy: 0.2286 - val_categorical_accuracy: 0.2286 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 2.2292 - accuracy: 0.2654 - categorical_accuracy: 0.2654 - mean_directional_accuracy: 0.5105 - val_loss: 2.1748 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 2.1283 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.0847 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 2.0445 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.0068 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.9699 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9355 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 1.9011 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8686 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 1.8361 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8057 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 1.7752 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7470 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.7183 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6922 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.6658 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6421 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 1.6194 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5997 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.5795 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5623 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.5445 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5301 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.5152 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5037 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4915 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4828 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.4725 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4656 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 1.4567 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4515 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.4442 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4406 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.4348 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4326 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4279 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4269 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.4230 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4229 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 1.4194 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4198 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4164 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4170 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.4143 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4154 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.4130 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4146 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.4122 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4140 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 1.4115 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4131 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.4107 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4125 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.4100 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4119 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4095 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4114 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.4092 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4111 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 63/300\n",
      "36/45 [=======================>......] - ETA: 0s - loss: 1.4089 - accuracy: 0.2669 - categorical_accuracy: 0.2669 - mean_directional_accuracy: 0.5078Restoring model weights from the end of the best epoch: 62.\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 1.4089 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4113 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 63: early stopping\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 1.4111 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.4111289978027344, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 62 \n",
      "\n",
      "Model time: 0.7180991545319557 minutes\n",
      "\n",
      "Total time: 5.619809757918119 minutes\n",
      "\n",
      "\n",
      "Model  15  out of  111\n",
      "Model type                FeedForward\n",
      "Hidden layers                       4\n",
      "Hidden units           [16, 2, 4, 64]\n",
      "Activation function              relu\n",
      "Dropout                           0.3\n",
      "L1                             0.0001\n",
      "L2                              100.0\n",
      "Batch size                        128\n",
      "Optimizer                     RMSprop\n",
      "Learning rate                    0.01\n",
      "Name: 3, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 3s 26ms/step - loss: 556.7947 - accuracy: 0.2643 - categorical_accuracy: 0.2643 - mean_directional_accuracy: 0.5035 - val_loss: 55.6753 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 26.3610 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.0048 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 9.8165 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 9.5064 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 4/300\n",
      "44/45 [============================>.] - ETA: 0s - loss: 9.5204 - accuracy: 0.2615 - categorical_accuracy: 0.2615 - mean_directional_accuracy: 0.4991Restoring model weights from the end of the best epoch: 3.\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 9.5204 - accuracy: 0.2617 - categorical_accuracy: 0.2617 - mean_directional_accuracy: 0.4988 - val_loss: 9.5208 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4: early stopping\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 9.5064 - accuracy: 0.2814 - categorical_accuracy: 0.2814 - mean_directional_accuracy: 0.5416\n",
      "{'loss': 9.506393432617188, 'accuracy': 0.28144571185112, 'categorical_accuracy': 0.28144571185112, 'mean_directional_accuracy': 0.5416231751441956} \n",
      " 3 \n",
      "\n",
      "Model time: 0.08811485394835472 minutes\n",
      "\n",
      "Total time: 5.707991316914558 minutes\n",
      "\n",
      "\n",
      "Model  16  out of  111\n",
      "Model type              FeedForward\n",
      "Hidden layers                     3\n",
      "Hidden units           [8, 32, 128]\n",
      "Activation function          linear\n",
      "Dropout                         0.2\n",
      "L1                             10.0\n",
      "L2                            0.001\n",
      "Batch size                      256\n",
      "Optimizer                      Adam\n",
      "Learning rate                 0.001\n",
      "Name: 4, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 4s 47ms/step - loss: 5577.5317 - accuracy: 0.2410 - categorical_accuracy: 0.2410 - mean_directional_accuracy: 0.5114 - val_loss: 4873.7637 - val_accuracy: 0.2300 - val_categorical_accuracy: 0.2300 - val_mean_directional_accuracy: 0.4897\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 4319.4976 - accuracy: 0.2288 - categorical_accuracy: 0.2288 - mean_directional_accuracy: 0.4903 - val_loss: 3703.2297 - val_accuracy: 0.2153 - val_categorical_accuracy: 0.2153 - val_mean_directional_accuracy: 0.4700\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 3224.3162 - accuracy: 0.2422 - categorical_accuracy: 0.2422 - mean_directional_accuracy: 0.5156 - val_loss: 2698.7295 - val_accuracy: 0.2259 - val_categorical_accuracy: 0.2259 - val_mean_directional_accuracy: 0.4742\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 2299.6677 - accuracy: 0.2490 - categorical_accuracy: 0.2490 - mean_directional_accuracy: 0.5005 - val_loss: 1866.5187 - val_accuracy: 0.2444 - val_categorical_accuracy: 0.2444 - val_mean_directional_accuracy: 0.4749\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1546.0205 - accuracy: 0.2650 - categorical_accuracy: 0.2650 - mean_directional_accuracy: 0.5088 - val_loss: 1201.9895 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 954.1780 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5104 - val_loss: 697.0191 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 522.5911 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 351.0980 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 256.1458 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 176.4276 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 146.7046 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 119.6993 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 105.1606 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 89.6266 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 78.6020 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 67.0270 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 58.1184 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 48.5269 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 41.8238 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 34.2596 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 28.7848 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 23.1762 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 19.3329 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.3395 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 12.8525 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 10.8774 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 9.8627 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 9.4443 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 9.2880 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 9.2109 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 9.2173 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 9.1463 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 9.1923 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 9.1132 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 9.1858 - accuracy: 0.2676 - categorical_accuracy: 0.2676 - mean_directional_accuracy: 0.5119Restoring model weights from the end of the best epoch: 20.\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 9.1835 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 9.1324 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21: early stopping\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 9.1132 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 9.113177299499512, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 20 \n",
      "\n",
      "Model time: 0.19166000932455063 minutes\n",
      "\n",
      "Total time: 5.899751354008913 minutes\n",
      "\n",
      "\n",
      "Model  17  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    3\n",
      "Hidden units             [2, 2, 2]\n",
      "Activation function           relu\n",
      "Dropout                        0.8\n",
      "L1                            10.0\n",
      "L2                           100.0\n",
      "Batch size                      32\n",
      "Optimizer                  RMSprop\n",
      "Learning rate               0.0001\n",
      "Name: 5, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 4s 11ms/step - loss: 1204.3387 - accuracy: 0.2669 - categorical_accuracy: 0.2669 - mean_directional_accuracy: 0.5116 - val_loss: 1109.5157 - val_accuracy: 0.2516 - val_categorical_accuracy: 0.2516 - val_mean_directional_accuracy: 0.5086\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1024.4105 - accuracy: 0.2684 - categorical_accuracy: 0.2684 - mean_directional_accuracy: 0.5111 - val_loss: 942.2744 - val_accuracy: 0.2483 - val_categorical_accuracy: 0.2483 - val_mean_directional_accuracy: 0.5014\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 868.9484 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5104 - val_loss: 798.6614 - val_accuracy: 0.2454 - val_categorical_accuracy: 0.2454 - val_mean_directional_accuracy: 0.4932\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 737.1211 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5095 - val_loss: 678.5967 - val_accuracy: 0.2364 - val_categorical_accuracy: 0.2364 - val_mean_directional_accuracy: 0.4823\n",
      "Epoch 5/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 628.0295 - accuracy: 0.2661 - categorical_accuracy: 0.2661 - mean_directional_accuracy: 0.5083 - val_loss: 580.0759 - val_accuracy: 0.2326 - val_categorical_accuracy: 0.2326 - val_mean_directional_accuracy: 0.4712\n",
      "Epoch 6/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 538.6600 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5097 - val_loss: 499.7737 - val_accuracy: 0.2273 - val_categorical_accuracy: 0.2273 - val_mean_directional_accuracy: 0.4582\n",
      "Epoch 7/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 466.4632 - accuracy: 0.2684 - categorical_accuracy: 0.2684 - mean_directional_accuracy: 0.5109 - val_loss: 435.2258 - val_accuracy: 0.2283 - val_categorical_accuracy: 0.2283 - val_mean_directional_accuracy: 0.4580\n",
      "Epoch 8/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 408.8208 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5104 - val_loss: 384.4109 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 364.2767 - accuracy: 0.2680 - categorical_accuracy: 0.2680 - mean_directional_accuracy: 0.5109 - val_loss: 345.8206 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 331.1235 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5102 - val_loss: 317.6680 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 306.9236 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 296.8079 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 287.2454 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 277.7131 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 268.5591 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 259.4392 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 250.6938 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 241.9869 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 233.6497 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 225.3555 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 217.4502 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 209.6303 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 202.1683 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 194.7486 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 187.6630 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 180.6239 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 173.9745 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 167.3854 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 161.1012 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 154.8625 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 148.9230 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 143.0332 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 137.4712 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 131.9965 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 126.8025 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 121.6567 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 116.8084 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 112.0761 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 25/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 107.6583 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 103.3161 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 99.1879 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 95.0897 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 91.1809 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 87.3047 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 28/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 83.6154 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 79.9614 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 29/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 76.5024 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 73.1188 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 30/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 69.9335 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 66.7786 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 31/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 63.7814 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 60.8171 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 32/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 58.0077 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 55.2335 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 33/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 52.6233 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 50.0885 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 47.7200 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 45.3812 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 43.1692 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 40.9891 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 38.9337 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 36.9120 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 35.0795 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 33.2850 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 31.6522 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 30.0543 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 28.5327 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 27.0312 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 25.6039 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 24.1977 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 22.8645 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 21.5533 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 20.3141 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 19.0982 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 17.9532 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.8323 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 15.7812 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.7554 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 13.7982 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.8674 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 12.0039 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 11.1680 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 10.4514 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 9.7799 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 9.1549 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 8.5495 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 7.9911 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 7.4825 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 7.0518 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 6.6331 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 6.2338 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.8468 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 5.4789 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.1237 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 4.7871 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.4636 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 4.1584 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.8666 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 3.5927 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.3326 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 3.0900 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.8615 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 57/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 2.6503 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.4534 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 58/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 2.2734 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.1081 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.9593 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8255 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.7078 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6055 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.5667 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5651 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "169/178 [===========================>..] - ETA: 0s - loss: 1.5622 - accuracy: 0.2679 - categorical_accuracy: 0.2679 - mean_directional_accuracy: 0.5113Restoring model weights from the end of the best epoch: 61.\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.5622 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5652 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62: early stopping\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 1.5651 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.5651195049285889, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 61 \n",
      "\n",
      "Model time: 1.363599307835102 minutes\n",
      "\n",
      "Total time: 7.263417329639196 minutes\n",
      "\n",
      "\n",
      "Model  18  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                   [4]\n",
      "Activation function        sigmoid\n",
      "Dropout                        0.3\n",
      "L1                           100.0\n",
      "L2                         0.00001\n",
      "Batch size                      16\n",
      "Optimizer                  RMSprop\n",
      "Learning rate                 0.01\n",
      "Name: 6, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 4s 8ms/step - loss: 452.8406 - accuracy: 0.2604 - categorical_accuracy: 0.2604 - mean_directional_accuracy: 0.5040 - val_loss: 342.5846 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "344/356 [===========================>..] - ETA: 0s - loss: 339.1946 - accuracy: 0.2640 - categorical_accuracy: 0.2640 - mean_directional_accuracy: 0.5007Restoring model weights from the end of the best epoch: 1.\n",
      "356/356 [==============================] - 2s 5ms/step - loss: 339.2248 - accuracy: 0.2629 - categorical_accuracy: 0.2629 - mean_directional_accuracy: 0.4998 - val_loss: 343.4258 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2: early stopping\n",
      "479/479 [==============================] - 2s 3ms/step - loss: 342.5846 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 342.5846252441406, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 1 \n",
      "\n",
      "Model time: 0.14136002957820892 minutes\n",
      "\n",
      "Total time: 7.40486067160964 minutes\n",
      "\n",
      "\n",
      "Model  19  out of  111\n",
      "Model type                     FeedForward\n",
      "Hidden layers                            5\n",
      "Hidden units           [32, 1, 16, 256, 8]\n",
      "Activation function                   relu\n",
      "Dropout                                0.3\n",
      "L1                                     0.0\n",
      "L2                                     0.0\n",
      "Batch size                              32\n",
      "Optimizer                             Adam\n",
      "Learning rate                         0.01\n",
      "Name: 7, dtype: object\n",
      "NN5Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 4s 13ms/step - loss: 1.3868 - accuracy: 0.2597 - categorical_accuracy: 0.2597 - mean_directional_accuracy: 0.5037 - val_loss: 1.3886 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.3864 - accuracy: 0.2631 - categorical_accuracy: 0.2631 - mean_directional_accuracy: 0.5070 - val_loss: 1.3878 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3862 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.5070 - val_loss: 1.3874 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.3858 - accuracy: 0.2680 - categorical_accuracy: 0.2680 - mean_directional_accuracy: 0.5102 - val_loss: 1.3872 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 5/300\n",
      "175/178 [============================>.] - ETA: 0s - loss: 1.3862 - accuracy: 0.2618 - categorical_accuracy: 0.2618 - mean_directional_accuracy: 0.5034Restoring model weights from the end of the best epoch: 4.\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.3863 - accuracy: 0.2606 - categorical_accuracy: 0.2606 - mean_directional_accuracy: 0.5018 - val_loss: 1.3902 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.3872 - accuracy: 0.2814 - categorical_accuracy: 0.2814 - mean_directional_accuracy: 0.5416\n",
      "{'loss': 1.3872252702713013, 'accuracy': 0.28144571185112, 'categorical_accuracy': 0.28144571185112, 'mean_directional_accuracy': 0.5416231751441956} \n",
      " 4 \n",
      "\n",
      "Model time: 0.18946129828691483 minutes\n",
      "\n",
      "Total time: 7.594405300915241 minutes\n",
      "\n",
      "\n",
      "Model  20  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                 [256]\n",
      "Activation function           relu\n",
      "Dropout                        0.7\n",
      "L1                         0.00001\n",
      "L2                             0.1\n",
      "Batch size                     256\n",
      "Optimizer                     Adam\n",
      "Learning rate                0.001\n",
      "Name: 8, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 2s 47ms/step - loss: 18.2818 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5019 - val_loss: 13.3808 - val_accuracy: 0.2657 - val_categorical_accuracy: 0.2657 - val_mean_directional_accuracy: 0.4902\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 10.5562 - accuracy: 0.2770 - categorical_accuracy: 0.2770 - mean_directional_accuracy: 0.5072 - val_loss: 7.7058 - val_accuracy: 0.2767 - val_categorical_accuracy: 0.2767 - val_mean_directional_accuracy: 0.5030\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 6.1030 - accuracy: 0.2864 - categorical_accuracy: 0.2864 - mean_directional_accuracy: 0.5151 - val_loss: 4.5641 - val_accuracy: 0.2749 - val_categorical_accuracy: 0.2749 - val_mean_directional_accuracy: 0.4974\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 3.7072 - accuracy: 0.3066 - categorical_accuracy: 0.3066 - mean_directional_accuracy: 0.5262 - val_loss: 2.9253 - val_accuracy: 0.2809 - val_categorical_accuracy: 0.2809 - val_mean_directional_accuracy: 0.4966\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 2.4913 - accuracy: 0.3145 - categorical_accuracy: 0.3145 - mean_directional_accuracy: 0.5341 - val_loss: 2.1086 - val_accuracy: 0.2882 - val_categorical_accuracy: 0.2882 - val_mean_directional_accuracy: 0.5070\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 1.8936 - accuracy: 0.3223 - categorical_accuracy: 0.3223 - mean_directional_accuracy: 0.5321 - val_loss: 1.7214 - val_accuracy: 0.2791 - val_categorical_accuracy: 0.2791 - val_mean_directional_accuracy: 0.4937\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 1.6173 - accuracy: 0.3217 - categorical_accuracy: 0.3217 - mean_directional_accuracy: 0.5351 - val_loss: 1.5434 - val_accuracy: 0.2762 - val_categorical_accuracy: 0.2762 - val_mean_directional_accuracy: 0.4909\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.4914 - accuracy: 0.3291 - categorical_accuracy: 0.3291 - mean_directional_accuracy: 0.5392 - val_loss: 1.4629 - val_accuracy: 0.2691 - val_categorical_accuracy: 0.2691 - val_mean_directional_accuracy: 0.4846\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.4349 - accuracy: 0.3138 - categorical_accuracy: 0.3138 - mean_directional_accuracy: 0.5323 - val_loss: 1.4268 - val_accuracy: 0.2625 - val_categorical_accuracy: 0.2625 - val_mean_directional_accuracy: 0.4757\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.4104 - accuracy: 0.3047 - categorical_accuracy: 0.3047 - mean_directional_accuracy: 0.5286 - val_loss: 1.4094 - val_accuracy: 0.2646 - val_categorical_accuracy: 0.2646 - val_mean_directional_accuracy: 0.4804\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3978 - accuracy: 0.2928 - categorical_accuracy: 0.2928 - mean_directional_accuracy: 0.5241 - val_loss: 1.4009 - val_accuracy: 0.2465 - val_categorical_accuracy: 0.2465 - val_mean_directional_accuracy: 0.4759\n",
      "Epoch 12/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3920 - accuracy: 0.2849 - categorical_accuracy: 0.2849 - mean_directional_accuracy: 0.5295 - val_loss: 1.3962 - val_accuracy: 0.2426 - val_categorical_accuracy: 0.2426 - val_mean_directional_accuracy: 0.4779\n",
      "Epoch 13/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3889 - accuracy: 0.2836 - categorical_accuracy: 0.2836 - mean_directional_accuracy: 0.5297 - val_loss: 1.3937 - val_accuracy: 0.2380 - val_categorical_accuracy: 0.2380 - val_mean_directional_accuracy: 0.4717\n",
      "Epoch 14/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3870 - accuracy: 0.2754 - categorical_accuracy: 0.2754 - mean_directional_accuracy: 0.5220 - val_loss: 1.3928 - val_accuracy: 0.2354 - val_categorical_accuracy: 0.2354 - val_mean_directional_accuracy: 0.4679\n",
      "Epoch 15/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3856 - accuracy: 0.2773 - categorical_accuracy: 0.2773 - mean_directional_accuracy: 0.5221 - val_loss: 1.3916 - val_accuracy: 0.2376 - val_categorical_accuracy: 0.2376 - val_mean_directional_accuracy: 0.4727\n",
      "Epoch 16/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3849 - accuracy: 0.2778 - categorical_accuracy: 0.2778 - mean_directional_accuracy: 0.5239 - val_loss: 1.3913 - val_accuracy: 0.2347 - val_categorical_accuracy: 0.2347 - val_mean_directional_accuracy: 0.4667\n",
      "Epoch 17/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 1.3845 - accuracy: 0.2743 - categorical_accuracy: 0.2743 - mean_directional_accuracy: 0.5225 - val_loss: 1.3911 - val_accuracy: 0.2319 - val_categorical_accuracy: 0.2319 - val_mean_directional_accuracy: 0.4635\n",
      "Epoch 18/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.3846 - accuracy: 0.2766 - categorical_accuracy: 0.2766 - mean_directional_accuracy: 0.5211 - val_loss: 1.3906 - val_accuracy: 0.2332 - val_categorical_accuracy: 0.2332 - val_mean_directional_accuracy: 0.4645\n",
      "Epoch 19/300\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3844 - accuracy: 0.2766 - categorical_accuracy: 0.2766 - mean_directional_accuracy: 0.5220Restoring model weights from the end of the best epoch: 18.\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.3843 - accuracy: 0.2768 - categorical_accuracy: 0.2768 - mean_directional_accuracy: 0.5227 - val_loss: 1.3906 - val_accuracy: 0.2333 - val_categorical_accuracy: 0.2333 - val_mean_directional_accuracy: 0.4661\n",
      "Epoch 19: early stopping\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3906 - accuracy: 0.2332 - categorical_accuracy: 0.2332 - mean_directional_accuracy: 0.4645\n",
      "{'loss': 1.3905813694000244, 'accuracy': 0.23316806554794312, 'categorical_accuracy': 0.23316806554794312, 'mean_directional_accuracy': 0.46450939774513245} \n",
      " 18 \n",
      "\n",
      "Model time: 0.1663726456463337 minutes\n",
      "\n",
      "Total time: 7.760861299932003 minutes\n",
      "\n",
      "\n",
      "Model  21  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                 [256]\n",
      "Activation function        sigmoid\n",
      "Dropout                        0.8\n",
      "L1                            0.01\n",
      "L2                            10.0\n",
      "Batch size                     128\n",
      "Optimizer                  RMSprop\n",
      "Learning rate                0.001\n",
      "Name: 9, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 6s 36ms/step - loss: 1047.7155 - accuracy: 0.2543 - categorical_accuracy: 0.2543 - mean_directional_accuracy: 0.5037 - val_loss: 445.2852 - val_accuracy: 0.2820 - val_categorical_accuracy: 0.2820 - val_mean_directional_accuracy: 0.5424\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 205.8423 - accuracy: 0.2557 - categorical_accuracy: 0.2557 - mean_directional_accuracy: 0.5126 - val_loss: 50.0371 - val_accuracy: 0.2300 - val_categorical_accuracy: 0.2300 - val_mean_directional_accuracy: 0.4597\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 15.1270 - accuracy: 0.2473 - categorical_accuracy: 0.2473 - mean_directional_accuracy: 0.5007 - val_loss: 2.0940 - val_accuracy: 0.2602 - val_categorical_accuracy: 0.2602 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 1.7688 - accuracy: 0.2578 - categorical_accuracy: 0.2578 - mean_directional_accuracy: 0.5049 - val_loss: 1.7253 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "44/45 [============================>.] - ETA: 0s - loss: 1.7254 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.5092Restoring model weights from the end of the best epoch: 4.\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 1.7255 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.5088 - val_loss: 1.7337 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5: early stopping\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 1.7253 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.7253073453903198, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 4 \n",
      "\n",
      "Model time: 0.21285811811685562 minutes\n",
      "\n",
      "Total time: 7.9738079980015755 minutes\n",
      "\n",
      "\n",
      "Model  22  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                  [16]\n",
      "Activation function           relu\n",
      "Dropout                        0.3\n",
      "L1                          0.0001\n",
      "L2                          0.0001\n",
      "Batch size                     128\n",
      "Optimizer                  RMSprop\n",
      "Learning rate               0.0001\n",
      "Name: 10, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 2s 15ms/step - loss: 1.6841 - accuracy: 0.2594 - categorical_accuracy: 0.2594 - mean_directional_accuracy: 0.5111 - val_loss: 1.5900 - val_accuracy: 0.2474 - val_categorical_accuracy: 0.2474 - val_mean_directional_accuracy: 0.4783\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 1.6380 - accuracy: 0.2608 - categorical_accuracy: 0.2608 - mean_directional_accuracy: 0.5170 - val_loss: 1.5580 - val_accuracy: 0.2493 - val_categorical_accuracy: 0.2493 - val_mean_directional_accuracy: 0.4798\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.6039 - accuracy: 0.2619 - categorical_accuracy: 0.2619 - mean_directional_accuracy: 0.5140 - val_loss: 1.5327 - val_accuracy: 0.2504 - val_categorical_accuracy: 0.2504 - val_mean_directional_accuracy: 0.4817\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.5656 - accuracy: 0.2689 - categorical_accuracy: 0.2689 - mean_directional_accuracy: 0.5123 - val_loss: 1.5135 - val_accuracy: 0.2505 - val_categorical_accuracy: 0.2505 - val_mean_directional_accuracy: 0.4821\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.5419 - accuracy: 0.2580 - categorical_accuracy: 0.2580 - mean_directional_accuracy: 0.5067 - val_loss: 1.4985 - val_accuracy: 0.2540 - val_categorical_accuracy: 0.2540 - val_mean_directional_accuracy: 0.4840\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.5259 - accuracy: 0.2680 - categorical_accuracy: 0.2680 - mean_directional_accuracy: 0.5128 - val_loss: 1.4852 - val_accuracy: 0.2547 - val_categorical_accuracy: 0.2547 - val_mean_directional_accuracy: 0.4854\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.5157 - accuracy: 0.2754 - categorical_accuracy: 0.2754 - mean_directional_accuracy: 0.5246 - val_loss: 1.4744 - val_accuracy: 0.2593 - val_categorical_accuracy: 0.2593 - val_mean_directional_accuracy: 0.4894\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4928 - accuracy: 0.2754 - categorical_accuracy: 0.2754 - mean_directional_accuracy: 0.5119 - val_loss: 1.4650 - val_accuracy: 0.2595 - val_categorical_accuracy: 0.2595 - val_mean_directional_accuracy: 0.4924\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4866 - accuracy: 0.2729 - categorical_accuracy: 0.2729 - mean_directional_accuracy: 0.5190 - val_loss: 1.4579 - val_accuracy: 0.2591 - val_categorical_accuracy: 0.2591 - val_mean_directional_accuracy: 0.4952\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4803 - accuracy: 0.2694 - categorical_accuracy: 0.2694 - mean_directional_accuracy: 0.5163 - val_loss: 1.4522 - val_accuracy: 0.2606 - val_categorical_accuracy: 0.2606 - val_mean_directional_accuracy: 0.4960\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4740 - accuracy: 0.2736 - categorical_accuracy: 0.2736 - mean_directional_accuracy: 0.5183 - val_loss: 1.4470 - val_accuracy: 0.2599 - val_categorical_accuracy: 0.2599 - val_mean_directional_accuracy: 0.4957\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 1.4623 - accuracy: 0.2782 - categorical_accuracy: 0.2782 - mean_directional_accuracy: 0.5202 - val_loss: 1.4424 - val_accuracy: 0.2630 - val_categorical_accuracy: 0.2630 - val_mean_directional_accuracy: 0.4977\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 1.4508 - accuracy: 0.2805 - categorical_accuracy: 0.2805 - mean_directional_accuracy: 0.5285 - val_loss: 1.4390 - val_accuracy: 0.2641 - val_categorical_accuracy: 0.2641 - val_mean_directional_accuracy: 0.5004\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.4556 - accuracy: 0.2741 - categorical_accuracy: 0.2741 - mean_directional_accuracy: 0.5104 - val_loss: 1.4356 - val_accuracy: 0.2654 - val_categorical_accuracy: 0.2654 - val_mean_directional_accuracy: 0.5023\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 1.4506 - accuracy: 0.2780 - categorical_accuracy: 0.2780 - mean_directional_accuracy: 0.5167 - val_loss: 1.4324 - val_accuracy: 0.2671 - val_categorical_accuracy: 0.2671 - val_mean_directional_accuracy: 0.5021\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 1.4399 - accuracy: 0.2885 - categorical_accuracy: 0.2885 - mean_directional_accuracy: 0.5241 - val_loss: 1.4292 - val_accuracy: 0.2702 - val_categorical_accuracy: 0.2702 - val_mean_directional_accuracy: 0.5031\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 1.4352 - accuracy: 0.2973 - categorical_accuracy: 0.2973 - mean_directional_accuracy: 0.5274 - val_loss: 1.4266 - val_accuracy: 0.2737 - val_categorical_accuracy: 0.2737 - val_mean_directional_accuracy: 0.5069\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 1.4366 - accuracy: 0.2824 - categorical_accuracy: 0.2824 - mean_directional_accuracy: 0.5225 - val_loss: 1.4246 - val_accuracy: 0.2747 - val_categorical_accuracy: 0.2747 - val_mean_directional_accuracy: 0.5078\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 1.4299 - accuracy: 0.2928 - categorical_accuracy: 0.2928 - mean_directional_accuracy: 0.5297 - val_loss: 1.4227 - val_accuracy: 0.2766 - val_categorical_accuracy: 0.2766 - val_mean_directional_accuracy: 0.5089\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.4243 - accuracy: 0.2943 - categorical_accuracy: 0.2943 - mean_directional_accuracy: 0.5307 - val_loss: 1.4211 - val_accuracy: 0.2767 - val_categorical_accuracy: 0.2767 - val_mean_directional_accuracy: 0.5103\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.4218 - accuracy: 0.2935 - categorical_accuracy: 0.2935 - mean_directional_accuracy: 0.5285 - val_loss: 1.4194 - val_accuracy: 0.2778 - val_categorical_accuracy: 0.2778 - val_mean_directional_accuracy: 0.5106\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4237 - accuracy: 0.2884 - categorical_accuracy: 0.2884 - mean_directional_accuracy: 0.5218 - val_loss: 1.4174 - val_accuracy: 0.2773 - val_categorical_accuracy: 0.2773 - val_mean_directional_accuracy: 0.5104\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 1.4166 - accuracy: 0.2915 - categorical_accuracy: 0.2915 - mean_directional_accuracy: 0.5207 - val_loss: 1.4161 - val_accuracy: 0.2795 - val_categorical_accuracy: 0.2795 - val_mean_directional_accuracy: 0.5121\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4138 - accuracy: 0.2945 - categorical_accuracy: 0.2945 - mean_directional_accuracy: 0.5258 - val_loss: 1.4151 - val_accuracy: 0.2781 - val_categorical_accuracy: 0.2781 - val_mean_directional_accuracy: 0.5133\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4129 - accuracy: 0.3008 - categorical_accuracy: 0.3008 - mean_directional_accuracy: 0.5360 - val_loss: 1.4138 - val_accuracy: 0.2784 - val_categorical_accuracy: 0.2784 - val_mean_directional_accuracy: 0.5127\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4109 - accuracy: 0.2979 - categorical_accuracy: 0.2979 - mean_directional_accuracy: 0.5353 - val_loss: 1.4124 - val_accuracy: 0.2807 - val_categorical_accuracy: 0.2807 - val_mean_directional_accuracy: 0.5146\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.4096 - accuracy: 0.3054 - categorical_accuracy: 0.3054 - mean_directional_accuracy: 0.5341 - val_loss: 1.4114 - val_accuracy: 0.2817 - val_categorical_accuracy: 0.2817 - val_mean_directional_accuracy: 0.5167\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4077 - accuracy: 0.2949 - categorical_accuracy: 0.2949 - mean_directional_accuracy: 0.5248 - val_loss: 1.4104 - val_accuracy: 0.2829 - val_categorical_accuracy: 0.2829 - val_mean_directional_accuracy: 0.5180\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4060 - accuracy: 0.2963 - categorical_accuracy: 0.2963 - mean_directional_accuracy: 0.5297 - val_loss: 1.4095 - val_accuracy: 0.2833 - val_categorical_accuracy: 0.2833 - val_mean_directional_accuracy: 0.5183\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.4059 - accuracy: 0.3003 - categorical_accuracy: 0.3003 - mean_directional_accuracy: 0.5281 - val_loss: 1.4087 - val_accuracy: 0.2856 - val_categorical_accuracy: 0.2856 - val_mean_directional_accuracy: 0.5204\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3984 - accuracy: 0.3109 - categorical_accuracy: 0.3109 - mean_directional_accuracy: 0.5427 - val_loss: 1.4080 - val_accuracy: 0.2860 - val_categorical_accuracy: 0.2860 - val_mean_directional_accuracy: 0.5202\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4013 - accuracy: 0.3026 - categorical_accuracy: 0.3026 - mean_directional_accuracy: 0.5378 - val_loss: 1.4073 - val_accuracy: 0.2878 - val_categorical_accuracy: 0.2878 - val_mean_directional_accuracy: 0.5215\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.3975 - accuracy: 0.3094 - categorical_accuracy: 0.3094 - mean_directional_accuracy: 0.5409 - val_loss: 1.4065 - val_accuracy: 0.2893 - val_categorical_accuracy: 0.2893 - val_mean_directional_accuracy: 0.5231\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3920 - accuracy: 0.3077 - categorical_accuracy: 0.3077 - mean_directional_accuracy: 0.5369 - val_loss: 1.4058 - val_accuracy: 0.2884 - val_categorical_accuracy: 0.2884 - val_mean_directional_accuracy: 0.5221\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3909 - accuracy: 0.3079 - categorical_accuracy: 0.3079 - mean_directional_accuracy: 0.5418 - val_loss: 1.4053 - val_accuracy: 0.2876 - val_categorical_accuracy: 0.2876 - val_mean_directional_accuracy: 0.5210\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3890 - accuracy: 0.3158 - categorical_accuracy: 0.3158 - mean_directional_accuracy: 0.5402 - val_loss: 1.4048 - val_accuracy: 0.2889 - val_categorical_accuracy: 0.2889 - val_mean_directional_accuracy: 0.5222\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3949 - accuracy: 0.3073 - categorical_accuracy: 0.3073 - mean_directional_accuracy: 0.5346 - val_loss: 1.4040 - val_accuracy: 0.2895 - val_categorical_accuracy: 0.2895 - val_mean_directional_accuracy: 0.5234\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3897 - accuracy: 0.3107 - categorical_accuracy: 0.3107 - mean_directional_accuracy: 0.5339 - val_loss: 1.4033 - val_accuracy: 0.2891 - val_categorical_accuracy: 0.2891 - val_mean_directional_accuracy: 0.5237\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3879 - accuracy: 0.3126 - categorical_accuracy: 0.3126 - mean_directional_accuracy: 0.5418 - val_loss: 1.4030 - val_accuracy: 0.2899 - val_categorical_accuracy: 0.2899 - val_mean_directional_accuracy: 0.5243\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.3885 - accuracy: 0.3163 - categorical_accuracy: 0.3163 - mean_directional_accuracy: 0.5455 - val_loss: 1.4024 - val_accuracy: 0.2894 - val_categorical_accuracy: 0.2894 - val_mean_directional_accuracy: 0.5248\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 1.3880 - accuracy: 0.3100 - categorical_accuracy: 0.3100 - mean_directional_accuracy: 0.5323 - val_loss: 1.4020 - val_accuracy: 0.2889 - val_categorical_accuracy: 0.2889 - val_mean_directional_accuracy: 0.5239\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3814 - accuracy: 0.3130 - categorical_accuracy: 0.3130 - mean_directional_accuracy: 0.5376 - val_loss: 1.4015 - val_accuracy: 0.2897 - val_categorical_accuracy: 0.2897 - val_mean_directional_accuracy: 0.5252\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3855 - accuracy: 0.3151 - categorical_accuracy: 0.3151 - mean_directional_accuracy: 0.5511 - val_loss: 1.4011 - val_accuracy: 0.2891 - val_categorical_accuracy: 0.2891 - val_mean_directional_accuracy: 0.5236\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3844 - accuracy: 0.3140 - categorical_accuracy: 0.3140 - mean_directional_accuracy: 0.5427 - val_loss: 1.4007 - val_accuracy: 0.2903 - val_categorical_accuracy: 0.2903 - val_mean_directional_accuracy: 0.5257\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.3837 - accuracy: 0.3084 - categorical_accuracy: 0.3084 - mean_directional_accuracy: 0.5430 - val_loss: 1.4001 - val_accuracy: 0.2916 - val_categorical_accuracy: 0.2916 - val_mean_directional_accuracy: 0.5273\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3791 - accuracy: 0.3258 - categorical_accuracy: 0.3258 - mean_directional_accuracy: 0.5490 - val_loss: 1.3998 - val_accuracy: 0.2921 - val_categorical_accuracy: 0.2921 - val_mean_directional_accuracy: 0.5271\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3796 - accuracy: 0.3223 - categorical_accuracy: 0.3223 - mean_directional_accuracy: 0.5499 - val_loss: 1.3994 - val_accuracy: 0.2932 - val_categorical_accuracy: 0.2932 - val_mean_directional_accuracy: 0.5283\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.3769 - accuracy: 0.3349 - categorical_accuracy: 0.3349 - mean_directional_accuracy: 0.5548 - val_loss: 1.3991 - val_accuracy: 0.2934 - val_categorical_accuracy: 0.2934 - val_mean_directional_accuracy: 0.5275\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3770 - accuracy: 0.3265 - categorical_accuracy: 0.3265 - mean_directional_accuracy: 0.5486 - val_loss: 1.3985 - val_accuracy: 0.2925 - val_categorical_accuracy: 0.2925 - val_mean_directional_accuracy: 0.5258\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3777 - accuracy: 0.3207 - categorical_accuracy: 0.3207 - mean_directional_accuracy: 0.5393 - val_loss: 1.3981 - val_accuracy: 0.2925 - val_categorical_accuracy: 0.2925 - val_mean_directional_accuracy: 0.5265\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3799 - accuracy: 0.3221 - categorical_accuracy: 0.3221 - mean_directional_accuracy: 0.5436 - val_loss: 1.3977 - val_accuracy: 0.2925 - val_categorical_accuracy: 0.2925 - val_mean_directional_accuracy: 0.5265\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.3764 - accuracy: 0.3242 - categorical_accuracy: 0.3242 - mean_directional_accuracy: 0.5446 - val_loss: 1.3974 - val_accuracy: 0.2927 - val_categorical_accuracy: 0.2927 - val_mean_directional_accuracy: 0.5257\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3771 - accuracy: 0.3291 - categorical_accuracy: 0.3291 - mean_directional_accuracy: 0.5574 - val_loss: 1.3971 - val_accuracy: 0.2931 - val_categorical_accuracy: 0.2931 - val_mean_directional_accuracy: 0.5261\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3745 - accuracy: 0.3286 - categorical_accuracy: 0.3286 - mean_directional_accuracy: 0.5522 - val_loss: 1.3969 - val_accuracy: 0.2940 - val_categorical_accuracy: 0.2940 - val_mean_directional_accuracy: 0.5262\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 1.3724 - accuracy: 0.3168 - categorical_accuracy: 0.3168 - mean_directional_accuracy: 0.5364 - val_loss: 1.3966 - val_accuracy: 0.2953 - val_categorical_accuracy: 0.2953 - val_mean_directional_accuracy: 0.5278\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.3750 - accuracy: 0.3279 - categorical_accuracy: 0.3279 - mean_directional_accuracy: 0.5527 - val_loss: 1.3965 - val_accuracy: 0.2940 - val_categorical_accuracy: 0.2940 - val_mean_directional_accuracy: 0.5264\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.3739 - accuracy: 0.3289 - categorical_accuracy: 0.3289 - mean_directional_accuracy: 0.5527 - val_loss: 1.3962 - val_accuracy: 0.2942 - val_categorical_accuracy: 0.2942 - val_mean_directional_accuracy: 0.5274\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3707 - accuracy: 0.3263 - categorical_accuracy: 0.3263 - mean_directional_accuracy: 0.5485 - val_loss: 1.3960 - val_accuracy: 0.2948 - val_categorical_accuracy: 0.2948 - val_mean_directional_accuracy: 0.5271\n",
      "Epoch 59/300\n",
      "43/45 [===========================>..] - ETA: 0s - loss: 1.3699 - accuracy: 0.3299 - categorical_accuracy: 0.3299 - mean_directional_accuracy: 0.5511Restoring model weights from the end of the best epoch: 58.\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3703 - accuracy: 0.3295 - categorical_accuracy: 0.3295 - mean_directional_accuracy: 0.5506 - val_loss: 1.3961 - val_accuracy: 0.2937 - val_categorical_accuracy: 0.2937 - val_mean_directional_accuracy: 0.5249\n",
      "Epoch 59: early stopping\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 1.3960 - accuracy: 0.2948 - categorical_accuracy: 0.2948 - mean_directional_accuracy: 0.5271\n",
      "{'loss': 1.3959685564041138, 'accuracy': 0.29475468397140503, 'categorical_accuracy': 0.29475468397140503, 'mean_directional_accuracy': 0.5271399021148682} \n",
      " 58 \n",
      "\n",
      "Model time: 0.4631696827709675 minutes\n",
      "\n",
      "Total time: 8.437061000615358 minutes\n",
      "\n",
      "\n",
      "Model  23  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                  [32]\n",
      "Activation function           tanh\n",
      "Dropout                        0.0\n",
      "L1                             0.0\n",
      "L2                             1.0\n",
      "Batch size                      32\n",
      "Optimizer                     Adam\n",
      "Learning rate                0.001\n",
      "Name: 11, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 3s 11ms/step - loss: 19.6963 - accuracy: 0.2882 - categorical_accuracy: 0.2882 - mean_directional_accuracy: 0.5272 - val_loss: 3.9890 - val_accuracy: 0.2843 - val_categorical_accuracy: 0.2843 - val_mean_directional_accuracy: 0.5313\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 2.3715 - accuracy: 0.2921 - categorical_accuracy: 0.2921 - mean_directional_accuracy: 0.5400 - val_loss: 1.6940 - val_accuracy: 0.2373 - val_categorical_accuracy: 0.2373 - val_mean_directional_accuracy: 0.4721\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.5255 - accuracy: 0.2685 - categorical_accuracy: 0.2685 - mean_directional_accuracy: 0.5125 - val_loss: 1.4346 - val_accuracy: 0.2290 - val_categorical_accuracy: 0.2290 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 1.4049 - accuracy: 0.2671 - categorical_accuracy: 0.2671 - mean_directional_accuracy: 0.5104 - val_loss: 1.3933 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3872 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3883 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3853 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3880 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "170/178 [===========================>..] - ETA: 0s - loss: 1.3851 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5112Restoring model weights from the end of the best epoch: 6.\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3883 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.3880 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3879966735839844, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 6 \n",
      "\n",
      "Model time: 0.1942790076136589 minutes\n",
      "\n",
      "Total time: 8.631506618112326 minutes\n",
      "\n",
      "\n",
      "Model  24  out of  111\n",
      "Model type                      FeedForward\n",
      "Hidden layers                             5\n",
      "Hidden units           [64, 1, 128, 128, 1]\n",
      "Activation function                    relu\n",
      "Dropout                                 0.9\n",
      "L1                                     0.01\n",
      "L2                                      0.1\n",
      "Batch size                               32\n",
      "Optimizer                              Adam\n",
      "Learning rate                        0.0001\n",
      "Name: 12, dtype: object\n",
      "NN5Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 4s 13ms/step - loss: 40.3173 - accuracy: 0.2696 - categorical_accuracy: 0.2696 - mean_directional_accuracy: 0.5095 - val_loss: 34.9515 - val_accuracy: 0.2289 - val_categorical_accuracy: 0.2289 - val_mean_directional_accuracy: 0.4626\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 30.8102 - accuracy: 0.2657 - categorical_accuracy: 0.2657 - mean_directional_accuracy: 0.5060 - val_loss: 26.6287 - val_accuracy: 0.2279 - val_categorical_accuracy: 0.2279 - val_mean_directional_accuracy: 0.4572\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 23.4383 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5098 - val_loss: 20.3317 - val_accuracy: 0.2282 - val_categorical_accuracy: 0.2282 - val_mean_directional_accuracy: 0.4581\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 2s 12ms/step - loss: 17.8257 - accuracy: 0.2661 - categorical_accuracy: 0.2661 - mean_directional_accuracy: 0.5090 - val_loss: 15.4692 - val_accuracy: 0.2283 - val_categorical_accuracy: 0.2283 - val_mean_directional_accuracy: 0.4581\n",
      "Epoch 5/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 13.5667 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5100 - val_loss: 11.7269 - val_accuracy: 0.2286 - val_categorical_accuracy: 0.2286 - val_mean_directional_accuracy: 0.4588\n",
      "Epoch 6/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 10.2668 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5097 - val_loss: 8.8907 - val_accuracy: 0.2281 - val_categorical_accuracy: 0.2281 - val_mean_directional_accuracy: 0.4582\n",
      "Epoch 7/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 7.8119 - accuracy: 0.2669 - categorical_accuracy: 0.2669 - mean_directional_accuracy: 0.5102 - val_loss: 6.7793 - val_accuracy: 0.2281 - val_categorical_accuracy: 0.2281 - val_mean_directional_accuracy: 0.4581\n",
      "Epoch 8/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 5.9824 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5100 - val_loss: 5.2425 - val_accuracy: 0.2283 - val_categorical_accuracy: 0.2283 - val_mean_directional_accuracy: 0.4580\n",
      "Epoch 9/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 4.7010 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5102 - val_loss: 4.1701 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 3.7954 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.4645 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 3.2325 - accuracy: 0.2671 - categorical_accuracy: 0.2671 - mean_directional_accuracy: 0.5102 - val_loss: 3.0374 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 2.9226 - accuracy: 0.2680 - categorical_accuracy: 0.2680 - mean_directional_accuracy: 0.5107 - val_loss: 2.8139 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 2.7333 - accuracy: 0.2671 - categorical_accuracy: 0.2671 - mean_directional_accuracy: 0.5102 - val_loss: 2.6568 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.5852 - accuracy: 0.2671 - categorical_accuracy: 0.2671 - mean_directional_accuracy: 0.5105 - val_loss: 2.5199 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 2.4644 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5107 - val_loss: 2.4019 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 2.3487 - accuracy: 0.2680 - categorical_accuracy: 0.2680 - mean_directional_accuracy: 0.5111 - val_loss: 2.3001 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.2554 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5098 - val_loss: 2.2068 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 2.1632 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5102 - val_loss: 2.1250 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 2.0861 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5104 - val_loss: 2.0515 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 2.0168 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5109 - val_loss: 1.9858 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.9528 - accuracy: 0.2680 - categorical_accuracy: 0.2680 - mean_directional_accuracy: 0.5112 - val_loss: 1.9260 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.8966 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5107 - val_loss: 1.8722 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8452 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5107 - val_loss: 1.8237 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.7989 - accuracy: 0.2669 - categorical_accuracy: 0.2669 - mean_directional_accuracy: 0.5102 - val_loss: 1.7799 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 25/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7567 - accuracy: 0.2671 - categorical_accuracy: 0.2671 - mean_directional_accuracy: 0.5102 - val_loss: 1.7393 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.7181 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7026 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6831 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5104 - val_loss: 1.6693 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 28/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6510 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6385 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 29/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6216 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6106 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 30/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.5951 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5855 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 31/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.5711 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5627 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 32/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.5494 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5104 - val_loss: 1.5421 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 33/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.5298 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5235 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.5122 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5070 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.4968 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4929 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.4836 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4805 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.4718 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4693 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.4612 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4593 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "178/178 [==============================] - 2s 12ms/step - loss: 1.4519 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4506 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.4437 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4430 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.4365 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4363 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.4302 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4304 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.4247 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4253 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.4199 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4208 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.4156 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4169 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.4120 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4136 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.4090 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4107 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.4063 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4082 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.4040 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4061 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.4019 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4042 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.4001 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4025 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.3985 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4010 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.3972 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3997 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.3960 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3987 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.3950 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3977 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.3940 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3968 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 57/300\n",
      "178/178 [==============================] - 2s 13ms/step - loss: 1.3932 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3960 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 58/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.3924 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3953 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.3917 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3947 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.3911 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3942 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.3907 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3937 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.3902 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3933 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 63/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.3899 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3931 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 64/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.3897 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3928 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 65/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.3894 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3926 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 66/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.3892 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3924 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 67/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.3891 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3923 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 68/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.3889 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3921 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 69/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.3888 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3920 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 70/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.3887 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3919 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 71/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.3886 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3919 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 72/300\n",
      "170/178 [===========================>..] - ETA: 0s - loss: 1.3885 - accuracy: 0.2688 - categorical_accuracy: 0.2688 - mean_directional_accuracy: 0.5138Restoring model weights from the end of the best epoch: 71.\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.3885 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3919 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 72: early stopping\n",
      "240/240 [==============================] - 1s 5ms/step - loss: 1.3919 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.391851782798767, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 71 \n",
      "\n",
      "Model time: 2.2098984345793724 minutes\n",
      "\n",
      "Total time: 10.841520864516497 minutes\n",
      "\n",
      "\n",
      "Model  25  out of  111\n",
      "Model type                FeedForward\n",
      "Hidden layers                       3\n",
      "Hidden units           [32, 256, 256]\n",
      "Activation function              relu\n",
      "Dropout                           0.5\n",
      "L1                                0.0\n",
      "L2                              0.001\n",
      "Batch size                        128\n",
      "Optimizer                     RMSprop\n",
      "Learning rate                   0.001\n",
      "Name: 13, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 3s 36ms/step - loss: 1.7824 - accuracy: 0.2671 - categorical_accuracy: 0.2671 - mean_directional_accuracy: 0.5107 - val_loss: 1.7113 - val_accuracy: 0.2401 - val_categorical_accuracy: 0.2401 - val_mean_directional_accuracy: 0.4596\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 1.7082 - accuracy: 0.2736 - categorical_accuracy: 0.2736 - mean_directional_accuracy: 0.5098 - val_loss: 1.6669 - val_accuracy: 0.2718 - val_categorical_accuracy: 0.2718 - val_mean_directional_accuracy: 0.5039\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 1s 24ms/step - loss: 1.6475 - accuracy: 0.2924 - categorical_accuracy: 0.2924 - mean_directional_accuracy: 0.5207 - val_loss: 1.6263 - val_accuracy: 0.2659 - val_categorical_accuracy: 0.2659 - val_mean_directional_accuracy: 0.4846\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 1.6072 - accuracy: 0.2900 - categorical_accuracy: 0.2900 - mean_directional_accuracy: 0.5195 - val_loss: 1.5887 - val_accuracy: 0.2787 - val_categorical_accuracy: 0.2787 - val_mean_directional_accuracy: 0.5089\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 1.5698 - accuracy: 0.2959 - categorical_accuracy: 0.2959 - mean_directional_accuracy: 0.5221 - val_loss: 1.5553 - val_accuracy: 0.2876 - val_categorical_accuracy: 0.2876 - val_mean_directional_accuracy: 0.5155\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 1.5342 - accuracy: 0.3017 - categorical_accuracy: 0.3017 - mean_directional_accuracy: 0.5255 - val_loss: 1.5262 - val_accuracy: 0.2914 - val_categorical_accuracy: 0.2914 - val_mean_directional_accuracy: 0.5151\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 1.5058 - accuracy: 0.3031 - categorical_accuracy: 0.3031 - mean_directional_accuracy: 0.5311 - val_loss: 1.5055 - val_accuracy: 0.2856 - val_categorical_accuracy: 0.2856 - val_mean_directional_accuracy: 0.5104\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 1.4826 - accuracy: 0.3235 - categorical_accuracy: 0.3235 - mean_directional_accuracy: 0.5413 - val_loss: 1.4828 - val_accuracy: 0.2888 - val_categorical_accuracy: 0.2888 - val_mean_directional_accuracy: 0.5130\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.4616 - accuracy: 0.3226 - categorical_accuracy: 0.3226 - mean_directional_accuracy: 0.5511 - val_loss: 1.4670 - val_accuracy: 0.2954 - val_categorical_accuracy: 0.2954 - val_mean_directional_accuracy: 0.5213\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4435 - accuracy: 0.3272 - categorical_accuracy: 0.3272 - mean_directional_accuracy: 0.5430 - val_loss: 1.4516 - val_accuracy: 0.3004 - val_categorical_accuracy: 0.3004 - val_mean_directional_accuracy: 0.5342\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 1.4318 - accuracy: 0.3295 - categorical_accuracy: 0.3295 - mean_directional_accuracy: 0.5518 - val_loss: 1.4455 - val_accuracy: 0.2946 - val_categorical_accuracy: 0.2946 - val_mean_directional_accuracy: 0.5174\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 1s 24ms/step - loss: 1.4206 - accuracy: 0.3337 - categorical_accuracy: 0.3337 - mean_directional_accuracy: 0.5543 - val_loss: 1.4338 - val_accuracy: 0.3041 - val_categorical_accuracy: 0.3041 - val_mean_directional_accuracy: 0.5358\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 1.4107 - accuracy: 0.3328 - categorical_accuracy: 0.3328 - mean_directional_accuracy: 0.5532 - val_loss: 1.4248 - val_accuracy: 0.3036 - val_categorical_accuracy: 0.3036 - val_mean_directional_accuracy: 0.5384\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 1.4036 - accuracy: 0.3383 - categorical_accuracy: 0.3383 - mean_directional_accuracy: 0.5685 - val_loss: 1.4196 - val_accuracy: 0.3027 - val_categorical_accuracy: 0.3027 - val_mean_directional_accuracy: 0.5296\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 1.3983 - accuracy: 0.3395 - categorical_accuracy: 0.3395 - mean_directional_accuracy: 0.5546 - val_loss: 1.4181 - val_accuracy: 0.2988 - val_categorical_accuracy: 0.2988 - val_mean_directional_accuracy: 0.5292\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 1.3879 - accuracy: 0.3500 - categorical_accuracy: 0.3500 - mean_directional_accuracy: 0.5688 - val_loss: 1.4098 - val_accuracy: 0.3043 - val_categorical_accuracy: 0.3043 - val_mean_directional_accuracy: 0.5322\n",
      "Epoch 17/300\n",
      "42/45 [===========================>..] - ETA: 0s - loss: 1.3855 - accuracy: 0.3454 - categorical_accuracy: 0.3454 - mean_directional_accuracy: 0.5647Restoring model weights from the end of the best epoch: 16.\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.3855 - accuracy: 0.3449 - categorical_accuracy: 0.3449 - mean_directional_accuracy: 0.5646 - val_loss: 1.4136 - val_accuracy: 0.2920 - val_categorical_accuracy: 0.2920 - val_mean_directional_accuracy: 0.5205\n",
      "Epoch 17: early stopping\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4098 - accuracy: 0.3043 - categorical_accuracy: 0.3043 - mean_directional_accuracy: 0.5322\n",
      "{'loss': 1.4097672700881958, 'accuracy': 0.30427974462509155, 'categorical_accuracy': 0.30427974462509155, 'mean_directional_accuracy': 0.5322285890579224} \n",
      " 16 \n",
      "\n",
      "Model time: 0.305221538990736 minutes\n",
      "\n",
      "Total time: 11.146975819021463 minutes\n",
      "\n",
      "\n",
      "Model  26  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                   [4]\n",
      "Activation function           tanh\n",
      "Dropout                        0.2\n",
      "L1                             0.1\n",
      "L2                             0.0\n",
      "Batch size                     256\n",
      "Optimizer                  RMSprop\n",
      "Learning rate                 0.01\n",
      "Name: 14, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 2s 36ms/step - loss: 3.1939 - accuracy: 0.2599 - categorical_accuracy: 0.2599 - mean_directional_accuracy: 0.5063 - val_loss: 2.0438 - val_accuracy: 0.2183 - val_categorical_accuracy: 0.2183 - val_mean_directional_accuracy: 0.4478\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.9155 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5098 - val_loss: 1.8242 - val_accuracy: 0.2342 - val_categorical_accuracy: 0.2342 - val_mean_directional_accuracy: 0.4656\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.7820 - accuracy: 0.2619 - categorical_accuracy: 0.2619 - mean_directional_accuracy: 0.5065 - val_loss: 1.7604 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7320 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7142 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.7276 - accuracy: 0.2667 - categorical_accuracy: 0.2667 - mean_directional_accuracy: 0.5101Restoring model weights from the end of the best epoch: 4.\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.7273 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7482 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5: early stopping\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.7142 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.7141814231872559, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 4 \n",
      "\n",
      "Model time: 0.07862572744488716 minutes\n",
      "\n",
      "Total time: 11.22566819563508 minutes\n",
      "\n",
      "\n",
      "Model  27  out of  111\n",
      "Model type                      FeedForward\n",
      "Hidden layers                             5\n",
      "Hidden units           [128, 1, 8, 128, 64]\n",
      "Activation function                    relu\n",
      "Dropout                                 0.3\n",
      "L1                                   0.0001\n",
      "L2                                    0.001\n",
      "Batch size                               16\n",
      "Optimizer                           RMSprop\n",
      "Learning rate                         0.001\n",
      "Name: 15, dtype: object\n",
      "NN5Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 7s 11ms/step - loss: 1.5161 - accuracy: 0.2634 - categorical_accuracy: 0.2634 - mean_directional_accuracy: 0.4972 - val_loss: 1.4230 - val_accuracy: 0.2486 - val_categorical_accuracy: 0.2486 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.4092 - accuracy: 0.2817 - categorical_accuracy: 0.2817 - mean_directional_accuracy: 0.5065 - val_loss: 1.4022 - val_accuracy: 0.2538 - val_categorical_accuracy: 0.2538 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3975 - accuracy: 0.2812 - categorical_accuracy: 0.2812 - mean_directional_accuracy: 0.5114 - val_loss: 1.4013 - val_accuracy: 0.2363 - val_categorical_accuracy: 0.2363 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3918 - accuracy: 0.2806 - categorical_accuracy: 0.2806 - mean_directional_accuracy: 0.5090 - val_loss: 1.3942 - val_accuracy: 0.2523 - val_categorical_accuracy: 0.2523 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3934 - accuracy: 0.2798 - categorical_accuracy: 0.2798 - mean_directional_accuracy: 0.5039 - val_loss: 1.3913 - val_accuracy: 0.2505 - val_categorical_accuracy: 0.2505 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "354/356 [============================>.] - ETA: 0s - loss: 1.3882 - accuracy: 0.2876 - categorical_accuracy: 0.2876 - mean_directional_accuracy: 0.5104Restoring model weights from the end of the best epoch: 5.\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3882 - accuracy: 0.2871 - categorical_accuracy: 0.2871 - mean_directional_accuracy: 0.5100 - val_loss: 1.3924 - val_accuracy: 0.3015 - val_categorical_accuracy: 0.3015 - val_mean_directional_accuracy: 0.5393\n",
      "Epoch 6: early stopping\n",
      "479/479 [==============================] - 2s 4ms/step - loss: 1.3913 - accuracy: 0.2505 - categorical_accuracy: 0.2505 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3913357257843018, 'accuracy': 0.2505219280719757, 'categorical_accuracy': 0.2505219280719757, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 5 \n",
      "\n",
      "Model time: 0.41292043775320053 minutes\n",
      "\n",
      "Total time: 11.638676255941391 minutes\n",
      "\n",
      "\n",
      "Model  28  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                 [256]\n",
      "Activation function         linear\n",
      "Dropout                        0.1\n",
      "L1                            10.0\n",
      "L2                          0.0001\n",
      "Batch size                     256\n",
      "Optimizer                     Adam\n",
      "Learning rate               0.0001\n",
      "Name: 16, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 2s 47ms/step - loss: 26201.6660 - accuracy: 0.2413 - categorical_accuracy: 0.2413 - mean_directional_accuracy: 0.5070 - val_loss: 25660.5898 - val_accuracy: 0.2475 - val_categorical_accuracy: 0.2475 - val_mean_directional_accuracy: 0.5120\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 25201.6406 - accuracy: 0.2388 - categorical_accuracy: 0.2388 - mean_directional_accuracy: 0.5025 - val_loss: 24670.2090 - val_accuracy: 0.2492 - val_categorical_accuracy: 0.2492 - val_mean_directional_accuracy: 0.5128\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 24219.6953 - accuracy: 0.2371 - categorical_accuracy: 0.2371 - mean_directional_accuracy: 0.5070 - val_loss: 23698.5352 - val_accuracy: 0.2491 - val_categorical_accuracy: 0.2491 - val_mean_directional_accuracy: 0.5125\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 23257.1953 - accuracy: 0.2396 - categorical_accuracy: 0.2396 - mean_directional_accuracy: 0.5077 - val_loss: 22746.7617 - val_accuracy: 0.2486 - val_categorical_accuracy: 0.2486 - val_mean_directional_accuracy: 0.5116\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 22314.4902 - accuracy: 0.2411 - categorical_accuracy: 0.2411 - mean_directional_accuracy: 0.5105 - val_loss: 21814.4102 - val_accuracy: 0.2487 - val_categorical_accuracy: 0.2487 - val_mean_directional_accuracy: 0.5116\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 21391.1621 - accuracy: 0.2390 - categorical_accuracy: 0.2390 - mean_directional_accuracy: 0.5091 - val_loss: 20901.5820 - val_accuracy: 0.2467 - val_categorical_accuracy: 0.2467 - val_mean_directional_accuracy: 0.5111\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 20487.2871 - accuracy: 0.2404 - categorical_accuracy: 0.2404 - mean_directional_accuracy: 0.5076 - val_loss: 20008.2852 - val_accuracy: 0.2461 - val_categorical_accuracy: 0.2461 - val_mean_directional_accuracy: 0.5108\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 19602.7656 - accuracy: 0.2429 - categorical_accuracy: 0.2429 - mean_directional_accuracy: 0.5130 - val_loss: 19133.7305 - val_accuracy: 0.2458 - val_categorical_accuracy: 0.2458 - val_mean_directional_accuracy: 0.5103\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 18737.1309 - accuracy: 0.2399 - categorical_accuracy: 0.2399 - mean_directional_accuracy: 0.5105 - val_loss: 18278.8828 - val_accuracy: 0.2454 - val_categorical_accuracy: 0.2454 - val_mean_directional_accuracy: 0.5098\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 17891.6152 - accuracy: 0.2448 - categorical_accuracy: 0.2448 - mean_directional_accuracy: 0.5146 - val_loss: 17443.8652 - val_accuracy: 0.2452 - val_categorical_accuracy: 0.2452 - val_mean_directional_accuracy: 0.5082\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 17065.2461 - accuracy: 0.2438 - categorical_accuracy: 0.2438 - mean_directional_accuracy: 0.5135 - val_loss: 16627.8418 - val_accuracy: 0.2452 - val_categorical_accuracy: 0.2452 - val_mean_directional_accuracy: 0.5064\n",
      "Epoch 12/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 16258.0361 - accuracy: 0.2482 - categorical_accuracy: 0.2482 - mean_directional_accuracy: 0.5170 - val_loss: 15831.1104 - val_accuracy: 0.2474 - val_categorical_accuracy: 0.2474 - val_mean_directional_accuracy: 0.5090\n",
      "Epoch 13/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 15470.6895 - accuracy: 0.2476 - categorical_accuracy: 0.2476 - mean_directional_accuracy: 0.5190 - val_loss: 15054.3740 - val_accuracy: 0.2484 - val_categorical_accuracy: 0.2484 - val_mean_directional_accuracy: 0.5090\n",
      "Epoch 14/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 14702.9072 - accuracy: 0.2506 - categorical_accuracy: 0.2506 - mean_directional_accuracy: 0.5232 - val_loss: 14297.1387 - val_accuracy: 0.2462 - val_categorical_accuracy: 0.2462 - val_mean_directional_accuracy: 0.5067\n",
      "Epoch 15/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 13954.7852 - accuracy: 0.2462 - categorical_accuracy: 0.2462 - mean_directional_accuracy: 0.5128 - val_loss: 13559.5215 - val_accuracy: 0.2465 - val_categorical_accuracy: 0.2465 - val_mean_directional_accuracy: 0.5057\n",
      "Epoch 16/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 13225.7520 - accuracy: 0.2464 - categorical_accuracy: 0.2464 - mean_directional_accuracy: 0.5198 - val_loss: 12840.6201 - val_accuracy: 0.2487 - val_categorical_accuracy: 0.2487 - val_mean_directional_accuracy: 0.5095\n",
      "Epoch 17/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 12516.0605 - accuracy: 0.2515 - categorical_accuracy: 0.2515 - mean_directional_accuracy: 0.5204 - val_loss: 12141.7422 - val_accuracy: 0.2484 - val_categorical_accuracy: 0.2484 - val_mean_directional_accuracy: 0.5081\n",
      "Epoch 18/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 11826.1846 - accuracy: 0.2555 - categorical_accuracy: 0.2555 - mean_directional_accuracy: 0.5214 - val_loss: 11462.1533 - val_accuracy: 0.2500 - val_categorical_accuracy: 0.2500 - val_mean_directional_accuracy: 0.5084\n",
      "Epoch 19/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 11155.2500 - accuracy: 0.2540 - categorical_accuracy: 0.2540 - mean_directional_accuracy: 0.5205 - val_loss: 10801.1582 - val_accuracy: 0.2495 - val_categorical_accuracy: 0.2495 - val_mean_directional_accuracy: 0.5067\n",
      "Epoch 20/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 10503.0166 - accuracy: 0.2478 - categorical_accuracy: 0.2478 - mean_directional_accuracy: 0.5190 - val_loss: 10159.5410 - val_accuracy: 0.2487 - val_categorical_accuracy: 0.2487 - val_mean_directional_accuracy: 0.5068\n",
      "Epoch 21/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 9870.5342 - accuracy: 0.2545 - categorical_accuracy: 0.2545 - mean_directional_accuracy: 0.5232 - val_loss: 9537.7393 - val_accuracy: 0.2479 - val_categorical_accuracy: 0.2479 - val_mean_directional_accuracy: 0.5033\n",
      "Epoch 22/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 9257.9219 - accuracy: 0.2559 - categorical_accuracy: 0.2559 - mean_directional_accuracy: 0.5242 - val_loss: 8935.6982 - val_accuracy: 0.2484 - val_categorical_accuracy: 0.2484 - val_mean_directional_accuracy: 0.5023\n",
      "Epoch 23/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 8664.6064 - accuracy: 0.2501 - categorical_accuracy: 0.2501 - mean_directional_accuracy: 0.5220 - val_loss: 8352.4463 - val_accuracy: 0.2490 - val_categorical_accuracy: 0.2490 - val_mean_directional_accuracy: 0.5022\n",
      "Epoch 24/300\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 8090.5234 - accuracy: 0.2601 - categorical_accuracy: 0.2601 - mean_directional_accuracy: 0.5246 - val_loss: 7788.9702 - val_accuracy: 0.2460 - val_categorical_accuracy: 0.2460 - val_mean_directional_accuracy: 0.5001\n",
      "Epoch 25/300\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 7536.1411 - accuracy: 0.2492 - categorical_accuracy: 0.2492 - mean_directional_accuracy: 0.5126 - val_loss: 7245.1724 - val_accuracy: 0.2461 - val_categorical_accuracy: 0.2461 - val_mean_directional_accuracy: 0.4988\n",
      "Epoch 26/300\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 7001.3223 - accuracy: 0.2520 - categorical_accuracy: 0.2520 - mean_directional_accuracy: 0.5213 - val_loss: 6720.9478 - val_accuracy: 0.2457 - val_categorical_accuracy: 0.2457 - val_mean_directional_accuracy: 0.4982\n",
      "Epoch 27/300\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 6486.3110 - accuracy: 0.2532 - categorical_accuracy: 0.2532 - mean_directional_accuracy: 0.5195 - val_loss: 6216.7124 - val_accuracy: 0.2447 - val_categorical_accuracy: 0.2447 - val_mean_directional_accuracy: 0.4965\n",
      "Epoch 28/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 5991.3032 - accuracy: 0.2534 - categorical_accuracy: 0.2534 - mean_directional_accuracy: 0.5151 - val_loss: 5732.6841 - val_accuracy: 0.2443 - val_categorical_accuracy: 0.2443 - val_mean_directional_accuracy: 0.4947\n",
      "Epoch 29/300\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 5516.5952 - accuracy: 0.2569 - categorical_accuracy: 0.2569 - mean_directional_accuracy: 0.5190 - val_loss: 5268.5410 - val_accuracy: 0.2433 - val_categorical_accuracy: 0.2433 - val_mean_directional_accuracy: 0.4945\n",
      "Epoch 30/300\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 5061.3501 - accuracy: 0.2596 - categorical_accuracy: 0.2596 - mean_directional_accuracy: 0.5223 - val_loss: 4823.6211 - val_accuracy: 0.2405 - val_categorical_accuracy: 0.2405 - val_mean_directional_accuracy: 0.4903\n",
      "Epoch 31/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 4625.3799 - accuracy: 0.2613 - categorical_accuracy: 0.2613 - mean_directional_accuracy: 0.5191 - val_loss: 4398.4937 - val_accuracy: 0.2403 - val_categorical_accuracy: 0.2403 - val_mean_directional_accuracy: 0.4888\n",
      "Epoch 32/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 4210.0420 - accuracy: 0.2608 - categorical_accuracy: 0.2608 - mean_directional_accuracy: 0.5174 - val_loss: 3994.3975 - val_accuracy: 0.2400 - val_categorical_accuracy: 0.2400 - val_mean_directional_accuracy: 0.4893\n",
      "Epoch 33/300\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 3815.2097 - accuracy: 0.2576 - categorical_accuracy: 0.2576 - mean_directional_accuracy: 0.5139 - val_loss: 3610.0298 - val_accuracy: 0.2389 - val_categorical_accuracy: 0.2389 - val_mean_directional_accuracy: 0.4880\n",
      "Epoch 34/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 3439.9297 - accuracy: 0.2629 - categorical_accuracy: 0.2629 - mean_directional_accuracy: 0.5163 - val_loss: 3245.2131 - val_accuracy: 0.2388 - val_categorical_accuracy: 0.2388 - val_mean_directional_accuracy: 0.4834\n",
      "Epoch 35/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 3084.0535 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.5191 - val_loss: 2899.9385 - val_accuracy: 0.2354 - val_categorical_accuracy: 0.2354 - val_mean_directional_accuracy: 0.4819\n",
      "Epoch 36/300\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 2747.9478 - accuracy: 0.2606 - categorical_accuracy: 0.2606 - mean_directional_accuracy: 0.5125 - val_loss: 2574.7517 - val_accuracy: 0.2346 - val_categorical_accuracy: 0.2346 - val_mean_directional_accuracy: 0.4787\n",
      "Epoch 37/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 2431.6719 - accuracy: 0.2619 - categorical_accuracy: 0.2619 - mean_directional_accuracy: 0.5169 - val_loss: 2268.4766 - val_accuracy: 0.2350 - val_categorical_accuracy: 0.2350 - val_mean_directional_accuracy: 0.4766\n",
      "Epoch 38/300\n",
      "23/23 [==============================] - 1s 33ms/step - loss: 2134.0918 - accuracy: 0.2624 - categorical_accuracy: 0.2624 - mean_directional_accuracy: 0.5177 - val_loss: 1981.1094 - val_accuracy: 0.2342 - val_categorical_accuracy: 0.2342 - val_mean_directional_accuracy: 0.4757\n",
      "Epoch 39/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 1855.5909 - accuracy: 0.2659 - categorical_accuracy: 0.2659 - mean_directional_accuracy: 0.5176 - val_loss: 1713.2592 - val_accuracy: 0.2325 - val_categorical_accuracy: 0.2325 - val_mean_directional_accuracy: 0.4746\n",
      "Epoch 40/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 1597.4154 - accuracy: 0.2682 - categorical_accuracy: 0.2682 - mean_directional_accuracy: 0.5198 - val_loss: 1466.5674 - val_accuracy: 0.2333 - val_categorical_accuracy: 0.2333 - val_mean_directional_accuracy: 0.4713\n",
      "Epoch 41/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 1360.1288 - accuracy: 0.2654 - categorical_accuracy: 0.2654 - mean_directional_accuracy: 0.5183 - val_loss: 1239.9486 - val_accuracy: 0.2307 - val_categorical_accuracy: 0.2307 - val_mean_directional_accuracy: 0.4663\n",
      "Epoch 42/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1142.6969 - accuracy: 0.2636 - categorical_accuracy: 0.2636 - mean_directional_accuracy: 0.5160 - val_loss: 1032.9658 - val_accuracy: 0.2281 - val_categorical_accuracy: 0.2281 - val_mean_directional_accuracy: 0.4629\n",
      "Epoch 43/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 944.8444 - accuracy: 0.2626 - categorical_accuracy: 0.2626 - mean_directional_accuracy: 0.5172 - val_loss: 845.6935 - val_accuracy: 0.2283 - val_categorical_accuracy: 0.2283 - val_mean_directional_accuracy: 0.4603\n",
      "Epoch 44/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 766.6149 - accuracy: 0.2592 - categorical_accuracy: 0.2592 - mean_directional_accuracy: 0.5119 - val_loss: 677.9930 - val_accuracy: 0.2264 - val_categorical_accuracy: 0.2264 - val_mean_directional_accuracy: 0.4563\n",
      "Epoch 45/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 607.8781 - accuracy: 0.2669 - categorical_accuracy: 0.2669 - mean_directional_accuracy: 0.5151 - val_loss: 529.6505 - val_accuracy: 0.2279 - val_categorical_accuracy: 0.2279 - val_mean_directional_accuracy: 0.4580\n",
      "Epoch 46/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 468.0115 - accuracy: 0.2684 - categorical_accuracy: 0.2684 - mean_directional_accuracy: 0.5135 - val_loss: 399.6902 - val_accuracy: 0.2283 - val_categorical_accuracy: 0.2283 - val_mean_directional_accuracy: 0.4580\n",
      "Epoch 47/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 346.9326 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5107 - val_loss: 289.1652 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 245.8914 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 199.1867 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 165.6569 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 130.3237 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 105.9145 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 81.3336 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 66.6670 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 53.3174 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 47.4570 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 42.9940 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 40.1634 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 37.2715 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 35.1252 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 32.7259 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 30.8620 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 28.7823 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 27.0215 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 25.0316 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 57/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 23.3773 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 21.5577 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 58/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 20.0648 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 18.4262 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 17.1609 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.7425 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 14.7062 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 13.5551 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 12.6316 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 11.6585 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 10.8925 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 10.0753 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 63/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 9.4670 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 8.8065 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 64/300\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 8.3181 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 7.7710 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 65/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 7.4445 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 7.1801 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 66/300\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 6.9988 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 6.9137 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 67/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 6.9051 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 6.8736 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 68/300\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 6.8942 - accuracy: 0.2646 - categorical_accuracy: 0.2646 - mean_directional_accuracy: 0.5109Restoring model weights from the end of the best epoch: 67.\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 6.8910 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 6.9023 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 68: early stopping\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 6.8736 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 6.8736395835876465, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 67 \n",
      "\n",
      "Model time: 0.5818167701363564 minutes\n",
      "\n",
      "Total time: 12.220643032342196 minutes\n",
      "\n",
      "\n",
      "Model  29  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                   [2]\n",
      "Activation function           relu\n",
      "Dropout                        0.4\n",
      "L1                           0.001\n",
      "L2                           100.0\n",
      "Batch size                      32\n",
      "Optimizer                  RMSprop\n",
      "Learning rate                0.001\n",
      "Name: 17, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 3s 11ms/step - loss: 350.2876 - accuracy: 0.2555 - categorical_accuracy: 0.2555 - mean_directional_accuracy: 0.5051 - val_loss: 189.3153 - val_accuracy: 0.2283 - val_categorical_accuracy: 0.2283 - val_mean_directional_accuracy: 0.4581\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 143.3085 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5098 - val_loss: 103.4777 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 73.7497 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 47.8104 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 30.1938 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.1565 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 8.4517 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.1970 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.7647 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3973 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "164/178 [==========================>...] - ETA: 0s - loss: 1.3942 - accuracy: 0.2654 - categorical_accuracy: 0.2654 - mean_directional_accuracy: 0.5095Restoring model weights from the end of the best epoch: 6.\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3940 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3974 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.3973 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3972570896148682, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 6 \n",
      "\n",
      "Model time: 0.20326637849211693 minutes\n",
      "\n",
      "Total time: 12.424009408801794 minutes\n",
      "\n",
      "\n",
      "Model  30  out of  111\n",
      "Model type                 FeedForward\n",
      "Hidden layers                        4\n",
      "Hidden units           [64, 4, 128, 4]\n",
      "Activation function               tanh\n",
      "Dropout                            0.7\n",
      "L1                             0.00001\n",
      "L2                                10.0\n",
      "Batch size                          32\n",
      "Optimizer                         Adam\n",
      "Learning rate                     0.01\n",
      "Name: 18, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 5s 15ms/step - loss: 42.7776 - accuracy: 0.2617 - categorical_accuracy: 0.2617 - mean_directional_accuracy: 0.4977 - val_loss: 1.3900 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1.3859 - accuracy: 0.2650 - categorical_accuracy: 0.2650 - mean_directional_accuracy: 0.5098 - val_loss: 1.3865 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 3/300\n",
      "173/178 [============================>.] - ETA: 0s - loss: 1.3864 - accuracy: 0.2567 - categorical_accuracy: 0.2567 - mean_directional_accuracy: 0.4951Restoring model weights from the end of the best epoch: 2.\n",
      "178/178 [==============================] - 2s 8ms/step - loss: 1.3863 - accuracy: 0.2580 - categorical_accuracy: 0.2580 - mean_directional_accuracy: 0.4961 - val_loss: 1.3891 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.3865 - accuracy: 0.2814 - categorical_accuracy: 0.2814 - mean_directional_accuracy: 0.5416\n",
      "{'loss': 1.3865437507629395, 'accuracy': 0.28144571185112, 'categorical_accuracy': 0.28144571185112, 'mean_directional_accuracy': 0.5416231751441956} \n",
      " 2 \n",
      "\n",
      "Model time: 0.14807535707950592 minutes\n",
      "\n",
      "Total time: 12.572184760123491 minutes\n",
      "\n",
      "\n",
      "Model  31  out of  111\n",
      "Model type                   FeedForward\n",
      "Hidden layers                          4\n",
      "Hidden units           [256, 32, 32, 16]\n",
      "Activation function               linear\n",
      "Dropout                              0.0\n",
      "L1                                   0.0\n",
      "L2                                   0.1\n",
      "Batch size                            64\n",
      "Optimizer                           Adam\n",
      "Learning rate                     0.0001\n",
      "Name: 19, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 3s 20ms/step - loss: 31.1455 - accuracy: 0.2729 - categorical_accuracy: 0.2729 - mean_directional_accuracy: 0.5079 - val_loss: 28.7782 - val_accuracy: 0.2540 - val_categorical_accuracy: 0.2540 - val_mean_directional_accuracy: 0.4646\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 26.7037 - accuracy: 0.2926 - categorical_accuracy: 0.2926 - mean_directional_accuracy: 0.5146 - val_loss: 24.7161 - val_accuracy: 0.2662 - val_categorical_accuracy: 0.2662 - val_mean_directional_accuracy: 0.4778\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 22.9286 - accuracy: 0.3005 - categorical_accuracy: 0.3005 - mean_directional_accuracy: 0.5188 - val_loss: 21.2229 - val_accuracy: 0.2747 - val_categorical_accuracy: 0.2747 - val_mean_directional_accuracy: 0.4873\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 19.6872 - accuracy: 0.3098 - categorical_accuracy: 0.3098 - mean_directional_accuracy: 0.5256 - val_loss: 18.2290 - val_accuracy: 0.2724 - val_categorical_accuracy: 0.2724 - val_mean_directional_accuracy: 0.4817\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 16.9105 - accuracy: 0.3182 - categorical_accuracy: 0.3182 - mean_directional_accuracy: 0.5270 - val_loss: 15.6649 - val_accuracy: 0.2797 - val_categorical_accuracy: 0.2797 - val_mean_directional_accuracy: 0.4935\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 14.5388 - accuracy: 0.3193 - categorical_accuracy: 0.3193 - mean_directional_accuracy: 0.5256 - val_loss: 13.4789 - val_accuracy: 0.2813 - val_categorical_accuracy: 0.2813 - val_mean_directional_accuracy: 0.4978\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 12.5208 - accuracy: 0.3188 - categorical_accuracy: 0.3188 - mean_directional_accuracy: 0.5256 - val_loss: 11.6213 - val_accuracy: 0.2841 - val_categorical_accuracy: 0.2841 - val_mean_directional_accuracy: 0.4996\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 10.8080 - accuracy: 0.3231 - categorical_accuracy: 0.3231 - mean_directional_accuracy: 0.5281 - val_loss: 10.0488 - val_accuracy: 0.2842 - val_categorical_accuracy: 0.2842 - val_mean_directional_accuracy: 0.4979\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 9.3611 - accuracy: 0.3237 - categorical_accuracy: 0.3237 - mean_directional_accuracy: 0.5330 - val_loss: 8.7224 - val_accuracy: 0.2829 - val_categorical_accuracy: 0.2829 - val_mean_directional_accuracy: 0.4958\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 1s 14ms/step - loss: 8.1420 - accuracy: 0.3249 - categorical_accuracy: 0.3249 - mean_directional_accuracy: 0.5313 - val_loss: 7.6060 - val_accuracy: 0.2826 - val_categorical_accuracy: 0.2826 - val_mean_directional_accuracy: 0.4923\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 7.1186 - accuracy: 0.3267 - categorical_accuracy: 0.3267 - mean_directional_accuracy: 0.5341 - val_loss: 6.6706 - val_accuracy: 0.2848 - val_categorical_accuracy: 0.2848 - val_mean_directional_accuracy: 0.4977\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 6.2616 - accuracy: 0.3238 - categorical_accuracy: 0.3238 - mean_directional_accuracy: 0.5332 - val_loss: 5.8878 - val_accuracy: 0.2846 - val_categorical_accuracy: 0.2846 - val_mean_directional_accuracy: 0.4980\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 5.5454 - accuracy: 0.3191 - categorical_accuracy: 0.3191 - mean_directional_accuracy: 0.5351 - val_loss: 5.2341 - val_accuracy: 0.2820 - val_categorical_accuracy: 0.2820 - val_mean_directional_accuracy: 0.4956\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 4.9475 - accuracy: 0.3224 - categorical_accuracy: 0.3224 - mean_directional_accuracy: 0.5414 - val_loss: 4.6884 - val_accuracy: 0.2773 - val_categorical_accuracy: 0.2773 - val_mean_directional_accuracy: 0.4960\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 4.4484 - accuracy: 0.3144 - categorical_accuracy: 0.3144 - mean_directional_accuracy: 0.5411 - val_loss: 4.2324 - val_accuracy: 0.2730 - val_categorical_accuracy: 0.2730 - val_mean_directional_accuracy: 0.4924\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 4.0315 - accuracy: 0.3080 - categorical_accuracy: 0.3080 - mean_directional_accuracy: 0.5423 - val_loss: 3.8512 - val_accuracy: 0.2632 - val_categorical_accuracy: 0.2632 - val_mean_directional_accuracy: 0.4881\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 3.6824 - accuracy: 0.2952 - categorical_accuracy: 0.2952 - mean_directional_accuracy: 0.5367 - val_loss: 3.5316 - val_accuracy: 0.2509 - val_categorical_accuracy: 0.2509 - val_mean_directional_accuracy: 0.4819\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 3.3892 - accuracy: 0.2864 - categorical_accuracy: 0.2864 - mean_directional_accuracy: 0.5313 - val_loss: 3.2622 - val_accuracy: 0.2418 - val_categorical_accuracy: 0.2418 - val_mean_directional_accuracy: 0.4723\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 3.1419 - accuracy: 0.2822 - categorical_accuracy: 0.2822 - mean_directional_accuracy: 0.5288 - val_loss: 3.0345 - val_accuracy: 0.2363 - val_categorical_accuracy: 0.2363 - val_mean_directional_accuracy: 0.4675\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 2.9320 - accuracy: 0.2771 - categorical_accuracy: 0.2771 - mean_directional_accuracy: 0.5221 - val_loss: 2.8407 - val_accuracy: 0.2342 - val_categorical_accuracy: 0.2342 - val_mean_directional_accuracy: 0.4640\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 2.7529 - accuracy: 0.2734 - categorical_accuracy: 0.2734 - mean_directional_accuracy: 0.5177 - val_loss: 2.6747 - val_accuracy: 0.2313 - val_categorical_accuracy: 0.2313 - val_mean_directional_accuracy: 0.4611\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 2.5990 - accuracy: 0.2713 - categorical_accuracy: 0.2713 - mean_directional_accuracy: 0.5144 - val_loss: 2.5316 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4577\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 2.4658 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5107 - val_loss: 2.4074 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 2.3497 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5105 - val_loss: 2.2989 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 2.2480 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.2034 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 2.1582 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.1189 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 2.0785 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.0439 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 2.0075 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9768 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.9440 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9167 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.8870 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8627 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.8357 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8140 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 1s 15ms/step - loss: 1.7895 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7702 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.7478 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7306 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.7102 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6949 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 1.6762 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6627 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.6456 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6337 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.6180 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6075 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.5932 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5840 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 1s 14ms/step - loss: 1.5708 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5628 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 1.5507 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5439 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.5327 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5269 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 1s 14ms/step - loss: 1.5166 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5116 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 1.5022 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4980 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4893 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4858 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4778 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4750 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4676 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4654 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4585 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4568 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 1s 15ms/step - loss: 1.4504 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4492 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4431 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4423 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 1.4367 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4363 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.4310 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4309 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.4260 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4262 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.4215 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4220 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4175 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4182 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4139 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4148 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4107 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4118 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 57/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.4079 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4092 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 58/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.4054 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4068 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.4031 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4047 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 1.4011 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4027 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.3993 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4011 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.3976 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3995 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 63/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.3962 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3982 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 64/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.3949 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3969 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 65/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3937 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3958 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 66/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.3927 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3949 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 67/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.3917 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3940 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 68/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3909 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3932 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 69/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.3902 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3925 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 70/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.3895 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3919 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 71/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.3889 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3913 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 72/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.3884 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3908 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 73/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3879 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3904 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 74/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.3875 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3901 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 75/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.3871 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3897 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 76/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3868 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3895 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 77/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 1.3866 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3892 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 78/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3863 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3890 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 79/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.3861 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3888 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 80/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3859 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3887 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 81/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 1.3858 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3885 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 82/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.3857 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3884 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 83/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3856 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3883 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 84/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3855 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3883 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 85/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 1.3854 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3882 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 86/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.3853 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3882 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 87/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3853 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3882 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 88/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3881 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 89/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.3852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3881 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 90/300\n",
      "88/89 [============================>.] - ETA: 0s - loss: 1.3852 - accuracy: 0.2676 - categorical_accuracy: 0.2676 - mean_directional_accuracy: 0.5107Restoring model weights from the end of the best epoch: 89.\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.3852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3881 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 90: early stopping\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 1.3881 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3881020545959473, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 89 \n",
      "\n",
      "Model time: 1.535682663321495 minutes\n",
      "\n",
      "Total time: 14.107934083789587 minutes\n",
      "\n",
      "\n",
      "Model  32  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                 [256]\n",
      "Activation function           tanh\n",
      "Dropout                        0.1\n",
      "L1                             0.1\n",
      "L2                         0.00001\n",
      "Batch size                      32\n",
      "Optimizer                     Adam\n",
      "Learning rate                 0.01\n",
      "Name: 20, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 4s 16ms/step - loss: 17.3052 - accuracy: 0.2594 - categorical_accuracy: 0.2594 - mean_directional_accuracy: 0.5065 - val_loss: 6.8407 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 6.7686 - accuracy: 0.2657 - categorical_accuracy: 0.2657 - mean_directional_accuracy: 0.5000 - val_loss: 6.6071 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "167/178 [===========================>..] - ETA: 0s - loss: 6.6824 - accuracy: 0.2618 - categorical_accuracy: 0.2618 - mean_directional_accuracy: 0.5011Restoring model weights from the end of the best epoch: 2.\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 6.6792 - accuracy: 0.2594 - categorical_accuracy: 0.2594 - mean_directional_accuracy: 0.5018 - val_loss: 6.6832 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3: early stopping\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 6.6071 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 6.6070685386657715, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 2 \n",
      "\n",
      "Model time: 0.21299952268600464 minutes\n",
      "\n",
      "Total time: 14.321000263094902 minutes\n",
      "\n",
      "\n",
      "Model  33  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    2\n",
      "Hidden units             [16, 256]\n",
      "Activation function         linear\n",
      "Dropout                        0.0\n",
      "L1                         0.00001\n",
      "L2                          0.0001\n",
      "Batch size                     256\n",
      "Optimizer                     Adam\n",
      "Learning rate                 0.01\n",
      "Name: 21, dtype: object\n",
      "NN2Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 3s 41ms/step - loss: 1.4601 - accuracy: 0.2873 - categorical_accuracy: 0.2873 - mean_directional_accuracy: 0.5232 - val_loss: 1.4229 - val_accuracy: 0.2788 - val_categorical_accuracy: 0.2788 - val_mean_directional_accuracy: 0.5037\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.4054 - accuracy: 0.3087 - categorical_accuracy: 0.3087 - mean_directional_accuracy: 0.5371 - val_loss: 1.4063 - val_accuracy: 0.2801 - val_categorical_accuracy: 0.2801 - val_mean_directional_accuracy: 0.5042\n",
      "Epoch 3/300\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 1.3689 - accuracy: 0.3279 - categorical_accuracy: 0.3279 - mean_directional_accuracy: 0.5432Restoring model weights from the end of the best epoch: 2.\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3694 - accuracy: 0.3268 - categorical_accuracy: 0.3268 - mean_directional_accuracy: 0.5443 - val_loss: 1.4124 - val_accuracy: 0.2868 - val_categorical_accuracy: 0.2868 - val_mean_directional_accuracy: 0.5020\n",
      "Epoch 3: early stopping\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4063 - accuracy: 0.2801 - categorical_accuracy: 0.2801 - mean_directional_accuracy: 0.5042\n",
      "{'loss': 1.4062691926956177, 'accuracy': 0.2801409065723419, 'categorical_accuracy': 0.2801409065723419, 'mean_directional_accuracy': 0.5041753649711609} \n",
      " 2 \n",
      "\n",
      "Model time: 0.08283242583274841 minutes\n",
      "\n",
      "Total time: 14.40398270264268 minutes\n",
      "\n",
      "\n",
      "Model  34  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                  [64]\n",
      "Activation function         linear\n",
      "Dropout                        0.6\n",
      "L1                            10.0\n",
      "L2                          0.0001\n",
      "Batch size                     128\n",
      "Optimizer                     Adam\n",
      "Learning rate               0.0001\n",
      "Name: 22, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 3s 21ms/step - loss: 8880.4033 - accuracy: 0.2585 - categorical_accuracy: 0.2585 - mean_directional_accuracy: 0.4956 - val_loss: 8627.2920 - val_accuracy: 0.2533 - val_categorical_accuracy: 0.2533 - val_mean_directional_accuracy: 0.4744\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 8396.2793 - accuracy: 0.2540 - categorical_accuracy: 0.2540 - mean_directional_accuracy: 0.4960 - val_loss: 8150.3623 - val_accuracy: 0.2552 - val_categorical_accuracy: 0.2552 - val_mean_directional_accuracy: 0.4756\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 7925.9312 - accuracy: 0.2585 - categorical_accuracy: 0.2585 - mean_directional_accuracy: 0.5111 - val_loss: 7687.2930 - val_accuracy: 0.2557 - val_categorical_accuracy: 0.2557 - val_mean_directional_accuracy: 0.4740\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 7469.2446 - accuracy: 0.2578 - categorical_accuracy: 0.2578 - mean_directional_accuracy: 0.4967 - val_loss: 7237.5107 - val_accuracy: 0.2547 - val_categorical_accuracy: 0.2547 - val_mean_directional_accuracy: 0.4733\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 7026.1602 - accuracy: 0.2578 - categorical_accuracy: 0.2578 - mean_directional_accuracy: 0.4981 - val_loss: 6801.5850 - val_accuracy: 0.2553 - val_categorical_accuracy: 0.2553 - val_mean_directional_accuracy: 0.4744\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 6596.4424 - accuracy: 0.2550 - categorical_accuracy: 0.2550 - mean_directional_accuracy: 0.4914 - val_loss: 6378.6460 - val_accuracy: 0.2560 - val_categorical_accuracy: 0.2560 - val_mean_directional_accuracy: 0.4744\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 6180.2305 - accuracy: 0.2587 - categorical_accuracy: 0.2587 - mean_directional_accuracy: 0.4891 - val_loss: 5969.6025 - val_accuracy: 0.2553 - val_categorical_accuracy: 0.2553 - val_mean_directional_accuracy: 0.4757\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 5777.1982 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.4977 - val_loss: 5572.9839 - val_accuracy: 0.2560 - val_categorical_accuracy: 0.2560 - val_mean_directional_accuracy: 0.4753\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 5386.8843 - accuracy: 0.2531 - categorical_accuracy: 0.2531 - mean_directional_accuracy: 0.4903 - val_loss: 5189.9893 - val_accuracy: 0.2570 - val_categorical_accuracy: 0.2570 - val_mean_directional_accuracy: 0.4773\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 5011.0493 - accuracy: 0.2531 - categorical_accuracy: 0.2531 - mean_directional_accuracy: 0.4916 - val_loss: 4821.7173 - val_accuracy: 0.2590 - val_categorical_accuracy: 0.2590 - val_mean_directional_accuracy: 0.4799\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 4649.8384 - accuracy: 0.2518 - categorical_accuracy: 0.2518 - mean_directional_accuracy: 0.4988 - val_loss: 4468.0522 - val_accuracy: 0.2608 - val_categorical_accuracy: 0.2608 - val_mean_directional_accuracy: 0.4817\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 4302.9141 - accuracy: 0.2550 - categorical_accuracy: 0.2550 - mean_directional_accuracy: 0.5046 - val_loss: 4128.4463 - val_accuracy: 0.2633 - val_categorical_accuracy: 0.2633 - val_mean_directional_accuracy: 0.4853\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 3969.9648 - accuracy: 0.2524 - categorical_accuracy: 0.2524 - mean_directional_accuracy: 0.4935 - val_loss: 3802.7004 - val_accuracy: 0.2634 - val_categorical_accuracy: 0.2634 - val_mean_directional_accuracy: 0.4871\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 3650.8608 - accuracy: 0.2629 - categorical_accuracy: 0.2629 - mean_directional_accuracy: 0.5083 - val_loss: 3490.5715 - val_accuracy: 0.2637 - val_categorical_accuracy: 0.2637 - val_mean_directional_accuracy: 0.4862\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 3345.3669 - accuracy: 0.2557 - categorical_accuracy: 0.2557 - mean_directional_accuracy: 0.4996 - val_loss: 3192.2048 - val_accuracy: 0.2645 - val_categorical_accuracy: 0.2645 - val_mean_directional_accuracy: 0.4859\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 3053.4019 - accuracy: 0.2576 - categorical_accuracy: 0.2576 - mean_directional_accuracy: 0.5081 - val_loss: 2907.1545 - val_accuracy: 0.2644 - val_categorical_accuracy: 0.2644 - val_mean_directional_accuracy: 0.4870\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 2774.8828 - accuracy: 0.2640 - categorical_accuracy: 0.2640 - mean_directional_accuracy: 0.4967 - val_loss: 2635.5803 - val_accuracy: 0.2612 - val_categorical_accuracy: 0.2612 - val_mean_directional_accuracy: 0.4870\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 2509.5298 - accuracy: 0.2497 - categorical_accuracy: 0.2497 - mean_directional_accuracy: 0.4995 - val_loss: 2376.9304 - val_accuracy: 0.2598 - val_categorical_accuracy: 0.2598 - val_mean_directional_accuracy: 0.4871\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 2257.3325 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.5058 - val_loss: 2131.5771 - val_accuracy: 0.2620 - val_categorical_accuracy: 0.2620 - val_mean_directional_accuracy: 0.4924\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2018.5601 - accuracy: 0.2604 - categorical_accuracy: 0.2604 - mean_directional_accuracy: 0.5086 - val_loss: 1900.0994 - val_accuracy: 0.2624 - val_categorical_accuracy: 0.2624 - val_mean_directional_accuracy: 0.4963\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 1793.8828 - accuracy: 0.2538 - categorical_accuracy: 0.2538 - mean_directional_accuracy: 0.5021 - val_loss: 1682.6025 - val_accuracy: 0.2580 - val_categorical_accuracy: 0.2580 - val_mean_directional_accuracy: 0.4950\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 1582.9264 - accuracy: 0.2450 - categorical_accuracy: 0.2450 - mean_directional_accuracy: 0.4998 - val_loss: 1478.6614 - val_accuracy: 0.2553 - val_categorical_accuracy: 0.2553 - val_mean_directional_accuracy: 0.4941\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 1385.9381 - accuracy: 0.2538 - categorical_accuracy: 0.2538 - mean_directional_accuracy: 0.5133 - val_loss: 1289.1641 - val_accuracy: 0.2526 - val_categorical_accuracy: 0.2526 - val_mean_directional_accuracy: 0.5017\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1203.4006 - accuracy: 0.2473 - categorical_accuracy: 0.2473 - mean_directional_accuracy: 0.5083 - val_loss: 1113.9495 - val_accuracy: 0.2557 - val_categorical_accuracy: 0.2557 - val_mean_directional_accuracy: 0.5094\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1034.8180 - accuracy: 0.2524 - categorical_accuracy: 0.2524 - mean_directional_accuracy: 0.5130 - val_loss: 952.5252 - val_accuracy: 0.2576 - val_categorical_accuracy: 0.2576 - val_mean_directional_accuracy: 0.5128\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 879.9285 - accuracy: 0.2541 - categorical_accuracy: 0.2541 - mean_directional_accuracy: 0.5128 - val_loss: 804.4928 - val_accuracy: 0.2582 - val_categorical_accuracy: 0.2582 - val_mean_directional_accuracy: 0.5155\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 737.9954 - accuracy: 0.2399 - categorical_accuracy: 0.2399 - mean_directional_accuracy: 0.4933 - val_loss: 669.2410 - val_accuracy: 0.2599 - val_categorical_accuracy: 0.2599 - val_mean_directional_accuracy: 0.5174\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 609.4256 - accuracy: 0.2392 - categorical_accuracy: 0.2392 - mean_directional_accuracy: 0.4944 - val_loss: 548.1157 - val_accuracy: 0.2603 - val_categorical_accuracy: 0.2603 - val_mean_directional_accuracy: 0.5163\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 495.3443 - accuracy: 0.2431 - categorical_accuracy: 0.2431 - mean_directional_accuracy: 0.5000 - val_loss: 441.1150 - val_accuracy: 0.2632 - val_categorical_accuracy: 0.2632 - val_mean_directional_accuracy: 0.5134\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 395.0253 - accuracy: 0.2457 - categorical_accuracy: 0.2457 - mean_directional_accuracy: 0.4921 - val_loss: 348.3253 - val_accuracy: 0.2620 - val_categorical_accuracy: 0.2620 - val_mean_directional_accuracy: 0.5128\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 308.9647 - accuracy: 0.2490 - categorical_accuracy: 0.2490 - mean_directional_accuracy: 0.4993 - val_loss: 269.1790 - val_accuracy: 0.2624 - val_categorical_accuracy: 0.2624 - val_mean_directional_accuracy: 0.5162\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 236.7339 - accuracy: 0.2455 - categorical_accuracy: 0.2455 - mean_directional_accuracy: 0.4960 - val_loss: 204.5372 - val_accuracy: 0.2602 - val_categorical_accuracy: 0.2602 - val_mean_directional_accuracy: 0.5142\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 179.0071 - accuracy: 0.2436 - categorical_accuracy: 0.2436 - mean_directional_accuracy: 0.4902 - val_loss: 154.2494 - val_accuracy: 0.2600 - val_categorical_accuracy: 0.2600 - val_mean_directional_accuracy: 0.5124\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 135.7495 - accuracy: 0.2506 - categorical_accuracy: 0.2506 - mean_directional_accuracy: 0.4946 - val_loss: 118.6018 - val_accuracy: 0.2679 - val_categorical_accuracy: 0.2679 - val_mean_directional_accuracy: 0.5277\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 107.1624 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.4967 - val_loss: 97.3919 - val_accuracy: 0.2822 - val_categorical_accuracy: 0.2822 - val_mean_directional_accuracy: 0.5423\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 92.6281 - accuracy: 0.2617 - categorical_accuracy: 0.2617 - mean_directional_accuracy: 0.4926 - val_loss: 89.1522 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 86.6562 - accuracy: 0.2503 - categorical_accuracy: 0.2503 - mean_directional_accuracy: 0.4893 - val_loss: 84.0637 - val_accuracy: 0.2817 - val_categorical_accuracy: 0.2817 - val_mean_directional_accuracy: 0.5421\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 81.6847 - accuracy: 0.2597 - categorical_accuracy: 0.2597 - mean_directional_accuracy: 0.4998 - val_loss: 79.1457 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 76.8213 - accuracy: 0.2648 - categorical_accuracy: 0.2648 - mean_directional_accuracy: 0.5044 - val_loss: 74.3455 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 72.0695 - accuracy: 0.2638 - categorical_accuracy: 0.2638 - mean_directional_accuracy: 0.5067 - val_loss: 69.6389 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 67.4222 - accuracy: 0.2657 - categorical_accuracy: 0.2657 - mean_directional_accuracy: 0.5076 - val_loss: 65.0611 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 62.9344 - accuracy: 0.2629 - categorical_accuracy: 0.2629 - mean_directional_accuracy: 0.5012 - val_loss: 60.6882 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 58.6211 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5095 - val_loss: 56.4485 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 54.4618 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5107 - val_loss: 52.3706 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 50.4598 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5104 - val_loss: 48.4391 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 46.5672 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5107 - val_loss: 44.5901 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 42.7916 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 40.8732 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 39.1412 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 37.3103 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 35.6372 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 33.8593 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 32.2190 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 30.4919 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 28.8910 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 27.1852 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 25.6779 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 24.0970 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 22.6592 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 21.1602 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 19.8422 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 18.4764 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 17.2708 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.0117 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 14.9361 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 13.8367 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 12.8919 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 11.9251 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 11.0472 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 10.1688 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 9.4011 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 8.5879 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 7.9043 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 7.2031 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 6.6230 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 6.0586 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 5.5677 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.0581 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 63/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 4.6504 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.2004 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 64/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 3.8118 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.4284 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 65/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 3.1621 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.9126 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 66/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.7590 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.6877 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 67/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 2.6959 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.6736 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 68/300\n",
      "41/45 [==========================>...] - ETA: 0s - loss: 2.6907 - accuracy: 0.2689 - categorical_accuracy: 0.2689 - mean_directional_accuracy: 0.5112Restoring model weights from the end of the best epoch: 67.\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 2.6914 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.7104 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 68: early stopping\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 2.6736 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 2.6735589504241943, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 67 \n",
      "\n",
      "Model time: 0.7198243774473667 minutes\n",
      "\n",
      "Total time: 15.123908042907715 minutes\n",
      "\n",
      "\n",
      "Model  35  out of  111\n",
      "Model type                    FeedForward\n",
      "Hidden layers                           4\n",
      "Hidden units           [256, 16, 64, 256]\n",
      "Activation function                linear\n",
      "Dropout                               0.2\n",
      "L1                                  0.001\n",
      "L2                                    0.1\n",
      "Batch size                            128\n",
      "Optimizer                         RMSprop\n",
      "Learning rate                      0.0001\n",
      "Name: 23, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 5s 45ms/step - loss: 40.3599 - accuracy: 0.2643 - categorical_accuracy: 0.2643 - mean_directional_accuracy: 0.5109 - val_loss: 38.3054 - val_accuracy: 0.2700 - val_categorical_accuracy: 0.2700 - val_mean_directional_accuracy: 0.5167\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 36.6757 - accuracy: 0.2650 - categorical_accuracy: 0.2650 - mean_directional_accuracy: 0.5132 - val_loss: 34.8504 - val_accuracy: 0.2730 - val_categorical_accuracy: 0.2730 - val_mean_directional_accuracy: 0.5198\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 33.2829 - accuracy: 0.2682 - categorical_accuracy: 0.2682 - mean_directional_accuracy: 0.5084 - val_loss: 31.5765 - val_accuracy: 0.2713 - val_categorical_accuracy: 0.2713 - val_mean_directional_accuracy: 0.5155\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 30.0947 - accuracy: 0.2638 - categorical_accuracy: 0.2638 - mean_directional_accuracy: 0.5140 - val_loss: 28.4949 - val_accuracy: 0.2794 - val_categorical_accuracy: 0.2794 - val_mean_directional_accuracy: 0.5215\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 27.0944 - accuracy: 0.2838 - categorical_accuracy: 0.2838 - mean_directional_accuracy: 0.5246 - val_loss: 25.6157 - val_accuracy: 0.2795 - val_categorical_accuracy: 0.2795 - val_mean_directional_accuracy: 0.5236\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 24.3052 - accuracy: 0.2835 - categorical_accuracy: 0.2835 - mean_directional_accuracy: 0.5198 - val_loss: 22.9357 - val_accuracy: 0.2811 - val_categorical_accuracy: 0.2811 - val_mean_directional_accuracy: 0.5185\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 21.7221 - accuracy: 0.2901 - categorical_accuracy: 0.2901 - mean_directional_accuracy: 0.5209 - val_loss: 20.4584 - val_accuracy: 0.2756 - val_categorical_accuracy: 0.2756 - val_mean_directional_accuracy: 0.5141\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 19.3349 - accuracy: 0.2798 - categorical_accuracy: 0.2798 - mean_directional_accuracy: 0.5209 - val_loss: 18.1759 - val_accuracy: 0.2630 - val_categorical_accuracy: 0.2630 - val_mean_directional_accuracy: 0.4927\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 17.1470 - accuracy: 0.2775 - categorical_accuracy: 0.2775 - mean_directional_accuracy: 0.5316 - val_loss: 16.0861 - val_accuracy: 0.2495 - val_categorical_accuracy: 0.2495 - val_mean_directional_accuracy: 0.4845\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 15.1488 - accuracy: 0.2777 - categorical_accuracy: 0.2777 - mean_directional_accuracy: 0.5235 - val_loss: 14.1851 - val_accuracy: 0.2426 - val_categorical_accuracy: 0.2426 - val_mean_directional_accuracy: 0.4759\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 13.3345 - accuracy: 0.2787 - categorical_accuracy: 0.2787 - mean_directional_accuracy: 0.5186 - val_loss: 12.4635 - val_accuracy: 0.2383 - val_categorical_accuracy: 0.2383 - val_mean_directional_accuracy: 0.4697\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 11.6978 - accuracy: 0.2719 - categorical_accuracy: 0.2719 - mean_directional_accuracy: 0.5156 - val_loss: 10.9143 - val_accuracy: 0.2354 - val_categorical_accuracy: 0.2354 - val_mean_directional_accuracy: 0.4653\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 10.2286 - accuracy: 0.2659 - categorical_accuracy: 0.2659 - mean_directional_accuracy: 0.5083 - val_loss: 9.5285 - val_accuracy: 0.2317 - val_categorical_accuracy: 0.2317 - val_mean_directional_accuracy: 0.4619\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 8.9181 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5112 - val_loss: 8.2971 - val_accuracy: 0.2300 - val_categorical_accuracy: 0.2300 - val_mean_directional_accuracy: 0.4598\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 7.7576 - accuracy: 0.2706 - categorical_accuracy: 0.2706 - mean_directional_accuracy: 0.5135 - val_loss: 7.2102 - val_accuracy: 0.2290 - val_categorical_accuracy: 0.2290 - val_mean_directional_accuracy: 0.4582\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 6.7369 - accuracy: 0.2687 - categorical_accuracy: 0.2687 - mean_directional_accuracy: 0.5114 - val_loss: 6.2581 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 5.8460 - accuracy: 0.2684 - categorical_accuracy: 0.2684 - mean_directional_accuracy: 0.5111 - val_loss: 5.4310 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 5.0757 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5104 - val_loss: 4.7195 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 4.4163 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.1140 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 3.8582 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.6047 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 3.3916 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.1821 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 3.0074 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.8372 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 2.6962 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.5602 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 1s 23ms/step - loss: 2.4486 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.3424 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 2.2556 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.1741 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 2.1078 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.0465 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 1s 25ms/step - loss: 1.9962 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9504 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 1s 26ms/step - loss: 1.9111 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8749 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 4s 99ms/step - loss: 1.8429 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8140 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 1.7881 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7650 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.7438 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7252 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.7071 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6911 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6749 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6608 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6462 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6336 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6200 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6085 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.5959 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5854 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.5735 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5640 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 1.5528 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5442 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.5338 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5260 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.5163 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5093 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.5002 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4941 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 1.4856 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4803 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.4724 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4677 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.4604 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4564 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.4496 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4463 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4400 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4372 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.4314 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4292 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4238 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4222 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4172 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4161 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 1.4114 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4108 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.4065 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4063 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.4024 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4025 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.3989 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3995 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.3961 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3970 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.3938 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3950 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3921 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3935 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.3907 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3924 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3898 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3916 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 1.3891 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3912 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.3887 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3909 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3885 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3908 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "40/45 [=========================>....] - ETA: 0s - loss: 1.3882 - accuracy: 0.2703 - categorical_accuracy: 0.2703 - mean_directional_accuracy: 0.5104Restoring model weights from the end of the best epoch: 61.\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3885 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3909 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62: early stopping\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3908 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3908414840698242, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 61 \n",
      "\n",
      "Model time: 1.4265908859670162 minutes\n",
      "\n",
      "Total time: 16.55056557431817 minutes\n",
      "\n",
      "\n",
      "Model  36  out of  111\n",
      "Model type                  FeedForward\n",
      "Hidden layers                         4\n",
      "Hidden units           [2, 256, 8, 128]\n",
      "Activation function                tanh\n",
      "Dropout                             0.0\n",
      "L1                               0.0001\n",
      "L2                                 0.01\n",
      "Batch size                          256\n",
      "Optimizer                       RMSprop\n",
      "Learning rate                      0.01\n",
      "Name: 24, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 3s 47ms/step - loss: 1.4630 - accuracy: 0.2606 - categorical_accuracy: 0.2606 - mean_directional_accuracy: 0.5074 - val_loss: 1.3878 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 2/300\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.3876 - accuracy: 0.2680 - categorical_accuracy: 0.2680 - mean_directional_accuracy: 0.5065Restoring model weights from the end of the best epoch: 1.\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.3878 - accuracy: 0.2647 - categorical_accuracy: 0.2647 - mean_directional_accuracy: 0.5039 - val_loss: 1.3908 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2: early stopping\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3878 - accuracy: 0.2814 - categorical_accuracy: 0.2814 - mean_directional_accuracy: 0.5416\n",
      "{'loss': 1.3877804279327393, 'accuracy': 0.28144571185112, 'categorical_accuracy': 0.28144571185112, 'mean_directional_accuracy': 0.5416231751441956} \n",
      " 1 \n",
      "\n",
      "Model time: 0.07852557674050331 minutes\n",
      "\n",
      "Total time: 16.629157822579145 minutes\n",
      "\n",
      "\n",
      "Model  37  out of  111\n",
      "Model type               FeedForward\n",
      "Hidden layers                      4\n",
      "Hidden units           [4, 64, 4, 1]\n",
      "Activation function             relu\n",
      "Dropout                          0.4\n",
      "L1                           0.00001\n",
      "L2                              0.01\n",
      "Batch size                        64\n",
      "Optimizer                    RMSprop\n",
      "Learning rate                 0.0001\n",
      "Name: 25, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 4s 14ms/step - loss: 1.6985 - accuracy: 0.2517 - categorical_accuracy: 0.2517 - mean_directional_accuracy: 0.4995 - val_loss: 1.6437 - val_accuracy: 0.2456 - val_categorical_accuracy: 0.2456 - val_mean_directional_accuracy: 0.5018\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.6617 - accuracy: 0.2476 - categorical_accuracy: 0.2476 - mean_directional_accuracy: 0.4879 - val_loss: 1.6282 - val_accuracy: 0.2597 - val_categorical_accuracy: 0.2597 - val_mean_directional_accuracy: 0.5265\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.6428 - accuracy: 0.2455 - categorical_accuracy: 0.2455 - mean_directional_accuracy: 0.4849 - val_loss: 1.6150 - val_accuracy: 0.2737 - val_categorical_accuracy: 0.2737 - val_mean_directional_accuracy: 0.5377\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.6234 - accuracy: 0.2545 - categorical_accuracy: 0.2545 - mean_directional_accuracy: 0.4928 - val_loss: 1.6026 - val_accuracy: 0.2807 - val_categorical_accuracy: 0.2807 - val_mean_directional_accuracy: 0.5415\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.6095 - accuracy: 0.2471 - categorical_accuracy: 0.2471 - mean_directional_accuracy: 0.4860 - val_loss: 1.5908 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5415\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.5897 - accuracy: 0.2538 - categorical_accuracy: 0.2538 - mean_directional_accuracy: 0.4903 - val_loss: 1.5786 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.5737 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.5084 - val_loss: 1.5656 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.5604 - accuracy: 0.2641 - categorical_accuracy: 0.2641 - mean_directional_accuracy: 0.5105 - val_loss: 1.5536 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.5484 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5105 - val_loss: 1.5415 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 1.5366 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5105 - val_loss: 1.5300 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.5252 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5105 - val_loss: 1.5192 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.5137 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5105 - val_loss: 1.5084 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.5029 - accuracy: 0.2671 - categorical_accuracy: 0.2671 - mean_directional_accuracy: 0.5105 - val_loss: 1.4978 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.4917 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5105 - val_loss: 1.4868 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4808 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5105 - val_loss: 1.4771 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.4713 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5105 - val_loss: 1.4676 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.4623 - accuracy: 0.2661 - categorical_accuracy: 0.2661 - mean_directional_accuracy: 0.5105 - val_loss: 1.4593 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 1s 14ms/step - loss: 1.4544 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4518 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.4472 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5105 - val_loss: 1.4451 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4405 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4385 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.4343 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4329 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 1.4288 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4278 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.4240 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4234 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 1.4198 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4196 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4162 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4164 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4133 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4137 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4107 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4115 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4086 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4095 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4068 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4079 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4052 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4066 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4039 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4054 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.4028 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4044 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4019 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4036 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4011 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4029 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4003 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4022 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.3997 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4017 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 1.3991 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4012 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.3986 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4007 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3981 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4003 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.3976 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3998 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3972 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3994 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3967 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3990 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3963 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3987 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3959 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3983 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3955 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3979 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3951 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3975 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3947 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3972 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3943 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3968 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3939 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3965 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3936 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3962 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3932 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3958 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3929 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3955 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3925 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3952 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3922 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3949 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3919 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3946 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3916 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3943 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 57/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3912 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3940 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 58/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3909 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3938 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3907 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3935 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.3904 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3932 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3901 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3930 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3898 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3927 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 63/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3896 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3925 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 64/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3893 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3922 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 65/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3891 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3920 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 66/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3888 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3918 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 67/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3886 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3916 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 68/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3884 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3914 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 69/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3882 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3911 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 70/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3880 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3909 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 71/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3878 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3908 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 72/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3876 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3906 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 73/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3874 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3904 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 74/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3872 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3903 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 75/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3870 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3901 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 76/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3869 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3899 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 77/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3867 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3898 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 78/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3866 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3897 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 79/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3865 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3895 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 80/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3863 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3894 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 81/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3862 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3893 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 82/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3861 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3892 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 83/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3860 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3891 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 84/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3859 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3890 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 85/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3858 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3890 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 86/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3857 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3889 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 87/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3857 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3888 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 88/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3856 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3888 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 89/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3856 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3887 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 90/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3855 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3887 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 91/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3855 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3887 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 92/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3854 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3886 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 93/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3854 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3886 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 94/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3853 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3886 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 95/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3853 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3885 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 96/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3853 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3885 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 97/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3853 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3885 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 98/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3885 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 99/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3884 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 100/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3884 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 101/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3884 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 102/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3884 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 103/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3851 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3884 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 104/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3851 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3884 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 105/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3851 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3884 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 106/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3851 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3884 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 107/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3851 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3884 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 108/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.3851 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3883 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 109/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3851 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3883 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 110/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3851 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3883 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 111/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3851 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3883 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 112/300\n",
      "78/89 [=========================>....] - ETA: 0s - loss: 1.3850 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5114Restoring model weights from the end of the best epoch: 111.\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3851 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3883 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 112: early stopping\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 1.3883 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.388319969177246, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 111 \n",
      "\n",
      "Model time: 1.5806396305561066 minutes\n",
      "\n",
      "Total time: 18.209864106029272 minutes\n",
      "\n",
      "\n",
      "Model  38  out of  111\n",
      "Model type              FeedForward\n",
      "Hidden layers                     3\n",
      "Hidden units           [2, 32, 128]\n",
      "Activation function          linear\n",
      "Dropout                         0.6\n",
      "L1                              1.0\n",
      "L2                              0.1\n",
      "Batch size                      256\n",
      "Optimizer                      Adam\n",
      "Learning rate                0.0001\n",
      "Name: 26, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 3s 41ms/step - loss: 500.3647 - accuracy: 0.2480 - categorical_accuracy: 0.2480 - mean_directional_accuracy: 0.5070 - val_loss: 493.9133 - val_accuracy: 0.2213 - val_categorical_accuracy: 0.2213 - val_mean_directional_accuracy: 0.4854\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 488.7301 - accuracy: 0.2490 - categorical_accuracy: 0.2490 - mean_directional_accuracy: 0.4982 - val_loss: 482.3564 - val_accuracy: 0.2204 - val_categorical_accuracy: 0.2204 - val_mean_directional_accuracy: 0.4834\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 477.2140 - accuracy: 0.2510 - categorical_accuracy: 0.2510 - mean_directional_accuracy: 0.4979 - val_loss: 470.9352 - val_accuracy: 0.2188 - val_categorical_accuracy: 0.2188 - val_mean_directional_accuracy: 0.4837\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 465.8561 - accuracy: 0.2408 - categorical_accuracy: 0.2408 - mean_directional_accuracy: 0.4944 - val_loss: 459.6320 - val_accuracy: 0.2159 - val_categorical_accuracy: 0.2159 - val_mean_directional_accuracy: 0.4800\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 454.5629 - accuracy: 0.2568 - categorical_accuracy: 0.2568 - mean_directional_accuracy: 0.5058 - val_loss: 448.4366 - val_accuracy: 0.2135 - val_categorical_accuracy: 0.2135 - val_mean_directional_accuracy: 0.4782\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 443.4374 - accuracy: 0.2506 - categorical_accuracy: 0.2506 - mean_directional_accuracy: 0.5016 - val_loss: 437.3789 - val_accuracy: 0.2131 - val_categorical_accuracy: 0.2131 - val_mean_directional_accuracy: 0.4773\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 432.4193 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.5079 - val_loss: 426.4678 - val_accuracy: 0.2144 - val_categorical_accuracy: 0.2144 - val_mean_directional_accuracy: 0.4786\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 421.5633 - accuracy: 0.2559 - categorical_accuracy: 0.2559 - mean_directional_accuracy: 0.5088 - val_loss: 415.6989 - val_accuracy: 0.2175 - val_categorical_accuracy: 0.2175 - val_mean_directional_accuracy: 0.4802\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 410.8492 - accuracy: 0.2504 - categorical_accuracy: 0.2504 - mean_directional_accuracy: 0.5012 - val_loss: 405.0756 - val_accuracy: 0.2165 - val_categorical_accuracy: 0.2165 - val_mean_directional_accuracy: 0.4787\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 400.3008 - accuracy: 0.2578 - categorical_accuracy: 0.2578 - mean_directional_accuracy: 0.5061 - val_loss: 394.6176 - val_accuracy: 0.2174 - val_categorical_accuracy: 0.2174 - val_mean_directional_accuracy: 0.4755\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 389.8928 - accuracy: 0.2531 - categorical_accuracy: 0.2531 - mean_directional_accuracy: 0.5039 - val_loss: 384.2859 - val_accuracy: 0.2197 - val_categorical_accuracy: 0.2197 - val_mean_directional_accuracy: 0.4749\n",
      "Epoch 12/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 379.6167 - accuracy: 0.2494 - categorical_accuracy: 0.2494 - mean_directional_accuracy: 0.5021 - val_loss: 374.0713 - val_accuracy: 0.2216 - val_categorical_accuracy: 0.2216 - val_mean_directional_accuracy: 0.4744\n",
      "Epoch 13/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 369.4419 - accuracy: 0.2504 - categorical_accuracy: 0.2504 - mean_directional_accuracy: 0.5026 - val_loss: 363.9690 - val_accuracy: 0.2217 - val_categorical_accuracy: 0.2217 - val_mean_directional_accuracy: 0.4731\n",
      "Epoch 14/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 359.3987 - accuracy: 0.2547 - categorical_accuracy: 0.2547 - mean_directional_accuracy: 0.5019 - val_loss: 354.0096 - val_accuracy: 0.2208 - val_categorical_accuracy: 0.2208 - val_mean_directional_accuracy: 0.4712\n",
      "Epoch 15/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 349.5065 - accuracy: 0.2536 - categorical_accuracy: 0.2536 - mean_directional_accuracy: 0.5083 - val_loss: 344.2044 - val_accuracy: 0.2193 - val_categorical_accuracy: 0.2193 - val_mean_directional_accuracy: 0.4669\n",
      "Epoch 16/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 339.7577 - accuracy: 0.2548 - categorical_accuracy: 0.2548 - mean_directional_accuracy: 0.5014 - val_loss: 334.5502 - val_accuracy: 0.2214 - val_categorical_accuracy: 0.2214 - val_mean_directional_accuracy: 0.4675\n",
      "Epoch 17/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 330.1796 - accuracy: 0.2541 - categorical_accuracy: 0.2541 - mean_directional_accuracy: 0.5014 - val_loss: 325.0456 - val_accuracy: 0.2208 - val_categorical_accuracy: 0.2208 - val_mean_directional_accuracy: 0.4654\n",
      "Epoch 18/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 320.7393 - accuracy: 0.2532 - categorical_accuracy: 0.2532 - mean_directional_accuracy: 0.5065 - val_loss: 315.7016 - val_accuracy: 0.2216 - val_categorical_accuracy: 0.2216 - val_mean_directional_accuracy: 0.4639\n",
      "Epoch 19/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 311.4696 - accuracy: 0.2557 - categorical_accuracy: 0.2557 - mean_directional_accuracy: 0.5061 - val_loss: 306.5251 - val_accuracy: 0.2214 - val_categorical_accuracy: 0.2214 - val_mean_directional_accuracy: 0.4628\n",
      "Epoch 20/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 302.3639 - accuracy: 0.2492 - categorical_accuracy: 0.2492 - mean_directional_accuracy: 0.5060 - val_loss: 297.4957 - val_accuracy: 0.2230 - val_categorical_accuracy: 0.2230 - val_mean_directional_accuracy: 0.4624\n",
      "Epoch 21/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 293.4002 - accuracy: 0.2483 - categorical_accuracy: 0.2483 - mean_directional_accuracy: 0.5105 - val_loss: 288.6248 - val_accuracy: 0.2240 - val_categorical_accuracy: 0.2240 - val_mean_directional_accuracy: 0.4605\n",
      "Epoch 22/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 284.6048 - accuracy: 0.2545 - categorical_accuracy: 0.2545 - mean_directional_accuracy: 0.5126 - val_loss: 279.9233 - val_accuracy: 0.2240 - val_categorical_accuracy: 0.2240 - val_mean_directional_accuracy: 0.4596\n",
      "Epoch 23/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 275.9699 - accuracy: 0.2548 - categorical_accuracy: 0.2548 - mean_directional_accuracy: 0.5102 - val_loss: 271.3686 - val_accuracy: 0.2249 - val_categorical_accuracy: 0.2249 - val_mean_directional_accuracy: 0.4602\n",
      "Epoch 24/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 267.4915 - accuracy: 0.2520 - categorical_accuracy: 0.2520 - mean_directional_accuracy: 0.5014 - val_loss: 262.9743 - val_accuracy: 0.2252 - val_categorical_accuracy: 0.2252 - val_mean_directional_accuracy: 0.4590\n",
      "Epoch 25/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 259.1460 - accuracy: 0.2541 - categorical_accuracy: 0.2541 - mean_directional_accuracy: 0.5021 - val_loss: 254.6929 - val_accuracy: 0.2255 - val_categorical_accuracy: 0.2255 - val_mean_directional_accuracy: 0.4590\n",
      "Epoch 26/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 250.9307 - accuracy: 0.2534 - categorical_accuracy: 0.2534 - mean_directional_accuracy: 0.5025 - val_loss: 246.5593 - val_accuracy: 0.2264 - val_categorical_accuracy: 0.2264 - val_mean_directional_accuracy: 0.4586\n",
      "Epoch 27/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 242.8532 - accuracy: 0.2566 - categorical_accuracy: 0.2566 - mean_directional_accuracy: 0.5072 - val_loss: 238.5555 - val_accuracy: 0.2265 - val_categorical_accuracy: 0.2265 - val_mean_directional_accuracy: 0.4579\n",
      "Epoch 28/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 234.9256 - accuracy: 0.2548 - categorical_accuracy: 0.2548 - mean_directional_accuracy: 0.5040 - val_loss: 230.7116 - val_accuracy: 0.2265 - val_categorical_accuracy: 0.2265 - val_mean_directional_accuracy: 0.4581\n",
      "Epoch 29/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 227.1434 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.5044 - val_loss: 223.0063 - val_accuracy: 0.2266 - val_categorical_accuracy: 0.2266 - val_mean_directional_accuracy: 0.4582\n",
      "Epoch 30/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 219.4981 - accuracy: 0.2545 - categorical_accuracy: 0.2545 - mean_directional_accuracy: 0.4981 - val_loss: 215.4267 - val_accuracy: 0.2270 - val_categorical_accuracy: 0.2270 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 31/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 211.9728 - accuracy: 0.2496 - categorical_accuracy: 0.2496 - mean_directional_accuracy: 0.4951 - val_loss: 207.9631 - val_accuracy: 0.2270 - val_categorical_accuracy: 0.2270 - val_mean_directional_accuracy: 0.4585\n",
      "Epoch 32/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 204.5721 - accuracy: 0.2559 - categorical_accuracy: 0.2559 - mean_directional_accuracy: 0.4986 - val_loss: 200.6363 - val_accuracy: 0.2269 - val_categorical_accuracy: 0.2269 - val_mean_directional_accuracy: 0.4593\n",
      "Epoch 33/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 197.3016 - accuracy: 0.2604 - categorical_accuracy: 0.2604 - mean_directional_accuracy: 0.5112 - val_loss: 193.4437 - val_accuracy: 0.2276 - val_categorical_accuracy: 0.2276 - val_mean_directional_accuracy: 0.4594\n",
      "Epoch 34/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 190.1709 - accuracy: 0.2545 - categorical_accuracy: 0.2545 - mean_directional_accuracy: 0.5053 - val_loss: 186.3821 - val_accuracy: 0.2276 - val_categorical_accuracy: 0.2276 - val_mean_directional_accuracy: 0.4594\n",
      "Epoch 35/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 183.1716 - accuracy: 0.2613 - categorical_accuracy: 0.2613 - mean_directional_accuracy: 0.5112 - val_loss: 179.4633 - val_accuracy: 0.2276 - val_categorical_accuracy: 0.2276 - val_mean_directional_accuracy: 0.4594\n",
      "Epoch 36/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 176.3131 - accuracy: 0.2564 - categorical_accuracy: 0.2564 - mean_directional_accuracy: 0.5035 - val_loss: 172.6722 - val_accuracy: 0.2277 - val_categorical_accuracy: 0.2277 - val_mean_directional_accuracy: 0.4597\n",
      "Epoch 37/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 169.5912 - accuracy: 0.2566 - categorical_accuracy: 0.2566 - mean_directional_accuracy: 0.5028 - val_loss: 166.0300 - val_accuracy: 0.2279 - val_categorical_accuracy: 0.2279 - val_mean_directional_accuracy: 0.4592\n",
      "Epoch 38/300\n",
      "23/23 [==============================] - 4s 176ms/step - loss: 163.0120 - accuracy: 0.2580 - categorical_accuracy: 0.2580 - mean_directional_accuracy: 0.5076 - val_loss: 159.5199 - val_accuracy: 0.2283 - val_categorical_accuracy: 0.2283 - val_mean_directional_accuracy: 0.4594\n",
      "Epoch 39/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 156.5610 - accuracy: 0.2518 - categorical_accuracy: 0.2518 - mean_directional_accuracy: 0.5011 - val_loss: 153.1375 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4593\n",
      "Epoch 40/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 150.2375 - accuracy: 0.2559 - categorical_accuracy: 0.2559 - mean_directional_accuracy: 0.5023 - val_loss: 146.8894 - val_accuracy: 0.2286 - val_categorical_accuracy: 0.2286 - val_mean_directional_accuracy: 0.4589\n",
      "Epoch 41/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 144.0546 - accuracy: 0.2540 - categorical_accuracy: 0.2540 - mean_directional_accuracy: 0.5063 - val_loss: 140.7860 - val_accuracy: 0.2286 - val_categorical_accuracy: 0.2286 - val_mean_directional_accuracy: 0.4586\n",
      "Epoch 42/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 138.0173 - accuracy: 0.2655 - categorical_accuracy: 0.2655 - mean_directional_accuracy: 0.5088 - val_loss: 134.8198 - val_accuracy: 0.2286 - val_categorical_accuracy: 0.2286 - val_mean_directional_accuracy: 0.4585\n",
      "Epoch 43/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 132.1160 - accuracy: 0.2615 - categorical_accuracy: 0.2615 - mean_directional_accuracy: 0.5070 - val_loss: 128.9968 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4585\n",
      "Epoch 44/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 126.3598 - accuracy: 0.2638 - categorical_accuracy: 0.2638 - mean_directional_accuracy: 0.5049 - val_loss: 123.3212 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 120.7632 - accuracy: 0.2594 - categorical_accuracy: 0.2594 - mean_directional_accuracy: 0.5037 - val_loss: 117.8116 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 115.3122 - accuracy: 0.2610 - categorical_accuracy: 0.2610 - mean_directional_accuracy: 0.5076 - val_loss: 112.4258 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 109.9895 - accuracy: 0.2669 - categorical_accuracy: 0.2669 - mean_directional_accuracy: 0.5139 - val_loss: 107.1789 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 104.8073 - accuracy: 0.2619 - categorical_accuracy: 0.2619 - mean_directional_accuracy: 0.5070 - val_loss: 102.0694 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 99.7537 - accuracy: 0.2627 - categorical_accuracy: 0.2627 - mean_directional_accuracy: 0.5090 - val_loss: 97.0875 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 94.8403 - accuracy: 0.2643 - categorical_accuracy: 0.2643 - mean_directional_accuracy: 0.5068 - val_loss: 92.2504 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 90.0528 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5130 - val_loss: 87.5275 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 85.3959 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5128 - val_loss: 82.9438 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 80.8653 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5100 - val_loss: 78.4704 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 76.4461 - accuracy: 0.2669 - categorical_accuracy: 0.2669 - mean_directional_accuracy: 0.5111 - val_loss: 74.1120 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 72.1407 - accuracy: 0.2652 - categorical_accuracy: 0.2652 - mean_directional_accuracy: 0.5102 - val_loss: 69.8795 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 67.9680 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5107 - val_loss: 65.7702 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 57/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 63.9244 - accuracy: 0.2671 - categorical_accuracy: 0.2671 - mean_directional_accuracy: 0.5107 - val_loss: 61.8017 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 58/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 60.0194 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5104 - val_loss: 57.9645 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 56.2267 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5084 - val_loss: 54.2347 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 52.5594 - accuracy: 0.2671 - categorical_accuracy: 0.2671 - mean_directional_accuracy: 0.5100 - val_loss: 50.6373 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 49.0293 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 47.1851 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 45.6424 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5102 - val_loss: 43.8750 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 63/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 42.4032 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5105 - val_loss: 40.7151 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 64/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 39.3063 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5107 - val_loss: 37.6916 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 65/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 36.3398 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 34.7958 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 66/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 33.5032 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 32.0268 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 67/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 30.7919 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 29.3866 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 68/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 28.2184 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 26.8851 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 69/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 25.7769 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 24.5183 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 70/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 23.4746 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 22.2841 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 71/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 21.3027 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 20.1869 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 72/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 19.2657 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 18.2233 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 73/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 17.3612 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.3787 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 74/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 15.5677 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.6553 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 75/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 13.9023 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 13.0495 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 76/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 12.3501 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 11.5694 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 77/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 10.9409 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 10.2423 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 78/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 9.6886 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 9.0747 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 79/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 8.5889 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 8.0510 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 80/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 7.6359 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 7.1799 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 81/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 6.8238 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 6.4359 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 82/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 6.1449 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.8369 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 83/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 5.6164 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.3902 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 84/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 5.2291 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.0693 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 85/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 4.9581 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.8536 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 86/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 4.7708 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.6825 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 87/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 4.6115 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.5346 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 88/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 4.4699 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.4005 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 89/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 4.3414 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.2794 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 90/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 4.2282 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.1748 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 91/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 4.1267 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.0777 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 92/300\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 4.0366 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.9961 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 93/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 3.9596 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.9248 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 94/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 3.8939 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.8625 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 95/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 3.8330 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.8039 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 96/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 3.7746 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.7459 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 97/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 3.7176 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.6903 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 98/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 3.6621 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.6340 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 99/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 3.6067 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.5782 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 100/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 3.5513 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.5240 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 101/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 3.4961 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.4693 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 102/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 3.4407 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.4137 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 103/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 3.3857 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.3579 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 104/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 3.3305 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.3028 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 105/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 3.2752 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.2477 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 106/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 3.2202 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.1928 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 107/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 3.1650 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.1391 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 108/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 3.1117 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.0868 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 109/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 3.0590 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.0329 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 110/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 3.0061 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.9803 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 111/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 2.9534 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.9269 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 112/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 2.9006 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.8752 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 113/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 2.8478 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.8220 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 114/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 2.7953 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.7688 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 115/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 2.7424 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.7161 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 116/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 2.6900 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.6634 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 117/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 2.6374 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.6113 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 118/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 2.5852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.5609 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 119/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 2.5347 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.5118 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 120/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 2.4872 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.4650 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 121/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 2.4423 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.4218 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 122/300\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 2.3997 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.3803 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 123/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 2.3586 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.3405 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 124/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 2.3200 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.3031 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 125/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 2.2836 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.2686 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 126/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 2.2505 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.2356 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 127/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 2.2182 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.2024 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 128/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 2.1879 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.1749 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 129/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 2.1600 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.1486 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 130/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 2.1326 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.1210 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 131/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 2.1050 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.0934 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 132/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 2.0777 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.0664 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 133/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 2.0504 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.0389 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 134/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 2.0230 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.0105 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 135/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.9954 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9838 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 136/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 1.9681 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9561 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 137/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.9409 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9303 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 138/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.9149 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9039 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 139/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.8900 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8788 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 140/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.8652 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8556 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 141/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.8422 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8319 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 142/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.8192 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8099 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 143/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.7967 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7873 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 144/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.7739 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7650 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 145/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.7511 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7414 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 146/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.7283 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7202 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 147/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.7056 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6962 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 148/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.6832 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6757 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 149/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.6637 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6575 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 150/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.6455 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6390 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 151/300\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 1.6279 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6220 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 152/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.6118 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6050 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 153/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 1.5956 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5907 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 154/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.5811 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5771 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 155/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.5677 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5641 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 156/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.5570 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5549 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 157/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.5475 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5465 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 158/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.5383 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5361 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 159/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.5295 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5269 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 160/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.5202 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5182 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 161/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.5112 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5089 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 162/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.5023 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5016 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 163/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.4952 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4953 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 164/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.4905 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4908 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 165/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 1.4857 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4870 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 166/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4821 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4836 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 167/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.4797 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4811 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 168/300\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 1.4773 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4791 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 169/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4749 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4766 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 170/300\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 1.4729 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4752 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 171/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 1.4704 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4725 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 172/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.4682 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4703 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 173/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.4660 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4680 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 174/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.4636 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4658 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 175/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4615 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4635 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 176/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.4591 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4610 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 177/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.4571 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4588 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 178/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.4548 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4563 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 179/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.4524 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4556 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 180/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.4500 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4519 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 181/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.4479 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4493 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 182/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.4456 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4476 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 183/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.4450 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4472 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 184/300\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.4446 - accuracy: 0.2691 - categorical_accuracy: 0.2691 - mean_directional_accuracy: 0.5063Restoring model weights from the end of the best epoch: 183.\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.4449 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4490 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 184: early stopping\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4472 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.4471930265426636, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 183 \n",
      "\n",
      "Model time: 1.273288618773222 minutes\n",
      "\n",
      "Total time: 19.483539573848248 minutes\n",
      "\n",
      "\n",
      "Model  39  out of  111\n",
      "Model type              FeedForward\n",
      "Hidden layers                     3\n",
      "Hidden units           [16, 4, 256]\n",
      "Activation function            tanh\n",
      "Dropout                         0.4\n",
      "L1                              0.1\n",
      "L2                             0.01\n",
      "Batch size                      128\n",
      "Optimizer                   RMSprop\n",
      "Learning rate                 0.001\n",
      "Name: 27, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 3s 27ms/step - loss: 30.9382 - accuracy: 0.2482 - categorical_accuracy: 0.2482 - mean_directional_accuracy: 0.4893 - val_loss: 21.5696 - val_accuracy: 0.2282 - val_categorical_accuracy: 0.2282 - val_mean_directional_accuracy: 0.4582\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 15.2780 - accuracy: 0.2652 - categorical_accuracy: 0.2652 - mean_directional_accuracy: 0.5074 - val_loss: 9.6124 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 6.1651 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.5116 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.6794 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.3120 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.2225 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.1368 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.0617 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9884 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.9249 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8678 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.8137 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7640 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 1.7227 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6907 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.6674 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6523 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 1.6353 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6292 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "37/45 [=======================>......] - ETA: 0s - loss: 1.6259 - accuracy: 0.2669 - categorical_accuracy: 0.2669 - mean_directional_accuracy: 0.5106Restoring model weights from the end of the best epoch: 11.\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.6260 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6293 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12: early stopping\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 1.6292 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.6291707754135132, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 11 \n",
      "\n",
      "Model time: 0.17523379623889923 minutes\n",
      "\n",
      "Total time: 19.658856701105833 minutes\n",
      "\n",
      "\n",
      "Model  40  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    3\n",
      "Hidden units             [8, 1, 4]\n",
      "Activation function           tanh\n",
      "Dropout                        0.0\n",
      "L1                         0.00001\n",
      "L2                             0.0\n",
      "Batch size                      32\n",
      "Optimizer                     Adam\n",
      "Learning rate                 0.01\n",
      "Name: 28, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 5s 12ms/step - loss: 1.3750 - accuracy: 0.2975 - categorical_accuracy: 0.2975 - mean_directional_accuracy: 0.5358 - val_loss: 1.3744 - val_accuracy: 0.3058 - val_categorical_accuracy: 0.3058 - val_mean_directional_accuracy: 0.5235\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - ETA: 0s - loss: 1.3638 - accuracy: 0.3093 - categorical_accuracy: 0.3093 - mean_directional_accuracy: 0.5485Restoring model weights from the end of the best epoch: 1.\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.3638 - accuracy: 0.3093 - categorical_accuracy: 0.3093 - mean_directional_accuracy: 0.5485 - val_loss: 1.3818 - val_accuracy: 0.2839 - val_categorical_accuracy: 0.2839 - val_mean_directional_accuracy: 0.5098\n",
      "Epoch 2: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.3744 - accuracy: 0.3058 - categorical_accuracy: 0.3058 - mean_directional_accuracy: 0.5235\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} \n",
      " 1 \n",
      "\n",
      "Model time: 0.12177862972021103 minutes\n",
      "\n",
      "Total time: 19.78070203587413 minutes\n",
      "\n",
      "\n",
      "Model  41  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                  [16]\n",
      "Activation function        sigmoid\n",
      "Dropout                        0.0\n",
      "L1                             1.0\n",
      "L2                           100.0\n",
      "Batch size                     128\n",
      "Optimizer                  RMSprop\n",
      "Learning rate                 0.01\n",
      "Name: 29, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 3s 23ms/step - loss: 366.8416 - accuracy: 0.2545 - categorical_accuracy: 0.2545 - mean_directional_accuracy: 0.4982 - val_loss: 25.1257 - val_accuracy: 0.2296 - val_categorical_accuracy: 0.2296 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 22.2518 - accuracy: 0.2545 - categorical_accuracy: 0.2545 - mean_directional_accuracy: 0.5081 - val_loss: 22.0742 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "33/45 [=====================>........] - ETA: 0s - loss: 22.1232 - accuracy: 0.2689 - categorical_accuracy: 0.2689 - mean_directional_accuracy: 0.5123Restoring model weights from the end of the best epoch: 2.\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 22.1292 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 22.1476 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3: early stopping\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 22.0742 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 22.07417106628418, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 2 \n",
      "\n",
      "Model time: 0.07815312966704369 minutes\n",
      "\n",
      "Total time: 19.85893850773573 minutes\n",
      "\n",
      "\n",
      "Model  42  out of  111\n",
      "Model type                 FeedForward\n",
      "Hidden layers                        4\n",
      "Hidden units           [8, 64, 64, 16]\n",
      "Activation function               tanh\n",
      "Dropout                            0.5\n",
      "L1                                0.01\n",
      "L2                               0.001\n",
      "Batch size                          32\n",
      "Optimizer                         Adam\n",
      "Learning rate                    0.001\n",
      "Name: 30, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 4s 11ms/step - loss: 7.2175 - accuracy: 0.2441 - categorical_accuracy: 0.2441 - mean_directional_accuracy: 0.4965 - val_loss: 4.7892 - val_accuracy: 0.2535 - val_categorical_accuracy: 0.2535 - val_mean_directional_accuracy: 0.4911\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 3.3111 - accuracy: 0.2554 - categorical_accuracy: 0.2554 - mean_directional_accuracy: 0.5033 - val_loss: 2.2543 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.8552 - accuracy: 0.2671 - categorical_accuracy: 0.2671 - mean_directional_accuracy: 0.5100 - val_loss: 1.6015 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 2s 12ms/step - loss: 1.5010 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5102 - val_loss: 1.4530 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.4331 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4210 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.4072 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4011 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3937 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3944 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "174/178 [============================>.] - ETA: 0s - loss: 1.3917 - accuracy: 0.2674 - categorical_accuracy: 0.2674 - mean_directional_accuracy: 0.5090Restoring model weights from the end of the best epoch: 7.\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3917 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3947 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8: early stopping\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 1.3944 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.394399881362915, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 7 \n",
      "\n",
      "Model time: 0.40187787637114525 minutes\n",
      "\n",
      "Total time: 20.260925233364105 minutes\n",
      "\n",
      "\n",
      "Model  43  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    3\n",
      "Hidden units            [16, 4, 4]\n",
      "Activation function           relu\n",
      "Dropout                        0.0\n",
      "L1                             0.1\n",
      "L2                         0.00001\n",
      "Batch size                     128\n",
      "Optimizer                     Adam\n",
      "Learning rate               0.0001\n",
      "Name: 31, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 3s 25ms/step - loss: 28.3016 - accuracy: 0.2582 - categorical_accuracy: 0.2582 - mean_directional_accuracy: 0.5188 - val_loss: 27.6494 - val_accuracy: 0.2392 - val_categorical_accuracy: 0.2392 - val_mean_directional_accuracy: 0.4653\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 27.0632 - accuracy: 0.2617 - categorical_accuracy: 0.2617 - mean_directional_accuracy: 0.5186 - val_loss: 26.4315 - val_accuracy: 0.2415 - val_categorical_accuracy: 0.2415 - val_mean_directional_accuracy: 0.4671\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 25.8625 - accuracy: 0.2613 - categorical_accuracy: 0.2613 - mean_directional_accuracy: 0.5170 - val_loss: 25.2504 - val_accuracy: 0.2435 - val_categorical_accuracy: 0.2435 - val_mean_directional_accuracy: 0.4679\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 24.6987 - accuracy: 0.2597 - categorical_accuracy: 0.2597 - mean_directional_accuracy: 0.5149 - val_loss: 24.1049 - val_accuracy: 0.2453 - val_categorical_accuracy: 0.2453 - val_mean_directional_accuracy: 0.4712\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 23.5680 - accuracy: 0.2587 - categorical_accuracy: 0.2587 - mean_directional_accuracy: 0.5128 - val_loss: 22.9916 - val_accuracy: 0.2495 - val_categorical_accuracy: 0.2495 - val_mean_directional_accuracy: 0.4748\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 22.4696 - accuracy: 0.2590 - categorical_accuracy: 0.2590 - mean_directional_accuracy: 0.5146 - val_loss: 21.9094 - val_accuracy: 0.2512 - val_categorical_accuracy: 0.2512 - val_mean_directional_accuracy: 0.4791\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 21.4017 - accuracy: 0.2594 - categorical_accuracy: 0.2594 - mean_directional_accuracy: 0.5151 - val_loss: 20.8582 - val_accuracy: 0.2547 - val_categorical_accuracy: 0.2547 - val_mean_directional_accuracy: 0.4802\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 20.3645 - accuracy: 0.2564 - categorical_accuracy: 0.2564 - mean_directional_accuracy: 0.5153 - val_loss: 19.8362 - val_accuracy: 0.2543 - val_categorical_accuracy: 0.2543 - val_mean_directional_accuracy: 0.4791\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 19.3565 - accuracy: 0.2554 - categorical_accuracy: 0.2554 - mean_directional_accuracy: 0.5125 - val_loss: 18.8444 - val_accuracy: 0.2496 - val_categorical_accuracy: 0.2496 - val_mean_directional_accuracy: 0.4783\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 18.3777 - accuracy: 0.2554 - categorical_accuracy: 0.2554 - mean_directional_accuracy: 0.5197 - val_loss: 17.8800 - val_accuracy: 0.2475 - val_categorical_accuracy: 0.2475 - val_mean_directional_accuracy: 0.4785\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 17.4252 - accuracy: 0.2525 - categorical_accuracy: 0.2525 - mean_directional_accuracy: 0.5216 - val_loss: 16.9419 - val_accuracy: 0.2475 - val_categorical_accuracy: 0.2475 - val_mean_directional_accuracy: 0.4824\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 16.5018 - accuracy: 0.2531 - categorical_accuracy: 0.2531 - mean_directional_accuracy: 0.5248 - val_loss: 16.0366 - val_accuracy: 0.2465 - val_categorical_accuracy: 0.2465 - val_mean_directional_accuracy: 0.4853\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 15.6124 - accuracy: 0.2511 - categorical_accuracy: 0.2511 - mean_directional_accuracy: 0.5200 - val_loss: 15.1636 - val_accuracy: 0.2478 - val_categorical_accuracy: 0.2478 - val_mean_directional_accuracy: 0.4845\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 14.7517 - accuracy: 0.2485 - categorical_accuracy: 0.2485 - mean_directional_accuracy: 0.5128 - val_loss: 14.3179 - val_accuracy: 0.2413 - val_categorical_accuracy: 0.2413 - val_mean_directional_accuracy: 0.4824\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 13.9194 - accuracy: 0.2455 - categorical_accuracy: 0.2455 - mean_directional_accuracy: 0.5063 - val_loss: 13.5014 - val_accuracy: 0.2398 - val_categorical_accuracy: 0.2398 - val_mean_directional_accuracy: 0.4723\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 13.1167 - accuracy: 0.2536 - categorical_accuracy: 0.2536 - mean_directional_accuracy: 0.5137 - val_loss: 12.7133 - val_accuracy: 0.2367 - val_categorical_accuracy: 0.2367 - val_mean_directional_accuracy: 0.4704\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 12.3447 - accuracy: 0.2596 - categorical_accuracy: 0.2596 - mean_directional_accuracy: 0.5151 - val_loss: 11.9594 - val_accuracy: 0.2343 - val_categorical_accuracy: 0.2343 - val_mean_directional_accuracy: 0.4689\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 11.6053 - accuracy: 0.2610 - categorical_accuracy: 0.2610 - mean_directional_accuracy: 0.5162 - val_loss: 11.2371 - val_accuracy: 0.2326 - val_categorical_accuracy: 0.2326 - val_mean_directional_accuracy: 0.4658\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 10.9004 - accuracy: 0.2619 - categorical_accuracy: 0.2619 - mean_directional_accuracy: 0.5151 - val_loss: 10.5512 - val_accuracy: 0.2330 - val_categorical_accuracy: 0.2330 - val_mean_directional_accuracy: 0.4654\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 10.2305 - accuracy: 0.2659 - categorical_accuracy: 0.2659 - mean_directional_accuracy: 0.5165 - val_loss: 9.8974 - val_accuracy: 0.2333 - val_categorical_accuracy: 0.2333 - val_mean_directional_accuracy: 0.4659\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 9.5912 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5177 - val_loss: 9.2732 - val_accuracy: 0.2334 - val_categorical_accuracy: 0.2334 - val_mean_directional_accuracy: 0.4652\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 8.9799 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5162 - val_loss: 8.6770 - val_accuracy: 0.2329 - val_categorical_accuracy: 0.2329 - val_mean_directional_accuracy: 0.4658\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 8.3994 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5162 - val_loss: 8.1128 - val_accuracy: 0.2313 - val_categorical_accuracy: 0.2313 - val_mean_directional_accuracy: 0.4644\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 7.8480 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5169 - val_loss: 7.5742 - val_accuracy: 0.2312 - val_categorical_accuracy: 0.2312 - val_mean_directional_accuracy: 0.4648\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 7.3236 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5169 - val_loss: 7.0667 - val_accuracy: 0.2308 - val_categorical_accuracy: 0.2308 - val_mean_directional_accuracy: 0.4635\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 6.8333 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5156 - val_loss: 6.5945 - val_accuracy: 0.2311 - val_categorical_accuracy: 0.2311 - val_mean_directional_accuracy: 0.4639\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 6.3760 - accuracy: 0.2680 - categorical_accuracy: 0.2680 - mean_directional_accuracy: 0.5149 - val_loss: 6.1531 - val_accuracy: 0.2308 - val_categorical_accuracy: 0.2308 - val_mean_directional_accuracy: 0.4633\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 5.9492 - accuracy: 0.2689 - categorical_accuracy: 0.2689 - mean_directional_accuracy: 0.5140 - val_loss: 5.7406 - val_accuracy: 0.2315 - val_categorical_accuracy: 0.2315 - val_mean_directional_accuracy: 0.4636\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 5.5500 - accuracy: 0.2705 - categorical_accuracy: 0.2705 - mean_directional_accuracy: 0.5155 - val_loss: 5.3556 - val_accuracy: 0.2309 - val_categorical_accuracy: 0.2309 - val_mean_directional_accuracy: 0.4627\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 5.1789 - accuracy: 0.2694 - categorical_accuracy: 0.2694 - mean_directional_accuracy: 0.5139 - val_loss: 5.0004 - val_accuracy: 0.2308 - val_categorical_accuracy: 0.2308 - val_mean_directional_accuracy: 0.4624\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 4.8396 - accuracy: 0.2691 - categorical_accuracy: 0.2691 - mean_directional_accuracy: 0.5126 - val_loss: 4.6773 - val_accuracy: 0.2302 - val_categorical_accuracy: 0.2302 - val_mean_directional_accuracy: 0.4620\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 4.5293 - accuracy: 0.2692 - categorical_accuracy: 0.2692 - mean_directional_accuracy: 0.5137 - val_loss: 4.3800 - val_accuracy: 0.2298 - val_categorical_accuracy: 0.2298 - val_mean_directional_accuracy: 0.4612\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 4.2459 - accuracy: 0.2685 - categorical_accuracy: 0.2685 - mean_directional_accuracy: 0.5121 - val_loss: 4.1128 - val_accuracy: 0.2298 - val_categorical_accuracy: 0.2298 - val_mean_directional_accuracy: 0.4612\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 3.9934 - accuracy: 0.2684 - categorical_accuracy: 0.2684 - mean_directional_accuracy: 0.5112 - val_loss: 3.8749 - val_accuracy: 0.2293 - val_categorical_accuracy: 0.2293 - val_mean_directional_accuracy: 0.4598\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 3.7700 - accuracy: 0.2691 - categorical_accuracy: 0.2691 - mean_directional_accuracy: 0.5128 - val_loss: 3.6682 - val_accuracy: 0.2289 - val_categorical_accuracy: 0.2289 - val_mean_directional_accuracy: 0.4590\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 3.5799 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5109 - val_loss: 3.4942 - val_accuracy: 0.2291 - val_categorical_accuracy: 0.2291 - val_mean_directional_accuracy: 0.4585\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 3.4205 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5107 - val_loss: 3.3514 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 3.2933 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.2413 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 3.1995 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.1636 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 3.1339 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.1110 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 3.0910 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.0750 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 3.0572 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.0426 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 3.0251 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.0106 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.9933 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.9790 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.9618 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.9476 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.9303 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.9163 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.8991 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.8852 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.8683 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.8546 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.8378 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.8245 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 2.8080 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.7952 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.7789 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.7664 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.7503 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.7379 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.7218 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.7093 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.6934 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.6810 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.6652 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.6531 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.6375 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.6256 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.6101 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.5986 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.5832 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.5719 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.5566 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.5454 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.5303 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.5193 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.5044 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.4936 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.4788 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.4681 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 63/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 2.4534 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.4429 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 64/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 2.4284 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.4182 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 65/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 2.4041 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.3943 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 66/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 2.3805 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.3712 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 67/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.3578 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.3487 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 68/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 2.3354 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.3265 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 69/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 2.3130 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.3044 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 70/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.2914 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.2831 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 71/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 2.2704 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.2624 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 72/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.2498 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.2419 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 73/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 2.2292 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.2215 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 74/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.2092 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.2019 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 75/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 2.1898 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.1825 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 76/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 2.1705 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.1633 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 77/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.1513 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.1440 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 78/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.1320 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.1249 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 79/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.1130 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.1061 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 80/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.0943 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.0877 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 81/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.0763 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.0701 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 82/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.0591 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.0531 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 83/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 2.0420 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.0360 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 84/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.0249 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.0191 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 85/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.0079 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.0020 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 86/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 1.9910 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9854 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 87/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 1.9747 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9694 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 88/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.9589 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9537 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 89/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.9433 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9387 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 90/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 1.9290 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9248 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 91/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.9151 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9108 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 92/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.9011 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8969 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 93/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.8872 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8830 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 94/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.8733 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8693 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 95/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.8598 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8558 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 96/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.8463 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8424 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 97/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.8328 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8288 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 98/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.8193 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8154 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 99/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.8060 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8022 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 100/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 1.7929 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7892 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 101/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.7799 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7762 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 102/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.7670 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7637 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 103/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.7548 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7516 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 104/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.7429 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7399 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 105/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.7312 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7282 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 106/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.7199 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7173 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 107/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.7090 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7065 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 108/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 1.6984 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6964 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 109/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.6887 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6870 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 110/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.6795 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6780 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 111/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.6707 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6693 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 112/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.6621 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6609 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 113/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 1.6539 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6530 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 114/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.6462 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6455 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 115/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.6391 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6386 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 116/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.6323 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6320 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 117/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.6256 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6253 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 118/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.6192 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6191 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 119/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 1.6131 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6132 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 120/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.6073 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6074 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 121/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.6014 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6015 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 122/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.5956 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5960 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 123/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 1.5901 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5905 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 124/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.5847 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5852 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 125/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.5794 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5798 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 126/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.5740 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5744 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 127/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.5686 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5691 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 128/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.5636 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5642 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 129/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.5586 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5592 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 130/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 1.5537 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5543 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 131/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.5487 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5493 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 132/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.5438 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5444 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 133/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.5388 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5394 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 134/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.5339 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5345 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 135/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.5289 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5296 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 136/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.5240 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5246 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 137/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.5190 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5196 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 138/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.5141 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5148 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 139/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.5091 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5097 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 140/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.5042 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5049 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 141/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4992 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4999 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 142/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4943 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4949 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 143/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 1.4893 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4900 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 144/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.4845 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4854 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 145/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.4799 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4809 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 146/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4754 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4764 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 147/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4709 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4718 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 148/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4665 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4674 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 149/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4619 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4629 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 150/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4575 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4584 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 151/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4529 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4538 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 152/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4485 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4494 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 153/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4439 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4449 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 154/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 1.4395 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4405 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 155/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4349 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4359 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 156/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.4309 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4323 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 157/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4272 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4287 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 158/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4237 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4252 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 159/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4205 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4224 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 160/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4181 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4202 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 161/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4158 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4179 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 162/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.4136 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4157 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 163/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4113 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4134 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 164/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4091 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4112 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 165/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 1.4068 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4089 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 166/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4046 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4067 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 167/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 1.4023 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4044 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 168/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4001 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4023 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 169/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3981 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4005 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 170/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.3963 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3987 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 171/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.3945 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3969 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 172/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3931 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3958 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 173/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3921 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3949 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 174/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3912 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3941 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 175/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 1.3903 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3931 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 176/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.3894 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3923 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 177/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3885 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3915 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 178/300\n",
      "42/45 [===========================>..] - ETA: 0s - loss: 1.3883 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5100Restoring model weights from the end of the best epoch: 177.\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.3882 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3916 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 178: early stopping\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 1.3915 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3915352821350098, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 177 \n",
      "\n",
      "Model time: 1.448587566614151 minutes\n",
      "\n",
      "Total time: 21.709579452872276 minutes\n",
      "\n",
      "\n",
      "Model  44  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                   [8]\n",
      "Activation function           relu\n",
      "Dropout                        0.4\n",
      "L1                           100.0\n",
      "L2                          0.0001\n",
      "Batch size                      64\n",
      "Optimizer                     Adam\n",
      "Learning rate                0.001\n",
      "Name: 32, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 3s 15ms/step - loss: 8386.5781 - accuracy: 0.2511 - categorical_accuracy: 0.2511 - mean_directional_accuracy: 0.5139 - val_loss: 4138.1543 - val_accuracy: 0.2488 - val_categorical_accuracy: 0.2488 - val_mean_directional_accuracy: 0.5194\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1831.3634 - accuracy: 0.2462 - categorical_accuracy: 0.2462 - mean_directional_accuracy: 0.5153 - val_loss: 505.4821 - val_accuracy: 0.2295 - val_categorical_accuracy: 0.2295 - val_mean_directional_accuracy: 0.4581\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 384.5016 - accuracy: 0.2478 - categorical_accuracy: 0.2478 - mean_directional_accuracy: 0.5105 - val_loss: 298.9333 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 229.6633 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 167.6825 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 125.6882 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 88.9887 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 63.4726 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 40.7685 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 28.3379 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 21.1834 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 19.5022 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 19.0781 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "66/89 [=====================>........] - ETA: 0s - loss: 19.1529 - accuracy: 0.2730 - categorical_accuracy: 0.2730 - mean_directional_accuracy: 0.5104Restoring model weights from the end of the best epoch: 8.\n",
      "89/89 [==============================] - 0s 6ms/step - loss: 19.1817 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 19.6468 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9: early stopping\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 19.0781 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 19.078123092651367, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 8 \n",
      "\n",
      "Model time: 0.1464638076722622 minutes\n",
      "\n",
      "Total time: 21.85612777993083 minutes\n",
      "\n",
      "\n",
      "Model  45  out of  111\n",
      "Model type                  FeedForward\n",
      "Hidden layers                         4\n",
      "Hidden units           [128, 64, 64, 1]\n",
      "Activation function                relu\n",
      "Dropout                             0.1\n",
      "L1                                  0.0\n",
      "L2                                100.0\n",
      "Batch size                           64\n",
      "Optimizer                       RMSprop\n",
      "Learning rate                      0.01\n",
      "Name: 33, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 5s 19ms/step - loss: 957.1559 - accuracy: 0.2592 - categorical_accuracy: 0.2592 - mean_directional_accuracy: 0.4981 - val_loss: 89.7160 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 86.2652 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5042 - val_loss: 86.3377 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "84/89 [===========================>..] - ETA: 0s - loss: 86.3559 - accuracy: 0.2643 - categorical_accuracy: 0.2643 - mean_directional_accuracy: 0.5091Restoring model weights from the end of the best epoch: 2.\n",
      "89/89 [==============================] - 1s 17ms/step - loss: 86.3557 - accuracy: 0.2626 - categorical_accuracy: 0.2626 - mean_directional_accuracy: 0.5063 - val_loss: 86.3583 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3: early stopping\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 86.3377 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 86.3376693725586, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 2 \n",
      "\n",
      "Model time: 0.1325523816049099 minutes\n",
      "\n",
      "Total time: 21.988772623240948 minutes\n",
      "\n",
      "\n",
      "Model  46  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    3\n",
      "Hidden units             [1, 1, 1]\n",
      "Activation function        sigmoid\n",
      "Dropout                        0.8\n",
      "L1                             0.1\n",
      "L2                             0.0\n",
      "Batch size                      32\n",
      "Optimizer                     Adam\n",
      "Learning rate                 0.01\n",
      "Name: 34, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 4s 12ms/step - loss: 1.6055 - accuracy: 0.2638 - categorical_accuracy: 0.2638 - mean_directional_accuracy: 0.4986 - val_loss: 1.4109 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.4082 - accuracy: 0.2634 - categorical_accuracy: 0.2634 - mean_directional_accuracy: 0.5060 - val_loss: 1.4104 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.4075 - accuracy: 0.2634 - categorical_accuracy: 0.2634 - mean_directional_accuracy: 0.5063 - val_loss: 1.4096 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "177/178 [============================>.] - ETA: 0s - loss: 1.4079 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5111Restoring model weights from the end of the best epoch: 3.\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.4079 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5105 - val_loss: 1.4105 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.4096 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.4095666408538818, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 3 \n",
      "\n",
      "Model time: 0.16003908962011337 minutes\n",
      "\n",
      "Total time: 22.148878384381533 minutes\n",
      "\n",
      "\n",
      "Model  47  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                  [16]\n",
      "Activation function         linear\n",
      "Dropout                        0.0\n",
      "L1                           0.001\n",
      "L2                           0.001\n",
      "Batch size                      32\n",
      "Optimizer                  RMSprop\n",
      "Learning rate                 0.01\n",
      "Name: 35, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 4s 13ms/step - loss: 1.5122 - accuracy: 0.3010 - categorical_accuracy: 0.3010 - mean_directional_accuracy: 0.5188 - val_loss: 1.4120 - val_accuracy: 0.3051 - val_categorical_accuracy: 0.3051 - val_mean_directional_accuracy: 0.5372\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.4089 - accuracy: 0.3075 - categorical_accuracy: 0.3075 - mean_directional_accuracy: 0.5344 - val_loss: 1.4105 - val_accuracy: 0.2989 - val_categorical_accuracy: 0.2989 - val_mean_directional_accuracy: 0.5150\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.4001 - accuracy: 0.3221 - categorical_accuracy: 0.3221 - mean_directional_accuracy: 0.5443 - val_loss: 1.4100 - val_accuracy: 0.3006 - val_categorical_accuracy: 0.3006 - val_mean_directional_accuracy: 0.5207\n",
      "Epoch 4/300\n",
      "163/178 [==========================>...] - ETA: 0s - loss: 1.3998 - accuracy: 0.3119 - categorical_accuracy: 0.3119 - mean_directional_accuracy: 0.5337Restoring model weights from the end of the best epoch: 3.\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.4014 - accuracy: 0.3151 - categorical_accuracy: 0.3151 - mean_directional_accuracy: 0.5362 - val_loss: 1.4104 - val_accuracy: 0.2963 - val_categorical_accuracy: 0.2963 - val_mean_directional_accuracy: 0.5249\n",
      "Epoch 4: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.4100 - accuracy: 0.3006 - categorical_accuracy: 0.3006 - mean_directional_accuracy: 0.5207\n",
      "{'loss': 1.410033941268921, 'accuracy': 0.30062630772590637, 'categorical_accuracy': 0.30062630772590637, 'mean_directional_accuracy': 0.5207463502883911} \n",
      " 3 \n",
      "\n",
      "Model time: 0.15761017054319382 minutes\n",
      "\n",
      "Total time: 22.306571874767542 minutes\n",
      "\n",
      "\n",
      "Model  48  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                  [64]\n",
      "Activation function         linear\n",
      "Dropout                        0.7\n",
      "L1                            10.0\n",
      "L2                            0.01\n",
      "Batch size                     256\n",
      "Optimizer                     Adam\n",
      "Learning rate                0.001\n",
      "Name: 36, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 3s 66ms/step - loss: 7916.2866 - accuracy: 0.2410 - categorical_accuracy: 0.2410 - mean_directional_accuracy: 0.4921 - val_loss: 6684.4932 - val_accuracy: 0.2373 - val_categorical_accuracy: 0.2373 - val_mean_directional_accuracy: 0.4980\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 5735.6963 - accuracy: 0.2473 - categorical_accuracy: 0.2473 - mean_directional_accuracy: 0.5051 - val_loss: 4693.4170 - val_accuracy: 0.2330 - val_categorical_accuracy: 0.2330 - val_mean_directional_accuracy: 0.4915\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 3911.3994 - accuracy: 0.2369 - categorical_accuracy: 0.2369 - mean_directional_accuracy: 0.4935 - val_loss: 3064.8645 - val_accuracy: 0.2282 - val_categorical_accuracy: 0.2282 - val_mean_directional_accuracy: 0.4793\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 2448.5022 - accuracy: 0.2462 - categorical_accuracy: 0.2462 - mean_directional_accuracy: 0.4995 - val_loss: 1795.3596 - val_accuracy: 0.2273 - val_categorical_accuracy: 0.2273 - val_mean_directional_accuracy: 0.4722\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 1340.9685 - accuracy: 0.2460 - categorical_accuracy: 0.2460 - mean_directional_accuracy: 0.5100 - val_loss: 879.9520 - val_accuracy: 0.2269 - val_categorical_accuracy: 0.2269 - val_mean_directional_accuracy: 0.4666\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 592.6558 - accuracy: 0.2520 - categorical_accuracy: 0.2520 - mean_directional_accuracy: 0.5091 - val_loss: 321.9036 - val_accuracy: 0.2279 - val_categorical_accuracy: 0.2279 - val_mean_directional_accuracy: 0.4599\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 1s 39ms/step - loss: 197.5806 - accuracy: 0.2633 - categorical_accuracy: 0.2633 - mean_directional_accuracy: 0.5130 - val_loss: 120.8404 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 95.7611 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 72.5339 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 1s 34ms/step - loss: 60.6084 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 48.4086 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 1s 34ms/step - loss: 40.3602 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 32.2601 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 27.4859 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 22.5664 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 19.8403 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 17.4185 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 16.1985 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.7320 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 15.4129 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.2612 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "14/23 [=================>............] - ETA: 0s - loss: 15.2897 - accuracy: 0.2676 - categorical_accuracy: 0.2676 - mean_directional_accuracy: 0.5103Restoring model weights from the end of the best epoch: 14.\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 15.2811 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.2650 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15: early stopping\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 15.2612 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 15.261226654052734, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 14 \n",
      "\n",
      "Model time: 0.20150339603424072 minutes\n",
      "\n",
      "Total time: 22.508144322782755 minutes\n",
      "\n",
      "\n",
      "Model  49  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    3\n",
      "Hidden units            [4, 16, 4]\n",
      "Activation function           relu\n",
      "Dropout                        0.1\n",
      "L1                           100.0\n",
      "L2                         0.00001\n",
      "Batch size                      32\n",
      "Optimizer                     Adam\n",
      "Learning rate                0.001\n",
      "Name: 37, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 4s 12ms/step - loss: 5563.3535 - accuracy: 0.2554 - categorical_accuracy: 0.2554 - mean_directional_accuracy: 0.5054 - val_loss: 2319.9729 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1462.6644 - accuracy: 0.2550 - categorical_accuracy: 0.2550 - mean_directional_accuracy: 0.4895 - val_loss: 749.9952 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 1s 5ms/step - loss: 340.8750 - accuracy: 0.2622 - categorical_accuracy: 0.2622 - mean_directional_accuracy: 0.5018 - val_loss: 76.1853 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 33.6999 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.0611 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 12.0641 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 11.8695 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "173/178 [============================>.] - ETA: 0s - loss: 12.0545 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5098Restoring model weights from the end of the best epoch: 5.\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 12.0482 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.3960 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 11.8695 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 11.869491577148438, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 5 \n",
      "\n",
      "Model time: 0.1881183460354805 minutes\n",
      "\n",
      "Total time: 22.696362655609846 minutes\n",
      "\n",
      "\n",
      "Model  50  out of  111\n",
      "Model type                FeedForward\n",
      "Hidden layers                       3\n",
      "Hidden units           [128, 64, 256]\n",
      "Activation function              relu\n",
      "Dropout                           0.6\n",
      "L1                                1.0\n",
      "L2                               0.01\n",
      "Batch size                         16\n",
      "Optimizer                     RMSprop\n",
      "Learning rate                   0.001\n",
      "Name: 38, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 7s 12ms/step - loss: 427.6559 - accuracy: 0.2636 - categorical_accuracy: 0.2636 - mean_directional_accuracy: 0.5051 - val_loss: 24.9760 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "345/356 [============================>.] - ETA: 0s - loss: 24.9949 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5105Restoring model weights from the end of the best epoch: 1.\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 24.9951 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 24.9763 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2: early stopping\n",
      "479/479 [==============================] - 1s 3ms/step - loss: 24.9760 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 24.975975036621094, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 1 \n",
      "\n",
      "Model time: 0.19263632968068123 minutes\n",
      "\n",
      "Total time: 22.889048982411623 minutes\n",
      "\n",
      "\n",
      "Model  51  out of  111\n",
      "Model type               FeedForward\n",
      "Hidden layers                      3\n",
      "Hidden units           [8, 256, 128]\n",
      "Activation function          sigmoid\n",
      "Dropout                          0.1\n",
      "L1                               1.0\n",
      "L2                            0.0001\n",
      "Batch size                        16\n",
      "Optimizer                    RMSprop\n",
      "Learning rate                 0.0001\n",
      "Name: 39, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 8s 11ms/step - loss: 1762.7318 - accuracy: 0.2345 - categorical_accuracy: 0.2345 - mean_directional_accuracy: 0.4895 - val_loss: 1233.9512 - val_accuracy: 0.2602 - val_categorical_accuracy: 0.2602 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 829.1661 - accuracy: 0.2345 - categorical_accuracy: 0.2345 - mean_directional_accuracy: 0.4895 - val_loss: 482.3929 - val_accuracy: 0.2602 - val_categorical_accuracy: 0.2602 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 3/300\n",
      "356/356 [==============================] - 6s 18ms/step - loss: 256.9819 - accuracy: 0.2348 - categorical_accuracy: 0.2348 - mean_directional_accuracy: 0.4900 - val_loss: 90.2307 - val_accuracy: 0.2602 - val_categorical_accuracy: 0.2602 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 4/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 37.2461 - accuracy: 0.2406 - categorical_accuracy: 0.2406 - mean_directional_accuracy: 0.4923 - val_loss: 15.6740 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 5/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 9.1317 - accuracy: 0.2620 - categorical_accuracy: 0.2620 - mean_directional_accuracy: 0.5037 - val_loss: 4.9291 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 3.7464 - accuracy: 0.2573 - categorical_accuracy: 0.2573 - mean_directional_accuracy: 0.5097 - val_loss: 3.2328 - val_accuracy: 0.2296 - val_categorical_accuracy: 0.2296 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "343/356 [===========================>..] - ETA: 0s - loss: 3.2205 - accuracy: 0.2493 - categorical_accuracy: 0.2493 - mean_directional_accuracy: 0.5104Restoring model weights from the end of the best epoch: 6.\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 3.2205 - accuracy: 0.2506 - categorical_accuracy: 0.2506 - mean_directional_accuracy: 0.5105 - val_loss: 3.2338 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7: early stopping\n",
      "479/479 [==============================] - 2s 3ms/step - loss: 3.2328 - accuracy: 0.2296 - categorical_accuracy: 0.2296 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 3.232759475708008, 'accuracy': 0.22964508831501007, 'categorical_accuracy': 0.22964508831501007, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 6 \n",
      "\n",
      "Model time: 0.5027425400912762 minutes\n",
      "\n",
      "Total time: 23.39185817912221 minutes\n",
      "\n",
      "\n",
      "Model  52  out of  111\n",
      "Model type                 FeedForward\n",
      "Hidden layers                        4\n",
      "Hidden units           [4, 16, 64, 16]\n",
      "Activation function               relu\n",
      "Dropout                            0.2\n",
      "L1                                 1.0\n",
      "L2                               100.0\n",
      "Batch size                          32\n",
      "Optimizer                         Adam\n",
      "Learning rate                   0.0001\n",
      "Name: 40, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 5s 15ms/step - loss: 6974.7563 - accuracy: 0.2576 - categorical_accuracy: 0.2576 - mean_directional_accuracy: 0.5049 - val_loss: 6349.1333 - val_accuracy: 0.2248 - val_categorical_accuracy: 0.2248 - val_mean_directional_accuracy: 0.4765\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 5802.7573 - accuracy: 0.2343 - categorical_accuracy: 0.2343 - mean_directional_accuracy: 0.5000 - val_loss: 5282.3711 - val_accuracy: 0.2306 - val_categorical_accuracy: 0.2306 - val_mean_directional_accuracy: 0.4866\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 4826.4565 - accuracy: 0.2431 - categorical_accuracy: 0.2431 - mean_directional_accuracy: 0.4903 - val_loss: 4391.8267 - val_accuracy: 0.2418 - val_categorical_accuracy: 0.2418 - val_mean_directional_accuracy: 0.4898\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 4010.6624 - accuracy: 0.2494 - categorical_accuracy: 0.2494 - mean_directional_accuracy: 0.4944 - val_loss: 3647.2188 - val_accuracy: 0.2370 - val_categorical_accuracy: 0.2370 - val_mean_directional_accuracy: 0.4776\n",
      "Epoch 5/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 3328.4846 - accuracy: 0.2513 - categorical_accuracy: 0.2513 - mean_directional_accuracy: 0.4935 - val_loss: 3024.6445 - val_accuracy: 0.2265 - val_categorical_accuracy: 0.2265 - val_mean_directional_accuracy: 0.4596\n",
      "Epoch 6/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 2758.3813 - accuracy: 0.2599 - categorical_accuracy: 0.2599 - mean_directional_accuracy: 0.5042 - val_loss: 2504.6885 - val_accuracy: 0.2283 - val_categorical_accuracy: 0.2283 - val_mean_directional_accuracy: 0.4579\n",
      "Epoch 7/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 2282.6323 - accuracy: 0.2652 - categorical_accuracy: 0.2652 - mean_directional_accuracy: 0.5090 - val_loss: 2071.1919 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1886.3828 - accuracy: 0.2654 - categorical_accuracy: 0.2654 - mean_directional_accuracy: 0.5083 - val_loss: 1710.5505 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1557.1084 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5112 - val_loss: 1411.2478 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1284.1791 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5107 - val_loss: 1163.5009 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1058.5638 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 958.9875 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 872.5409 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 790.5880 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 719.5566 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 652.2612 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 594.0264 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 538.8898 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 491.2216 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 446.1105 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 407.1273 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 370.2383 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 338.3485 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 308.1660 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 282.0544 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 257.3234 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 235.8853 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 215.5620 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 197.8949 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 181.1098 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 166.4546 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 152.5043 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 140.2582 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 128.5713 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 118.2698 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 108.4214 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 99.6981 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 91.3407 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 25/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 83.9196 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 76.8045 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 70.4690 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 64.3851 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27/300\n",
      "178/178 [==============================] - 2s 8ms/step - loss: 58.9538 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 53.7351 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 28/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 49.0567 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 44.5509 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 29/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 40.5137 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 36.6353 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 30/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 33.1786 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 29.8680 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 31/300\n",
      "178/178 [==============================] - 2s 8ms/step - loss: 26.9341 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 24.1342 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 32/300\n",
      "178/178 [==============================] - 2s 8ms/step - loss: 21.6689 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 19.3247 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 33/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 17.2747 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.3344 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 13.6508 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.0651 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 10.6992 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 9.4197 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "178/178 [==============================] - 2s 8ms/step - loss: 8.3284 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 7.3132 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 6.4563 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.6655 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 5.0064 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.4038 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 3.9081 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.4596 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 3.0935 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.7667 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 2.5042 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.2730 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 2.0891 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9313 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.8077 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7051 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.6266 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5643 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "178/178 [==============================] - 2s 8ms/step - loss: 1.5156 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4803 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.4537 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4378 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.4248 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4218 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.4179 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4208 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "172/178 [===========================>..] - ETA: 0s - loss: 1.4183 - accuracy: 0.2674 - categorical_accuracy: 0.2674 - mean_directional_accuracy: 0.5111Restoring model weights from the end of the best epoch: 48.\n",
      "178/178 [==============================] - 2s 8ms/step - loss: 1.4182 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4215 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.4208 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.4208219051361084, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 48 \n",
      "\n",
      "Model time: 1.2885011807084084 minutes\n",
      "\n",
      "Total time: 24.680426008999348 minutes\n",
      "\n",
      "\n",
      "Model  53  out of  111\n",
      "Model type                    FeedForward\n",
      "Hidden layers                           4\n",
      "Hidden units           [32, 128, 64, 128]\n",
      "Activation function                linear\n",
      "Dropout                               0.8\n",
      "L1                                    0.0\n",
      "L2                                    0.1\n",
      "Batch size                             16\n",
      "Optimizer                            Adam\n",
      "Learning rate                        0.01\n",
      "Name: 41, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 6s 11ms/step - loss: 9.8120 - accuracy: 0.2513 - categorical_accuracy: 0.2513 - mean_directional_accuracy: 0.5030 - val_loss: 1.8808 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.5165 - accuracy: 0.2531 - categorical_accuracy: 0.2531 - mean_directional_accuracy: 0.4970 - val_loss: 1.4045 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3917 - accuracy: 0.2613 - categorical_accuracy: 0.2613 - mean_directional_accuracy: 0.5028 - val_loss: 1.3980 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3876 - accuracy: 0.2612 - categorical_accuracy: 0.2612 - mean_directional_accuracy: 0.5002 - val_loss: 1.3971 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3868 - accuracy: 0.2597 - categorical_accuracy: 0.2597 - mean_directional_accuracy: 0.5121 - val_loss: 1.3866 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "351/356 [============================>.] - ETA: 0s - loss: 1.3873 - accuracy: 0.2580 - categorical_accuracy: 0.2580 - mean_directional_accuracy: 0.4945Restoring model weights from the end of the best epoch: 5.\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3873 - accuracy: 0.2582 - categorical_accuracy: 0.2582 - mean_directional_accuracy: 0.4953 - val_loss: 1.3884 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6: early stopping\n",
      "479/479 [==============================] - 2s 3ms/step - loss: 1.3866 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3865879774093628, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 5 \n",
      "\n",
      "Model time: 0.40716452896595 minutes\n",
      "\n",
      "Total time: 25.087673868983984 minutes\n",
      "\n",
      "\n",
      "Model  54  out of  111\n",
      "Model type              FeedForward\n",
      "Hidden layers                     4\n",
      "Hidden units           [1, 8, 4, 4]\n",
      "Activation function         sigmoid\n",
      "Dropout                         0.8\n",
      "L1                           0.0001\n",
      "L2                              0.1\n",
      "Batch size                       16\n",
      "Optimizer                   RMSprop\n",
      "Learning rate                 0.001\n",
      "Name: 42, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 6s 9ms/step - loss: 2.3375 - accuracy: 0.2531 - categorical_accuracy: 0.2531 - mean_directional_accuracy: 0.5012 - val_loss: 1.7116 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.5647 - accuracy: 0.2406 - categorical_accuracy: 0.2406 - mean_directional_accuracy: 0.5058 - val_loss: 1.4405 - val_accuracy: 0.2296 - val_categorical_accuracy: 0.2296 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.4171 - accuracy: 0.2515 - categorical_accuracy: 0.2515 - mean_directional_accuracy: 0.5007 - val_loss: 1.4007 - val_accuracy: 0.2296 - val_categorical_accuracy: 0.2296 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3942 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.5090 - val_loss: 1.3913 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3877 - accuracy: 0.2620 - categorical_accuracy: 0.2620 - mean_directional_accuracy: 0.5070 - val_loss: 1.3887 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 6/300\n",
      "352/356 [============================>.] - ETA: 0s - loss: 1.3866 - accuracy: 0.2608 - categorical_accuracy: 0.2608 - mean_directional_accuracy: 0.5014Restoring model weights from the end of the best epoch: 5.\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3866 - accuracy: 0.2615 - categorical_accuracy: 0.2615 - mean_directional_accuracy: 0.5023 - val_loss: 1.3888 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6: early stopping\n",
      "479/479 [==============================] - 2s 3ms/step - loss: 1.3887 - accuracy: 0.2814 - categorical_accuracy: 0.2814 - mean_directional_accuracy: 0.5416\n",
      "{'loss': 1.3887158632278442, 'accuracy': 0.28144571185112, 'categorical_accuracy': 0.28144571185112, 'mean_directional_accuracy': 0.5416231751441956} \n",
      " 5 \n",
      "\n",
      "Model time: 0.3490302711725235 minutes\n",
      "\n",
      "Total time: 25.43685408681631 minutes\n",
      "\n",
      "\n",
      "Model  55  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                 [256]\n",
      "Activation function           relu\n",
      "Dropout                        0.9\n",
      "L1                             0.1\n",
      "L2                           100.0\n",
      "Batch size                      64\n",
      "Optimizer                  RMSprop\n",
      "Learning rate               0.0001\n",
      "Name: 43, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 4s 21ms/step - loss: 18727.4219 - accuracy: 0.2627 - categorical_accuracy: 0.2627 - mean_directional_accuracy: 0.5072 - val_loss: 16581.9512 - val_accuracy: 0.2663 - val_categorical_accuracy: 0.2663 - val_mean_directional_accuracy: 0.5069\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 14729.4639 - accuracy: 0.2624 - categorical_accuracy: 0.2624 - mean_directional_accuracy: 0.5074 - val_loss: 12934.3691 - val_accuracy: 0.2629 - val_categorical_accuracy: 0.2629 - val_mean_directional_accuracy: 0.5050\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 11372.3115 - accuracy: 0.2573 - categorical_accuracy: 0.2573 - mean_directional_accuracy: 0.5044 - val_loss: 9866.6318 - val_accuracy: 0.2637 - val_categorical_accuracy: 0.2637 - val_mean_directional_accuracy: 0.5099\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 8571.0107 - accuracy: 0.2522 - categorical_accuracy: 0.2522 - mean_directional_accuracy: 0.4933 - val_loss: 7329.8564 - val_accuracy: 0.2589 - val_categorical_accuracy: 0.2589 - val_mean_directional_accuracy: 0.5042\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 6275.9424 - accuracy: 0.2589 - categorical_accuracy: 0.2589 - mean_directional_accuracy: 0.5037 - val_loss: 5273.8169 - val_accuracy: 0.2504 - val_categorical_accuracy: 0.2504 - val_mean_directional_accuracy: 0.4990\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 4436.5757 - accuracy: 0.2450 - categorical_accuracy: 0.2450 - mean_directional_accuracy: 0.4889 - val_loss: 3647.7607 - val_accuracy: 0.2523 - val_categorical_accuracy: 0.2523 - val_mean_directional_accuracy: 0.5009\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 3001.9829 - accuracy: 0.2532 - categorical_accuracy: 0.2532 - mean_directional_accuracy: 0.4954 - val_loss: 2400.4783 - val_accuracy: 0.2447 - val_categorical_accuracy: 0.2447 - val_mean_directional_accuracy: 0.4868\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1920.5665 - accuracy: 0.2536 - categorical_accuracy: 0.2536 - mean_directional_accuracy: 0.5121 - val_loss: 1480.1680 - val_accuracy: 0.2370 - val_categorical_accuracy: 0.2370 - val_mean_directional_accuracy: 0.4721\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1140.7946 - accuracy: 0.2573 - categorical_accuracy: 0.2573 - mean_directional_accuracy: 0.5051 - val_loss: 835.7482 - val_accuracy: 0.2304 - val_categorical_accuracy: 0.2304 - val_mean_directional_accuracy: 0.4601\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 612.0728 - accuracy: 0.2620 - categorical_accuracy: 0.2620 - mean_directional_accuracy: 0.5056 - val_loss: 416.9629 - val_accuracy: 0.2291 - val_categorical_accuracy: 0.2291 - val_mean_directional_accuracy: 0.4586\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 284.1533 - accuracy: 0.2525 - categorical_accuracy: 0.2525 - mean_directional_accuracy: 0.4986 - val_loss: 173.6337 - val_accuracy: 0.2290 - val_categorical_accuracy: 0.2290 - val_mean_directional_accuracy: 0.4588\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 107.1917 - accuracy: 0.2661 - categorical_accuracy: 0.2661 - mean_directional_accuracy: 0.5107 - val_loss: 56.2789 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 31.9288 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5097 - val_loss: 15.9818 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 10.6454 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 7.1696 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 5.1606 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.5356 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 2.6352 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9936 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.7512 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6297 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6190 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6197 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "79/89 [=========================>....] - ETA: 0s - loss: 1.6178 - accuracy: 0.2682 - categorical_accuracy: 0.2682 - mean_directional_accuracy: 0.5129Restoring model weights from the end of the best epoch: 18.\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.6178 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6197 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19: early stopping\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 1.6197 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.6196986436843872, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 18 \n",
      "\n",
      "Model time: 0.3899179846048355 minutes\n",
      "\n",
      "Total time: 25.826922081410885 minutes\n",
      "\n",
      "\n",
      "Model  56  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    3\n",
      "Hidden units            [8, 64, 4]\n",
      "Activation function        sigmoid\n",
      "Dropout                        0.3\n",
      "L1                             0.0\n",
      "L2                         0.00001\n",
      "Batch size                      64\n",
      "Optimizer                  RMSprop\n",
      "Learning rate               0.0001\n",
      "Name: 44, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 4s 17ms/step - loss: 1.5024 - accuracy: 0.2687 - categorical_accuracy: 0.2687 - mean_directional_accuracy: 0.5107 - val_loss: 1.4836 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4626 - accuracy: 0.2650 - categorical_accuracy: 0.2650 - mean_directional_accuracy: 0.5086 - val_loss: 1.4510 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4470 - accuracy: 0.2585 - categorical_accuracy: 0.2585 - mean_directional_accuracy: 0.5056 - val_loss: 1.4286 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4253 - accuracy: 0.2622 - categorical_accuracy: 0.2622 - mean_directional_accuracy: 0.5065 - val_loss: 1.4147 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4115 - accuracy: 0.2599 - categorical_accuracy: 0.2599 - mean_directional_accuracy: 0.5076 - val_loss: 1.4057 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 1.3994 - accuracy: 0.2659 - categorical_accuracy: 0.2659 - mean_directional_accuracy: 0.5114 - val_loss: 1.4001 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 1.3993 - accuracy: 0.2590 - categorical_accuracy: 0.2590 - mean_directional_accuracy: 0.5002 - val_loss: 1.3961 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.3925 - accuracy: 0.2592 - categorical_accuracy: 0.2592 - mean_directional_accuracy: 0.5177 - val_loss: 1.3936 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3919 - accuracy: 0.2596 - categorical_accuracy: 0.2596 - mean_directional_accuracy: 0.5032 - val_loss: 1.3918 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3935 - accuracy: 0.2468 - categorical_accuracy: 0.2468 - mean_directional_accuracy: 0.5023 - val_loss: 1.3905 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 1.3900 - accuracy: 0.2645 - categorical_accuracy: 0.2645 - mean_directional_accuracy: 0.5119 - val_loss: 1.3897 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.3881 - accuracy: 0.2597 - categorical_accuracy: 0.2597 - mean_directional_accuracy: 0.5058 - val_loss: 1.3891 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3879 - accuracy: 0.2631 - categorical_accuracy: 0.2631 - mean_directional_accuracy: 0.5121 - val_loss: 1.3886 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3860 - accuracy: 0.2687 - categorical_accuracy: 0.2687 - mean_directional_accuracy: 0.5109 - val_loss: 1.3883 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3879 - accuracy: 0.2592 - categorical_accuracy: 0.2592 - mean_directional_accuracy: 0.4991 - val_loss: 1.3879 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3876 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.4965 - val_loss: 1.3875 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3870 - accuracy: 0.2589 - categorical_accuracy: 0.2589 - mean_directional_accuracy: 0.4975 - val_loss: 1.3874 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3873 - accuracy: 0.2585 - categorical_accuracy: 0.2585 - mean_directional_accuracy: 0.5121 - val_loss: 1.3872 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3858 - accuracy: 0.2627 - categorical_accuracy: 0.2627 - mean_directional_accuracy: 0.5004 - val_loss: 1.3870 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3872 - accuracy: 0.2576 - categorical_accuracy: 0.2576 - mean_directional_accuracy: 0.5121 - val_loss: 1.3869 - val_accuracy: 0.2289 - val_categorical_accuracy: 0.2289 - val_mean_directional_accuracy: 0.4585\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3865 - accuracy: 0.2531 - categorical_accuracy: 0.2531 - mean_directional_accuracy: 0.5025 - val_loss: 1.3868 - val_accuracy: 0.2289 - val_categorical_accuracy: 0.2289 - val_mean_directional_accuracy: 0.4585\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3857 - accuracy: 0.2596 - categorical_accuracy: 0.2596 - mean_directional_accuracy: 0.5002 - val_loss: 1.3868 - val_accuracy: 0.2293 - val_categorical_accuracy: 0.2293 - val_mean_directional_accuracy: 0.4589\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 1.3862 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5105 - val_loss: 1.3866 - val_accuracy: 0.2295 - val_categorical_accuracy: 0.2295 - val_mean_directional_accuracy: 0.4593\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3856 - accuracy: 0.2629 - categorical_accuracy: 0.2629 - mean_directional_accuracy: 0.4974 - val_loss: 1.3865 - val_accuracy: 0.2298 - val_categorical_accuracy: 0.2298 - val_mean_directional_accuracy: 0.4602\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3862 - accuracy: 0.2610 - categorical_accuracy: 0.2610 - mean_directional_accuracy: 0.5068 - val_loss: 1.3864 - val_accuracy: 0.2341 - val_categorical_accuracy: 0.2341 - val_mean_directional_accuracy: 0.4649\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3866 - accuracy: 0.2543 - categorical_accuracy: 0.2543 - mean_directional_accuracy: 0.4963 - val_loss: 1.3863 - val_accuracy: 0.2342 - val_categorical_accuracy: 0.2342 - val_mean_directional_accuracy: 0.4650\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3859 - accuracy: 0.2557 - categorical_accuracy: 0.2557 - mean_directional_accuracy: 0.5079 - val_loss: 1.3863 - val_accuracy: 0.2353 - val_categorical_accuracy: 0.2353 - val_mean_directional_accuracy: 0.4663\n",
      "Epoch 28/300\n",
      "73/89 [=======================>......] - ETA: 0s - loss: 1.3857 - accuracy: 0.2586 - categorical_accuracy: 0.2586 - mean_directional_accuracy: 0.5049Restoring model weights from the end of the best epoch: 27.\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3861 - accuracy: 0.2587 - categorical_accuracy: 0.2587 - mean_directional_accuracy: 0.5054 - val_loss: 1.3863 - val_accuracy: 0.2343 - val_categorical_accuracy: 0.2343 - val_mean_directional_accuracy: 0.4654\n",
      "Epoch 28: early stopping\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.3863 - accuracy: 0.2353 - categorical_accuracy: 0.2353 - mean_directional_accuracy: 0.4663\n",
      "{'loss': 1.3862577676773071, 'accuracy': 0.23525574803352356, 'categorical_accuracy': 0.23525574803352356, 'mean_directional_accuracy': 0.46633613109588623} \n",
      " 27 \n",
      "\n",
      "Model time: 0.4385038949549198 minutes\n",
      "\n",
      "Total time: 26.265515975654125 minutes\n",
      "\n",
      "\n",
      "Model  57  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                 [256]\n",
      "Activation function         linear\n",
      "Dropout                        0.4\n",
      "L1                         0.00001\n",
      "L2                          0.0001\n",
      "Batch size                      16\n",
      "Optimizer                     Adam\n",
      "Learning rate               0.0001\n",
      "Name: 45, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 6s 11ms/step - loss: 1.7006 - accuracy: 0.2631 - categorical_accuracy: 0.2631 - mean_directional_accuracy: 0.5051 - val_loss: 1.4597 - val_accuracy: 0.2803 - val_categorical_accuracy: 0.2803 - val_mean_directional_accuracy: 0.5061\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.5909 - accuracy: 0.2829 - categorical_accuracy: 0.2829 - mean_directional_accuracy: 0.5126 - val_loss: 1.4509 - val_accuracy: 0.2811 - val_categorical_accuracy: 0.2811 - val_mean_directional_accuracy: 0.5016\n",
      "Epoch 3/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.5659 - accuracy: 0.2977 - categorical_accuracy: 0.2977 - mean_directional_accuracy: 0.5193 - val_loss: 1.4428 - val_accuracy: 0.2868 - val_categorical_accuracy: 0.2868 - val_mean_directional_accuracy: 0.5020\n",
      "Epoch 4/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 1.5296 - accuracy: 0.3044 - categorical_accuracy: 0.3044 - mean_directional_accuracy: 0.5314 - val_loss: 1.4425 - val_accuracy: 0.2846 - val_categorical_accuracy: 0.2846 - val_mean_directional_accuracy: 0.5042\n",
      "Epoch 5/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.4965 - accuracy: 0.3105 - categorical_accuracy: 0.3105 - mean_directional_accuracy: 0.5242 - val_loss: 1.4356 - val_accuracy: 0.2838 - val_categorical_accuracy: 0.2838 - val_mean_directional_accuracy: 0.5034\n",
      "Epoch 6/300\n",
      "348/356 [============================>.] - ETA: 0s - loss: 1.5091 - accuracy: 0.3105 - categorical_accuracy: 0.3105 - mean_directional_accuracy: 0.5365Restoring model weights from the end of the best epoch: 5.\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.5099 - accuracy: 0.3100 - categorical_accuracy: 0.3100 - mean_directional_accuracy: 0.5364 - val_loss: 1.4472 - val_accuracy: 0.2795 - val_categorical_accuracy: 0.2795 - val_mean_directional_accuracy: 0.4979\n",
      "Epoch 6: early stopping\n",
      "479/479 [==============================] - 2s 3ms/step - loss: 1.4356 - accuracy: 0.2838 - categorical_accuracy: 0.2838 - mean_directional_accuracy: 0.5034\n",
      "{'loss': 1.4355524778366089, 'accuracy': 0.2837943732738495, 'categorical_accuracy': 0.2837943732738495, 'mean_directional_accuracy': 0.5033924579620361} \n",
      " 5 \n",
      "\n",
      "Model time: 0.36734845116734505 minutes\n",
      "\n",
      "Total time: 26.63293109461665 minutes\n",
      "\n",
      "\n",
      "Model  58  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                   [4]\n",
      "Activation function           tanh\n",
      "Dropout                        0.3\n",
      "L1                             1.0\n",
      "L2                             0.1\n",
      "Batch size                     128\n",
      "Optimizer                  RMSprop\n",
      "Learning rate                0.001\n",
      "Name: 46, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 3s 24ms/step - loss: 54.4194 - accuracy: 0.2525 - categorical_accuracy: 0.2525 - mean_directional_accuracy: 0.5044 - val_loss: 40.5214 - val_accuracy: 0.2440 - val_categorical_accuracy: 0.2440 - val_mean_directional_accuracy: 0.4851\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 30.9029 - accuracy: 0.2480 - categorical_accuracy: 0.2480 - mean_directional_accuracy: 0.5011 - val_loss: 21.8648 - val_accuracy: 0.2513 - val_categorical_accuracy: 0.2513 - val_mean_directional_accuracy: 0.5044\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 15.8674 - accuracy: 0.2562 - categorical_accuracy: 0.2562 - mean_directional_accuracy: 0.5144 - val_loss: 10.6512 - val_accuracy: 0.2593 - val_categorical_accuracy: 0.2593 - val_mean_directional_accuracy: 0.5140\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 8.1883 - accuracy: 0.2590 - categorical_accuracy: 0.2590 - mean_directional_accuracy: 0.5137 - val_loss: 6.8246 - val_accuracy: 0.2379 - val_categorical_accuracy: 0.2379 - val_mean_directional_accuracy: 0.4789\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 6.4989 - accuracy: 0.2626 - categorical_accuracy: 0.2626 - mean_directional_accuracy: 0.4989 - val_loss: 6.1564 - val_accuracy: 0.2286 - val_categorical_accuracy: 0.2286 - val_mean_directional_accuracy: 0.4586\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 5.8314 - accuracy: 0.2657 - categorical_accuracy: 0.2657 - mean_directional_accuracy: 0.5104 - val_loss: 5.4884 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 5.1923 - accuracy: 0.2699 - categorical_accuracy: 0.2699 - mean_directional_accuracy: 0.5140 - val_loss: 4.8791 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 4.5825 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5105 - val_loss: 4.2926 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 4.0593 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.8315 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 3.6378 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.4553 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 3.2912 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.1400 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 3.0049 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.8613 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.7255 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.5820 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.4704 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.3631 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 2.2652 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.1661 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 2.0814 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9968 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 1.9096 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8419 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 1.8008 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7666 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 1.7398 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7291 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "41/45 [==========================>...] - ETA: 0s - loss: 1.7310 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5118Restoring model weights from the end of the best epoch: 19.\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.7311 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7393 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20: early stopping\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 1.7291 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.729062795639038, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 19 \n",
      "\n",
      "Model time: 0.20480314269661903 minutes\n",
      "\n",
      "Total time: 26.837818253785372 minutes\n",
      "\n",
      "\n",
      "Model  59  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    3\n",
      "Hidden units             [2, 1, 4]\n",
      "Activation function        sigmoid\n",
      "Dropout                        0.2\n",
      "L1                            10.0\n",
      "L2                          0.0001\n",
      "Batch size                      64\n",
      "Optimizer                     Adam\n",
      "Learning rate                0.001\n",
      "Name: 47, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 4s 17ms/step - loss: 305.5458 - accuracy: 0.2511 - categorical_accuracy: 0.2511 - mean_directional_accuracy: 0.5098 - val_loss: 191.5272 - val_accuracy: 0.2296 - val_categorical_accuracy: 0.2296 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 130.2301 - accuracy: 0.2460 - categorical_accuracy: 0.2460 - mean_directional_accuracy: 0.5009 - val_loss: 91.3106 - val_accuracy: 0.2296 - val_categorical_accuracy: 0.2296 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 82.1510 - accuracy: 0.2503 - categorical_accuracy: 0.2503 - mean_directional_accuracy: 0.5079 - val_loss: 73.6795 - val_accuracy: 0.2296 - val_categorical_accuracy: 0.2296 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 65.7737 - accuracy: 0.2531 - categorical_accuracy: 0.2531 - mean_directional_accuracy: 0.5033 - val_loss: 57.6585 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 50.0400 - accuracy: 0.2483 - categorical_accuracy: 0.2483 - mean_directional_accuracy: 0.5044 - val_loss: 42.5868 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 36.3898 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5121 - val_loss: 30.5875 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 25.8298 - accuracy: 0.2585 - categorical_accuracy: 0.2585 - mean_directional_accuracy: 0.4954 - val_loss: 21.1938 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 17.4163 - accuracy: 0.2617 - categorical_accuracy: 0.2617 - mean_directional_accuracy: 0.5012 - val_loss: 13.9001 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 11.4270 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 9.2969 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 7.5447 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.8885 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 4.5692 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.2256 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 2.3116 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8543 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.8304 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8460 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8281 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8147 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "86/89 [===========================>..] - ETA: 0s - loss: 1.8271 - accuracy: 0.2698 - categorical_accuracy: 0.2698 - mean_directional_accuracy: 0.5142Restoring model weights from the end of the best epoch: 14.\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.8277 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8430 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15: early stopping\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.8147 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.8146750926971436, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 14 \n",
      "\n",
      "Model time: 0.25141380727291107 minutes\n",
      "\n",
      "Total time: 27.089332073926926 minutes\n",
      "\n",
      "\n",
      "Model  60  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    2\n",
      "Hidden units              [2, 256]\n",
      "Activation function           relu\n",
      "Dropout                        0.7\n",
      "L1                           100.0\n",
      "L2                            10.0\n",
      "Batch size                      64\n",
      "Optimizer                     Adam\n",
      "Learning rate                 0.01\n",
      "Name: 48, dtype: object\n",
      "NN2Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 4s 19ms/step - loss: 1612.5560 - accuracy: 0.2654 - categorical_accuracy: 0.2654 - mean_directional_accuracy: 0.5046 - val_loss: 247.7832 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 243.0005 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 243.6789 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 240.5262 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5088 - val_loss: 241.6072 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 239.1634 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5123 - val_loss: 229.3501 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 237.1059 - accuracy: 0.2631 - categorical_accuracy: 0.2631 - mean_directional_accuracy: 0.5076 - val_loss: 227.2600 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "88/89 [============================>.] - ETA: 0s - loss: 236.7862 - accuracy: 0.2591 - categorical_accuracy: 0.2591 - mean_directional_accuracy: 0.4995Restoring model weights from the end of the best epoch: 5.\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 236.8472 - accuracy: 0.2601 - categorical_accuracy: 0.2601 - mean_directional_accuracy: 0.4998 - val_loss: 236.9234 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6: early stopping\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 227.2600 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 227.25999450683594, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 5 \n",
      "\n",
      "Model time: 0.16328369081020355 minutes\n",
      "\n",
      "Total time: 27.25270412489772 minutes\n",
      "\n",
      "\n",
      "Model  61  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                   [8]\n",
      "Activation function           tanh\n",
      "Dropout                        0.2\n",
      "L1                          0.0001\n",
      "L2                            0.01\n",
      "Batch size                      16\n",
      "Optimizer                     Adam\n",
      "Learning rate               0.0001\n",
      "Name: 49, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 4s 7ms/step - loss: 1.7174 - accuracy: 0.2648 - categorical_accuracy: 0.2648 - mean_directional_accuracy: 0.4958 - val_loss: 1.6443 - val_accuracy: 0.2513 - val_categorical_accuracy: 0.2513 - val_mean_directional_accuracy: 0.4816\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.6486 - accuracy: 0.2636 - categorical_accuracy: 0.2636 - mean_directional_accuracy: 0.4972 - val_loss: 1.5970 - val_accuracy: 0.2614 - val_categorical_accuracy: 0.2614 - val_mean_directional_accuracy: 0.4947\n",
      "Epoch 3/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.5943 - accuracy: 0.2787 - categorical_accuracy: 0.2787 - mean_directional_accuracy: 0.5091 - val_loss: 1.5633 - val_accuracy: 0.2688 - val_categorical_accuracy: 0.2688 - val_mean_directional_accuracy: 0.5008\n",
      "Epoch 4/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 1.5589 - accuracy: 0.2785 - categorical_accuracy: 0.2785 - mean_directional_accuracy: 0.5047 - val_loss: 1.5389 - val_accuracy: 0.2775 - val_categorical_accuracy: 0.2775 - val_mean_directional_accuracy: 0.5093\n",
      "Epoch 5/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.5332 - accuracy: 0.2949 - categorical_accuracy: 0.2949 - mean_directional_accuracy: 0.5207 - val_loss: 1.5196 - val_accuracy: 0.2838 - val_categorical_accuracy: 0.2838 - val_mean_directional_accuracy: 0.5177\n",
      "Epoch 6/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.5171 - accuracy: 0.2989 - categorical_accuracy: 0.2989 - mean_directional_accuracy: 0.5276 - val_loss: 1.5046 - val_accuracy: 0.2874 - val_categorical_accuracy: 0.2874 - val_mean_directional_accuracy: 0.5221\n",
      "Epoch 7/300\n",
      "356/356 [==============================] - 2s 5ms/step - loss: 1.4960 - accuracy: 0.3061 - categorical_accuracy: 0.3061 - mean_directional_accuracy: 0.5281 - val_loss: 1.4927 - val_accuracy: 0.2868 - val_categorical_accuracy: 0.2868 - val_mean_directional_accuracy: 0.5234\n",
      "Epoch 8/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4812 - accuracy: 0.3082 - categorical_accuracy: 0.3082 - mean_directional_accuracy: 0.5335 - val_loss: 1.4825 - val_accuracy: 0.2912 - val_categorical_accuracy: 0.2912 - val_mean_directional_accuracy: 0.5283\n",
      "Epoch 9/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4708 - accuracy: 0.3116 - categorical_accuracy: 0.3116 - mean_directional_accuracy: 0.5353 - val_loss: 1.4738 - val_accuracy: 0.2968 - val_categorical_accuracy: 0.2968 - val_mean_directional_accuracy: 0.5343\n",
      "Epoch 10/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.4618 - accuracy: 0.3147 - categorical_accuracy: 0.3147 - mean_directional_accuracy: 0.5386 - val_loss: 1.4667 - val_accuracy: 0.2976 - val_categorical_accuracy: 0.2976 - val_mean_directional_accuracy: 0.5339\n",
      "Epoch 11/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4561 - accuracy: 0.3214 - categorical_accuracy: 0.3214 - mean_directional_accuracy: 0.5462 - val_loss: 1.4602 - val_accuracy: 0.3023 - val_categorical_accuracy: 0.3023 - val_mean_directional_accuracy: 0.5398\n",
      "Epoch 12/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.4445 - accuracy: 0.3265 - categorical_accuracy: 0.3265 - mean_directional_accuracy: 0.5474 - val_loss: 1.4553 - val_accuracy: 0.2981 - val_categorical_accuracy: 0.2981 - val_mean_directional_accuracy: 0.5358\n",
      "Epoch 13/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4421 - accuracy: 0.3251 - categorical_accuracy: 0.3251 - mean_directional_accuracy: 0.5427 - val_loss: 1.4503 - val_accuracy: 0.2993 - val_categorical_accuracy: 0.2993 - val_mean_directional_accuracy: 0.5376\n",
      "Epoch 14/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.4347 - accuracy: 0.3242 - categorical_accuracy: 0.3242 - mean_directional_accuracy: 0.5430 - val_loss: 1.4461 - val_accuracy: 0.2996 - val_categorical_accuracy: 0.2996 - val_mean_directional_accuracy: 0.5385\n",
      "Epoch 15/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4265 - accuracy: 0.3303 - categorical_accuracy: 0.3303 - mean_directional_accuracy: 0.5501 - val_loss: 1.4422 - val_accuracy: 0.2989 - val_categorical_accuracy: 0.2989 - val_mean_directional_accuracy: 0.5373\n",
      "Epoch 16/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4244 - accuracy: 0.3351 - categorical_accuracy: 0.3351 - mean_directional_accuracy: 0.5548 - val_loss: 1.4386 - val_accuracy: 0.3014 - val_categorical_accuracy: 0.3014 - val_mean_directional_accuracy: 0.5371\n",
      "Epoch 17/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.4207 - accuracy: 0.3323 - categorical_accuracy: 0.3323 - mean_directional_accuracy: 0.5594 - val_loss: 1.4357 - val_accuracy: 0.3002 - val_categorical_accuracy: 0.3002 - val_mean_directional_accuracy: 0.5339\n",
      "Epoch 18/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.4166 - accuracy: 0.3354 - categorical_accuracy: 0.3354 - mean_directional_accuracy: 0.5567 - val_loss: 1.4324 - val_accuracy: 0.3015 - val_categorical_accuracy: 0.3015 - val_mean_directional_accuracy: 0.5342\n",
      "Epoch 19/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.4114 - accuracy: 0.3346 - categorical_accuracy: 0.3346 - mean_directional_accuracy: 0.5595 - val_loss: 1.4298 - val_accuracy: 0.3002 - val_categorical_accuracy: 0.3002 - val_mean_directional_accuracy: 0.5313\n",
      "Epoch 20/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.4094 - accuracy: 0.3319 - categorical_accuracy: 0.3319 - mean_directional_accuracy: 0.5518 - val_loss: 1.4274 - val_accuracy: 0.2998 - val_categorical_accuracy: 0.2998 - val_mean_directional_accuracy: 0.5283\n",
      "Epoch 21/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.4057 - accuracy: 0.3423 - categorical_accuracy: 0.3423 - mean_directional_accuracy: 0.5623 - val_loss: 1.4249 - val_accuracy: 0.3004 - val_categorical_accuracy: 0.3004 - val_mean_directional_accuracy: 0.5300\n",
      "Epoch 22/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.4029 - accuracy: 0.3386 - categorical_accuracy: 0.3386 - mean_directional_accuracy: 0.5594 - val_loss: 1.4227 - val_accuracy: 0.3026 - val_categorical_accuracy: 0.3026 - val_mean_directional_accuracy: 0.5313\n",
      "Epoch 23/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.4022 - accuracy: 0.3411 - categorical_accuracy: 0.3411 - mean_directional_accuracy: 0.5581 - val_loss: 1.4210 - val_accuracy: 0.3004 - val_categorical_accuracy: 0.3004 - val_mean_directional_accuracy: 0.5295\n",
      "Epoch 24/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3971 - accuracy: 0.3425 - categorical_accuracy: 0.3425 - mean_directional_accuracy: 0.5569 - val_loss: 1.4192 - val_accuracy: 0.2989 - val_categorical_accuracy: 0.2989 - val_mean_directional_accuracy: 0.5279\n",
      "Epoch 25/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3961 - accuracy: 0.3372 - categorical_accuracy: 0.3372 - mean_directional_accuracy: 0.5645 - val_loss: 1.4176 - val_accuracy: 0.2978 - val_categorical_accuracy: 0.2978 - val_mean_directional_accuracy: 0.5254\n",
      "Epoch 26/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3927 - accuracy: 0.3397 - categorical_accuracy: 0.3397 - mean_directional_accuracy: 0.5636 - val_loss: 1.4156 - val_accuracy: 0.3001 - val_categorical_accuracy: 0.3001 - val_mean_directional_accuracy: 0.5257\n",
      "Epoch 27/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3946 - accuracy: 0.3397 - categorical_accuracy: 0.3397 - mean_directional_accuracy: 0.5588 - val_loss: 1.4137 - val_accuracy: 0.3028 - val_categorical_accuracy: 0.3028 - val_mean_directional_accuracy: 0.5287\n",
      "Epoch 28/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 1.3898 - accuracy: 0.3476 - categorical_accuracy: 0.3476 - mean_directional_accuracy: 0.5616 - val_loss: 1.4123 - val_accuracy: 0.3022 - val_categorical_accuracy: 0.3022 - val_mean_directional_accuracy: 0.5288\n",
      "Epoch 29/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3882 - accuracy: 0.3414 - categorical_accuracy: 0.3414 - mean_directional_accuracy: 0.5653 - val_loss: 1.4110 - val_accuracy: 0.2996 - val_categorical_accuracy: 0.2996 - val_mean_directional_accuracy: 0.5256\n",
      "Epoch 30/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3875 - accuracy: 0.3400 - categorical_accuracy: 0.3400 - mean_directional_accuracy: 0.5604 - val_loss: 1.4095 - val_accuracy: 0.3000 - val_categorical_accuracy: 0.3000 - val_mean_directional_accuracy: 0.5253\n",
      "Epoch 31/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3844 - accuracy: 0.3390 - categorical_accuracy: 0.3390 - mean_directional_accuracy: 0.5616 - val_loss: 1.4081 - val_accuracy: 0.3001 - val_categorical_accuracy: 0.3001 - val_mean_directional_accuracy: 0.5258\n",
      "Epoch 32/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3850 - accuracy: 0.3481 - categorical_accuracy: 0.3481 - mean_directional_accuracy: 0.5695 - val_loss: 1.4072 - val_accuracy: 0.2998 - val_categorical_accuracy: 0.2998 - val_mean_directional_accuracy: 0.5241\n",
      "Epoch 33/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3822 - accuracy: 0.3456 - categorical_accuracy: 0.3456 - mean_directional_accuracy: 0.5653 - val_loss: 1.4062 - val_accuracy: 0.3001 - val_categorical_accuracy: 0.3001 - val_mean_directional_accuracy: 0.5243\n",
      "Epoch 34/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3827 - accuracy: 0.3381 - categorical_accuracy: 0.3381 - mean_directional_accuracy: 0.5576 - val_loss: 1.4050 - val_accuracy: 0.2993 - val_categorical_accuracy: 0.2993 - val_mean_directional_accuracy: 0.5241\n",
      "Epoch 35/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3816 - accuracy: 0.3391 - categorical_accuracy: 0.3391 - mean_directional_accuracy: 0.5657 - val_loss: 1.4037 - val_accuracy: 0.2998 - val_categorical_accuracy: 0.2998 - val_mean_directional_accuracy: 0.5240\n",
      "Epoch 36/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3776 - accuracy: 0.3481 - categorical_accuracy: 0.3481 - mean_directional_accuracy: 0.5623 - val_loss: 1.4029 - val_accuracy: 0.3000 - val_categorical_accuracy: 0.3000 - val_mean_directional_accuracy: 0.5236\n",
      "Epoch 37/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3771 - accuracy: 0.3502 - categorical_accuracy: 0.3502 - mean_directional_accuracy: 0.5722 - val_loss: 1.4016 - val_accuracy: 0.3013 - val_categorical_accuracy: 0.3013 - val_mean_directional_accuracy: 0.5260\n",
      "Epoch 38/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3760 - accuracy: 0.3444 - categorical_accuracy: 0.3444 - mean_directional_accuracy: 0.5664 - val_loss: 1.4010 - val_accuracy: 0.3026 - val_categorical_accuracy: 0.3026 - val_mean_directional_accuracy: 0.5244\n",
      "Epoch 39/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3751 - accuracy: 0.3502 - categorical_accuracy: 0.3502 - mean_directional_accuracy: 0.5648 - val_loss: 1.4003 - val_accuracy: 0.3001 - val_categorical_accuracy: 0.3001 - val_mean_directional_accuracy: 0.5256\n",
      "Epoch 40/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3746 - accuracy: 0.3462 - categorical_accuracy: 0.3462 - mean_directional_accuracy: 0.5638 - val_loss: 1.3993 - val_accuracy: 0.3025 - val_categorical_accuracy: 0.3025 - val_mean_directional_accuracy: 0.5279\n",
      "Epoch 41/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3723 - accuracy: 0.3474 - categorical_accuracy: 0.3474 - mean_directional_accuracy: 0.5629 - val_loss: 1.3983 - val_accuracy: 0.3015 - val_categorical_accuracy: 0.3015 - val_mean_directional_accuracy: 0.5273\n",
      "Epoch 42/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3730 - accuracy: 0.3458 - categorical_accuracy: 0.3458 - mean_directional_accuracy: 0.5592 - val_loss: 1.3979 - val_accuracy: 0.2981 - val_categorical_accuracy: 0.2981 - val_mean_directional_accuracy: 0.5232\n",
      "Epoch 43/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3717 - accuracy: 0.3451 - categorical_accuracy: 0.3451 - mean_directional_accuracy: 0.5587 - val_loss: 1.3970 - val_accuracy: 0.2988 - val_categorical_accuracy: 0.2988 - val_mean_directional_accuracy: 0.5237\n",
      "Epoch 44/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3739 - accuracy: 0.3440 - categorical_accuracy: 0.3440 - mean_directional_accuracy: 0.5645 - val_loss: 1.3960 - val_accuracy: 0.3017 - val_categorical_accuracy: 0.3017 - val_mean_directional_accuracy: 0.5257\n",
      "Epoch 45/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3722 - accuracy: 0.3402 - categorical_accuracy: 0.3402 - mean_directional_accuracy: 0.5620 - val_loss: 1.3955 - val_accuracy: 0.3005 - val_categorical_accuracy: 0.3005 - val_mean_directional_accuracy: 0.5251\n",
      "Epoch 46/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3697 - accuracy: 0.3469 - categorical_accuracy: 0.3469 - mean_directional_accuracy: 0.5685 - val_loss: 1.3949 - val_accuracy: 0.2983 - val_categorical_accuracy: 0.2983 - val_mean_directional_accuracy: 0.5232\n",
      "Epoch 47/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3676 - accuracy: 0.3498 - categorical_accuracy: 0.3498 - mean_directional_accuracy: 0.5678 - val_loss: 1.3946 - val_accuracy: 0.2970 - val_categorical_accuracy: 0.2970 - val_mean_directional_accuracy: 0.5224\n",
      "Epoch 48/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3690 - accuracy: 0.3539 - categorical_accuracy: 0.3539 - mean_directional_accuracy: 0.5685 - val_loss: 1.3942 - val_accuracy: 0.2967 - val_categorical_accuracy: 0.2967 - val_mean_directional_accuracy: 0.5223\n",
      "Epoch 49/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3694 - accuracy: 0.3500 - categorical_accuracy: 0.3500 - mean_directional_accuracy: 0.5769 - val_loss: 1.3935 - val_accuracy: 0.2974 - val_categorical_accuracy: 0.2974 - val_mean_directional_accuracy: 0.5241\n",
      "Epoch 50/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3681 - accuracy: 0.3458 - categorical_accuracy: 0.3458 - mean_directional_accuracy: 0.5625 - val_loss: 1.3929 - val_accuracy: 0.2962 - val_categorical_accuracy: 0.2962 - val_mean_directional_accuracy: 0.5234\n",
      "Epoch 51/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3657 - accuracy: 0.3500 - categorical_accuracy: 0.3500 - mean_directional_accuracy: 0.5669 - val_loss: 1.3923 - val_accuracy: 0.2971 - val_categorical_accuracy: 0.2971 - val_mean_directional_accuracy: 0.5244\n",
      "Epoch 52/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3659 - accuracy: 0.3528 - categorical_accuracy: 0.3528 - mean_directional_accuracy: 0.5687 - val_loss: 1.3921 - val_accuracy: 0.2962 - val_categorical_accuracy: 0.2962 - val_mean_directional_accuracy: 0.5214\n",
      "Epoch 53/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3682 - accuracy: 0.3442 - categorical_accuracy: 0.3442 - mean_directional_accuracy: 0.5627 - val_loss: 1.3913 - val_accuracy: 0.2974 - val_categorical_accuracy: 0.2974 - val_mean_directional_accuracy: 0.5204\n",
      "Epoch 54/300\n",
      "341/356 [===========================>..] - ETA: 0s - loss: 1.3678 - accuracy: 0.3429 - categorical_accuracy: 0.3429 - mean_directional_accuracy: 0.5594Restoring model weights from the end of the best epoch: 53.\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3667 - accuracy: 0.3447 - categorical_accuracy: 0.3447 - mean_directional_accuracy: 0.5622 - val_loss: 1.3916 - val_accuracy: 0.2976 - val_categorical_accuracy: 0.2976 - val_mean_directional_accuracy: 0.5198\n",
      "Epoch 54: early stopping\n",
      "479/479 [==============================] - 2s 3ms/step - loss: 1.3913 - accuracy: 0.2974 - categorical_accuracy: 0.2974 - mean_directional_accuracy: 0.5204\n",
      "{'loss': 1.3912888765335083, 'accuracy': 0.2973642945289612, 'categorical_accuracy': 0.2973642945289612, 'mean_directional_accuracy': 0.5203549265861511} \n",
      " 53 \n",
      "\n",
      "Model time: 2.2108063213527203 minutes\n",
      "\n",
      "Total time: 29.46357710659504 minutes\n",
      "\n",
      "\n",
      "Model  62  out of  111\n",
      "Model type                FeedForward\n",
      "Hidden layers                       4\n",
      "Hidden units           [128, 1, 1, 4]\n",
      "Activation function            linear\n",
      "Dropout                           0.4\n",
      "L1                              100.0\n",
      "L2                                1.0\n",
      "Batch size                        256\n",
      "Optimizer                     RMSprop\n",
      "Learning rate                   0.001\n",
      "Name: 50, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 4s 40ms/step - loss: 121779.9062 - accuracy: 0.2499 - categorical_accuracy: 0.2499 - mean_directional_accuracy: 0.5002 - val_loss: 95073.9453 - val_accuracy: 0.2479 - val_categorical_accuracy: 0.2479 - val_mean_directional_accuracy: 0.4841\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 77969.9297 - accuracy: 0.2480 - categorical_accuracy: 0.2480 - mean_directional_accuracy: 0.4956 - val_loss: 59910.5117 - val_accuracy: 0.2487 - val_categorical_accuracy: 0.2487 - val_mean_directional_accuracy: 0.4910\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 47009.3047 - accuracy: 0.2525 - categorical_accuracy: 0.2525 - mean_directional_accuracy: 0.4989 - val_loss: 33443.6250 - val_accuracy: 0.2512 - val_categorical_accuracy: 0.2512 - val_mean_directional_accuracy: 0.4927\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 24267.6523 - accuracy: 0.2585 - categorical_accuracy: 0.2585 - mean_directional_accuracy: 0.5035 - val_loss: 15054.7539 - val_accuracy: 0.2551 - val_categorical_accuracy: 0.2551 - val_mean_directional_accuracy: 0.4975\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 9574.1240 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5076 - val_loss: 4660.8457 - val_accuracy: 0.2316 - val_categorical_accuracy: 0.2316 - val_mean_directional_accuracy: 0.4616\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 1s 34ms/step - loss: 2875.9714 - accuracy: 0.2687 - categorical_accuracy: 0.2687 - mean_directional_accuracy: 0.5133 - val_loss: 2109.4023 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2052.0286 - accuracy: 0.2669 - categorical_accuracy: 0.2669 - mean_directional_accuracy: 0.5077 - val_loss: 1990.4449 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1943.3739 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1892.6816 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1861.5065 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1833.0137 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1810.9911 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1785.4401 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1764.6681 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1740.5924 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1719.6913 - accuracy: 0.2671 - categorical_accuracy: 0.2671 - mean_directional_accuracy: 0.5102 - val_loss: 1695.4409 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1676.6166 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5104 - val_loss: 1654.8256 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1634.9683 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1611.9397 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1593.3412 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1571.9614 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1553.3462 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1532.3754 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1515.8730 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1496.8524 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1480.3700 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1461.8154 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1447.5208 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1431.2574 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 1417.0367 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1401.1997 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1389.0336 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1374.7808 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1361.3303 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1346.0839 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1334.9742 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1321.9534 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1310.1049 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1296.8806 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 25/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1286.8895 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1275.1643 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1263.8232 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1250.7200 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1240.7523 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1229.4238 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 28/300\n",
      "23/23 [==============================] - 1s 27ms/step - loss: 1219.1334 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1207.2572 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 29/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1198.8105 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1189.4819 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 30/300\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 1180.2657 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1170.0892 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 31/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1163.3529 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1155.4127 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 32/300\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 1147.2416 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1137.8361 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 33/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1131.1152 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1123.5327 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1118.1866 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1112.4269 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1108.9092 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1104.8805 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1100.9161 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1096.4171 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1094.8093 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1093.0753 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1090.8344 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1089.5320 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1090.2930 - accuracy: 0.2635 - categorical_accuracy: 0.2635 - mean_directional_accuracy: 0.5087Restoring model weights from the end of the best epoch: 38.\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1090.2843 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1091.0596 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39: early stopping\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1089.5320 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1089.531982421875, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 38 \n",
      "\n",
      "Model time: 0.31291551142930984 minutes\n",
      "\n",
      "Total time: 29.776558596640825 minutes\n",
      "\n",
      "\n",
      "Model  63  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                 [256]\n",
      "Activation function        sigmoid\n",
      "Dropout                        0.0\n",
      "L1                            10.0\n",
      "L2                           100.0\n",
      "Batch size                      16\n",
      "Optimizer                     Adam\n",
      "Learning rate                 0.01\n",
      "Name: 51, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 5s 10ms/step - loss: 1415.1757 - accuracy: 0.2650 - categorical_accuracy: 0.2650 - mean_directional_accuracy: 0.5130 - val_loss: 610.2416 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "353/356 [============================>.] - ETA: 0s - loss: 648.2559 - accuracy: 0.2573 - categorical_accuracy: 0.2573 - mean_directional_accuracy: 0.5025Restoring model weights from the end of the best epoch: 1.\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 648.0577 - accuracy: 0.2573 - categorical_accuracy: 0.2573 - mean_directional_accuracy: 0.5032 - val_loss: 719.0222 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 2: early stopping\n",
      "479/479 [==============================] - 2s 4ms/step - loss: 610.2416 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 610.2416381835938, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 1 \n",
      "\n",
      "Model time: 0.17253414541482925 minutes\n",
      "\n",
      "Total time: 29.9491427578032 minutes\n",
      "\n",
      "\n",
      "Model  64  out of  111\n",
      "Model type                    FeedForward\n",
      "Hidden layers                           4\n",
      "Hidden units           [256, 256, 256, 4]\n",
      "Activation function                  tanh\n",
      "Dropout                               0.2\n",
      "L1                                    0.0\n",
      "L2                                   0.01\n",
      "Batch size                             16\n",
      "Optimizer                            Adam\n",
      "Learning rate                       0.001\n",
      "Name: 52, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 8s 15ms/step - loss: 3.5113 - accuracy: 0.2973 - categorical_accuracy: 0.2973 - mean_directional_accuracy: 0.5248 - val_loss: 1.6618 - val_accuracy: 0.3031 - val_categorical_accuracy: 0.3031 - val_mean_directional_accuracy: 0.5360\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - 4s 11ms/step - loss: 1.4883 - accuracy: 0.3131 - categorical_accuracy: 0.3131 - mean_directional_accuracy: 0.5325 - val_loss: 1.4370 - val_accuracy: 0.2882 - val_categorical_accuracy: 0.2882 - val_mean_directional_accuracy: 0.5176\n",
      "Epoch 3/300\n",
      "356/356 [==============================] - 5s 13ms/step - loss: 1.4185 - accuracy: 0.2982 - categorical_accuracy: 0.2982 - mean_directional_accuracy: 0.5244 - val_loss: 1.4120 - val_accuracy: 0.2961 - val_categorical_accuracy: 0.2961 - val_mean_directional_accuracy: 0.5189\n",
      "Epoch 4/300\n",
      "356/356 [==============================] - 4s 11ms/step - loss: 1.4042 - accuracy: 0.3000 - categorical_accuracy: 0.3000 - mean_directional_accuracy: 0.5277 - val_loss: 1.4069 - val_accuracy: 0.2542 - val_categorical_accuracy: 0.2542 - val_mean_directional_accuracy: 0.4846\n",
      "Epoch 5/300\n",
      "356/356 [==============================] - 4s 12ms/step - loss: 1.3982 - accuracy: 0.2893 - categorical_accuracy: 0.2893 - mean_directional_accuracy: 0.5218 - val_loss: 1.4050 - val_accuracy: 0.2465 - val_categorical_accuracy: 0.2465 - val_mean_directional_accuracy: 0.4636\n",
      "Epoch 6/300\n",
      "356/356 [==============================] - 4s 10ms/step - loss: 1.3952 - accuracy: 0.2950 - categorical_accuracy: 0.2950 - mean_directional_accuracy: 0.5153 - val_loss: 1.4020 - val_accuracy: 0.2788 - val_categorical_accuracy: 0.2788 - val_mean_directional_accuracy: 0.4897\n",
      "Epoch 7/300\n",
      "356/356 [==============================] - 4s 12ms/step - loss: 1.3947 - accuracy: 0.2914 - categorical_accuracy: 0.2914 - mean_directional_accuracy: 0.5239 - val_loss: 1.3992 - val_accuracy: 0.2586 - val_categorical_accuracy: 0.2586 - val_mean_directional_accuracy: 0.4731\n",
      "Epoch 8/300\n",
      "348/356 [============================>.] - ETA: 0s - loss: 1.3932 - accuracy: 0.2875 - categorical_accuracy: 0.2875 - mean_directional_accuracy: 0.5287Restoring model weights from the end of the best epoch: 7.\n",
      "356/356 [==============================] - 4s 12ms/step - loss: 1.3936 - accuracy: 0.2866 - categorical_accuracy: 0.2866 - mean_directional_accuracy: 0.5263 - val_loss: 1.4031 - val_accuracy: 0.2899 - val_categorical_accuracy: 0.2899 - val_mean_directional_accuracy: 0.5017\n",
      "Epoch 8: early stopping\n",
      "479/479 [==============================] - 2s 4ms/step - loss: 1.3992 - accuracy: 0.2586 - categorical_accuracy: 0.2586 - mean_directional_accuracy: 0.4731\n",
      "{'loss': 1.3991670608520508, 'accuracy': 0.25861167907714844, 'categorical_accuracy': 0.25861167907714844, 'mean_directional_accuracy': 0.4731210768222809} \n",
      " 7 \n",
      "\n",
      "Model time: 0.7709221132099628 minutes\n",
      "\n",
      "Total time: 30.72038770467043 minutes\n",
      "\n",
      "\n",
      "Model  65  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                   [8]\n",
      "Activation function        sigmoid\n",
      "Dropout                        0.9\n",
      "L1                             0.1\n",
      "L2                           0.001\n",
      "Batch size                     128\n",
      "Optimizer                     Adam\n",
      "Learning rate               0.0001\n",
      "Name: 53, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 3s 27ms/step - loss: 15.5549 - accuracy: 0.2448 - categorical_accuracy: 0.2448 - mean_directional_accuracy: 0.4916 - val_loss: 14.4746 - val_accuracy: 0.2801 - val_categorical_accuracy: 0.2801 - val_mean_directional_accuracy: 0.5406\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 14.9221 - accuracy: 0.2554 - categorical_accuracy: 0.2554 - mean_directional_accuracy: 0.4924 - val_loss: 13.8815 - val_accuracy: 0.2807 - val_categorical_accuracy: 0.2807 - val_mean_directional_accuracy: 0.5412\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 14.3656 - accuracy: 0.2392 - categorical_accuracy: 0.2392 - mean_directional_accuracy: 0.4872 - val_loss: 13.3072 - val_accuracy: 0.2808 - val_categorical_accuracy: 0.2808 - val_mean_directional_accuracy: 0.5412\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 13.7827 - accuracy: 0.2455 - categorical_accuracy: 0.2455 - mean_directional_accuracy: 0.4919 - val_loss: 12.7508 - val_accuracy: 0.2805 - val_categorical_accuracy: 0.2805 - val_mean_directional_accuracy: 0.5410\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 13.2561 - accuracy: 0.2415 - categorical_accuracy: 0.2415 - mean_directional_accuracy: 0.4909 - val_loss: 12.2088 - val_accuracy: 0.2809 - val_categorical_accuracy: 0.2809 - val_mean_directional_accuracy: 0.5415\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 12.6633 - accuracy: 0.2445 - categorical_accuracy: 0.2445 - mean_directional_accuracy: 0.4858 - val_loss: 11.6831 - val_accuracy: 0.2812 - val_categorical_accuracy: 0.2812 - val_mean_directional_accuracy: 0.5419\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 12.1436 - accuracy: 0.2583 - categorical_accuracy: 0.2583 - mean_directional_accuracy: 0.5053 - val_loss: 11.1718 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 11.6271 - accuracy: 0.2464 - categorical_accuracy: 0.2464 - mean_directional_accuracy: 0.4863 - val_loss: 10.6762 - val_accuracy: 0.2813 - val_categorical_accuracy: 0.2813 - val_mean_directional_accuracy: 0.5412\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 11.1478 - accuracy: 0.2436 - categorical_accuracy: 0.2436 - mean_directional_accuracy: 0.4988 - val_loss: 10.1956 - val_accuracy: 0.2812 - val_categorical_accuracy: 0.2812 - val_mean_directional_accuracy: 0.5414\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 10.6679 - accuracy: 0.2376 - categorical_accuracy: 0.2376 - mean_directional_accuracy: 0.4863 - val_loss: 9.7280 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5419\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 10.1175 - accuracy: 0.2557 - categorical_accuracy: 0.2557 - mean_directional_accuracy: 0.4956 - val_loss: 9.2721 - val_accuracy: 0.2818 - val_categorical_accuracy: 0.2818 - val_mean_directional_accuracy: 0.5420\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 9.6858 - accuracy: 0.2510 - categorical_accuracy: 0.2510 - mean_directional_accuracy: 0.4998 - val_loss: 8.8315 - val_accuracy: 0.2818 - val_categorical_accuracy: 0.2818 - val_mean_directional_accuracy: 0.5419\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 9.2421 - accuracy: 0.2513 - categorical_accuracy: 0.2513 - mean_directional_accuracy: 0.5044 - val_loss: 8.4040 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5415\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 8.8343 - accuracy: 0.2471 - categorical_accuracy: 0.2471 - mean_directional_accuracy: 0.4975 - val_loss: 7.9914 - val_accuracy: 0.2817 - val_categorical_accuracy: 0.2817 - val_mean_directional_accuracy: 0.5420\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 8.3851 - accuracy: 0.2480 - categorical_accuracy: 0.2480 - mean_directional_accuracy: 0.4932 - val_loss: 7.5923 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5415\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 7.9912 - accuracy: 0.2485 - categorical_accuracy: 0.2485 - mean_directional_accuracy: 0.4889 - val_loss: 7.2074 - val_accuracy: 0.2812 - val_categorical_accuracy: 0.2812 - val_mean_directional_accuracy: 0.5412\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 7.6081 - accuracy: 0.2532 - categorical_accuracy: 0.2532 - mean_directional_accuracy: 0.5081 - val_loss: 6.8368 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 7.2430 - accuracy: 0.2483 - categorical_accuracy: 0.2483 - mean_directional_accuracy: 0.4968 - val_loss: 6.4782 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 6.8434 - accuracy: 0.2432 - categorical_accuracy: 0.2432 - mean_directional_accuracy: 0.4919 - val_loss: 6.1334 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 6.5365 - accuracy: 0.2450 - categorical_accuracy: 0.2450 - mean_directional_accuracy: 0.4826 - val_loss: 5.8015 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 6.1886 - accuracy: 0.2550 - categorical_accuracy: 0.2550 - mean_directional_accuracy: 0.5042 - val_loss: 5.4838 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 5.8575 - accuracy: 0.2417 - categorical_accuracy: 0.2417 - mean_directional_accuracy: 0.4916 - val_loss: 5.1826 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 5.6100 - accuracy: 0.2367 - categorical_accuracy: 0.2367 - mean_directional_accuracy: 0.4902 - val_loss: 4.8946 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 5.2713 - accuracy: 0.2464 - categorical_accuracy: 0.2464 - mean_directional_accuracy: 0.4996 - val_loss: 4.6222 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 4.9874 - accuracy: 0.2499 - categorical_accuracy: 0.2499 - mean_directional_accuracy: 0.4946 - val_loss: 4.3624 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 4.7277 - accuracy: 0.2396 - categorical_accuracy: 0.2396 - mean_directional_accuracy: 0.4949 - val_loss: 4.1174 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 4.4909 - accuracy: 0.2445 - categorical_accuracy: 0.2445 - mean_directional_accuracy: 0.4928 - val_loss: 3.8862 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 4.2466 - accuracy: 0.2490 - categorical_accuracy: 0.2490 - mean_directional_accuracy: 0.4986 - val_loss: 3.6670 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 4.0352 - accuracy: 0.2417 - categorical_accuracy: 0.2417 - mean_directional_accuracy: 0.5009 - val_loss: 3.4615 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 3.7985 - accuracy: 0.2525 - categorical_accuracy: 0.2525 - mean_directional_accuracy: 0.4960 - val_loss: 3.2725 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 3.6225 - accuracy: 0.2471 - categorical_accuracy: 0.2471 - mean_directional_accuracy: 0.5004 - val_loss: 3.0987 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 3.4298 - accuracy: 0.2476 - categorical_accuracy: 0.2476 - mean_directional_accuracy: 0.4944 - val_loss: 2.9424 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 3.3148 - accuracy: 0.2350 - categorical_accuracy: 0.2350 - mean_directional_accuracy: 0.4888 - val_loss: 2.8034 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 3.1333 - accuracy: 0.2501 - categorical_accuracy: 0.2501 - mean_directional_accuracy: 0.4960 - val_loss: 2.6760 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.9947 - accuracy: 0.2511 - categorical_accuracy: 0.2511 - mean_directional_accuracy: 0.4935 - val_loss: 2.5632 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 2.9369 - accuracy: 0.2392 - categorical_accuracy: 0.2392 - mean_directional_accuracy: 0.4953 - val_loss: 2.4625 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.8025 - accuracy: 0.2497 - categorical_accuracy: 0.2497 - mean_directional_accuracy: 0.4984 - val_loss: 2.3733 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.7051 - accuracy: 0.2494 - categorical_accuracy: 0.2494 - mean_directional_accuracy: 0.4995 - val_loss: 2.2959 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.6176 - accuracy: 0.2483 - categorical_accuracy: 0.2483 - mean_directional_accuracy: 0.4923 - val_loss: 2.2328 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 2.5847 - accuracy: 0.2490 - categorical_accuracy: 0.2490 - mean_directional_accuracy: 0.4891 - val_loss: 2.1852 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.5124 - accuracy: 0.2543 - categorical_accuracy: 0.2543 - mean_directional_accuracy: 0.4989 - val_loss: 2.1535 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 2.4888 - accuracy: 0.2388 - categorical_accuracy: 0.2388 - mean_directional_accuracy: 0.4907 - val_loss: 2.1327 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.4612 - accuracy: 0.2415 - categorical_accuracy: 0.2415 - mean_directional_accuracy: 0.4935 - val_loss: 2.1174 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.4353 - accuracy: 0.2471 - categorical_accuracy: 0.2471 - mean_directional_accuracy: 0.4867 - val_loss: 2.1052 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.4197 - accuracy: 0.2460 - categorical_accuracy: 0.2460 - mean_directional_accuracy: 0.4935 - val_loss: 2.0943 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.3883 - accuracy: 0.2599 - categorical_accuracy: 0.2599 - mean_directional_accuracy: 0.5047 - val_loss: 2.0833 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.3691 - accuracy: 0.2524 - categorical_accuracy: 0.2524 - mean_directional_accuracy: 0.4928 - val_loss: 2.0723 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.3670 - accuracy: 0.2489 - categorical_accuracy: 0.2489 - mean_directional_accuracy: 0.4867 - val_loss: 2.0612 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 2.3322 - accuracy: 0.2508 - categorical_accuracy: 0.2508 - mean_directional_accuracy: 0.4981 - val_loss: 2.0505 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.3281 - accuracy: 0.2471 - categorical_accuracy: 0.2471 - mean_directional_accuracy: 0.5023 - val_loss: 2.0396 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.2960 - accuracy: 0.2482 - categorical_accuracy: 0.2482 - mean_directional_accuracy: 0.4991 - val_loss: 2.0290 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.2923 - accuracy: 0.2452 - categorical_accuracy: 0.2452 - mean_directional_accuracy: 0.4877 - val_loss: 2.0184 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.2727 - accuracy: 0.2511 - categorical_accuracy: 0.2511 - mean_directional_accuracy: 0.4981 - val_loss: 2.0079 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.2528 - accuracy: 0.2392 - categorical_accuracy: 0.2392 - mean_directional_accuracy: 0.4896 - val_loss: 1.9975 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 2.2345 - accuracy: 0.2515 - categorical_accuracy: 0.2515 - mean_directional_accuracy: 0.4961 - val_loss: 1.9871 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 2.1937 - accuracy: 0.2575 - categorical_accuracy: 0.2575 - mean_directional_accuracy: 0.5067 - val_loss: 1.9767 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.1911 - accuracy: 0.2497 - categorical_accuracy: 0.2497 - mean_directional_accuracy: 0.4991 - val_loss: 1.9668 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.1918 - accuracy: 0.2506 - categorical_accuracy: 0.2506 - mean_directional_accuracy: 0.4968 - val_loss: 1.9570 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 2.1905 - accuracy: 0.2415 - categorical_accuracy: 0.2415 - mean_directional_accuracy: 0.4939 - val_loss: 1.9472 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.1498 - accuracy: 0.2492 - categorical_accuracy: 0.2492 - mean_directional_accuracy: 0.5044 - val_loss: 1.9377 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.1369 - accuracy: 0.2441 - categorical_accuracy: 0.2441 - mean_directional_accuracy: 0.4979 - val_loss: 1.9284 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 62/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 2.1173 - accuracy: 0.2508 - categorical_accuracy: 0.2508 - mean_directional_accuracy: 0.5002 - val_loss: 1.9192 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 63/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 2.1062 - accuracy: 0.2508 - categorical_accuracy: 0.2508 - mean_directional_accuracy: 0.4979 - val_loss: 1.9101 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 64/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.1131 - accuracy: 0.2401 - categorical_accuracy: 0.2401 - mean_directional_accuracy: 0.4879 - val_loss: 1.9012 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 65/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 2.0767 - accuracy: 0.2499 - categorical_accuracy: 0.2499 - mean_directional_accuracy: 0.4989 - val_loss: 1.8923 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 66/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.0636 - accuracy: 0.2527 - categorical_accuracy: 0.2527 - mean_directional_accuracy: 0.4993 - val_loss: 1.8836 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 67/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.0412 - accuracy: 0.2503 - categorical_accuracy: 0.2503 - mean_directional_accuracy: 0.5002 - val_loss: 1.8748 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 68/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 2.0288 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.5046 - val_loss: 1.8666 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 69/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.0340 - accuracy: 0.2443 - categorical_accuracy: 0.2443 - mean_directional_accuracy: 0.5021 - val_loss: 1.8586 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 70/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.0003 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.4942 - val_loss: 1.8513 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 71/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.0112 - accuracy: 0.2446 - categorical_accuracy: 0.2446 - mean_directional_accuracy: 0.5030 - val_loss: 1.8439 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 72/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 2.0032 - accuracy: 0.2417 - categorical_accuracy: 0.2417 - mean_directional_accuracy: 0.4874 - val_loss: 1.8364 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 73/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 1.9950 - accuracy: 0.2397 - categorical_accuracy: 0.2397 - mean_directional_accuracy: 0.4949 - val_loss: 1.8288 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 74/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.9640 - accuracy: 0.2424 - categorical_accuracy: 0.2424 - mean_directional_accuracy: 0.4933 - val_loss: 1.8216 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 75/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.9570 - accuracy: 0.2464 - categorical_accuracy: 0.2464 - mean_directional_accuracy: 0.4928 - val_loss: 1.8143 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 76/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.9347 - accuracy: 0.2475 - categorical_accuracy: 0.2475 - mean_directional_accuracy: 0.4968 - val_loss: 1.8070 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 77/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.9315 - accuracy: 0.2448 - categorical_accuracy: 0.2448 - mean_directional_accuracy: 0.4895 - val_loss: 1.7998 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 78/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.8994 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.5025 - val_loss: 1.7926 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 79/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.9055 - accuracy: 0.2471 - categorical_accuracy: 0.2471 - mean_directional_accuracy: 0.5049 - val_loss: 1.7856 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 80/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.8908 - accuracy: 0.2434 - categorical_accuracy: 0.2434 - mean_directional_accuracy: 0.4888 - val_loss: 1.7787 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 81/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.8913 - accuracy: 0.2378 - categorical_accuracy: 0.2378 - mean_directional_accuracy: 0.4937 - val_loss: 1.7718 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 82/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.8691 - accuracy: 0.2511 - categorical_accuracy: 0.2511 - mean_directional_accuracy: 0.5007 - val_loss: 1.7652 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 83/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.8749 - accuracy: 0.2360 - categorical_accuracy: 0.2360 - mean_directional_accuracy: 0.4870 - val_loss: 1.7583 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 84/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.8561 - accuracy: 0.2392 - categorical_accuracy: 0.2392 - mean_directional_accuracy: 0.4884 - val_loss: 1.7515 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 85/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.8367 - accuracy: 0.2492 - categorical_accuracy: 0.2492 - mean_directional_accuracy: 0.5030 - val_loss: 1.7450 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 86/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.8263 - accuracy: 0.2527 - categorical_accuracy: 0.2527 - mean_directional_accuracy: 0.5016 - val_loss: 1.7384 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 87/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.8309 - accuracy: 0.2378 - categorical_accuracy: 0.2378 - mean_directional_accuracy: 0.4932 - val_loss: 1.7315 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 88/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.8147 - accuracy: 0.2390 - categorical_accuracy: 0.2390 - mean_directional_accuracy: 0.4939 - val_loss: 1.7253 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 89/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.8042 - accuracy: 0.2517 - categorical_accuracy: 0.2517 - mean_directional_accuracy: 0.4972 - val_loss: 1.7189 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 90/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.7980 - accuracy: 0.2441 - categorical_accuracy: 0.2441 - mean_directional_accuracy: 0.4886 - val_loss: 1.7126 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 91/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.7858 - accuracy: 0.2422 - categorical_accuracy: 0.2422 - mean_directional_accuracy: 0.4900 - val_loss: 1.7063 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 92/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.7789 - accuracy: 0.2531 - categorical_accuracy: 0.2531 - mean_directional_accuracy: 0.5023 - val_loss: 1.6999 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 93/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.7678 - accuracy: 0.2417 - categorical_accuracy: 0.2417 - mean_directional_accuracy: 0.4851 - val_loss: 1.6936 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 94/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.7616 - accuracy: 0.2446 - categorical_accuracy: 0.2446 - mean_directional_accuracy: 0.4860 - val_loss: 1.6874 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 95/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 1.7566 - accuracy: 0.2387 - categorical_accuracy: 0.2387 - mean_directional_accuracy: 0.4882 - val_loss: 1.6810 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 96/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.7338 - accuracy: 0.2503 - categorical_accuracy: 0.2503 - mean_directional_accuracy: 0.4991 - val_loss: 1.6747 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 97/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.7370 - accuracy: 0.2431 - categorical_accuracy: 0.2431 - mean_directional_accuracy: 0.4865 - val_loss: 1.6684 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 98/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.7180 - accuracy: 0.2411 - categorical_accuracy: 0.2411 - mean_directional_accuracy: 0.5065 - val_loss: 1.6621 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 99/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.7159 - accuracy: 0.2431 - categorical_accuracy: 0.2431 - mean_directional_accuracy: 0.4861 - val_loss: 1.6559 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 100/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.7121 - accuracy: 0.2396 - categorical_accuracy: 0.2396 - mean_directional_accuracy: 0.4935 - val_loss: 1.6497 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 101/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.6885 - accuracy: 0.2403 - categorical_accuracy: 0.2403 - mean_directional_accuracy: 0.4916 - val_loss: 1.6436 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 102/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.6868 - accuracy: 0.2417 - categorical_accuracy: 0.2417 - mean_directional_accuracy: 0.4951 - val_loss: 1.6375 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 103/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 1.6840 - accuracy: 0.2394 - categorical_accuracy: 0.2394 - mean_directional_accuracy: 0.4884 - val_loss: 1.6313 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 104/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.6712 - accuracy: 0.2383 - categorical_accuracy: 0.2383 - mean_directional_accuracy: 0.4939 - val_loss: 1.6253 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 105/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.6617 - accuracy: 0.2390 - categorical_accuracy: 0.2390 - mean_directional_accuracy: 0.4816 - val_loss: 1.6194 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 106/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 1.6517 - accuracy: 0.2480 - categorical_accuracy: 0.2480 - mean_directional_accuracy: 0.5037 - val_loss: 1.6135 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 107/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.6457 - accuracy: 0.2431 - categorical_accuracy: 0.2431 - mean_directional_accuracy: 0.4939 - val_loss: 1.6075 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 108/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.6337 - accuracy: 0.2459 - categorical_accuracy: 0.2459 - mean_directional_accuracy: 0.5016 - val_loss: 1.6017 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 109/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.6313 - accuracy: 0.2459 - categorical_accuracy: 0.2459 - mean_directional_accuracy: 0.4930 - val_loss: 1.5958 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 110/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.6279 - accuracy: 0.2411 - categorical_accuracy: 0.2411 - mean_directional_accuracy: 0.4942 - val_loss: 1.5899 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 111/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.6249 - accuracy: 0.2327 - categorical_accuracy: 0.2327 - mean_directional_accuracy: 0.4826 - val_loss: 1.5839 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 112/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.6090 - accuracy: 0.2385 - categorical_accuracy: 0.2385 - mean_directional_accuracy: 0.4958 - val_loss: 1.5783 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 113/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.6112 - accuracy: 0.2323 - categorical_accuracy: 0.2323 - mean_directional_accuracy: 0.4849 - val_loss: 1.5727 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 114/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.6002 - accuracy: 0.2462 - categorical_accuracy: 0.2462 - mean_directional_accuracy: 0.5002 - val_loss: 1.5674 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 115/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.5917 - accuracy: 0.2429 - categorical_accuracy: 0.2429 - mean_directional_accuracy: 0.4989 - val_loss: 1.5622 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 116/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.5834 - accuracy: 0.2459 - categorical_accuracy: 0.2459 - mean_directional_accuracy: 0.4940 - val_loss: 1.5573 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 117/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 1.5760 - accuracy: 0.2436 - categorical_accuracy: 0.2436 - mean_directional_accuracy: 0.5033 - val_loss: 1.5525 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 118/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.5760 - accuracy: 0.2373 - categorical_accuracy: 0.2373 - mean_directional_accuracy: 0.4975 - val_loss: 1.5477 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 119/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.5660 - accuracy: 0.2424 - categorical_accuracy: 0.2424 - mean_directional_accuracy: 0.4975 - val_loss: 1.5428 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 120/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.5598 - accuracy: 0.2394 - categorical_accuracy: 0.2394 - mean_directional_accuracy: 0.4914 - val_loss: 1.5379 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 121/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.5472 - accuracy: 0.2418 - categorical_accuracy: 0.2418 - mean_directional_accuracy: 0.4917 - val_loss: 1.5331 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 122/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.5434 - accuracy: 0.2501 - categorical_accuracy: 0.2501 - mean_directional_accuracy: 0.5025 - val_loss: 1.5288 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 123/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.5424 - accuracy: 0.2436 - categorical_accuracy: 0.2436 - mean_directional_accuracy: 0.4863 - val_loss: 1.5247 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 124/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.5311 - accuracy: 0.2469 - categorical_accuracy: 0.2469 - mean_directional_accuracy: 0.4967 - val_loss: 1.5204 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 125/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.5276 - accuracy: 0.2459 - categorical_accuracy: 0.2459 - mean_directional_accuracy: 0.4965 - val_loss: 1.5164 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 126/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.5241 - accuracy: 0.2478 - categorical_accuracy: 0.2478 - mean_directional_accuracy: 0.4986 - val_loss: 1.5124 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 127/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.5196 - accuracy: 0.2436 - categorical_accuracy: 0.2436 - mean_directional_accuracy: 0.5030 - val_loss: 1.5082 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 128/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.5170 - accuracy: 0.2439 - categorical_accuracy: 0.2439 - mean_directional_accuracy: 0.4968 - val_loss: 1.5040 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 129/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.5123 - accuracy: 0.2489 - categorical_accuracy: 0.2489 - mean_directional_accuracy: 0.4982 - val_loss: 1.4999 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 130/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.5017 - accuracy: 0.2531 - categorical_accuracy: 0.2531 - mean_directional_accuracy: 0.4958 - val_loss: 1.4958 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 131/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.5022 - accuracy: 0.2388 - categorical_accuracy: 0.2388 - mean_directional_accuracy: 0.4903 - val_loss: 1.4917 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 132/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4963 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.5033 - val_loss: 1.4874 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 133/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4951 - accuracy: 0.2366 - categorical_accuracy: 0.2366 - mean_directional_accuracy: 0.4914 - val_loss: 1.4831 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 134/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4889 - accuracy: 0.2429 - categorical_accuracy: 0.2429 - mean_directional_accuracy: 0.4956 - val_loss: 1.4789 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 135/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4819 - accuracy: 0.2383 - categorical_accuracy: 0.2383 - mean_directional_accuracy: 0.4907 - val_loss: 1.4747 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 136/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4776 - accuracy: 0.2464 - categorical_accuracy: 0.2464 - mean_directional_accuracy: 0.4991 - val_loss: 1.4706 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 137/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4726 - accuracy: 0.2459 - categorical_accuracy: 0.2459 - mean_directional_accuracy: 0.4947 - val_loss: 1.4666 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 138/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4667 - accuracy: 0.2478 - categorical_accuracy: 0.2478 - mean_directional_accuracy: 0.4935 - val_loss: 1.4631 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 139/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4631 - accuracy: 0.2434 - categorical_accuracy: 0.2434 - mean_directional_accuracy: 0.4951 - val_loss: 1.4598 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 140/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 1.4585 - accuracy: 0.2411 - categorical_accuracy: 0.2411 - mean_directional_accuracy: 0.4933 - val_loss: 1.4565 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 141/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4568 - accuracy: 0.2415 - categorical_accuracy: 0.2415 - mean_directional_accuracy: 0.4893 - val_loss: 1.4530 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 142/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 1.4541 - accuracy: 0.2392 - categorical_accuracy: 0.2392 - mean_directional_accuracy: 0.4919 - val_loss: 1.4496 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 143/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4469 - accuracy: 0.2448 - categorical_accuracy: 0.2448 - mean_directional_accuracy: 0.4951 - val_loss: 1.4462 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 144/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.4447 - accuracy: 0.2415 - categorical_accuracy: 0.2415 - mean_directional_accuracy: 0.5000 - val_loss: 1.4428 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 145/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4384 - accuracy: 0.2439 - categorical_accuracy: 0.2439 - mean_directional_accuracy: 0.4968 - val_loss: 1.4395 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 146/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.4358 - accuracy: 0.2476 - categorical_accuracy: 0.2476 - mean_directional_accuracy: 0.4912 - val_loss: 1.4363 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 147/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4347 - accuracy: 0.2406 - categorical_accuracy: 0.2406 - mean_directional_accuracy: 0.4972 - val_loss: 1.4334 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 148/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.4302 - accuracy: 0.2438 - categorical_accuracy: 0.2438 - mean_directional_accuracy: 0.5000 - val_loss: 1.4306 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 149/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4282 - accuracy: 0.2357 - categorical_accuracy: 0.2357 - mean_directional_accuracy: 0.4889 - val_loss: 1.4277 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 150/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4234 - accuracy: 0.2448 - categorical_accuracy: 0.2448 - mean_directional_accuracy: 0.4851 - val_loss: 1.4254 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 151/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4225 - accuracy: 0.2376 - categorical_accuracy: 0.2376 - mean_directional_accuracy: 0.4917 - val_loss: 1.4233 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 152/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4201 - accuracy: 0.2427 - categorical_accuracy: 0.2427 - mean_directional_accuracy: 0.4923 - val_loss: 1.4214 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 153/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4163 - accuracy: 0.2462 - categorical_accuracy: 0.2462 - mean_directional_accuracy: 0.5000 - val_loss: 1.4194 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 154/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 1.4167 - accuracy: 0.2364 - categorical_accuracy: 0.2364 - mean_directional_accuracy: 0.4968 - val_loss: 1.4176 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 155/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4137 - accuracy: 0.2418 - categorical_accuracy: 0.2418 - mean_directional_accuracy: 0.4953 - val_loss: 1.4159 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 156/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.4128 - accuracy: 0.2443 - categorical_accuracy: 0.2443 - mean_directional_accuracy: 0.4916 - val_loss: 1.4144 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 157/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4103 - accuracy: 0.2406 - categorical_accuracy: 0.2406 - mean_directional_accuracy: 0.4909 - val_loss: 1.4127 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 158/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4085 - accuracy: 0.2425 - categorical_accuracy: 0.2425 - mean_directional_accuracy: 0.4935 - val_loss: 1.4112 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 159/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4078 - accuracy: 0.2380 - categorical_accuracy: 0.2380 - mean_directional_accuracy: 0.4937 - val_loss: 1.4096 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 160/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.4051 - accuracy: 0.2464 - categorical_accuracy: 0.2464 - mean_directional_accuracy: 0.4968 - val_loss: 1.4081 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 161/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4043 - accuracy: 0.2436 - categorical_accuracy: 0.2436 - mean_directional_accuracy: 0.4954 - val_loss: 1.4064 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 162/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4014 - accuracy: 0.2457 - categorical_accuracy: 0.2457 - mean_directional_accuracy: 0.4958 - val_loss: 1.4049 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 163/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4011 - accuracy: 0.2531 - categorical_accuracy: 0.2531 - mean_directional_accuracy: 0.5044 - val_loss: 1.4034 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 164/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.3993 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4019 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 165/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3975 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4004 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 166/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3959 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3989 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 167/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3947 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3975 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 168/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3935 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3964 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 169/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.3923 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3953 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 170/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.3913 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3942 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 171/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3902 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3933 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 172/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3894 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3926 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 173/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3889 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3924 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 174/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3888 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3922 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 175/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.3887 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3922 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 176/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3886 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3921 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 177/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3885 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3920 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 178/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3884 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3919 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 179/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3883 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3918 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 180/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3882 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3918 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 181/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.3882 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3917 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 182/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3881 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3916 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 183/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3880 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3915 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 184/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3880 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3914 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 185/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3879 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3914 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 186/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3878 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3913 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 187/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3878 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3912 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 188/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3877 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3911 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 189/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3876 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3911 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 190/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3876 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3911 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 191/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3875 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3910 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 192/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3875 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3910 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 193/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3874 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3909 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 194/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 1.3874 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3909 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 195/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 1.3874 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3908 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 196/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3873 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3908 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 197/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3873 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3907 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 198/300\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.3873 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105Restoring model weights from the end of the best epoch: 197.\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.3873 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3907 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 198: early stopping\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 1.3907 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3907265663146973, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 197 \n",
      "\n",
      "Model time: 2.4072544910013676 minutes\n",
      "\n",
      "Total time: 33.12772552669048 minutes\n",
      "\n",
      "\n",
      "Model  66  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    3\n",
      "Hidden units           [64, 8, 16]\n",
      "Activation function        sigmoid\n",
      "Dropout                        0.1\n",
      "L1                          0.0001\n",
      "L2                             0.1\n",
      "Batch size                      16\n",
      "Optimizer                     Adam\n",
      "Learning rate                 0.01\n",
      "Name: 54, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 7s 14ms/step - loss: 1.6511 - accuracy: 0.2661 - categorical_accuracy: 0.2661 - mean_directional_accuracy: 0.5037 - val_loss: 1.3889 - val_accuracy: 0.2296 - val_categorical_accuracy: 0.2296 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - ETA: 0s - loss: 1.3921 - accuracy: 0.2540 - categorical_accuracy: 0.2540 - mean_directional_accuracy: 0.4912Restoring model weights from the end of the best epoch: 1.\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3921 - accuracy: 0.2540 - categorical_accuracy: 0.2540 - mean_directional_accuracy: 0.4912 - val_loss: 1.3905 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 2: early stopping\n",
      "479/479 [==============================] - 2s 3ms/step - loss: 1.3889 - accuracy: 0.2296 - categorical_accuracy: 0.2296 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3888918161392212, 'accuracy': 0.22964508831501007, 'categorical_accuracy': 0.22964508831501007, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 1 \n",
      "\n",
      "Model time: 0.19219595566391945 minutes\n",
      "\n",
      "Total time: 33.32017148286104 minutes\n",
      "\n",
      "\n",
      "Model  67  out of  111\n",
      "Model type                     FeedForward\n",
      "Hidden layers                            5\n",
      "Hidden units           [256, 8, 256, 8, 4]\n",
      "Activation function                sigmoid\n",
      "Dropout                                0.1\n",
      "L1                                     0.0\n",
      "L2                                  0.0001\n",
      "Batch size                             128\n",
      "Optimizer                             Adam\n",
      "Learning rate                         0.01\n",
      "Name: 55, dtype: object\n",
      "NN5Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 5s 45ms/step - loss: 1.3944 - accuracy: 0.2587 - categorical_accuracy: 0.2587 - mean_directional_accuracy: 0.4981 - val_loss: 1.3885 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "42/45 [===========================>..] - ETA: 0s - loss: 1.3901 - accuracy: 0.2608 - categorical_accuracy: 0.2608 - mean_directional_accuracy: 0.5041Restoring model weights from the end of the best epoch: 1.\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.3899 - accuracy: 0.2606 - categorical_accuracy: 0.2606 - mean_directional_accuracy: 0.5058 - val_loss: 1.3893 - val_accuracy: 0.2296 - val_categorical_accuracy: 0.2296 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2: early stopping\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 1.3885 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.388505458831787, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 1 \n",
      "\n",
      "Model time: 0.10936099663376808 minutes\n",
      "\n",
      "Total time: 33.42966681718826 minutes\n",
      "\n",
      "\n",
      "Model  68  out of  111\n",
      "Model type              FeedForward\n",
      "Hidden layers                     3\n",
      "Hidden units           [16, 4, 128]\n",
      "Activation function          linear\n",
      "Dropout                         0.1\n",
      "L1                             10.0\n",
      "L2                             0.01\n",
      "Batch size                       64\n",
      "Optimizer                   RMSprop\n",
      "Learning rate                0.0001\n",
      "Name: 56, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 4s 15ms/step - loss: 3510.7263 - accuracy: 0.2592 - categorical_accuracy: 0.2592 - mean_directional_accuracy: 0.5079 - val_loss: 3343.1489 - val_accuracy: 0.2462 - val_categorical_accuracy: 0.2462 - val_mean_directional_accuracy: 0.5077\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 3187.2520 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.5123 - val_loss: 3030.1689 - val_accuracy: 0.2457 - val_categorical_accuracy: 0.2457 - val_mean_directional_accuracy: 0.5059\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 2881.6819 - accuracy: 0.2417 - categorical_accuracy: 0.2417 - mean_directional_accuracy: 0.5097 - val_loss: 2731.9419 - val_accuracy: 0.2444 - val_categorical_accuracy: 0.2444 - val_mean_directional_accuracy: 0.5035\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 2590.8684 - accuracy: 0.2508 - categorical_accuracy: 0.2508 - mean_directional_accuracy: 0.5151 - val_loss: 2449.1086 - val_accuracy: 0.2443 - val_categorical_accuracy: 0.2443 - val_mean_directional_accuracy: 0.5037\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2315.6392 - accuracy: 0.2517 - categorical_accuracy: 0.2517 - mean_directional_accuracy: 0.5146 - val_loss: 2181.9009 - val_accuracy: 0.2458 - val_categorical_accuracy: 0.2458 - val_mean_directional_accuracy: 0.5014\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 2056.7788 - accuracy: 0.2534 - categorical_accuracy: 0.2534 - mean_directional_accuracy: 0.5158 - val_loss: 1931.6573 - val_accuracy: 0.2490 - val_categorical_accuracy: 0.2490 - val_mean_directional_accuracy: 0.4982\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 1814.8424 - accuracy: 0.2590 - categorical_accuracy: 0.2590 - mean_directional_accuracy: 0.5132 - val_loss: 1697.9579 - val_accuracy: 0.2483 - val_categorical_accuracy: 0.2483 - val_mean_directional_accuracy: 0.4906\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1588.8851 - accuracy: 0.2626 - categorical_accuracy: 0.2626 - mean_directional_accuracy: 0.5158 - val_loss: 1479.7064 - val_accuracy: 0.2428 - val_categorical_accuracy: 0.2428 - val_mean_directional_accuracy: 0.4768\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1377.9550 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5162 - val_loss: 1276.5920 - val_accuracy: 0.2393 - val_categorical_accuracy: 0.2393 - val_mean_directional_accuracy: 0.4696\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1182.8888 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5167 - val_loss: 1089.2107 - val_accuracy: 0.2373 - val_categorical_accuracy: 0.2373 - val_mean_directional_accuracy: 0.4670\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1002.8571 - accuracy: 0.2684 - categorical_accuracy: 0.2684 - mean_directional_accuracy: 0.5119 - val_loss: 917.2966 - val_accuracy: 0.2346 - val_categorical_accuracy: 0.2346 - val_mean_directional_accuracy: 0.4654\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 839.5615 - accuracy: 0.2689 - categorical_accuracy: 0.2689 - mean_directional_accuracy: 0.5139 - val_loss: 762.7901 - val_accuracy: 0.2321 - val_categorical_accuracy: 0.2321 - val_mean_directional_accuracy: 0.4642\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 692.6714 - accuracy: 0.2691 - categorical_accuracy: 0.2691 - mean_directional_accuracy: 0.5114 - val_loss: 623.7485 - val_accuracy: 0.2308 - val_categorical_accuracy: 0.2308 - val_mean_directional_accuracy: 0.4609\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 562.1839 - accuracy: 0.2698 - categorical_accuracy: 0.2698 - mean_directional_accuracy: 0.5149 - val_loss: 502.0367 - val_accuracy: 0.2289 - val_categorical_accuracy: 0.2289 - val_mean_directional_accuracy: 0.4582\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 448.3862 - accuracy: 0.2682 - categorical_accuracy: 0.2682 - mean_directional_accuracy: 0.5112 - val_loss: 396.1913 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 350.6297 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 306.5790 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 268.5590 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 232.3499 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 202.1675 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 173.6877 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 150.9604 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 130.5161 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 115.9496 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 103.7236 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 95.8374 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 88.5678 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 82.4287 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 76.6999 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 72.1816 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 68.1893 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 65.5951 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 63.4461 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 61.6929 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 59.9396 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 58.3245 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 56.7023 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 55.0966 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 53.4785 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 51.9712 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 50.4652 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 49.0291 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 47.5670 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 46.2369 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 44.9017 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 43.5780 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 42.2707 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 41.0630 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 39.8308 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 38.5709 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 37.2855 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 36.1189 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 34.9354 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 33.7822 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 32.6458 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 31.6520 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 30.6448 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 29.6048 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 28.5449 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 27.5881 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 26.6255 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 25.6291 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 24.6141 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 23.6701 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 22.7087 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 21.7217 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 20.7532 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 19.8966 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 19.0247 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 18.1390 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 17.2742 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 16.5201 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.7748 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 15.0035 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.2413 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 13.6285 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 13.0231 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 12.4111 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 11.8019 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 11.2522 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 10.6948 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 10.1355 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 9.5722 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 9.0667 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 8.5547 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 8.0187 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 7.4960 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 7.0795 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 6.6919 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 6.2681 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.8399 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 5.4996 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.2112 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 4.9363 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.7001 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 4.5457 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.3933 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 57/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 4.1902 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.9881 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 58/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 3.8374 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.7073 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 3.5570 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.4316 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 3.3870 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.3654 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 3.2999 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.2586 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "85/89 [===========================>..] - ETA: 0s - loss: 3.2797 - accuracy: 0.2688 - categorical_accuracy: 0.2688 - mean_directional_accuracy: 0.5114Restoring model weights from the end of the best epoch: 61.\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 3.2798 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.3074 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62: early stopping\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 3.2586 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 3.2585885524749756, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 61 \n",
      "\n",
      "Model time: 0.9253734536468983 minutes\n",
      "\n",
      "Total time: 34.35510693117976 minutes\n",
      "\n",
      "\n",
      "Model  69  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    2\n",
      "Hidden units               [4, 16]\n",
      "Activation function        sigmoid\n",
      "Dropout                        0.4\n",
      "L1                          0.0001\n",
      "L2                             1.0\n",
      "Batch size                     256\n",
      "Optimizer                  RMSprop\n",
      "Learning rate               0.0001\n",
      "Name: 57, dtype: object\n",
      "NN2Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 4s 53ms/step - loss: 22.3318 - accuracy: 0.2478 - categorical_accuracy: 0.2478 - mean_directional_accuracy: 0.4965 - val_loss: 21.9497 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 21.8411 - accuracy: 0.2568 - categorical_accuracy: 0.2568 - mean_directional_accuracy: 0.5049 - val_loss: 21.5073 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 21.3984 - accuracy: 0.2541 - categorical_accuracy: 0.2541 - mean_directional_accuracy: 0.5076 - val_loss: 21.0798 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 20.9838 - accuracy: 0.2496 - categorical_accuracy: 0.2496 - mean_directional_accuracy: 0.4984 - val_loss: 20.6608 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 20.5615 - accuracy: 0.2525 - categorical_accuracy: 0.2525 - mean_directional_accuracy: 0.5067 - val_loss: 20.2499 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 20.1452 - accuracy: 0.2571 - categorical_accuracy: 0.2571 - mean_directional_accuracy: 0.5061 - val_loss: 19.8468 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 19.7412 - accuracy: 0.2633 - categorical_accuracy: 0.2633 - mean_directional_accuracy: 0.5158 - val_loss: 19.4515 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 19.3507 - accuracy: 0.2619 - categorical_accuracy: 0.2619 - mean_directional_accuracy: 0.5149 - val_loss: 19.0637 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 18.9614 - accuracy: 0.2615 - categorical_accuracy: 0.2615 - mean_directional_accuracy: 0.5049 - val_loss: 18.6835 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 18.5862 - accuracy: 0.2511 - categorical_accuracy: 0.2511 - mean_directional_accuracy: 0.5005 - val_loss: 18.3110 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 18.2203 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.5030 - val_loss: 17.9458 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 12/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 17.8556 - accuracy: 0.2436 - categorical_accuracy: 0.2436 - mean_directional_accuracy: 0.4924 - val_loss: 17.5879 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 13/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 17.4975 - accuracy: 0.2575 - categorical_accuracy: 0.2575 - mean_directional_accuracy: 0.4974 - val_loss: 17.2373 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 14/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 17.1655 - accuracy: 0.2490 - categorical_accuracy: 0.2490 - mean_directional_accuracy: 0.4930 - val_loss: 16.8937 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 15/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 16.8077 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.4991 - val_loss: 16.5571 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 16/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 16.4733 - accuracy: 0.2568 - categorical_accuracy: 0.2568 - mean_directional_accuracy: 0.5084 - val_loss: 16.2275 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 17/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 16.1551 - accuracy: 0.2508 - categorical_accuracy: 0.2508 - mean_directional_accuracy: 0.4953 - val_loss: 15.9046 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 18/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 15.8324 - accuracy: 0.2506 - categorical_accuracy: 0.2506 - mean_directional_accuracy: 0.5012 - val_loss: 15.5884 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 19/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 15.5145 - accuracy: 0.2569 - categorical_accuracy: 0.2569 - mean_directional_accuracy: 0.5047 - val_loss: 15.2786 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 20/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 15.2140 - accuracy: 0.2564 - categorical_accuracy: 0.2564 - mean_directional_accuracy: 0.4975 - val_loss: 14.9755 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 21/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 14.9047 - accuracy: 0.2517 - categorical_accuracy: 0.2517 - mean_directional_accuracy: 0.5026 - val_loss: 14.6788 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 22/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 14.6033 - accuracy: 0.2538 - categorical_accuracy: 0.2538 - mean_directional_accuracy: 0.4951 - val_loss: 14.3882 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 23/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 14.3212 - accuracy: 0.2515 - categorical_accuracy: 0.2515 - mean_directional_accuracy: 0.5061 - val_loss: 14.1039 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 24/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 14.0351 - accuracy: 0.2643 - categorical_accuracy: 0.2643 - mean_directional_accuracy: 0.5114 - val_loss: 13.8255 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 25/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 13.7603 - accuracy: 0.2610 - categorical_accuracy: 0.2610 - mean_directional_accuracy: 0.5067 - val_loss: 13.5531 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 26/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 13.4904 - accuracy: 0.2464 - categorical_accuracy: 0.2464 - mean_directional_accuracy: 0.4924 - val_loss: 13.2869 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 27/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 13.2210 - accuracy: 0.2647 - categorical_accuracy: 0.2647 - mean_directional_accuracy: 0.5156 - val_loss: 13.0262 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 28/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 12.9665 - accuracy: 0.2517 - categorical_accuracy: 0.2517 - mean_directional_accuracy: 0.5056 - val_loss: 12.7712 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 29/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 12.7237 - accuracy: 0.2485 - categorical_accuracy: 0.2485 - mean_directional_accuracy: 0.5104 - val_loss: 12.5219 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 30/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 12.4630 - accuracy: 0.2633 - categorical_accuracy: 0.2633 - mean_directional_accuracy: 0.5058 - val_loss: 12.2782 - val_accuracy: 0.2816 - val_categorical_accuracy: 0.2816 - val_mean_directional_accuracy: 0.5418\n",
      "Epoch 31/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 12.2290 - accuracy: 0.2508 - categorical_accuracy: 0.2508 - mean_directional_accuracy: 0.4988 - val_loss: 12.0400 - val_accuracy: 0.2816 - val_categorical_accuracy: 0.2816 - val_mean_directional_accuracy: 0.5418\n",
      "Epoch 32/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 11.9967 - accuracy: 0.2536 - categorical_accuracy: 0.2536 - mean_directional_accuracy: 0.4972 - val_loss: 11.8071 - val_accuracy: 0.2817 - val_categorical_accuracy: 0.2817 - val_mean_directional_accuracy: 0.5420\n",
      "Epoch 33/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 11.7602 - accuracy: 0.2510 - categorical_accuracy: 0.2510 - mean_directional_accuracy: 0.4981 - val_loss: 11.5797 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5419\n",
      "Epoch 34/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 11.5311 - accuracy: 0.2652 - categorical_accuracy: 0.2652 - mean_directional_accuracy: 0.5093 - val_loss: 11.3574 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5418\n",
      "Epoch 35/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 11.3144 - accuracy: 0.2489 - categorical_accuracy: 0.2489 - mean_directional_accuracy: 0.5063 - val_loss: 11.1401 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5424\n",
      "Epoch 36/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 11.0975 - accuracy: 0.2420 - categorical_accuracy: 0.2420 - mean_directional_accuracy: 0.4998 - val_loss: 10.9280 - val_accuracy: 0.2812 - val_categorical_accuracy: 0.2812 - val_mean_directional_accuracy: 0.5424\n",
      "Epoch 37/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 10.8902 - accuracy: 0.2513 - categorical_accuracy: 0.2513 - mean_directional_accuracy: 0.5084 - val_loss: 10.7209 - val_accuracy: 0.2808 - val_categorical_accuracy: 0.2808 - val_mean_directional_accuracy: 0.5420\n",
      "Epoch 38/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 10.6780 - accuracy: 0.2524 - categorical_accuracy: 0.2524 - mean_directional_accuracy: 0.5019 - val_loss: 10.5186 - val_accuracy: 0.2804 - val_categorical_accuracy: 0.2804 - val_mean_directional_accuracy: 0.5418\n",
      "Epoch 39/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 10.4805 - accuracy: 0.2520 - categorical_accuracy: 0.2520 - mean_directional_accuracy: 0.5035 - val_loss: 10.3211 - val_accuracy: 0.2801 - val_categorical_accuracy: 0.2801 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 40/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 10.2881 - accuracy: 0.2441 - categorical_accuracy: 0.2441 - mean_directional_accuracy: 0.4951 - val_loss: 10.1284 - val_accuracy: 0.2799 - val_categorical_accuracy: 0.2799 - val_mean_directional_accuracy: 0.5418\n",
      "Epoch 41/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 10.0784 - accuracy: 0.2633 - categorical_accuracy: 0.2633 - mean_directional_accuracy: 0.5025 - val_loss: 9.9405 - val_accuracy: 0.2792 - val_categorical_accuracy: 0.2792 - val_mean_directional_accuracy: 0.5410\n",
      "Epoch 42/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 9.8951 - accuracy: 0.2633 - categorical_accuracy: 0.2633 - mean_directional_accuracy: 0.5061 - val_loss: 9.7571 - val_accuracy: 0.2792 - val_categorical_accuracy: 0.2792 - val_mean_directional_accuracy: 0.5412\n",
      "Epoch 43/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 9.7178 - accuracy: 0.2515 - categorical_accuracy: 0.2515 - mean_directional_accuracy: 0.5044 - val_loss: 9.5781 - val_accuracy: 0.2775 - val_categorical_accuracy: 0.2775 - val_mean_directional_accuracy: 0.5394\n",
      "Epoch 44/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 9.5418 - accuracy: 0.2511 - categorical_accuracy: 0.2511 - mean_directional_accuracy: 0.5000 - val_loss: 9.4035 - val_accuracy: 0.2761 - val_categorical_accuracy: 0.2761 - val_mean_directional_accuracy: 0.5388\n",
      "Epoch 45/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 9.3746 - accuracy: 0.2538 - categorical_accuracy: 0.2538 - mean_directional_accuracy: 0.5030 - val_loss: 9.2333 - val_accuracy: 0.2741 - val_categorical_accuracy: 0.2741 - val_mean_directional_accuracy: 0.5369\n",
      "Epoch 46/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 9.1934 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.4986 - val_loss: 9.0674 - val_accuracy: 0.2689 - val_categorical_accuracy: 0.2689 - val_mean_directional_accuracy: 0.5342\n",
      "Epoch 47/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 9.0385 - accuracy: 0.2469 - categorical_accuracy: 0.2469 - mean_directional_accuracy: 0.5032 - val_loss: 8.9054 - val_accuracy: 0.2649 - val_categorical_accuracy: 0.2649 - val_mean_directional_accuracy: 0.5308\n",
      "Epoch 48/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 8.8732 - accuracy: 0.2460 - categorical_accuracy: 0.2460 - mean_directional_accuracy: 0.5039 - val_loss: 8.7477 - val_accuracy: 0.2663 - val_categorical_accuracy: 0.2663 - val_mean_directional_accuracy: 0.5335\n",
      "Epoch 49/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 8.7135 - accuracy: 0.2568 - categorical_accuracy: 0.2568 - mean_directional_accuracy: 0.5009 - val_loss: 8.5940 - val_accuracy: 0.2580 - val_categorical_accuracy: 0.2580 - val_mean_directional_accuracy: 0.5279\n",
      "Epoch 50/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 8.5678 - accuracy: 0.2422 - categorical_accuracy: 0.2422 - mean_directional_accuracy: 0.4947 - val_loss: 8.4441 - val_accuracy: 0.2461 - val_categorical_accuracy: 0.2461 - val_mean_directional_accuracy: 0.5027\n",
      "Epoch 51/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 8.4146 - accuracy: 0.2561 - categorical_accuracy: 0.2561 - mean_directional_accuracy: 0.5002 - val_loss: 8.2980 - val_accuracy: 0.2308 - val_categorical_accuracy: 0.2308 - val_mean_directional_accuracy: 0.4783\n",
      "Epoch 52/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 8.2713 - accuracy: 0.2445 - categorical_accuracy: 0.2445 - mean_directional_accuracy: 0.4919 - val_loss: 8.1556 - val_accuracy: 0.2180 - val_categorical_accuracy: 0.2180 - val_mean_directional_accuracy: 0.4633\n",
      "Epoch 53/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 8.1290 - accuracy: 0.2515 - categorical_accuracy: 0.2515 - mean_directional_accuracy: 0.5028 - val_loss: 8.0168 - val_accuracy: 0.2161 - val_categorical_accuracy: 0.2161 - val_mean_directional_accuracy: 0.4602\n",
      "Epoch 54/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 7.9884 - accuracy: 0.2564 - categorical_accuracy: 0.2564 - mean_directional_accuracy: 0.5112 - val_loss: 7.8815 - val_accuracy: 0.2154 - val_categorical_accuracy: 0.2154 - val_mean_directional_accuracy: 0.4590\n",
      "Epoch 55/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 7.8540 - accuracy: 0.2524 - categorical_accuracy: 0.2524 - mean_directional_accuracy: 0.5004 - val_loss: 7.7496 - val_accuracy: 0.2135 - val_categorical_accuracy: 0.2135 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 7.7230 - accuracy: 0.2534 - categorical_accuracy: 0.2534 - mean_directional_accuracy: 0.5090 - val_loss: 7.6210 - val_accuracy: 0.2126 - val_categorical_accuracy: 0.2126 - val_mean_directional_accuracy: 0.4585\n",
      "Epoch 57/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 7.6001 - accuracy: 0.2459 - categorical_accuracy: 0.2459 - mean_directional_accuracy: 0.4937 - val_loss: 7.4958 - val_accuracy: 0.2123 - val_categorical_accuracy: 0.2123 - val_mean_directional_accuracy: 0.4585\n",
      "Epoch 58/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 7.4671 - accuracy: 0.2597 - categorical_accuracy: 0.2597 - mean_directional_accuracy: 0.5126 - val_loss: 7.3737 - val_accuracy: 0.2115 - val_categorical_accuracy: 0.2115 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 7.3494 - accuracy: 0.2490 - categorical_accuracy: 0.2490 - mean_directional_accuracy: 0.4910 - val_loss: 7.2546 - val_accuracy: 0.2122 - val_categorical_accuracy: 0.2122 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 7.2347 - accuracy: 0.2413 - categorical_accuracy: 0.2413 - mean_directional_accuracy: 0.4907 - val_loss: 7.1383 - val_accuracy: 0.2135 - val_categorical_accuracy: 0.2135 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 7.1225 - accuracy: 0.2453 - categorical_accuracy: 0.2453 - mean_directional_accuracy: 0.5051 - val_loss: 7.0250 - val_accuracy: 0.2122 - val_categorical_accuracy: 0.2122 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 7.0053 - accuracy: 0.2492 - categorical_accuracy: 0.2492 - mean_directional_accuracy: 0.4988 - val_loss: 6.9145 - val_accuracy: 0.2133 - val_categorical_accuracy: 0.2133 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 63/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 6.8979 - accuracy: 0.2510 - categorical_accuracy: 0.2510 - mean_directional_accuracy: 0.4946 - val_loss: 6.8068 - val_accuracy: 0.2162 - val_categorical_accuracy: 0.2162 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 64/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 6.7927 - accuracy: 0.2466 - categorical_accuracy: 0.2466 - mean_directional_accuracy: 0.5004 - val_loss: 6.7017 - val_accuracy: 0.2159 - val_categorical_accuracy: 0.2159 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 65/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 6.6868 - accuracy: 0.2525 - categorical_accuracy: 0.2525 - mean_directional_accuracy: 0.4986 - val_loss: 6.5993 - val_accuracy: 0.2219 - val_categorical_accuracy: 0.2219 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 66/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 6.5741 - accuracy: 0.2640 - categorical_accuracy: 0.2640 - mean_directional_accuracy: 0.5061 - val_loss: 6.4992 - val_accuracy: 0.2184 - val_categorical_accuracy: 0.2184 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 67/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 6.4847 - accuracy: 0.2534 - categorical_accuracy: 0.2534 - mean_directional_accuracy: 0.5009 - val_loss: 6.4015 - val_accuracy: 0.2196 - val_categorical_accuracy: 0.2196 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 68/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 6.3781 - accuracy: 0.2585 - categorical_accuracy: 0.2585 - mean_directional_accuracy: 0.5111 - val_loss: 6.3062 - val_accuracy: 0.2249 - val_categorical_accuracy: 0.2249 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 69/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 6.2872 - accuracy: 0.2496 - categorical_accuracy: 0.2496 - mean_directional_accuracy: 0.5047 - val_loss: 6.2129 - val_accuracy: 0.2264 - val_categorical_accuracy: 0.2264 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 70/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 6.1904 - accuracy: 0.2626 - categorical_accuracy: 0.2626 - mean_directional_accuracy: 0.5072 - val_loss: 6.1217 - val_accuracy: 0.2260 - val_categorical_accuracy: 0.2260 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 71/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 6.1041 - accuracy: 0.2587 - categorical_accuracy: 0.2587 - mean_directional_accuracy: 0.5105 - val_loss: 6.0324 - val_accuracy: 0.2285 - val_categorical_accuracy: 0.2285 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 72/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 6.0147 - accuracy: 0.2503 - categorical_accuracy: 0.2503 - mean_directional_accuracy: 0.5005 - val_loss: 5.9451 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 73/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 5.9318 - accuracy: 0.2445 - categorical_accuracy: 0.2445 - mean_directional_accuracy: 0.4982 - val_loss: 5.8595 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 74/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 5.8434 - accuracy: 0.2482 - categorical_accuracy: 0.2482 - mean_directional_accuracy: 0.5039 - val_loss: 5.7756 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 75/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 5.7572 - accuracy: 0.2568 - categorical_accuracy: 0.2568 - mean_directional_accuracy: 0.5042 - val_loss: 5.6935 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 76/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 5.6842 - accuracy: 0.2513 - categorical_accuracy: 0.2513 - mean_directional_accuracy: 0.5000 - val_loss: 5.6127 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 77/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 5.5931 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.5100 - val_loss: 5.5336 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 78/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 5.5169 - accuracy: 0.2511 - categorical_accuracy: 0.2511 - mean_directional_accuracy: 0.5061 - val_loss: 5.4557 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 79/300\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 5.4414 - accuracy: 0.2410 - categorical_accuracy: 0.2410 - mean_directional_accuracy: 0.4858 - val_loss: 5.3791 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 80/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 5.3655 - accuracy: 0.2497 - categorical_accuracy: 0.2497 - mean_directional_accuracy: 0.4967 - val_loss: 5.3035 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 81/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 5.2897 - accuracy: 0.2515 - categorical_accuracy: 0.2515 - mean_directional_accuracy: 0.5084 - val_loss: 5.2291 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 82/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 5.2143 - accuracy: 0.2525 - categorical_accuracy: 0.2525 - mean_directional_accuracy: 0.4967 - val_loss: 5.1558 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 83/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 5.1419 - accuracy: 0.2468 - categorical_accuracy: 0.2468 - mean_directional_accuracy: 0.5007 - val_loss: 5.0834 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 84/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 5.0664 - accuracy: 0.2575 - categorical_accuracy: 0.2575 - mean_directional_accuracy: 0.5016 - val_loss: 5.0120 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 85/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 5.0013 - accuracy: 0.2496 - categorical_accuracy: 0.2496 - mean_directional_accuracy: 0.5176 - val_loss: 4.9412 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 86/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 4.9243 - accuracy: 0.2575 - categorical_accuracy: 0.2575 - mean_directional_accuracy: 0.5076 - val_loss: 4.8716 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 87/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 4.8559 - accuracy: 0.2575 - categorical_accuracy: 0.2575 - mean_directional_accuracy: 0.4975 - val_loss: 4.8028 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 88/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 4.7885 - accuracy: 0.2543 - categorical_accuracy: 0.2543 - mean_directional_accuracy: 0.5067 - val_loss: 4.7348 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 89/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.7237 - accuracy: 0.2520 - categorical_accuracy: 0.2520 - mean_directional_accuracy: 0.5035 - val_loss: 4.6678 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 90/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.6543 - accuracy: 0.2499 - categorical_accuracy: 0.2499 - mean_directional_accuracy: 0.4960 - val_loss: 4.6014 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 91/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 4.5877 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.5056 - val_loss: 4.5360 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 92/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 4.5218 - accuracy: 0.2499 - categorical_accuracy: 0.2499 - mean_directional_accuracy: 0.5039 - val_loss: 4.4713 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 93/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 4.4638 - accuracy: 0.2492 - categorical_accuracy: 0.2492 - mean_directional_accuracy: 0.5086 - val_loss: 4.4075 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 94/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 4.3947 - accuracy: 0.2464 - categorical_accuracy: 0.2464 - mean_directional_accuracy: 0.4944 - val_loss: 4.3445 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 95/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 4.3300 - accuracy: 0.2545 - categorical_accuracy: 0.2545 - mean_directional_accuracy: 0.4998 - val_loss: 4.2825 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 96/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 4.2699 - accuracy: 0.2452 - categorical_accuracy: 0.2452 - mean_directional_accuracy: 0.5065 - val_loss: 4.2212 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 97/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 4.2065 - accuracy: 0.2446 - categorical_accuracy: 0.2446 - mean_directional_accuracy: 0.4942 - val_loss: 4.1607 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 98/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 4.1452 - accuracy: 0.2550 - categorical_accuracy: 0.2550 - mean_directional_accuracy: 0.5076 - val_loss: 4.1009 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 99/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4.0866 - accuracy: 0.2520 - categorical_accuracy: 0.2520 - mean_directional_accuracy: 0.4974 - val_loss: 4.0421 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 100/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 4.0287 - accuracy: 0.2497 - categorical_accuracy: 0.2497 - mean_directional_accuracy: 0.4979 - val_loss: 3.9839 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 101/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 3.9694 - accuracy: 0.2536 - categorical_accuracy: 0.2536 - mean_directional_accuracy: 0.5065 - val_loss: 3.9267 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 102/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 3.9166 - accuracy: 0.2452 - categorical_accuracy: 0.2452 - mean_directional_accuracy: 0.4961 - val_loss: 3.8702 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 103/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 3.8582 - accuracy: 0.2446 - categorical_accuracy: 0.2446 - mean_directional_accuracy: 0.4965 - val_loss: 3.8144 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 104/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 3.7952 - accuracy: 0.2547 - categorical_accuracy: 0.2547 - mean_directional_accuracy: 0.5005 - val_loss: 3.7596 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 105/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.7481 - accuracy: 0.2462 - categorical_accuracy: 0.2462 - mean_directional_accuracy: 0.5009 - val_loss: 3.7054 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 106/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.6895 - accuracy: 0.2599 - categorical_accuracy: 0.2599 - mean_directional_accuracy: 0.5061 - val_loss: 3.6521 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 107/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 3.6365 - accuracy: 0.2524 - categorical_accuracy: 0.2524 - mean_directional_accuracy: 0.5040 - val_loss: 3.5995 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 108/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 3.5802 - accuracy: 0.2543 - categorical_accuracy: 0.2543 - mean_directional_accuracy: 0.4975 - val_loss: 3.5477 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 109/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 3.5348 - accuracy: 0.2506 - categorical_accuracy: 0.2506 - mean_directional_accuracy: 0.4956 - val_loss: 3.4966 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 110/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 3.4869 - accuracy: 0.2525 - categorical_accuracy: 0.2525 - mean_directional_accuracy: 0.4940 - val_loss: 3.4464 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 111/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 3.4323 - accuracy: 0.2513 - categorical_accuracy: 0.2513 - mean_directional_accuracy: 0.5007 - val_loss: 3.3971 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 112/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 3.3798 - accuracy: 0.2527 - categorical_accuracy: 0.2527 - mean_directional_accuracy: 0.5011 - val_loss: 3.3484 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 113/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 3.3342 - accuracy: 0.2468 - categorical_accuracy: 0.2468 - mean_directional_accuracy: 0.4895 - val_loss: 3.3004 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 114/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3.2868 - accuracy: 0.2536 - categorical_accuracy: 0.2536 - mean_directional_accuracy: 0.5011 - val_loss: 3.2532 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 115/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 3.2392 - accuracy: 0.2515 - categorical_accuracy: 0.2515 - mean_directional_accuracy: 0.4946 - val_loss: 3.2068 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 116/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 3.1915 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.4956 - val_loss: 3.1611 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 117/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 3.1474 - accuracy: 0.2594 - categorical_accuracy: 0.2594 - mean_directional_accuracy: 0.5053 - val_loss: 3.1162 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 118/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 3.0993 - accuracy: 0.2559 - categorical_accuracy: 0.2559 - mean_directional_accuracy: 0.4921 - val_loss: 3.0719 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 119/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 3.0590 - accuracy: 0.2561 - categorical_accuracy: 0.2561 - mean_directional_accuracy: 0.4977 - val_loss: 3.0285 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 120/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 3.0130 - accuracy: 0.2531 - categorical_accuracy: 0.2531 - mean_directional_accuracy: 0.4953 - val_loss: 2.9858 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 121/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 2.9725 - accuracy: 0.2506 - categorical_accuracy: 0.2506 - mean_directional_accuracy: 0.4988 - val_loss: 2.9439 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 122/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2.9287 - accuracy: 0.2561 - categorical_accuracy: 0.2561 - mean_directional_accuracy: 0.5084 - val_loss: 2.9026 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 123/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2.8910 - accuracy: 0.2604 - categorical_accuracy: 0.2604 - mean_directional_accuracy: 0.5042 - val_loss: 2.8621 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 124/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2.8513 - accuracy: 0.2418 - categorical_accuracy: 0.2418 - mean_directional_accuracy: 0.4939 - val_loss: 2.8222 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 125/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.8097 - accuracy: 0.2548 - categorical_accuracy: 0.2548 - mean_directional_accuracy: 0.5026 - val_loss: 2.7832 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 126/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 2.7702 - accuracy: 0.2527 - categorical_accuracy: 0.2527 - mean_directional_accuracy: 0.5023 - val_loss: 2.7448 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 127/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 2.7333 - accuracy: 0.2478 - categorical_accuracy: 0.2478 - mean_directional_accuracy: 0.5068 - val_loss: 2.7071 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 128/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.6916 - accuracy: 0.2615 - categorical_accuracy: 0.2615 - mean_directional_accuracy: 0.5040 - val_loss: 2.6702 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 129/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 2.6587 - accuracy: 0.2522 - categorical_accuracy: 0.2522 - mean_directional_accuracy: 0.4986 - val_loss: 2.6338 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 130/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 2.6219 - accuracy: 0.2510 - categorical_accuracy: 0.2510 - mean_directional_accuracy: 0.5081 - val_loss: 2.5980 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 131/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 2.5858 - accuracy: 0.2540 - categorical_accuracy: 0.2540 - mean_directional_accuracy: 0.5042 - val_loss: 2.5631 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 132/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 2.5514 - accuracy: 0.2541 - categorical_accuracy: 0.2541 - mean_directional_accuracy: 0.5093 - val_loss: 2.5289 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 133/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2.5178 - accuracy: 0.2527 - categorical_accuracy: 0.2527 - mean_directional_accuracy: 0.5035 - val_loss: 2.4952 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 134/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 2.4820 - accuracy: 0.2694 - categorical_accuracy: 0.2694 - mean_directional_accuracy: 0.5174 - val_loss: 2.4623 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 135/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.4509 - accuracy: 0.2564 - categorical_accuracy: 0.2564 - mean_directional_accuracy: 0.5139 - val_loss: 2.4300 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 136/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.4177 - accuracy: 0.2536 - categorical_accuracy: 0.2536 - mean_directional_accuracy: 0.4958 - val_loss: 2.3985 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 137/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 2.3872 - accuracy: 0.2480 - categorical_accuracy: 0.2480 - mean_directional_accuracy: 0.4933 - val_loss: 2.3676 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 138/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.3565 - accuracy: 0.2518 - categorical_accuracy: 0.2518 - mean_directional_accuracy: 0.5042 - val_loss: 2.3373 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 139/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.3284 - accuracy: 0.2450 - categorical_accuracy: 0.2450 - mean_directional_accuracy: 0.5004 - val_loss: 2.3076 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 140/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.2931 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5091 - val_loss: 2.2786 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 141/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 2.2673 - accuracy: 0.2571 - categorical_accuracy: 0.2571 - mean_directional_accuracy: 0.5025 - val_loss: 2.2502 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 142/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 2.2406 - accuracy: 0.2608 - categorical_accuracy: 0.2608 - mean_directional_accuracy: 0.5126 - val_loss: 2.2225 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 143/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.2124 - accuracy: 0.2513 - categorical_accuracy: 0.2513 - mean_directional_accuracy: 0.4919 - val_loss: 2.1955 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 144/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 2.1856 - accuracy: 0.2604 - categorical_accuracy: 0.2604 - mean_directional_accuracy: 0.5119 - val_loss: 2.1690 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 145/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 2.1570 - accuracy: 0.2604 - categorical_accuracy: 0.2604 - mean_directional_accuracy: 0.5030 - val_loss: 2.1432 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 146/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.1322 - accuracy: 0.2613 - categorical_accuracy: 0.2613 - mean_directional_accuracy: 0.5054 - val_loss: 2.1178 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 147/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.1079 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.5012 - val_loss: 2.0931 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 148/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 2.0848 - accuracy: 0.2538 - categorical_accuracy: 0.2538 - mean_directional_accuracy: 0.5053 - val_loss: 2.0691 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 149/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 2.0586 - accuracy: 0.2508 - categorical_accuracy: 0.2508 - mean_directional_accuracy: 0.4928 - val_loss: 2.0456 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 150/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 2.0367 - accuracy: 0.2520 - categorical_accuracy: 0.2520 - mean_directional_accuracy: 0.4972 - val_loss: 2.0225 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 151/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.0155 - accuracy: 0.2466 - categorical_accuracy: 0.2466 - mean_directional_accuracy: 0.4954 - val_loss: 2.0001 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 152/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.9900 - accuracy: 0.2696 - categorical_accuracy: 0.2696 - mean_directional_accuracy: 0.5133 - val_loss: 1.9781 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 153/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.9684 - accuracy: 0.2601 - categorical_accuracy: 0.2601 - mean_directional_accuracy: 0.5076 - val_loss: 1.9568 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 154/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.9491 - accuracy: 0.2573 - categorical_accuracy: 0.2573 - mean_directional_accuracy: 0.5058 - val_loss: 1.9358 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 155/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.9264 - accuracy: 0.2571 - categorical_accuracy: 0.2571 - mean_directional_accuracy: 0.5070 - val_loss: 1.9155 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 156/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.9076 - accuracy: 0.2515 - categorical_accuracy: 0.2515 - mean_directional_accuracy: 0.5035 - val_loss: 1.8956 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 157/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.8866 - accuracy: 0.2561 - categorical_accuracy: 0.2561 - mean_directional_accuracy: 0.5056 - val_loss: 1.8763 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 158/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.8667 - accuracy: 0.2655 - categorical_accuracy: 0.2655 - mean_directional_accuracy: 0.5191 - val_loss: 1.8575 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 159/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.8501 - accuracy: 0.2548 - categorical_accuracy: 0.2548 - mean_directional_accuracy: 0.4977 - val_loss: 1.8391 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 160/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.8290 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5163 - val_loss: 1.8212 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 161/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.8128 - accuracy: 0.2562 - categorical_accuracy: 0.2562 - mean_directional_accuracy: 0.5033 - val_loss: 1.8037 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 162/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.7956 - accuracy: 0.2543 - categorical_accuracy: 0.2543 - mean_directional_accuracy: 0.4954 - val_loss: 1.7868 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 163/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.7783 - accuracy: 0.2592 - categorical_accuracy: 0.2592 - mean_directional_accuracy: 0.5005 - val_loss: 1.7703 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 164/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.7622 - accuracy: 0.2568 - categorical_accuracy: 0.2568 - mean_directional_accuracy: 0.4939 - val_loss: 1.7543 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 165/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.7455 - accuracy: 0.2710 - categorical_accuracy: 0.2710 - mean_directional_accuracy: 0.5205 - val_loss: 1.7386 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 166/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.7305 - accuracy: 0.2578 - categorical_accuracy: 0.2578 - mean_directional_accuracy: 0.5039 - val_loss: 1.7235 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 167/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7166 - accuracy: 0.2541 - categorical_accuracy: 0.2541 - mean_directional_accuracy: 0.5019 - val_loss: 1.7086 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 168/300\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 1.7016 - accuracy: 0.2640 - categorical_accuracy: 0.2640 - mean_directional_accuracy: 0.5074 - val_loss: 1.6941 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 169/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6875 - accuracy: 0.2564 - categorical_accuracy: 0.2564 - mean_directional_accuracy: 0.5054 - val_loss: 1.6802 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 170/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.6728 - accuracy: 0.2561 - categorical_accuracy: 0.2561 - mean_directional_accuracy: 0.5028 - val_loss: 1.6666 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 171/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.6600 - accuracy: 0.2566 - categorical_accuracy: 0.2566 - mean_directional_accuracy: 0.5163 - val_loss: 1.6536 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 172/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.6457 - accuracy: 0.2652 - categorical_accuracy: 0.2652 - mean_directional_accuracy: 0.5084 - val_loss: 1.6409 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 173/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.6348 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.5068 - val_loss: 1.6285 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 174/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.6238 - accuracy: 0.2543 - categorical_accuracy: 0.2543 - mean_directional_accuracy: 0.4991 - val_loss: 1.6165 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 175/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.6102 - accuracy: 0.2571 - categorical_accuracy: 0.2571 - mean_directional_accuracy: 0.5019 - val_loss: 1.6051 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 176/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5983 - accuracy: 0.2612 - categorical_accuracy: 0.2612 - mean_directional_accuracy: 0.5032 - val_loss: 1.5941 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 177/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 1.5878 - accuracy: 0.2641 - categorical_accuracy: 0.2641 - mean_directional_accuracy: 0.5061 - val_loss: 1.5833 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 178/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5765 - accuracy: 0.2655 - categorical_accuracy: 0.2655 - mean_directional_accuracy: 0.5046 - val_loss: 1.5730 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 179/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5683 - accuracy: 0.2532 - categorical_accuracy: 0.2532 - mean_directional_accuracy: 0.5025 - val_loss: 1.5630 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 180/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5576 - accuracy: 0.2601 - categorical_accuracy: 0.2601 - mean_directional_accuracy: 0.5040 - val_loss: 1.5534 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 181/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5479 - accuracy: 0.2641 - categorical_accuracy: 0.2641 - mean_directional_accuracy: 0.5095 - val_loss: 1.5443 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 182/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.5394 - accuracy: 0.2613 - categorical_accuracy: 0.2613 - mean_directional_accuracy: 0.5079 - val_loss: 1.5354 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 183/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.5307 - accuracy: 0.2654 - categorical_accuracy: 0.2654 - mean_directional_accuracy: 0.5033 - val_loss: 1.5269 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 184/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.5220 - accuracy: 0.2612 - categorical_accuracy: 0.2612 - mean_directional_accuracy: 0.5072 - val_loss: 1.5187 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 185/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.5131 - accuracy: 0.2643 - categorical_accuracy: 0.2643 - mean_directional_accuracy: 0.5109 - val_loss: 1.5108 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 186/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.5055 - accuracy: 0.2694 - categorical_accuracy: 0.2694 - mean_directional_accuracy: 0.5111 - val_loss: 1.5034 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 187/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 1.4984 - accuracy: 0.2701 - categorical_accuracy: 0.2701 - mean_directional_accuracy: 0.5130 - val_loss: 1.4963 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 188/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4915 - accuracy: 0.2671 - categorical_accuracy: 0.2671 - mean_directional_accuracy: 0.5123 - val_loss: 1.4894 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 189/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4849 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5142 - val_loss: 1.4828 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 190/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4792 - accuracy: 0.2626 - categorical_accuracy: 0.2626 - mean_directional_accuracy: 0.5058 - val_loss: 1.4766 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 191/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4721 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5077 - val_loss: 1.4708 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 192/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 1.4667 - accuracy: 0.2687 - categorical_accuracy: 0.2687 - mean_directional_accuracy: 0.5132 - val_loss: 1.4652 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 193/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4611 - accuracy: 0.2627 - categorical_accuracy: 0.2627 - mean_directional_accuracy: 0.5074 - val_loss: 1.4598 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 194/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.4554 - accuracy: 0.2659 - categorical_accuracy: 0.2659 - mean_directional_accuracy: 0.5088 - val_loss: 1.4547 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 195/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.4505 - accuracy: 0.2687 - categorical_accuracy: 0.2687 - mean_directional_accuracy: 0.5112 - val_loss: 1.4499 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 196/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.4467 - accuracy: 0.2638 - categorical_accuracy: 0.2638 - mean_directional_accuracy: 0.5077 - val_loss: 1.4453 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 197/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4413 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5095 - val_loss: 1.4409 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 198/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4379 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5102 - val_loss: 1.4367 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 199/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.4329 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5100 - val_loss: 1.4329 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 200/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4296 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4292 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 201/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4261 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4256 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 202/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4223 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5107 - val_loss: 1.4225 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 203/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4194 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5107 - val_loss: 1.4194 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 204/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.4169 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4165 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 205/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.4140 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4138 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 206/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.4115 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4114 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 207/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4087 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4091 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 208/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4065 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4069 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 209/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.4045 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4049 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 210/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.4026 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4030 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 211/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.4007 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4013 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 212/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3990 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3997 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 213/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3974 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3982 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 214/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3962 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3969 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 215/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3950 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3957 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 216/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3938 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3946 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 217/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3927 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3937 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 218/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3918 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3927 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 219/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3910 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3920 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 220/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3904 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3913 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 221/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3896 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3906 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 222/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3890 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3901 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 223/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3884 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3897 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 224/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3880 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3893 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 225/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3877 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3889 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 226/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3872 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3886 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 227/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3870 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3882 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 228/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3868 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3881 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 229/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3866 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3879 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 230/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3864 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3878 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 231/300\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 1.3863 - accuracy: 0.2654 - categorical_accuracy: 0.2654 - mean_directional_accuracy: 0.5059Restoring model weights from the end of the best epoch: 230.\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3863 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3879 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 231: early stopping\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3878 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3877944946289062, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 230 \n",
      "\n",
      "Model time: 1.2393781542778015 minutes\n",
      "\n",
      "Total time: 35.59455178305507 minutes\n",
      "\n",
      "\n",
      "Model  70  out of  111\n",
      "Model type               FeedForward\n",
      "Hidden layers                      4\n",
      "Hidden units           [2, 8, 16, 2]\n",
      "Activation function          sigmoid\n",
      "Dropout                          0.9\n",
      "L1                              10.0\n",
      "L2                               0.0\n",
      "Batch size                        64\n",
      "Optimizer                       Adam\n",
      "Learning rate                   0.01\n",
      "Name: 58, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 5s 22ms/step - loss: 155.8364 - accuracy: 0.2494 - categorical_accuracy: 0.2494 - mean_directional_accuracy: 0.4963 - val_loss: 8.8270 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 13ms/step - loss: 7.8504 - accuracy: 0.2689 - categorical_accuracy: 0.2689 - mean_directional_accuracy: 0.5140 - val_loss: 7.3968 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 7.6757 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5107 - val_loss: 7.3263 - val_accuracy: 0.2296 - val_categorical_accuracy: 0.2296 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "70/89 [======================>.......] - ETA: 0s - loss: 7.6611 - accuracy: 0.2558 - categorical_accuracy: 0.2558 - mean_directional_accuracy: 0.5143Restoring model weights from the end of the best epoch: 3.\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 7.6736 - accuracy: 0.2592 - categorical_accuracy: 0.2592 - mean_directional_accuracy: 0.5111 - val_loss: 7.6409 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4: early stopping\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 7.3263 - accuracy: 0.2296 - categorical_accuracy: 0.2296 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 7.326256275177002, 'accuracy': 0.22964508831501007, 'categorical_accuracy': 0.22964508831501007, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 3 \n",
      "\n",
      "Model time: 0.13842755556106567 minutes\n",
      "\n",
      "Total time: 35.733056765049696 minutes\n",
      "\n",
      "\n",
      "Model  71  out of  111\n",
      "Model type                         FeedForward\n",
      "Hidden layers                                5\n",
      "Hidden units           [256, 128, 64, 16, 256]\n",
      "Activation function                       tanh\n",
      "Dropout                                    0.9\n",
      "L1                                        10.0\n",
      "L2                                       0.001\n",
      "Batch size                                  64\n",
      "Optimizer                                 Adam\n",
      "Learning rate                            0.001\n",
      "Name: 59, dtype: object\n",
      "NN5Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 6s 27ms/step - loss: 28312.8262 - accuracy: 0.2536 - categorical_accuracy: 0.2536 - mean_directional_accuracy: 0.5018 - val_loss: 6695.2964 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 16ms/step - loss: 1809.7583 - accuracy: 0.2573 - categorical_accuracy: 0.2573 - mean_directional_accuracy: 0.5032 - val_loss: 310.4792 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 15ms/step - loss: 183.4458 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 117.6781 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 16ms/step - loss: 115.7824 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 115.0064 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - ETA: 0s - loss: 115.3809 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105Restoring model weights from the end of the best epoch: 4.\n",
      "89/89 [==============================] - 2s 18ms/step - loss: 115.3809 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 115.6556 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5: early stopping\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 115.0064 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 115.00635528564453, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 4 \n",
      "\n",
      "Model time: 0.2080514132976532 minutes\n",
      "\n",
      "Total time: 35.941191483289 minutes\n",
      "\n",
      "\n",
      "Model  72  out of  111\n",
      "Model type                      FeedForward\n",
      "Hidden layers                             5\n",
      "Hidden units           [256, 128, 2, 2, 16]\n",
      "Activation function                    tanh\n",
      "Dropout                                 0.1\n",
      "L1                                     10.0\n",
      "L2                                      0.1\n",
      "Batch size                               32\n",
      "Optimizer                              Adam\n",
      "Learning rate                          0.01\n",
      "Name: 60, dtype: object\n",
      "NN5Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 7s 18ms/step - loss: 2849.5654 - accuracy: 0.2601 - categorical_accuracy: 0.2601 - mean_directional_accuracy: 0.5040 - val_loss: 988.4565 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 987.1665 - accuracy: 0.2617 - categorical_accuracy: 0.2617 - mean_directional_accuracy: 0.5046 - val_loss: 957.2204 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "174/178 [============================>.] - ETA: 0s - loss: 985.3425 - accuracy: 0.2663 - categorical_accuracy: 0.2663 - mean_directional_accuracy: 0.5043Restoring model weights from the end of the best epoch: 2.\n",
      "178/178 [==============================] - 2s 12ms/step - loss: 985.3375 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5046 - val_loss: 994.6448 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3: early stopping\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 957.2204 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 957.2203979492188, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 2 \n",
      "\n",
      "Model time: 0.21019848436117172 minutes\n",
      "\n",
      "Total time: 36.151498172432184 minutes\n",
      "\n",
      "\n",
      "Model  73  out of  111\n",
      "Model type                  FeedForward\n",
      "Hidden layers                         5\n",
      "Hidden units           [8, 64, 4, 1, 2]\n",
      "Activation function                tanh\n",
      "Dropout                             0.8\n",
      "L1                                100.0\n",
      "L2                              0.00001\n",
      "Batch size                           32\n",
      "Optimizer                          Adam\n",
      "Learning rate                    0.0001\n",
      "Name: 61, dtype: object\n",
      "NN5Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 6s 15ms/step - loss: 22688.5059 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5111 - val_loss: 20882.4648 - val_accuracy: 0.2426 - val_categorical_accuracy: 0.2426 - val_mean_directional_accuracy: 0.4602\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 19200.7832 - accuracy: 0.2631 - categorical_accuracy: 0.2631 - mean_directional_accuracy: 0.5098 - val_loss: 17544.0703 - val_accuracy: 0.2380 - val_categorical_accuracy: 0.2380 - val_mean_directional_accuracy: 0.4598\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 16019.2217 - accuracy: 0.2655 - categorical_accuracy: 0.2655 - mean_directional_accuracy: 0.5105 - val_loss: 14537.1865 - val_accuracy: 0.2362 - val_categorical_accuracy: 0.2362 - val_mean_directional_accuracy: 0.4602\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 13177.8896 - accuracy: 0.2650 - categorical_accuracy: 0.2650 - mean_directional_accuracy: 0.5090 - val_loss: 11854.8232 - val_accuracy: 0.2373 - val_categorical_accuracy: 0.2373 - val_mean_directional_accuracy: 0.4619\n",
      "Epoch 5/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 10651.0850 - accuracy: 0.2657 - categorical_accuracy: 0.2657 - mean_directional_accuracy: 0.5097 - val_loss: 9480.2764 - val_accuracy: 0.2379 - val_categorical_accuracy: 0.2379 - val_mean_directional_accuracy: 0.4722\n",
      "Epoch 6/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 8421.1816 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5125 - val_loss: 7404.0044 - val_accuracy: 0.2440 - val_categorical_accuracy: 0.2440 - val_mean_directional_accuracy: 0.4902\n",
      "Epoch 7/300\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 6513.0615 - accuracy: 0.2684 - categorical_accuracy: 0.2684 - mean_directional_accuracy: 0.5126 - val_loss: 5668.7729 - val_accuracy: 0.2647 - val_categorical_accuracy: 0.2647 - val_mean_directional_accuracy: 0.5191\n",
      "Epoch 8/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 4942.8081 - accuracy: 0.2645 - categorical_accuracy: 0.2645 - mean_directional_accuracy: 0.5097 - val_loss: 4263.8032 - val_accuracy: 0.2329 - val_categorical_accuracy: 0.2329 - val_mean_directional_accuracy: 0.4653\n",
      "Epoch 9/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 3693.6221 - accuracy: 0.2652 - categorical_accuracy: 0.2652 - mean_directional_accuracy: 0.5095 - val_loss: 3168.9995 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 2753.4153 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5109 - val_loss: 2380.5898 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2108.0151 - accuracy: 0.2661 - categorical_accuracy: 0.2661 - mean_directional_accuracy: 0.5102 - val_loss: 1861.5583 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1651.6866 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5104 - val_loss: 1455.8416 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1286.7489 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5105 - val_loss: 1124.4325 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 986.9536 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 859.7751 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 759.1069 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5105 - val_loss: 673.2278 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 620.9590 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5105 - val_loss: 581.8103 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 563.8961 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 550.2444 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 538.7375 - accuracy: 0.2669 - categorical_accuracy: 0.2669 - mean_directional_accuracy: 0.5105 - val_loss: 527.0293 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 516.2080 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 505.4912 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 495.0218 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 484.9155 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 475.1997 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 465.3501 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 455.6182 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 445.7753 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 436.0398 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 426.2375 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 416.8409 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 407.8014 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 25/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 398.9301 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 389.9503 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 381.1310 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 372.2063 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 363.3292 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 354.3116 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 28/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 345.5306 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 336.5807 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 29/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 327.7307 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 318.7822 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 30/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 309.9310 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 300.9517 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 31/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 292.1313 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 283.1586 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 32/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 274.3312 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 265.4606 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 33/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 257.0506 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 248.9534 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 240.9756 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 232.9772 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 224.9570 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 216.9234 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 208.9359 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 200.8798 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 192.9158 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 184.8717 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 176.8981 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 168.8164 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 161.3828 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 154.7368 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 148.5630 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 142.6542 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 137.3214 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 131.9175 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 126.6403 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 121.2508 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 116.7716 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 112.3298 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 107.8659 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 103.3988 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 98.9675 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 94.4590 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 90.0667 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 85.5980 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 81.7176 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 78.1232 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 75.2190 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 72.4974 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 69.8610 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 67.2006 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 64.5216 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 61.8462 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 59.1812 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 56.4752 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 53.8420 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 51.1943 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 48.5012 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 45.8178 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 43.1616 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 40.5323 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 37.8235 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 35.1283 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 33.0744 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 31.3221 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 57/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 30.1444 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 29.2568 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 58/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 28.3453 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 27.4013 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 26.5650 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 25.7100 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 24.7852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 23.8442 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 23.0057 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 22.0593 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 21.2237 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 20.3714 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 63/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 19.4461 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 18.5110 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 64/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 17.6647 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.7959 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 65/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 15.8837 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.0022 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 66/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 14.1046 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 13.2474 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 67/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 12.3245 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 11.4971 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 68/300\n",
      "178/178 [==============================] - 2s 8ms/step - loss: 10.5449 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 9.6959 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 69/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 8.7646 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 7.8399 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 70/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 6.9844 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 6.0197 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 71/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 5.2053 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.3525 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 72/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 4.0880 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.0794 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 73/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 4.0684 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.0629 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 74/300\n",
      "176/178 [============================>.] - ETA: 0s - loss: 4.0683 - accuracy: 0.2676 - categorical_accuracy: 0.2676 - mean_directional_accuracy: 0.5103Restoring model weights from the end of the best epoch: 73.\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 4.0683 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.0856 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 74: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 4.0629 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 4.062904357910156, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 73 \n",
      "\n",
      "Model time: 1.9246906451880932 minutes\n",
      "\n",
      "Total time: 38.07627972215414 minutes\n",
      "\n",
      "\n",
      "Model  74  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    2\n",
      "Hidden units              [8, 256]\n",
      "Activation function         linear\n",
      "Dropout                        0.2\n",
      "L1                             0.1\n",
      "L2                         0.00001\n",
      "Batch size                     256\n",
      "Optimizer                  RMSprop\n",
      "Learning rate                0.001\n",
      "Name: 62, dtype: object\n",
      "NN2Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 4s 50ms/step - loss: 30.4850 - accuracy: 0.2575 - categorical_accuracy: 0.2575 - mean_directional_accuracy: 0.5054 - val_loss: 24.9765 - val_accuracy: 0.2341 - val_categorical_accuracy: 0.2341 - val_mean_directional_accuracy: 0.4594\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 21.3434 - accuracy: 0.2446 - categorical_accuracy: 0.2446 - mean_directional_accuracy: 0.4982 - val_loss: 17.4336 - val_accuracy: 0.2277 - val_categorical_accuracy: 0.2277 - val_mean_directional_accuracy: 0.4581\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 14.5327 - accuracy: 0.2657 - categorical_accuracy: 0.2657 - mean_directional_accuracy: 0.5126 - val_loss: 11.4135 - val_accuracy: 0.2274 - val_categorical_accuracy: 0.2274 - val_mean_directional_accuracy: 0.4573\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 9.1413 - accuracy: 0.2622 - categorical_accuracy: 0.2622 - mean_directional_accuracy: 0.5086 - val_loss: 6.7618 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 5.1898 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5104 - val_loss: 3.6554 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 2.8038 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.0994 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.8536 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6670 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.6187 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6053 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.6061 - accuracy: 0.2704 - categorical_accuracy: 0.2704 - mean_directional_accuracy: 0.5139Restoring model weights from the end of the best epoch: 8.\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.6062 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6113 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9: early stopping\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.6053 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.6053271293640137, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 8 \n",
      "\n",
      "Model time: 0.12551670894026756 minutes\n",
      "\n",
      "Total time: 38.202722270041704 minutes\n",
      "\n",
      "\n",
      "Model  75  out of  111\n",
      "Model type                 FeedForward\n",
      "Hidden layers                        4\n",
      "Hidden units           [16, 256, 2, 1]\n",
      "Activation function            sigmoid\n",
      "Dropout                            0.5\n",
      "L1                                0.01\n",
      "L2                              0.0001\n",
      "Batch size                         256\n",
      "Optimizer                      RMSprop\n",
      "Learning rate                     0.01\n",
      "Name: 63, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 5s 58ms/step - loss: 2.5343 - accuracy: 0.2392 - categorical_accuracy: 0.2392 - mean_directional_accuracy: 0.4837 - val_loss: 1.8036 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.7773 - accuracy: 0.2587 - categorical_accuracy: 0.2587 - mean_directional_accuracy: 0.5139 - val_loss: 1.7548 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 3/300\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 1.7660 - accuracy: 0.2600 - categorical_accuracy: 0.2600 - mean_directional_accuracy: 0.5032Restoring model weights from the end of the best epoch: 2.\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.7647 - accuracy: 0.2640 - categorical_accuracy: 0.2640 - mean_directional_accuracy: 0.5037 - val_loss: 1.7737 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3: early stopping\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.7548 - accuracy: 0.2814 - categorical_accuracy: 0.2814 - mean_directional_accuracy: 0.5416\n",
      "{'loss': 1.7547961473464966, 'accuracy': 0.28144571185112, 'categorical_accuracy': 0.28144571185112, 'mean_directional_accuracy': 0.5416231751441956} \n",
      " 2 \n",
      "\n",
      "Model time: 0.11608294025063515 minutes\n",
      "\n",
      "Total time: 38.318886678665876 minutes\n",
      "\n",
      "\n",
      "Model  76  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    2\n",
      "Hidden units               [4, 16]\n",
      "Activation function           relu\n",
      "Dropout                        0.2\n",
      "L1                          0.0001\n",
      "L2                         0.00001\n",
      "Batch size                      64\n",
      "Optimizer                     Adam\n",
      "Learning rate                 0.01\n",
      "Name: 64, dtype: object\n",
      "NN2Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 4s 17ms/step - loss: 1.3899 - accuracy: 0.2775 - categorical_accuracy: 0.2775 - mean_directional_accuracy: 0.5019 - val_loss: 1.3815 - val_accuracy: 0.2764 - val_categorical_accuracy: 0.2764 - val_mean_directional_accuracy: 0.4909\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.3819 - accuracy: 0.2878 - categorical_accuracy: 0.2878 - mean_directional_accuracy: 0.5060Restoring model weights from the end of the best epoch: 1.\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3819 - accuracy: 0.2878 - categorical_accuracy: 0.2878 - mean_directional_accuracy: 0.5060 - val_loss: 1.3832 - val_accuracy: 0.3014 - val_categorical_accuracy: 0.3014 - val_mean_directional_accuracy: 0.5324\n",
      "Epoch 2: early stopping\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 1.3815 - accuracy: 0.2764 - categorical_accuracy: 0.2764 - mean_directional_accuracy: 0.4909\n",
      "{'loss': 1.3815499544143677, 'accuracy': 0.2763569951057434, 'categorical_accuracy': 0.2763569951057434, 'mean_directional_accuracy': 0.49086639285087585} \n",
      " 1 \n",
      "\n",
      "Model time: 0.08592764660716057 minutes\n",
      "\n",
      "Total time: 38.40488099306822 minutes\n",
      "\n",
      "\n",
      "Model  77  out of  111\n",
      "Model type               FeedForward\n",
      "Hidden layers                      4\n",
      "Hidden units           [8, 1, 8, 16]\n",
      "Activation function           linear\n",
      "Dropout                          0.3\n",
      "L1                               0.0\n",
      "L2                            0.0001\n",
      "Batch size                        16\n",
      "Optimizer                    RMSprop\n",
      "Learning rate                   0.01\n",
      "Name: 65, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 6s 8ms/step - loss: 1.4116 - accuracy: 0.2654 - categorical_accuracy: 0.2654 - mean_directional_accuracy: 0.5084 - val_loss: 1.3913 - val_accuracy: 0.2507 - val_categorical_accuracy: 0.2507 - val_mean_directional_accuracy: 0.4580\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 1.3866 - accuracy: 0.2626 - categorical_accuracy: 0.2626 - mean_directional_accuracy: 0.4958 - val_loss: 1.3797 - val_accuracy: 0.3023 - val_categorical_accuracy: 0.3023 - val_mean_directional_accuracy: 0.5553\n",
      "Epoch 3/300\n",
      "342/356 [===========================>..] - ETA: 0s - loss: 1.3857 - accuracy: 0.2721 - categorical_accuracy: 0.2721 - mean_directional_accuracy: 0.5029Restoring model weights from the end of the best epoch: 2.\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 1.3859 - accuracy: 0.2710 - categorical_accuracy: 0.2710 - mean_directional_accuracy: 0.5026 - val_loss: 1.3815 - val_accuracy: 0.2910 - val_categorical_accuracy: 0.2910 - val_mean_directional_accuracy: 0.5072\n",
      "Epoch 3: early stopping\n",
      "479/479 [==============================] - 1s 3ms/step - loss: 1.3797 - accuracy: 0.3023 - categorical_accuracy: 0.3023 - mean_directional_accuracy: 0.5553\n",
      "{'loss': 1.3796697854995728, 'accuracy': 0.30232253670692444, 'categorical_accuracy': 0.30232253670692444, 'mean_directional_accuracy': 0.555323600769043} \n",
      " 2 \n",
      "\n",
      "Model time: 0.21344683691859245 minutes\n",
      "\n",
      "Total time: 38.61839448660612 minutes\n",
      "\n",
      "\n",
      "Model  78  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    2\n",
      "Hidden units              [128, 4]\n",
      "Activation function           tanh\n",
      "Dropout                        0.8\n",
      "L1                            10.0\n",
      "L2                             0.0\n",
      "Batch size                     128\n",
      "Optimizer                  RMSprop\n",
      "Learning rate               0.0001\n",
      "Name: 66, dtype: object\n",
      "NN2Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 4s 33ms/step - loss: 15351.9297 - accuracy: 0.2555 - categorical_accuracy: 0.2555 - mean_directional_accuracy: 0.5065 - val_loss: 14821.7070 - val_accuracy: 0.2381 - val_categorical_accuracy: 0.2381 - val_mean_directional_accuracy: 0.4819\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 14363.1650 - accuracy: 0.2504 - categorical_accuracy: 0.2504 - mean_directional_accuracy: 0.5011 - val_loss: 13877.8047 - val_accuracy: 0.2414 - val_categorical_accuracy: 0.2414 - val_mean_directional_accuracy: 0.4840\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 13435.7949 - accuracy: 0.2460 - categorical_accuracy: 0.2460 - mean_directional_accuracy: 0.4905 - val_loss: 12967.3926 - val_accuracy: 0.2431 - val_categorical_accuracy: 0.2431 - val_mean_directional_accuracy: 0.4841\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 12539.9883 - accuracy: 0.2540 - categorical_accuracy: 0.2540 - mean_directional_accuracy: 0.5090 - val_loss: 12087.2861 - val_accuracy: 0.2449 - val_categorical_accuracy: 0.2449 - val_mean_directional_accuracy: 0.4862\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 11674.0352 - accuracy: 0.2487 - categorical_accuracy: 0.2487 - mean_directional_accuracy: 0.4967 - val_loss: 11236.5664 - val_accuracy: 0.2470 - val_categorical_accuracy: 0.2470 - val_mean_directional_accuracy: 0.4901\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 10838.7148 - accuracy: 0.2547 - categorical_accuracy: 0.2547 - mean_directional_accuracy: 0.4974 - val_loss: 10417.7949 - val_accuracy: 0.2482 - val_categorical_accuracy: 0.2482 - val_mean_directional_accuracy: 0.4918\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 10034.8037 - accuracy: 0.2608 - categorical_accuracy: 0.2608 - mean_directional_accuracy: 0.5076 - val_loss: 9629.6250 - val_accuracy: 0.2508 - val_categorical_accuracy: 0.2508 - val_mean_directional_accuracy: 0.4956\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 9261.4932 - accuracy: 0.2483 - categorical_accuracy: 0.2483 - mean_directional_accuracy: 0.4961 - val_loss: 8872.6152 - val_accuracy: 0.2525 - val_categorical_accuracy: 0.2525 - val_mean_directional_accuracy: 0.5000\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 8519.3486 - accuracy: 0.2531 - categorical_accuracy: 0.2531 - mean_directional_accuracy: 0.5114 - val_loss: 8145.9795 - val_accuracy: 0.2529 - val_categorical_accuracy: 0.2529 - val_mean_directional_accuracy: 0.5046\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 7807.3867 - accuracy: 0.2557 - categorical_accuracy: 0.2557 - mean_directional_accuracy: 0.4937 - val_loss: 7450.1504 - val_accuracy: 0.2539 - val_categorical_accuracy: 0.2539 - val_mean_directional_accuracy: 0.5070\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 7127.2827 - accuracy: 0.2466 - categorical_accuracy: 0.2466 - mean_directional_accuracy: 0.4977 - val_loss: 6787.1816 - val_accuracy: 0.2578 - val_categorical_accuracy: 0.2578 - val_mean_directional_accuracy: 0.5099\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 6480.2642 - accuracy: 0.2589 - categorical_accuracy: 0.2589 - mean_directional_accuracy: 0.5032 - val_loss: 6157.1621 - val_accuracy: 0.2574 - val_categorical_accuracy: 0.2574 - val_mean_directional_accuracy: 0.5086\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 5865.5439 - accuracy: 0.2596 - categorical_accuracy: 0.2596 - mean_directional_accuracy: 0.4949 - val_loss: 5558.6592 - val_accuracy: 0.2559 - val_categorical_accuracy: 0.2559 - val_mean_directional_accuracy: 0.5093\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 5282.1045 - accuracy: 0.2529 - categorical_accuracy: 0.2529 - mean_directional_accuracy: 0.5032 - val_loss: 4991.1309 - val_accuracy: 0.2587 - val_categorical_accuracy: 0.2587 - val_mean_directional_accuracy: 0.5116\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 4728.9141 - accuracy: 0.2548 - categorical_accuracy: 0.2548 - mean_directional_accuracy: 0.4988 - val_loss: 4453.7188 - val_accuracy: 0.2564 - val_categorical_accuracy: 0.2564 - val_mean_directional_accuracy: 0.5142\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 4206.7007 - accuracy: 0.2501 - categorical_accuracy: 0.2501 - mean_directional_accuracy: 0.4909 - val_loss: 3947.5374 - val_accuracy: 0.2628 - val_categorical_accuracy: 0.2628 - val_mean_directional_accuracy: 0.5191\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 3715.7566 - accuracy: 0.2622 - categorical_accuracy: 0.2622 - mean_directional_accuracy: 0.4965 - val_loss: 3473.2434 - val_accuracy: 0.2644 - val_categorical_accuracy: 0.2644 - val_mean_directional_accuracy: 0.5214\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 3256.8992 - accuracy: 0.2492 - categorical_accuracy: 0.2492 - mean_directional_accuracy: 0.4986 - val_loss: 3030.7378 - val_accuracy: 0.2645 - val_categorical_accuracy: 0.2645 - val_mean_directional_accuracy: 0.5265\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 2829.8604 - accuracy: 0.2583 - categorical_accuracy: 0.2583 - mean_directional_accuracy: 0.5042 - val_loss: 2620.3718 - val_accuracy: 0.2667 - val_categorical_accuracy: 0.2667 - val_mean_directional_accuracy: 0.5284\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 2434.3962 - accuracy: 0.2638 - categorical_accuracy: 0.2638 - mean_directional_accuracy: 0.5019 - val_loss: 2240.7861 - val_accuracy: 0.2705 - val_categorical_accuracy: 0.2705 - val_mean_directional_accuracy: 0.5321\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 2069.7031 - accuracy: 0.2489 - categorical_accuracy: 0.2489 - mean_directional_accuracy: 0.4951 - val_loss: 1892.0555 - val_accuracy: 0.2737 - val_categorical_accuracy: 0.2737 - val_mean_directional_accuracy: 0.5397\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1736.6018 - accuracy: 0.2492 - categorical_accuracy: 0.2492 - mean_directional_accuracy: 0.4939 - val_loss: 1575.9769 - val_accuracy: 0.2740 - val_categorical_accuracy: 0.2740 - val_mean_directional_accuracy: 0.5354\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1436.1687 - accuracy: 0.2578 - categorical_accuracy: 0.2578 - mean_directional_accuracy: 0.5012 - val_loss: 1291.9497 - val_accuracy: 0.2767 - val_categorical_accuracy: 0.2767 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1167.1055 - accuracy: 0.2483 - categorical_accuracy: 0.2483 - mean_directional_accuracy: 0.4867 - val_loss: 1039.0287 - val_accuracy: 0.2783 - val_categorical_accuracy: 0.2783 - val_mean_directional_accuracy: 0.5406\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 928.7972 - accuracy: 0.2492 - categorical_accuracy: 0.2492 - mean_directional_accuracy: 0.4996 - val_loss: 816.3513 - val_accuracy: 0.2809 - val_categorical_accuracy: 0.2809 - val_mean_directional_accuracy: 0.5398\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 721.0466 - accuracy: 0.2541 - categorical_accuracy: 0.2541 - mean_directional_accuracy: 0.4921 - val_loss: 624.1412 - val_accuracy: 0.2812 - val_categorical_accuracy: 0.2812 - val_mean_directional_accuracy: 0.5414\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 543.5571 - accuracy: 0.2555 - categorical_accuracy: 0.2555 - mean_directional_accuracy: 0.4958 - val_loss: 462.7374 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 397.2987 - accuracy: 0.2622 - categorical_accuracy: 0.2622 - mean_directional_accuracy: 0.5011 - val_loss: 332.9480 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 282.6352 - accuracy: 0.2538 - categorical_accuracy: 0.2538 - mean_directional_accuracy: 0.4940 - val_loss: 234.0090 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 198.7692 - accuracy: 0.2489 - categorical_accuracy: 0.2489 - mean_directional_accuracy: 0.4867 - val_loss: 166.6287 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 146.0985 - accuracy: 0.2522 - categorical_accuracy: 0.2522 - mean_directional_accuracy: 0.4867 - val_loss: 129.3813 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 123.0489 - accuracy: 0.2573 - categorical_accuracy: 0.2573 - mean_directional_accuracy: 0.4923 - val_loss: 118.1516 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 114.0620 - accuracy: 0.2543 - categorical_accuracy: 0.2543 - mean_directional_accuracy: 0.4923 - val_loss: 109.8060 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 106.0784 - accuracy: 0.2573 - categorical_accuracy: 0.2573 - mean_directional_accuracy: 0.4903 - val_loss: 102.2032 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 98.7285 - accuracy: 0.2534 - categorical_accuracy: 0.2534 - mean_directional_accuracy: 0.4875 - val_loss: 95.1014 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 91.9265 - accuracy: 0.2515 - categorical_accuracy: 0.2515 - mean_directional_accuracy: 0.4831 - val_loss: 88.6200 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 85.5854 - accuracy: 0.2587 - categorical_accuracy: 0.2587 - mean_directional_accuracy: 0.4968 - val_loss: 82.3826 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 79.5608 - accuracy: 0.2629 - categorical_accuracy: 0.2629 - mean_directional_accuracy: 0.5014 - val_loss: 76.6535 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 74.0625 - accuracy: 0.2617 - categorical_accuracy: 0.2617 - mean_directional_accuracy: 0.5002 - val_loss: 71.3578 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 69.0622 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5153 - val_loss: 66.6916 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 64.5504 - accuracy: 0.2648 - categorical_accuracy: 0.2648 - mean_directional_accuracy: 0.5065 - val_loss: 62.3484 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 60.4916 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5105 - val_loss: 58.6047 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 56.9574 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 55.2781 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 53.9597 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 52.6613 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 51.5669 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 50.5289 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 49.9057 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 49.3991 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 49.0204 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 48.6746 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 48.4567 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 48.2284 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 47.9623 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 47.6827 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 47.4666 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 47.2385 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 46.9722 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 46.6927 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 46.4768 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 46.2485 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 45.9823 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 45.7028 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 1s 33ms/step - loss: 45.4867 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 45.2586 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 1s 28ms/step - loss: 44.9922 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 44.7138 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 44.4966 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 44.2676 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 1s 22ms/step - loss: 44.0023 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 43.7239 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 43.5067 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 43.2777 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 43.0122 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 42.7339 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 42.5167 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 42.2877 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 42.0223 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 41.7439 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 41.5267 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 41.2977 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 63/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 41.0322 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 40.7540 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 64/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 40.5366 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 40.3078 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 65/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 40.0489 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 39.7902 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 66/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 39.5995 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 39.4112 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 67/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 39.1893 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 38.9582 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 68/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 38.7838 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 38.6012 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 69/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 38.3793 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 38.1483 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 70/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 37.9737 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 37.7913 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 71/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 37.5692 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 37.3383 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 72/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 37.1790 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 37.0193 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 73/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 36.8190 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 36.6111 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 74/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 36.4584 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 36.2993 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 75/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 36.0990 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 35.8911 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 76/300\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 35.7384 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 35.5793 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 77/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 35.3789 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 35.1711 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 78/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 35.0184 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 34.8593 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 79/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 34.6589 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 34.4511 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 80/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 34.2983 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 34.1393 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 81/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 33.9388 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 33.7311 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 82/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 33.5783 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 33.4193 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 83/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 33.2189 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 33.0112 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 84/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 32.8584 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 32.6994 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 85/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 32.4989 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 32.2912 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 86/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 32.1383 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 31.9794 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 87/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 31.7789 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 31.5712 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 88/300\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 31.4183 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 31.2595 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 89/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 31.0590 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 30.8513 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 90/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 30.6984 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 30.5395 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 91/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 30.3389 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 30.1313 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 92/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 29.9784 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 29.8195 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 93/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 29.6189 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 29.4113 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 94/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 29.2583 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 29.0996 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 95/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 28.8989 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 28.6914 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 96/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 28.5384 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 28.3796 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 97/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 28.1788 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 27.9714 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 98/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 27.8184 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 27.6596 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 99/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 27.4589 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 27.2514 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 100/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 27.0984 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 26.9397 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 101/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 26.7388 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 26.5314 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 102/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 26.3784 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 26.2197 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 103/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 26.0189 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 25.8114 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 104/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 25.6584 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 25.4997 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 105/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 25.2989 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 25.0914 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 106/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 24.9384 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 24.7797 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 107/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 24.5789 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 24.3714 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 108/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 24.2184 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 24.0597 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 109/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 23.8589 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 23.6514 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 110/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 23.4984 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 23.3437 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 111/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 23.1642 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 22.9794 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 112/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 22.8485 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 22.7137 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 113/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 22.5342 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 22.3494 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 114/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 22.2186 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 22.0837 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 115/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 21.9041 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 21.7194 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 116/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 21.5890 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 21.4612 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 117/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 21.3031 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 21.1415 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 118/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 21.0325 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 20.9212 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 119/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 20.7631 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 20.6015 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 120/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 20.4925 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 20.3813 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 121/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 20.2232 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 20.0615 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 122/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 19.9526 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 19.8412 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 123/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 19.6831 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 19.5215 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 124/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 19.4126 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 19.3013 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 125/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 19.1431 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 18.9815 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 126/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 18.8726 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 18.7613 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 127/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 18.6031 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 18.4415 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 128/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 18.3326 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 18.2213 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 129/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 18.0631 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 17.9015 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 130/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 17.7927 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 17.6863 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 131/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 17.5498 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 17.4115 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 132/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 17.3243 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 17.2363 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 133/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 17.0998 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.9615 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 134/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 16.8743 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.7863 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 135/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 16.6537 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.5312 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 136/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 16.4654 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.4003 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 137/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 16.2859 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.1713 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 138/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 16.1054 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.0403 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 139/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 15.9259 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.8113 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 140/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 15.7454 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.6804 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 141/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 15.5660 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.4533 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 142/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 15.4094 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.3679 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 143/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 15.2749 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.1833 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 144/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 15.1394 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.0979 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 145/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 15.0049 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.9133 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 146/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 14.8694 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.8278 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 147/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 14.7349 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.6433 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 148/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 14.5994 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.5579 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 149/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 14.4649 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.3733 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 150/300\n",
      "38/45 [========================>.....] - ETA: 0s - loss: 14.3740 - accuracy: 0.2664 - categorical_accuracy: 0.2664 - mean_directional_accuracy: 0.5103Restoring model weights from the end of the best epoch: 149.\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 14.3704 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.3758 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 150: early stopping\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 14.3733 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 14.373327255249023, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 149 \n",
      "\n",
      "Model time: 1.4708306416869164 minutes\n",
      "\n",
      "Total time: 40.08929416909814 minutes\n",
      "\n",
      "\n",
      "Model  79  out of  111\n",
      "Model type                   FeedForward\n",
      "Hidden layers                          4\n",
      "Hidden units           [256, 16, 64, 16]\n",
      "Activation function                 relu\n",
      "Dropout                              0.7\n",
      "L1                                 100.0\n",
      "L2                                0.0001\n",
      "Batch size                            16\n",
      "Optimizer                           Adam\n",
      "Learning rate                      0.001\n",
      "Name: 67, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 7s 12ms/step - loss: 42375.6367 - accuracy: 0.2576 - categorical_accuracy: 0.2576 - mean_directional_accuracy: 0.5049 - val_loss: 837.5510 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - 3s 10ms/step - loss: 666.4649 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 624.6252 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "349/356 [============================>.] - ETA: 0s - loss: 626.9802 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5100Restoring model weights from the end of the best epoch: 2.\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 626.9266 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 628.6428 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3: early stopping\n",
      "479/479 [==============================] - 2s 3ms/step - loss: 624.6252 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 624.6251831054688, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 2 \n",
      "\n",
      "Model time: 0.26299722120165825 minutes\n",
      "\n",
      "Total time: 40.35235803946853 minutes\n",
      "\n",
      "\n",
      "Model  80  out of  111\n",
      "Model type              FeedForward\n",
      "Hidden layers                     3\n",
      "Hidden units           [1, 256, 32]\n",
      "Activation function            relu\n",
      "Dropout                         0.0\n",
      "L1                             10.0\n",
      "L2                             0.01\n",
      "Batch size                       32\n",
      "Optimizer                   RMSprop\n",
      "Learning rate                0.0001\n",
      "Name: 68, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 5s 14ms/step - loss: 5693.7500 - accuracy: 0.2654 - categorical_accuracy: 0.2654 - mean_directional_accuracy: 0.5051 - val_loss: 4975.9873 - val_accuracy: 0.2294 - val_categorical_accuracy: 0.2294 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 4332.0098 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5112 - val_loss: 3714.1809 - val_accuracy: 0.2289 - val_categorical_accuracy: 0.2289 - val_mean_directional_accuracy: 0.4586\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 3169.9729 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5105 - val_loss: 2652.4578 - val_accuracy: 0.2293 - val_categorical_accuracy: 0.2293 - val_mean_directional_accuracy: 0.4585\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 2201.2585 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5109 - val_loss: 1773.4105 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 1408.3376 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1069.8910 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 799.6431 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 558.1577 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 383.0773 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 238.8784 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 159.4836 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 110.2724 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "178/178 [==============================] - 1s 6ms/step - loss: 100.0543 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 91.2157 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 83.6679 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 76.6349 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 70.8191 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 65.1864 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 59.9319 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 54.6951 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 49.9784 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 45.3275 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 41.1686 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 37.1327 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 33.5754 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 30.1500 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 26.9961 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 23.8212 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 20.9585 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 18.3223 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 16.1550 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.0427 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 12.4018 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 10.9087 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 9.7059 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 8.6115 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 7.7727 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 7.0017 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 6.4129 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.9619 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 5.8219 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.7226 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "172/178 [===========================>..] - ETA: 0s - loss: 5.7422 - accuracy: 0.2669 - categorical_accuracy: 0.2669 - mean_directional_accuracy: 0.5094Restoring model weights from the end of the best epoch: 23.\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 5.7422 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.7227 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24: early stopping\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 5.7226 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 5.722614765167236, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 23 \n",
      "\n",
      "Model time: 0.6223365664482117 minutes\n",
      "\n",
      "Total time: 40.97482838481665 minutes\n",
      "\n",
      "\n",
      "Model  81  out of  111\n",
      "Model type              FeedForward\n",
      "Hidden layers                     3\n",
      "Hidden units           [8, 128, 16]\n",
      "Activation function            tanh\n",
      "Dropout                         0.8\n",
      "L1                             0.01\n",
      "L2                             10.0\n",
      "Batch size                       16\n",
      "Optimizer                   RMSprop\n",
      "Learning rate                  0.01\n",
      "Name: 69, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 7s 9ms/step - loss: 9.6752 - accuracy: 0.2585 - categorical_accuracy: 0.2585 - mean_directional_accuracy: 0.5032 - val_loss: 2.7326 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "354/356 [============================>.] - ETA: 0s - loss: 2.7345 - accuracy: 0.2597 - categorical_accuracy: 0.2597 - mean_directional_accuracy: 0.5037Restoring model weights from the end of the best epoch: 1.\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 2.7344 - accuracy: 0.2604 - categorical_accuracy: 0.2604 - mean_directional_accuracy: 0.5040 - val_loss: 2.7339 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2: early stopping\n",
      "479/479 [==============================] - 1s 3ms/step - loss: 2.7326 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 2.7325921058654785, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 1 \n",
      "\n",
      "Model time: 0.18552466854453087 minutes\n",
      "\n",
      "Total time: 41.1604030020535 minutes\n",
      "\n",
      "\n",
      "Model  82  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    2\n",
      "Hidden units              [16, 16]\n",
      "Activation function         linear\n",
      "Dropout                        0.1\n",
      "L1                           100.0\n",
      "L2                             1.0\n",
      "Batch size                     256\n",
      "Optimizer                     Adam\n",
      "Learning rate                0.001\n",
      "Name: 70, dtype: object\n",
      "NN2Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 3s 46ms/step - loss: 28323.5039 - accuracy: 0.2566 - categorical_accuracy: 0.2566 - mean_directional_accuracy: 0.5077 - val_loss: 24894.4766 - val_accuracy: 0.2637 - val_categorical_accuracy: 0.2637 - val_mean_directional_accuracy: 0.5104\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 22203.1133 - accuracy: 0.2590 - categorical_accuracy: 0.2590 - mean_directional_accuracy: 0.5149 - val_loss: 19218.3418 - val_accuracy: 0.2632 - val_categorical_accuracy: 0.2632 - val_mean_directional_accuracy: 0.5146\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 16902.1504 - accuracy: 0.2604 - categorical_accuracy: 0.2604 - mean_directional_accuracy: 0.5242 - val_loss: 14350.3545 - val_accuracy: 0.2647 - val_categorical_accuracy: 0.2647 - val_mean_directional_accuracy: 0.5162\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 12411.8838 - accuracy: 0.2587 - categorical_accuracy: 0.2587 - mean_directional_accuracy: 0.5179 - val_loss: 10314.1104 - val_accuracy: 0.2702 - val_categorical_accuracy: 0.2702 - val_mean_directional_accuracy: 0.5240\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 8774.8174 - accuracy: 0.2641 - categorical_accuracy: 0.2641 - mean_directional_accuracy: 0.5162 - val_loss: 7123.2856 - val_accuracy: 0.2713 - val_categorical_accuracy: 0.2713 - val_mean_directional_accuracy: 0.5231\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 5953.5771 - accuracy: 0.2633 - categorical_accuracy: 0.2633 - mean_directional_accuracy: 0.5190 - val_loss: 4756.7300 - val_accuracy: 0.2628 - val_categorical_accuracy: 0.2628 - val_mean_directional_accuracy: 0.5005\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 3994.0515 - accuracy: 0.2634 - categorical_accuracy: 0.2634 - mean_directional_accuracy: 0.5095 - val_loss: 3258.4609 - val_accuracy: 0.2358 - val_categorical_accuracy: 0.2358 - val_mean_directional_accuracy: 0.4656\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2871.1284 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5107 - val_loss: 2551.4607 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2335.5879 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2106.7795 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1931.6185 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1737.3647 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1580.8137 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1403.9657 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1264.2334 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1108.9525 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 987.8314 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 853.0180 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 751.8994 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 639.4821 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 554.2485 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 459.8360 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 384.6644 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 305.0466 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 249.0449 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 191.2275 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 155.2460 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 117.7273 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 95.8808 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 77.9802 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 67.2974 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 56.1777 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 48.4494 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 42.7875 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 40.6153 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 40.2439 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 39.3935 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 38.9363 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 39.3428 - accuracy: 0.2693 - categorical_accuracy: 0.2693 - mean_directional_accuracy: 0.5113Restoring model weights from the end of the best epoch: 23.\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 39.3375 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 39.2719 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24: early stopping\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 38.9363 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 38.936317443847656, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 23 \n",
      "\n",
      "Model time: 0.18109331652522087 minutes\n",
      "\n",
      "Total time: 41.34187963977456 minutes\n",
      "\n",
      "\n",
      "Model  83  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    2\n",
      "Hidden units              [128, 2]\n",
      "Activation function         linear\n",
      "Dropout                        0.4\n",
      "L1                            10.0\n",
      "L2                            0.01\n",
      "Batch size                      64\n",
      "Optimizer                  RMSprop\n",
      "Learning rate               0.0001\n",
      "Name: 71, dtype: object\n",
      "NN2Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 4s 22ms/step - loss: 14596.8906 - accuracy: 0.2657 - categorical_accuracy: 0.2657 - mean_directional_accuracy: 0.5098 - val_loss: 13643.0781 - val_accuracy: 0.2692 - val_categorical_accuracy: 0.2692 - val_mean_directional_accuracy: 0.5162\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 12766.6260 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.5068 - val_loss: 11890.3242 - val_accuracy: 0.2739 - val_categorical_accuracy: 0.2739 - val_mean_directional_accuracy: 0.5193\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 11075.4102 - accuracy: 0.2590 - categorical_accuracy: 0.2590 - mean_directional_accuracy: 0.5063 - val_loss: 10262.1045 - val_accuracy: 0.2779 - val_categorical_accuracy: 0.2779 - val_mean_directional_accuracy: 0.5228\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 9508.4033 - accuracy: 0.2613 - categorical_accuracy: 0.2613 - mean_directional_accuracy: 0.4995 - val_loss: 8756.4414 - val_accuracy: 0.2837 - val_categorical_accuracy: 0.2837 - val_mean_directional_accuracy: 0.5275\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 8059.7422 - accuracy: 0.2590 - categorical_accuracy: 0.2590 - mean_directional_accuracy: 0.5098 - val_loss: 7367.4141 - val_accuracy: 0.2858 - val_categorical_accuracy: 0.2858 - val_mean_directional_accuracy: 0.5266\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 6728.5312 - accuracy: 0.2497 - categorical_accuracy: 0.2497 - mean_directional_accuracy: 0.4991 - val_loss: 6094.8774 - val_accuracy: 0.2858 - val_categorical_accuracy: 0.2858 - val_mean_directional_accuracy: 0.5295\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 5516.8882 - accuracy: 0.2626 - categorical_accuracy: 0.2626 - mean_directional_accuracy: 0.5139 - val_loss: 4946.2256 - val_accuracy: 0.2865 - val_categorical_accuracy: 0.2865 - val_mean_directional_accuracy: 0.5312\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 4427.7241 - accuracy: 0.2650 - categorical_accuracy: 0.2650 - mean_directional_accuracy: 0.5077 - val_loss: 3918.0977 - val_accuracy: 0.2838 - val_categorical_accuracy: 0.2838 - val_mean_directional_accuracy: 0.5329\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 3460.5085 - accuracy: 0.2708 - categorical_accuracy: 0.2708 - mean_directional_accuracy: 0.5292 - val_loss: 3012.7498 - val_accuracy: 0.2781 - val_categorical_accuracy: 0.2781 - val_mean_directional_accuracy: 0.5324\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 2614.9922 - accuracy: 0.2580 - categorical_accuracy: 0.2580 - mean_directional_accuracy: 0.5072 - val_loss: 2227.6509 - val_accuracy: 0.2761 - val_categorical_accuracy: 0.2761 - val_mean_directional_accuracy: 0.5305\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1888.3340 - accuracy: 0.2715 - categorical_accuracy: 0.2715 - mean_directional_accuracy: 0.5249 - val_loss: 1561.8018 - val_accuracy: 0.2727 - val_categorical_accuracy: 0.2727 - val_mean_directional_accuracy: 0.5309\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1284.0834 - accuracy: 0.2647 - categorical_accuracy: 0.2647 - mean_directional_accuracy: 0.5181 - val_loss: 1019.1844 - val_accuracy: 0.2677 - val_categorical_accuracy: 0.2677 - val_mean_directional_accuracy: 0.5138\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 800.4069 - accuracy: 0.2782 - categorical_accuracy: 0.2782 - mean_directional_accuracy: 0.5225 - val_loss: 597.1475 - val_accuracy: 0.2530 - val_categorical_accuracy: 0.2530 - val_mean_directional_accuracy: 0.4909\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 439.1325 - accuracy: 0.2685 - categorical_accuracy: 0.2685 - mean_directional_accuracy: 0.5155 - val_loss: 298.9631 - val_accuracy: 0.2330 - val_categorical_accuracy: 0.2330 - val_mean_directional_accuracy: 0.4639\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 202.4576 - accuracy: 0.2713 - categorical_accuracy: 0.2713 - mean_directional_accuracy: 0.5193 - val_loss: 124.0068 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 88.9557 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5104 - val_loss: 72.2955 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 68.1072 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 64.0181 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 60.4929 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 57.0895 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 53.9237 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 50.8242 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 48.2757 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 45.7978 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 43.7048 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 41.7349 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 40.2755 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 38.9343 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 37.7253 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 36.6667 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 36.1536 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 35.7751 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 35.4487 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 35.1370 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 34.9102 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 34.6824 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 34.4137 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 34.1502 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 33.9671 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 33.7839 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 33.5225 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 33.2595 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 33.0761 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 32.8932 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 32.6317 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 32.3688 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 32.1854 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 32.0026 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 31.7408 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 31.4781 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 31.2947 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 31.1119 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 30.8502 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 30.5874 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 30.4040 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 30.2213 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 29.9593 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 29.6968 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 29.5133 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 29.3307 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 29.0688 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 28.8062 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 28.6227 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 28.4402 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 28.1782 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 27.9157 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 27.7320 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 27.5497 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 27.2878 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 27.0252 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 26.8415 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 26.6592 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 26.3971 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 26.1347 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 25.9510 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 25.7687 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 25.5067 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 25.2443 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 25.0606 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 24.8783 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 24.6163 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 24.3538 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 24.1702 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 23.9879 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 23.7309 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 23.4944 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 23.3547 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 23.2176 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 22.9995 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 22.7821 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 22.6424 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 22.5052 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 22.2872 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 22.0698 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 56/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 21.9300 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 21.7929 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 57/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 21.5746 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 21.3575 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 58/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 21.2177 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 21.0806 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 20.8792 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 20.7005 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 20.6046 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 20.5123 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 20.3382 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 20.1663 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 20.0703 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 19.9781 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 63/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 19.8040 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 19.6320 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 64/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 19.5360 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 19.4438 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 65/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 19.2698 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 19.0977 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 66/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 19.0019 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 18.9095 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 67/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 18.7355 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 18.5635 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 68/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 18.4676 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 18.3753 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 69/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 18.2014 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 18.0292 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 70/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 17.9334 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 17.8411 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 71/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 17.6672 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 17.4950 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 72/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 17.3991 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 17.3069 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 73/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 17.1641 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 17.0355 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 74/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 16.9838 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.9367 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 75/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 16.8066 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.6794 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 76/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 16.6276 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.5806 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 77/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 16.4504 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.3233 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 78/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 16.2716 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.2245 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 79/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 16.0944 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.9671 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 80/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 15.9154 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.8683 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 81/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 15.7382 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.6110 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 82/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 15.5592 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.5122 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 83/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 15.3821 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.2549 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 84/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 15.2031 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.1561 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 85/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 15.0260 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.8989 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 86/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 14.8470 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.8001 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 87/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 14.6699 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.5428 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 88/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 14.4910 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.4440 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 89/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 14.3138 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.1867 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 90/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 14.1349 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.0880 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 91/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 13.9577 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 13.8307 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 92/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 13.7789 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 13.7319 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 93/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 13.6017 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 13.4747 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 94/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 13.4228 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 13.3759 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 95/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 13.2486 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 13.1426 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 96/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 13.1343 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 13.1320 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 97/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 13.0462 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.9646 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 98/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 12.9563 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.9540 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 99/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 12.8681 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.7866 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 100/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 12.7783 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.7759 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 101/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 12.6901 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.6086 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 102/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 12.6003 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.5979 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 103/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 12.5121 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.4306 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 104/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 12.4222 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.4199 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 105/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 12.3437 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.2946 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 106/300\n",
      "79/89 [=========================>....] - ETA: 0s - loss: 12.3299 - accuracy: 0.2706 - categorical_accuracy: 0.2706 - mean_directional_accuracy: 0.5136Restoring model weights from the end of the best epoch: 105.\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 12.3301 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.3728 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 106: early stopping\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 12.2946 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 12.294588088989258, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 105 \n",
      "\n",
      "Model time: 1.6371589004993439 minutes\n",
      "\n",
      "Total time: 42.97910522297025 minutes\n",
      "\n",
      "\n",
      "Model  84  out of  111\n",
      "Model type              FeedForward\n",
      "Hidden layers                     3\n",
      "Hidden units           [64, 2, 128]\n",
      "Activation function          linear\n",
      "Dropout                         0.3\n",
      "L1                           0.0001\n",
      "L2                            100.0\n",
      "Batch size                       16\n",
      "Optimizer                      Adam\n",
      "Learning rate                0.0001\n",
      "Name: 72, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 6s 10ms/step - loss: 8042.5298 - accuracy: 0.2599 - categorical_accuracy: 0.2599 - mean_directional_accuracy: 0.5028 - val_loss: 5695.2485 - val_accuracy: 0.2383 - val_categorical_accuracy: 0.2383 - val_mean_directional_accuracy: 0.4739\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 4139.7534 - accuracy: 0.2650 - categorical_accuracy: 0.2650 - mean_directional_accuracy: 0.5114 - val_loss: 2864.2910 - val_accuracy: 0.2268 - val_categorical_accuracy: 0.2268 - val_mean_directional_accuracy: 0.4620\n",
      "Epoch 3/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 2034.4801 - accuracy: 0.2619 - categorical_accuracy: 0.2619 - mean_directional_accuracy: 0.5032 - val_loss: 1363.7427 - val_accuracy: 0.2283 - val_categorical_accuracy: 0.2283 - val_mean_directional_accuracy: 0.4582\n",
      "Epoch 4/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 942.8745 - accuracy: 0.2691 - categorical_accuracy: 0.2691 - mean_directional_accuracy: 0.5104 - val_loss: 609.8333 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 410.6276 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 256.9610 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 169.7943 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 104.2344 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 68.7082 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 42.4387 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 28.4780 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 18.1542 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 12.5708 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 8.3804 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 6.0372 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.2585 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 3.2525 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.4943 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 2.0764 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7708 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.6115 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5017 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 1.4481 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4155 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 1.3995 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3929 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.3877 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3882 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3855 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3876 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3875 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "344/356 [===========================>..] - ETA: 0s - loss: 1.3852 - accuracy: 0.2667 - categorical_accuracy: 0.2667 - mean_directional_accuracy: 0.5093Restoring model weights from the end of the best epoch: 18.\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3876 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19: early stopping\n",
      "479/479 [==============================] - 1s 2ms/step - loss: 1.3875 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3875281810760498, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 18 \n",
      "\n",
      "Model time: 0.9318932257592678 minutes\n",
      "\n",
      "Total time: 43.91108179837465 minutes\n",
      "\n",
      "\n",
      "Model  85  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    2\n",
      "Hidden units               [64, 1]\n",
      "Activation function         linear\n",
      "Dropout                        0.6\n",
      "L1                           100.0\n",
      "L2                          0.0001\n",
      "Batch size                     256\n",
      "Optimizer                     Adam\n",
      "Learning rate                0.001\n",
      "Name: 73, dtype: object\n",
      "NN2Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 3s 51ms/step - loss: 76713.9922 - accuracy: 0.2590 - categorical_accuracy: 0.2590 - mean_directional_accuracy: 0.5016 - val_loss: 64598.7695 - val_accuracy: 0.2634 - val_categorical_accuracy: 0.2634 - val_mean_directional_accuracy: 0.5222\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 55280.3047 - accuracy: 0.2608 - categorical_accuracy: 0.2608 - mean_directional_accuracy: 0.4989 - val_loss: 45073.4375 - val_accuracy: 0.2525 - val_categorical_accuracy: 0.2525 - val_mean_directional_accuracy: 0.5057\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 37417.7969 - accuracy: 0.2559 - categorical_accuracy: 0.2559 - mean_directional_accuracy: 0.4989 - val_loss: 29166.1191 - val_accuracy: 0.2409 - val_categorical_accuracy: 0.2409 - val_mean_directional_accuracy: 0.4871\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 23182.8105 - accuracy: 0.2608 - categorical_accuracy: 0.2608 - mean_directional_accuracy: 0.5004 - val_loss: 16842.4023 - val_accuracy: 0.2278 - val_categorical_accuracy: 0.2278 - val_mean_directional_accuracy: 0.4714\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 12452.8076 - accuracy: 0.2636 - categorical_accuracy: 0.2636 - mean_directional_accuracy: 0.5105 - val_loss: 7961.2188 - val_accuracy: 0.2193 - val_categorical_accuracy: 0.2193 - val_mean_directional_accuracy: 0.4603\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 5182.6099 - accuracy: 0.2682 - categorical_accuracy: 0.2682 - mean_directional_accuracy: 0.5169 - val_loss: 2616.9163 - val_accuracy: 0.2290 - val_categorical_accuracy: 0.2290 - val_mean_directional_accuracy: 0.4582\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1491.4417 - accuracy: 0.2664 - categorical_accuracy: 0.2664 - mean_directional_accuracy: 0.5112 - val_loss: 837.0612 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 666.2488 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 526.3589 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 474.3413 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 429.5760 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 401.7948 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 374.3891 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 356.4149 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 336.7347 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 322.7893 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 308.3898 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 297.5740 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 288.5639 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 283.2028 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 277.8981 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 273.5005 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 267.2815 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 263.6530 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 259.5023 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 255.0793 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 250.0500 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 245.8882 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 242.6511 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 237.9864 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 233.8626 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 230.8671 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 227.2462 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 224.0662 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 221.4708 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 217.4293 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 212.8115 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 210.2956 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 205.0469 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 203.6031 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 200.3722 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 25/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 197.5570 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 195.0495 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 193.1899 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 190.5491 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 188.2967 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 185.4116 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 28/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 183.9150 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 181.5063 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 29/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 179.0081 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 177.8773 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 30/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 174.6929 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 171.8286 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 31/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 169.9456 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 168.5150 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 32/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 165.3083 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 163.4408 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 33/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 160.9599 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 158.9061 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 156.0406 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 153.1246 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 151.9488 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 149.1534 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 148.2956 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 147.3972 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 145.9925 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 145.2850 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 143.5981 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 142.6389 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 141.6370 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 139.8657 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 140.1904 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 138.9234 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "23/23 [==============================] - ETA: 0s - loss: 139.6955 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105Restoring model weights from the end of the best epoch: 40.\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 139.6955 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 140.4706 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41: early stopping\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 138.9234 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 138.92337036132812, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 40 \n",
      "\n",
      "Model time: 0.2764371894299984 minutes\n",
      "\n",
      "Total time: 44.18758566305041 minutes\n",
      "\n",
      "\n",
      "Model  86  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    3\n",
      "Hidden units           [2, 32, 64]\n",
      "Activation function           relu\n",
      "Dropout                        0.3\n",
      "L1                          0.0001\n",
      "L2                           100.0\n",
      "Batch size                      32\n",
      "Optimizer                  RMSprop\n",
      "Learning rate                0.001\n",
      "Name: 74, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 5s 12ms/step - loss: 1942.9573 - accuracy: 0.2664 - categorical_accuracy: 0.2664 - mean_directional_accuracy: 0.5061 - val_loss: 232.7735 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 47.0212 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2.7369 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.6081 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4556 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "176/178 [============================>.] - ETA: 0s - loss: 1.4530 - accuracy: 0.2665 - categorical_accuracy: 0.2665 - mean_directional_accuracy: 0.5092Restoring model weights from the end of the best epoch: 3.\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.4530 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4559 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.4556 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.4555723667144775, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 3 \n",
      "\n",
      "Model time: 0.17735231667757034 minutes\n",
      "\n",
      "Total time: 44.36502135545015 minutes\n",
      "\n",
      "\n",
      "Model  87  out of  111\n",
      "Model type                      FeedForward\n",
      "Hidden layers                             5\n",
      "Hidden units           [32, 1, 16, 32, 256]\n",
      "Activation function                    tanh\n",
      "Dropout                                 0.6\n",
      "L1                                    0.001\n",
      "L2                                    0.001\n",
      "Batch size                               32\n",
      "Optimizer                           RMSprop\n",
      "Learning rate                          0.01\n",
      "Name: 75, dtype: object\n",
      "NN5Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 7s 14ms/step - loss: 1.7010 - accuracy: 0.2529 - categorical_accuracy: 0.2529 - mean_directional_accuracy: 0.4933 - val_loss: 1.4770 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - ETA: 0s - loss: 1.4775 - accuracy: 0.2501 - categorical_accuracy: 0.2501 - mean_directional_accuracy: 0.5014Restoring model weights from the end of the best epoch: 1.\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.4775 - accuracy: 0.2501 - categorical_accuracy: 0.2501 - mean_directional_accuracy: 0.5014 - val_loss: 1.4810 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 2: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.4770 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.477005124092102, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 1 \n",
      "\n",
      "Model time: 0.16191020607948303 minutes\n",
      "\n",
      "Total time: 44.5269982367754 minutes\n",
      "\n",
      "\n",
      "Model  88  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    2\n",
      "Hidden units                [4, 2]\n",
      "Activation function           tanh\n",
      "Dropout                        0.1\n",
      "L1                           0.001\n",
      "L2                         0.00001\n",
      "Batch size                     256\n",
      "Optimizer                  RMSprop\n",
      "Learning rate                 0.01\n",
      "Name: 76, dtype: object\n",
      "NN2Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 4s 51ms/step - loss: 1.4112 - accuracy: 0.2822 - categorical_accuracy: 0.2822 - mean_directional_accuracy: 0.5139 - val_loss: 1.4020 - val_accuracy: 0.2783 - val_categorical_accuracy: 0.2783 - val_mean_directional_accuracy: 0.4949\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.3840 - accuracy: 0.3217 - categorical_accuracy: 0.3217 - mean_directional_accuracy: 0.5369 - val_loss: 1.3892 - val_accuracy: 0.3053 - val_categorical_accuracy: 0.3053 - val_mean_directional_accuracy: 0.5240\n",
      "Epoch 3/300\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 1.3778 - accuracy: 0.3249 - categorical_accuracy: 0.3249 - mean_directional_accuracy: 0.5434Restoring model weights from the end of the best epoch: 2.\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3786 - accuracy: 0.3275 - categorical_accuracy: 0.3275 - mean_directional_accuracy: 0.5485 - val_loss: 1.3974 - val_accuracy: 0.2835 - val_categorical_accuracy: 0.2835 - val_mean_directional_accuracy: 0.5012\n",
      "Epoch 3: early stopping\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.3892 - accuracy: 0.3053 - categorical_accuracy: 0.3053 - mean_directional_accuracy: 0.5240\n",
      "{'loss': 1.3892143964767456, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} \n",
      " 2 \n",
      "\n",
      "Model time: 0.08281682804226875 minutes\n",
      "\n",
      "Total time: 44.6098983772099 minutes\n",
      "\n",
      "\n",
      "Model  89  out of  111\n",
      "Model type                FeedForward\n",
      "Hidden layers                       3\n",
      "Hidden units           [256, 64, 256]\n",
      "Activation function           sigmoid\n",
      "Dropout                           0.3\n",
      "L1                               10.0\n",
      "L2                               10.0\n",
      "Batch size                        128\n",
      "Optimizer                     RMSprop\n",
      "Learning rate                   0.001\n",
      "Name: 77, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 6s 43ms/step - loss: 31612.6992 - accuracy: 0.2518 - categorical_accuracy: 0.2518 - mean_directional_accuracy: 0.4921 - val_loss: 17299.0469 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 9303.6582 - accuracy: 0.2619 - categorical_accuracy: 0.2619 - mean_directional_accuracy: 0.5063 - val_loss: 3064.4563 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1103.8220 - accuracy: 0.2525 - categorical_accuracy: 0.2525 - mean_directional_accuracy: 0.4996 - val_loss: 390.6412 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 387.1071 - accuracy: 0.2550 - categorical_accuracy: 0.2550 - mean_directional_accuracy: 0.4896 - val_loss: 385.8429 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 5/300\n",
      "44/45 [============================>.] - ETA: 0s - loss: 386.8993 - accuracy: 0.2553 - categorical_accuracy: 0.2553 - mean_directional_accuracy: 0.4890Restoring model weights from the end of the best epoch: 4.\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 386.8888 - accuracy: 0.2550 - categorical_accuracy: 0.2550 - mean_directional_accuracy: 0.4895 - val_loss: 387.8694 - val_accuracy: 0.2814 - val_categorical_accuracy: 0.2814 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 5: early stopping\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 385.8429 - accuracy: 0.2814 - categorical_accuracy: 0.2814 - mean_directional_accuracy: 0.5416\n",
      "{'loss': 385.8428649902344, 'accuracy': 0.28144571185112, 'categorical_accuracy': 0.28144571185112, 'mean_directional_accuracy': 0.5416231751441956} \n",
      " 4 \n",
      "\n",
      "Model time: 0.17630710080266 minutes\n",
      "\n",
      "Total time: 44.78628881275654 minutes\n",
      "\n",
      "\n",
      "Model  90  out of  111\n",
      "Model type               FeedForward\n",
      "Hidden layers                      4\n",
      "Hidden units           [2, 32, 4, 1]\n",
      "Activation function           linear\n",
      "Dropout                          0.4\n",
      "L1                              0.01\n",
      "L2                              0.01\n",
      "Batch size                       128\n",
      "Optimizer                    RMSprop\n",
      "Learning rate                  0.001\n",
      "Name: 78, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 5s 37ms/step - loss: 2.2322 - accuracy: 0.2569 - categorical_accuracy: 0.2569 - mean_directional_accuracy: 0.5037 - val_loss: 2.0923 - val_accuracy: 0.2440 - val_categorical_accuracy: 0.2440 - val_mean_directional_accuracy: 0.4876\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.0207 - accuracy: 0.2604 - categorical_accuracy: 0.2604 - mean_directional_accuracy: 0.5060 - val_loss: 1.9225 - val_accuracy: 0.2470 - val_categorical_accuracy: 0.2470 - val_mean_directional_accuracy: 0.4876\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.8488 - accuracy: 0.2610 - categorical_accuracy: 0.2610 - mean_directional_accuracy: 0.5090 - val_loss: 1.7690 - val_accuracy: 0.2307 - val_categorical_accuracy: 0.2307 - val_mean_directional_accuracy: 0.4623\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.7066 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5121 - val_loss: 1.6503 - val_accuracy: 0.2289 - val_categorical_accuracy: 0.2289 - val_mean_directional_accuracy: 0.4585\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.6046 - accuracy: 0.2650 - categorical_accuracy: 0.2650 - mean_directional_accuracy: 0.5076 - val_loss: 1.5654 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 1.5338 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5100 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4875 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4716 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 1.4550 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4452 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4337 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4288 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4214 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4203 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.4152 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4157 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 1.4106 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4115 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.4067 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4076 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.4029 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4043 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3999 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4016 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.3974 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3991 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3950 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3968 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3928 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3947 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 1.3909 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3933 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3899 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3927 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 1.3894 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3923 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3888 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3918 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.3883 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3913 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3879 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3912 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 1.3878 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3911 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26/300\n",
      "41/45 [==========================>...] - ETA: 0s - loss: 1.3878 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5103Restoring model weights from the end of the best epoch: 25.\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1.3878 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3912 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26: early stopping\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 1.3911 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3910752534866333, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 25 \n",
      "\n",
      "Model time: 0.29141389578580856 minutes\n",
      "\n",
      "Total time: 45.07780271396041 minutes\n",
      "\n",
      "\n",
      "Model  91  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                   [4]\n",
      "Activation function           tanh\n",
      "Dropout                        0.1\n",
      "L1                            10.0\n",
      "L2                            10.0\n",
      "Batch size                      16\n",
      "Optimizer                  RMSprop\n",
      "Learning rate                0.001\n",
      "Name: 79, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 5s 8ms/step - loss: 183.6799 - accuracy: 0.2499 - categorical_accuracy: 0.2499 - mean_directional_accuracy: 0.4995 - val_loss: 41.9665 - val_accuracy: 0.2308 - val_categorical_accuracy: 0.2308 - val_mean_directional_accuracy: 0.4598\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 20.6594 - accuracy: 0.2680 - categorical_accuracy: 0.2680 - mean_directional_accuracy: 0.5112 - val_loss: 6.3761 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 4.9939 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.8614 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 4.8470 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.8558 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 4.8470 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.8529 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 4.8470 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.8516 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 4.8470 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.8512 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 4.8470 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.8508 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 4.8470 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.8503 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 4.8470 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.8500 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "350/356 [============================>.] - ETA: 0s - loss: 4.8472 - accuracy: 0.2661 - categorical_accuracy: 0.2661 - mean_directional_accuracy: 0.5102Restoring model weights from the end of the best epoch: 10.\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 4.8470 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.8505 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11: early stopping\n",
      "479/479 [==============================] - 1s 3ms/step - loss: 4.8500 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 4.850003242492676, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 10 \n",
      "\n",
      "Model time: 0.5002717562019825 minutes\n",
      "\n",
      "Total time: 45.57814114168286 minutes\n",
      "\n",
      "\n",
      "Model  92  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    3\n",
      "Hidden units           [32, 8, 64]\n",
      "Activation function         linear\n",
      "Dropout                        0.2\n",
      "L1                            10.0\n",
      "L2                           100.0\n",
      "Batch size                     256\n",
      "Optimizer                  RMSprop\n",
      "Learning rate               0.0001\n",
      "Name: 80, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 4s 52ms/step - loss: 14985.2334 - accuracy: 0.2527 - categorical_accuracy: 0.2527 - mean_directional_accuracy: 0.5035 - val_loss: 14714.2305 - val_accuracy: 0.2496 - val_categorical_accuracy: 0.2496 - val_mean_directional_accuracy: 0.4736\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 14514.6738 - accuracy: 0.2460 - categorical_accuracy: 0.2460 - mean_directional_accuracy: 0.4954 - val_loss: 14286.4443 - val_accuracy: 0.2491 - val_categorical_accuracy: 0.2491 - val_mean_directional_accuracy: 0.4738\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 14095.1133 - accuracy: 0.2585 - categorical_accuracy: 0.2585 - mean_directional_accuracy: 0.5132 - val_loss: 13873.8223 - val_accuracy: 0.2490 - val_categorical_accuracy: 0.2490 - val_mean_directional_accuracy: 0.4735\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 13686.7480 - accuracy: 0.2545 - categorical_accuracy: 0.2545 - mean_directional_accuracy: 0.5074 - val_loss: 13470.1475 - val_accuracy: 0.2486 - val_categorical_accuracy: 0.2486 - val_mean_directional_accuracy: 0.4717\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 13287.0146 - accuracy: 0.2641 - categorical_accuracy: 0.2641 - mean_directional_accuracy: 0.5042 - val_loss: 13074.9209 - val_accuracy: 0.2487 - val_categorical_accuracy: 0.2487 - val_mean_directional_accuracy: 0.4718\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 12895.5156 - accuracy: 0.2566 - categorical_accuracy: 0.2566 - mean_directional_accuracy: 0.4953 - val_loss: 12687.8311 - val_accuracy: 0.2474 - val_categorical_accuracy: 0.2474 - val_mean_directional_accuracy: 0.4717\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 12512.2051 - accuracy: 0.2589 - categorical_accuracy: 0.2589 - mean_directional_accuracy: 0.5139 - val_loss: 12308.8447 - val_accuracy: 0.2491 - val_categorical_accuracy: 0.2491 - val_mean_directional_accuracy: 0.4734\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 12136.8301 - accuracy: 0.2582 - categorical_accuracy: 0.2582 - mean_directional_accuracy: 0.5076 - val_loss: 11937.6865 - val_accuracy: 0.2499 - val_categorical_accuracy: 0.2499 - val_mean_directional_accuracy: 0.4726\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 11769.1875 - accuracy: 0.2610 - categorical_accuracy: 0.2610 - mean_directional_accuracy: 0.5077 - val_loss: 11574.1924 - val_accuracy: 0.2508 - val_categorical_accuracy: 0.2508 - val_mean_directional_accuracy: 0.4742\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 11409.2061 - accuracy: 0.2610 - categorical_accuracy: 0.2610 - mean_directional_accuracy: 0.5093 - val_loss: 11218.2666 - val_accuracy: 0.2504 - val_categorical_accuracy: 0.2504 - val_mean_directional_accuracy: 0.4731\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 11056.8008 - accuracy: 0.2494 - categorical_accuracy: 0.2494 - mean_directional_accuracy: 0.4995 - val_loss: 10869.9688 - val_accuracy: 0.2507 - val_categorical_accuracy: 0.2507 - val_mean_directional_accuracy: 0.4729\n",
      "Epoch 12/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 10711.9492 - accuracy: 0.2499 - categorical_accuracy: 0.2499 - mean_directional_accuracy: 0.5114 - val_loss: 10529.1221 - val_accuracy: 0.2500 - val_categorical_accuracy: 0.2500 - val_mean_directional_accuracy: 0.4739\n",
      "Epoch 13/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 10374.5205 - accuracy: 0.2627 - categorical_accuracy: 0.2627 - mean_directional_accuracy: 0.5242 - val_loss: 10195.6953 - val_accuracy: 0.2492 - val_categorical_accuracy: 0.2492 - val_mean_directional_accuracy: 0.4746\n",
      "Epoch 14/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 10044.4756 - accuracy: 0.2606 - categorical_accuracy: 0.2606 - mean_directional_accuracy: 0.5132 - val_loss: 9869.6191 - val_accuracy: 0.2473 - val_categorical_accuracy: 0.2473 - val_mean_directional_accuracy: 0.4746\n",
      "Epoch 15/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 9721.9014 - accuracy: 0.2680 - categorical_accuracy: 0.2680 - mean_directional_accuracy: 0.5205 - val_loss: 9551.1484 - val_accuracy: 0.2477 - val_categorical_accuracy: 0.2477 - val_mean_directional_accuracy: 0.4743\n",
      "Epoch 16/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 9406.8330 - accuracy: 0.2659 - categorical_accuracy: 0.2659 - mean_directional_accuracy: 0.5170 - val_loss: 9239.9414 - val_accuracy: 0.2453 - val_categorical_accuracy: 0.2453 - val_mean_directional_accuracy: 0.4726\n",
      "Epoch 17/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 9098.9111 - accuracy: 0.2548 - categorical_accuracy: 0.2548 - mean_directional_accuracy: 0.5128 - val_loss: 8935.8975 - val_accuracy: 0.2449 - val_categorical_accuracy: 0.2449 - val_mean_directional_accuracy: 0.4718\n",
      "Epoch 18/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 8798.1709 - accuracy: 0.2545 - categorical_accuracy: 0.2545 - mean_directional_accuracy: 0.5056 - val_loss: 8638.9785 - val_accuracy: 0.2461 - val_categorical_accuracy: 0.2461 - val_mean_directional_accuracy: 0.4716\n",
      "Epoch 19/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 8504.4814 - accuracy: 0.2610 - categorical_accuracy: 0.2610 - mean_directional_accuracy: 0.5133 - val_loss: 8349.0225 - val_accuracy: 0.2463 - val_categorical_accuracy: 0.2463 - val_mean_directional_accuracy: 0.4708\n",
      "Epoch 20/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 8217.6602 - accuracy: 0.2643 - categorical_accuracy: 0.2643 - mean_directional_accuracy: 0.5160 - val_loss: 8065.8799 - val_accuracy: 0.2471 - val_categorical_accuracy: 0.2471 - val_mean_directional_accuracy: 0.4713\n",
      "Epoch 21/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 7937.6592 - accuracy: 0.2601 - categorical_accuracy: 0.2601 - mean_directional_accuracy: 0.5111 - val_loss: 7789.5288 - val_accuracy: 0.2475 - val_categorical_accuracy: 0.2475 - val_mean_directional_accuracy: 0.4727\n",
      "Epoch 22/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 7664.4600 - accuracy: 0.2587 - categorical_accuracy: 0.2587 - mean_directional_accuracy: 0.5205 - val_loss: 7520.0322 - val_accuracy: 0.2461 - val_categorical_accuracy: 0.2461 - val_mean_directional_accuracy: 0.4710\n",
      "Epoch 23/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 7398.0972 - accuracy: 0.2599 - categorical_accuracy: 0.2599 - mean_directional_accuracy: 0.5130 - val_loss: 7257.2861 - val_accuracy: 0.2462 - val_categorical_accuracy: 0.2462 - val_mean_directional_accuracy: 0.4714\n",
      "Epoch 24/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 7138.3857 - accuracy: 0.2619 - categorical_accuracy: 0.2619 - mean_directional_accuracy: 0.5144 - val_loss: 7001.0664 - val_accuracy: 0.2452 - val_categorical_accuracy: 0.2452 - val_mean_directional_accuracy: 0.4705\n",
      "Epoch 25/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 6885.0781 - accuracy: 0.2622 - categorical_accuracy: 0.2622 - mean_directional_accuracy: 0.5179 - val_loss: 6751.1543 - val_accuracy: 0.2439 - val_categorical_accuracy: 0.2439 - val_mean_directional_accuracy: 0.4683\n",
      "Epoch 26/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 6638.1006 - accuracy: 0.2640 - categorical_accuracy: 0.2640 - mean_directional_accuracy: 0.5242 - val_loss: 6507.5449 - val_accuracy: 0.2413 - val_categorical_accuracy: 0.2413 - val_mean_directional_accuracy: 0.4676\n",
      "Epoch 27/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 6397.4097 - accuracy: 0.2648 - categorical_accuracy: 0.2648 - mean_directional_accuracy: 0.5249 - val_loss: 6270.2920 - val_accuracy: 0.2406 - val_categorical_accuracy: 0.2406 - val_mean_directional_accuracy: 0.4667\n",
      "Epoch 28/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 6163.0088 - accuracy: 0.2622 - categorical_accuracy: 0.2622 - mean_directional_accuracy: 0.5230 - val_loss: 6039.2119 - val_accuracy: 0.2400 - val_categorical_accuracy: 0.2400 - val_mean_directional_accuracy: 0.4659\n",
      "Epoch 29/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 5934.8916 - accuracy: 0.2568 - categorical_accuracy: 0.2568 - mean_directional_accuracy: 0.5228 - val_loss: 5814.5376 - val_accuracy: 0.2398 - val_categorical_accuracy: 0.2398 - val_mean_directional_accuracy: 0.4665\n",
      "Epoch 30/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 5712.9556 - accuracy: 0.2624 - categorical_accuracy: 0.2624 - mean_directional_accuracy: 0.5205 - val_loss: 5595.6689 - val_accuracy: 0.2376 - val_categorical_accuracy: 0.2376 - val_mean_directional_accuracy: 0.4644\n",
      "Epoch 31/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 5496.7422 - accuracy: 0.2684 - categorical_accuracy: 0.2684 - mean_directional_accuracy: 0.5249 - val_loss: 5382.6763 - val_accuracy: 0.2377 - val_categorical_accuracy: 0.2377 - val_mean_directional_accuracy: 0.4639\n",
      "Epoch 32/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 5286.5137 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5205 - val_loss: 5175.5977 - val_accuracy: 0.2367 - val_categorical_accuracy: 0.2367 - val_mean_directional_accuracy: 0.4607\n",
      "Epoch 33/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 5082.2319 - accuracy: 0.2608 - categorical_accuracy: 0.2608 - mean_directional_accuracy: 0.5083 - val_loss: 4974.5640 - val_accuracy: 0.2366 - val_categorical_accuracy: 0.2366 - val_mean_directional_accuracy: 0.4599\n",
      "Epoch 34/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4883.8838 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5195 - val_loss: 4779.3394 - val_accuracy: 0.2356 - val_categorical_accuracy: 0.2356 - val_mean_directional_accuracy: 0.4603\n",
      "Epoch 35/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 4691.2417 - accuracy: 0.2622 - categorical_accuracy: 0.2622 - mean_directional_accuracy: 0.5176 - val_loss: 4589.6499 - val_accuracy: 0.2350 - val_categorical_accuracy: 0.2350 - val_mean_directional_accuracy: 0.4590\n",
      "Epoch 36/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 4504.1401 - accuracy: 0.2601 - categorical_accuracy: 0.2601 - mean_directional_accuracy: 0.5191 - val_loss: 4405.6084 - val_accuracy: 0.2328 - val_categorical_accuracy: 0.2328 - val_mean_directional_accuracy: 0.4586\n",
      "Epoch 37/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 4322.6914 - accuracy: 0.2624 - categorical_accuracy: 0.2624 - mean_directional_accuracy: 0.5153 - val_loss: 4227.1284 - val_accuracy: 0.2313 - val_categorical_accuracy: 0.2313 - val_mean_directional_accuracy: 0.4568\n",
      "Epoch 38/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 4146.7593 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5188 - val_loss: 4054.2417 - val_accuracy: 0.2317 - val_categorical_accuracy: 0.2317 - val_mean_directional_accuracy: 0.4586\n",
      "Epoch 39/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 3976.4602 - accuracy: 0.2599 - categorical_accuracy: 0.2599 - mean_directional_accuracy: 0.5079 - val_loss: 3886.8394 - val_accuracy: 0.2312 - val_categorical_accuracy: 0.2312 - val_mean_directional_accuracy: 0.4588\n",
      "Epoch 40/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 3811.4995 - accuracy: 0.2696 - categorical_accuracy: 0.2696 - mean_directional_accuracy: 0.5179 - val_loss: 3724.7307 - val_accuracy: 0.2329 - val_categorical_accuracy: 0.2329 - val_mean_directional_accuracy: 0.4589\n",
      "Epoch 41/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3651.9009 - accuracy: 0.2655 - categorical_accuracy: 0.2655 - mean_directional_accuracy: 0.5213 - val_loss: 3568.0632 - val_accuracy: 0.2316 - val_categorical_accuracy: 0.2316 - val_mean_directional_accuracy: 0.4590\n",
      "Epoch 42/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 3497.6023 - accuracy: 0.2648 - categorical_accuracy: 0.2648 - mean_directional_accuracy: 0.5228 - val_loss: 3416.4719 - val_accuracy: 0.2317 - val_categorical_accuracy: 0.2317 - val_mean_directional_accuracy: 0.4603\n",
      "Epoch 43/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 3348.3301 - accuracy: 0.2720 - categorical_accuracy: 0.2720 - mean_directional_accuracy: 0.5205 - val_loss: 3269.8706 - val_accuracy: 0.2302 - val_categorical_accuracy: 0.2302 - val_mean_directional_accuracy: 0.4594\n",
      "Epoch 44/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3204.0374 - accuracy: 0.2576 - categorical_accuracy: 0.2576 - mean_directional_accuracy: 0.5051 - val_loss: 3128.3279 - val_accuracy: 0.2303 - val_categorical_accuracy: 0.2303 - val_mean_directional_accuracy: 0.4606\n",
      "Epoch 45/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 3064.7712 - accuracy: 0.2612 - categorical_accuracy: 0.2612 - mean_directional_accuracy: 0.5105 - val_loss: 2991.6096 - val_accuracy: 0.2308 - val_categorical_accuracy: 0.2308 - val_mean_directional_accuracy: 0.4607\n",
      "Epoch 46/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 2930.1995 - accuracy: 0.2643 - categorical_accuracy: 0.2643 - mean_directional_accuracy: 0.5137 - val_loss: 2859.5706 - val_accuracy: 0.2291 - val_categorical_accuracy: 0.2291 - val_mean_directional_accuracy: 0.4597\n",
      "Epoch 47/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 2800.4497 - accuracy: 0.2610 - categorical_accuracy: 0.2610 - mean_directional_accuracy: 0.5086 - val_loss: 2732.4116 - val_accuracy: 0.2296 - val_categorical_accuracy: 0.2296 - val_mean_directional_accuracy: 0.4597\n",
      "Epoch 48/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2675.2920 - accuracy: 0.2657 - categorical_accuracy: 0.2657 - mean_directional_accuracy: 0.5151 - val_loss: 2609.5952 - val_accuracy: 0.2299 - val_categorical_accuracy: 0.2299 - val_mean_directional_accuracy: 0.4593\n",
      "Epoch 49/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 2554.5098 - accuracy: 0.2645 - categorical_accuracy: 0.2645 - mean_directional_accuracy: 0.5123 - val_loss: 2491.1475 - val_accuracy: 0.2300 - val_categorical_accuracy: 0.2300 - val_mean_directional_accuracy: 0.4592\n",
      "Epoch 50/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 2438.0154 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.5104 - val_loss: 2377.0178 - val_accuracy: 0.2300 - val_categorical_accuracy: 0.2300 - val_mean_directional_accuracy: 0.4588\n",
      "Epoch 51/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 2326.0759 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5130 - val_loss: 2267.5022 - val_accuracy: 0.2294 - val_categorical_accuracy: 0.2294 - val_mean_directional_accuracy: 0.4582\n",
      "Epoch 52/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2218.4194 - accuracy: 0.2640 - categorical_accuracy: 0.2640 - mean_directional_accuracy: 0.5133 - val_loss: 2162.0649 - val_accuracy: 0.2290 - val_categorical_accuracy: 0.2290 - val_mean_directional_accuracy: 0.4580\n",
      "Epoch 53/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 2114.9988 - accuracy: 0.2657 - categorical_accuracy: 0.2657 - mean_directional_accuracy: 0.5112 - val_loss: 2060.9202 - val_accuracy: 0.2291 - val_categorical_accuracy: 0.2291 - val_mean_directional_accuracy: 0.4581\n",
      "Epoch 54/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 2015.7693 - accuracy: 0.2657 - categorical_accuracy: 0.2657 - mean_directional_accuracy: 0.5095 - val_loss: 1963.9523 - val_accuracy: 0.2286 - val_categorical_accuracy: 0.2286 - val_mean_directional_accuracy: 0.4579\n",
      "Epoch 55/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1920.6748 - accuracy: 0.2650 - categorical_accuracy: 0.2650 - mean_directional_accuracy: 0.5112 - val_loss: 1871.0632 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4579\n",
      "Epoch 56/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1829.6993 - accuracy: 0.2641 - categorical_accuracy: 0.2641 - mean_directional_accuracy: 0.5111 - val_loss: 1782.2872 - val_accuracy: 0.2285 - val_categorical_accuracy: 0.2285 - val_mean_directional_accuracy: 0.4581\n",
      "Epoch 57/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1742.7729 - accuracy: 0.2657 - categorical_accuracy: 0.2657 - mean_directional_accuracy: 0.5086 - val_loss: 1697.5092 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4582\n",
      "Epoch 58/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1659.8595 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5109 - val_loss: 1616.7142 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 59/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1580.7975 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5112 - val_loss: 1539.5959 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 60/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1505.2443 - accuracy: 0.2669 - categorical_accuracy: 0.2669 - mean_directional_accuracy: 0.5098 - val_loss: 1465.8639 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 61/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1433.1290 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5102 - val_loss: 1395.6780 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 62/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1364.4557 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5104 - val_loss: 1328.6709 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 63/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1299.0363 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1265.2198 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 64/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1237.1516 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5104 - val_loss: 1205.0132 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 65/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1178.4185 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5107 - val_loss: 1148.0375 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 66/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1122.8090 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1093.9701 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 67/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1070.0784 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1042.8278 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 68/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1020.2762 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 994.5913 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 69/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 973.4552 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 949.4175 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 70/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 929.6838 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 907.2974 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 71/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 889.0698 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 868.3984 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 72/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 851.3864 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 832.0742 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 73/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 816.2759 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 798.3519 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 74/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 783.6805 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 767.0173 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 75/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 753.3739 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 737.9371 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 76/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 725.3233 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 710.8241 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 77/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 698.6555 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 684.6348 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 78/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 672.7706 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 659.0861 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 79/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 647.6082 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 634.3740 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 80/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 623.1691 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 610.2537 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 81/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 599.3687 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 586.8054 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 82/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 576.1891 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 563.9376 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 83/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 553.6079 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 541.6873 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 84/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 531.6107 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 519.9872 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 85/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 510.1943 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 498.8943 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 86/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 489.3542 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 478.3428 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 87/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 469.0820 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 458.4066 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 88/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 449.4419 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 439.1361 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 89/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 430.4870 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 420.5189 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 90/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 412.0926 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 402.3886 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 91/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 394.2184 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 384.8012 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 92/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 376.8570 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 367.7326 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 93/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 360.0631 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 351.2054 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 94/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 343.7080 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 335.0858 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 95/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 327.8565 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 319.5312 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 96/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 312.5272 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 304.4882 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 97/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 297.7373 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 289.9680 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 98/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 283.4272 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 275.9144 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 99/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 269.6144 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 262.3680 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 100/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 256.2778 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 249.2733 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 101/300\n",
      "23/23 [==============================] - 0s 21ms/step - loss: 243.4054 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 236.6533 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 102/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 230.9840 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 224.4835 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 103/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 219.0788 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 212.8886 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 104/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 207.7100 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 201.7729 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 105/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 196.8201 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 191.1363 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 106/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 186.3658 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 180.8914 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 107/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 176.3288 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 171.1088 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 108/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 166.7314 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 161.6915 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 109/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 157.4727 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 152.6434 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 110/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 148.5943 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 143.9852 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 111/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 140.2306 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 135.9305 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 112/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 132.3791 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 128.3417 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 113/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 125.0033 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 121.1791 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 114/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 117.9662 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 114.2719 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 115/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 111.2273 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 107.7617 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 116/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 104.8923 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 101.6189 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 117/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 98.9170 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 95.8342 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 118/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 93.2723 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 90.3492 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 119/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 87.9248 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 85.1569 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 120/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 82.8600 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 80.2518 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 121/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 78.1015 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 75.6453 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 122/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 73.6026 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 71.2913 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 123/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 69.4045 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 67.2502 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 124/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 65.4560 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 63.4174 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 125/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 61.7515 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 59.8538 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 126/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 58.2946 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 56.5535 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 127/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 55.1151 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 53.4712 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 128/300\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 52.1056 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 50.5596 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 129/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 49.2856 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 47.8458 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 130/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 46.6340 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 45.2397 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 131/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 44.0614 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 42.7071 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 132/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 41.5661 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 40.2602 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 133/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 39.1746 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 37.9400 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 134/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 36.8947 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 35.6936 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 135/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 34.6906 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 33.5453 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 136/300\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 32.5921 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 31.5019 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 137/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 30.5908 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 29.5514 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 138/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 28.7036 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 27.7407 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 139/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 26.9310 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 26.0077 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 140/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 25.2257 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 24.3382 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 141/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 23.5989 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 22.7609 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 142/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 22.0768 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 21.2978 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 143/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 20.6389 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 19.8864 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 144/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 19.2633 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 18.5532 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 145/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 17.9510 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 17.2624 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 146/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 16.6794 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 16.0193 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 147/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 15.4795 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 14.8776 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 148/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 14.3671 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 13.7848 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 149/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 13.2924 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 12.7308 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 150/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 12.2701 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 11.7708 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 151/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 11.3701 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 10.9142 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 152/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 10.5255 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 10.0838 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 153/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 9.7090 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 9.2854 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 154/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 8.9438 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 8.5651 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 155/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 8.2496 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 7.9053 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 156/300\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 7.6198 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 7.3102 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 157/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 7.0818 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 6.8459 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 158/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 6.6568 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 6.4539 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 159/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 6.2938 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 6.1157 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 160/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 5.9694 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.8092 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 161/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 5.6706 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.5174 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 162/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 5.3862 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.2533 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 163/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 5.1578 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.0624 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 164/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 4.9782 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.8878 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 165/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 4.8223 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.7581 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 166/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.7118 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.6747 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 167/300\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 4.6408 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.6194 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 168/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.6078 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.6060 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 169/300\n",
      "23/23 [==============================] - 1s 27ms/step - loss: 4.6027 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.6052 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 170/300\n",
      "11/23 [=============>................] - ETA: 0s - loss: 4.6025 - accuracy: 0.2653 - categorical_accuracy: 0.2653 - mean_directional_accuracy: 0.5000Restoring model weights from the end of the best epoch: 169.\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 4.6027 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.6060 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 170: early stopping\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 4.6052 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 4.60519552230835, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 169 \n",
      "\n",
      "Model time: 1.0063257738947868 minutes\n",
      "\n",
      "Total time: 46.584574434906244 minutes\n",
      "\n",
      "\n",
      "Model  93  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    2\n",
      "Hidden units              [8, 128]\n",
      "Activation function         linear\n",
      "Dropout                        0.3\n",
      "L1                            0.01\n",
      "L2                           0.001\n",
      "Batch size                      64\n",
      "Optimizer                     Adam\n",
      "Learning rate               0.0001\n",
      "Name: 81, dtype: object\n",
      "NN2Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 3s 15ms/step - loss: 4.4130 - accuracy: 0.2346 - categorical_accuracy: 0.2346 - mean_directional_accuracy: 0.4898 - val_loss: 4.2236 - val_accuracy: 0.2581 - val_categorical_accuracy: 0.2581 - val_mean_directional_accuracy: 0.5160\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 4.2063 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.5090 - val_loss: 4.0764 - val_accuracy: 0.2603 - val_categorical_accuracy: 0.2603 - val_mean_directional_accuracy: 0.5174\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 4.0490 - accuracy: 0.2557 - categorical_accuracy: 0.2557 - mean_directional_accuracy: 0.5046 - val_loss: 3.9331 - val_accuracy: 0.2603 - val_categorical_accuracy: 0.2603 - val_mean_directional_accuracy: 0.5067\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 3.8931 - accuracy: 0.2504 - categorical_accuracy: 0.2504 - mean_directional_accuracy: 0.4926 - val_loss: 3.7927 - val_accuracy: 0.2649 - val_categorical_accuracy: 0.2649 - val_mean_directional_accuracy: 0.5055\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 3.7421 - accuracy: 0.2741 - categorical_accuracy: 0.2741 - mean_directional_accuracy: 0.5155 - val_loss: 3.6554 - val_accuracy: 0.2668 - val_categorical_accuracy: 0.2668 - val_mean_directional_accuracy: 0.5047\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 3.6047 - accuracy: 0.2671 - categorical_accuracy: 0.2671 - mean_directional_accuracy: 0.5126 - val_loss: 3.5206 - val_accuracy: 0.2620 - val_categorical_accuracy: 0.2620 - val_mean_directional_accuracy: 0.4991\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 1s 6ms/step - loss: 3.4651 - accuracy: 0.2696 - categorical_accuracy: 0.2696 - mean_directional_accuracy: 0.5061 - val_loss: 3.3892 - val_accuracy: 0.2581 - val_categorical_accuracy: 0.2581 - val_mean_directional_accuracy: 0.4933\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 3.3287 - accuracy: 0.2708 - categorical_accuracy: 0.2708 - mean_directional_accuracy: 0.5181 - val_loss: 3.2610 - val_accuracy: 0.2612 - val_categorical_accuracy: 0.2612 - val_mean_directional_accuracy: 0.4949\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 3.2005 - accuracy: 0.2687 - categorical_accuracy: 0.2687 - mean_directional_accuracy: 0.5172 - val_loss: 3.1353 - val_accuracy: 0.2567 - val_categorical_accuracy: 0.2567 - val_mean_directional_accuracy: 0.4885\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 3.0728 - accuracy: 0.2799 - categorical_accuracy: 0.2799 - mean_directional_accuracy: 0.5207 - val_loss: 3.0130 - val_accuracy: 0.2552 - val_categorical_accuracy: 0.2552 - val_mean_directional_accuracy: 0.4833\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 2.9487 - accuracy: 0.2845 - categorical_accuracy: 0.2845 - mean_directional_accuracy: 0.5235 - val_loss: 2.8943 - val_accuracy: 0.2531 - val_categorical_accuracy: 0.2531 - val_mean_directional_accuracy: 0.4799\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.8311 - accuracy: 0.2726 - categorical_accuracy: 0.2726 - mean_directional_accuracy: 0.5177 - val_loss: 2.7785 - val_accuracy: 0.2520 - val_categorical_accuracy: 0.2520 - val_mean_directional_accuracy: 0.4787\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 2.7179 - accuracy: 0.2838 - categorical_accuracy: 0.2838 - mean_directional_accuracy: 0.5156 - val_loss: 2.6654 - val_accuracy: 0.2503 - val_categorical_accuracy: 0.2503 - val_mean_directional_accuracy: 0.4765\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 2.6073 - accuracy: 0.2726 - categorical_accuracy: 0.2726 - mean_directional_accuracy: 0.5097 - val_loss: 2.5565 - val_accuracy: 0.2517 - val_categorical_accuracy: 0.2517 - val_mean_directional_accuracy: 0.4766\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 2.4984 - accuracy: 0.2843 - categorical_accuracy: 0.2843 - mean_directional_accuracy: 0.5223 - val_loss: 2.4527 - val_accuracy: 0.2540 - val_categorical_accuracy: 0.2540 - val_mean_directional_accuracy: 0.4763\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 2.3968 - accuracy: 0.2806 - categorical_accuracy: 0.2806 - mean_directional_accuracy: 0.5126 - val_loss: 2.3528 - val_accuracy: 0.2518 - val_categorical_accuracy: 0.2518 - val_mean_directional_accuracy: 0.4740\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 2.2984 - accuracy: 0.2833 - categorical_accuracy: 0.2833 - mean_directional_accuracy: 0.5190 - val_loss: 2.2574 - val_accuracy: 0.2496 - val_categorical_accuracy: 0.2496 - val_mean_directional_accuracy: 0.4744\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 2.2049 - accuracy: 0.2847 - categorical_accuracy: 0.2847 - mean_directional_accuracy: 0.5183 - val_loss: 2.1669 - val_accuracy: 0.2461 - val_categorical_accuracy: 0.2461 - val_mean_directional_accuracy: 0.4749\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 2.1176 - accuracy: 0.2952 - categorical_accuracy: 0.2952 - mean_directional_accuracy: 0.5281 - val_loss: 2.0842 - val_accuracy: 0.2432 - val_categorical_accuracy: 0.2432 - val_mean_directional_accuracy: 0.4731\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 2.0387 - accuracy: 0.2849 - categorical_accuracy: 0.2849 - mean_directional_accuracy: 0.5211 - val_loss: 2.0085 - val_accuracy: 0.2428 - val_categorical_accuracy: 0.2428 - val_mean_directional_accuracy: 0.4736\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9672 - accuracy: 0.2856 - categorical_accuracy: 0.2856 - mean_directional_accuracy: 0.5220 - val_loss: 1.9393 - val_accuracy: 0.2420 - val_categorical_accuracy: 0.2420 - val_mean_directional_accuracy: 0.4739\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.9012 - accuracy: 0.2787 - categorical_accuracy: 0.2787 - mean_directional_accuracy: 0.5176 - val_loss: 1.8766 - val_accuracy: 0.2402 - val_categorical_accuracy: 0.2402 - val_mean_directional_accuracy: 0.4738\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8409 - accuracy: 0.2845 - categorical_accuracy: 0.2845 - mean_directional_accuracy: 0.5249 - val_loss: 1.8181 - val_accuracy: 0.2402 - val_categorical_accuracy: 0.2402 - val_mean_directional_accuracy: 0.4739\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7850 - accuracy: 0.2859 - categorical_accuracy: 0.2859 - mean_directional_accuracy: 0.5293 - val_loss: 1.7635 - val_accuracy: 0.2389 - val_categorical_accuracy: 0.2389 - val_mean_directional_accuracy: 0.4722\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.7329 - accuracy: 0.2805 - categorical_accuracy: 0.2805 - mean_directional_accuracy: 0.5230 - val_loss: 1.7123 - val_accuracy: 0.2375 - val_categorical_accuracy: 0.2375 - val_mean_directional_accuracy: 0.4712\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6838 - accuracy: 0.2805 - categorical_accuracy: 0.2805 - mean_directional_accuracy: 0.5232 - val_loss: 1.6645 - val_accuracy: 0.2359 - val_categorical_accuracy: 0.2359 - val_mean_directional_accuracy: 0.4687\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.6388 - accuracy: 0.2777 - categorical_accuracy: 0.2777 - mean_directional_accuracy: 0.5220 - val_loss: 1.6211 - val_accuracy: 0.2334 - val_categorical_accuracy: 0.2334 - val_mean_directional_accuracy: 0.4649\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.5977 - accuracy: 0.2777 - categorical_accuracy: 0.2777 - mean_directional_accuracy: 0.5211 - val_loss: 1.5814 - val_accuracy: 0.2320 - val_categorical_accuracy: 0.2320 - val_mean_directional_accuracy: 0.4639\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.5606 - accuracy: 0.2749 - categorical_accuracy: 0.2749 - mean_directional_accuracy: 0.5213 - val_loss: 1.5461 - val_accuracy: 0.2300 - val_categorical_accuracy: 0.2300 - val_mean_directional_accuracy: 0.4606\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.5277 - accuracy: 0.2733 - categorical_accuracy: 0.2733 - mean_directional_accuracy: 0.5177 - val_loss: 1.5151 - val_accuracy: 0.2290 - val_categorical_accuracy: 0.2290 - val_mean_directional_accuracy: 0.4580\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4993 - accuracy: 0.2698 - categorical_accuracy: 0.2698 - mean_directional_accuracy: 0.5140 - val_loss: 1.4885 - val_accuracy: 0.2290 - val_categorical_accuracy: 0.2290 - val_mean_directional_accuracy: 0.4580\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4748 - accuracy: 0.2701 - categorical_accuracy: 0.2701 - mean_directional_accuracy: 0.5130 - val_loss: 1.4655 - val_accuracy: 0.2293 - val_categorical_accuracy: 0.2293 - val_mean_directional_accuracy: 0.4588\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4539 - accuracy: 0.2685 - categorical_accuracy: 0.2685 - mean_directional_accuracy: 0.5112 - val_loss: 1.4468 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4371 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5105 - val_loss: 1.4319 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4242 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4206 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4141 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4115 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4060 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4043 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3997 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3990 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3953 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3954 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3921 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3928 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3898 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3909 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3882 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3897 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3872 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3890 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3866 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3886 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3863 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3883 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3860 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3880 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3857 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3879 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3855 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3878 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "89/89 [==============================] - ETA: 0s - loss: 1.3855 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105Restoring model weights from the end of the best epoch: 48.\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3855 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3878 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49: early stopping\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.3878 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3878040313720703, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 48 \n",
      "\n",
      "Model time: 0.6984058395028114 minutes\n",
      "\n",
      "Total time: 47.28318103030324 minutes\n",
      "\n",
      "\n",
      "Model  94  out of  111\n",
      "Model type                    FeedForward\n",
      "Hidden layers                           5\n",
      "Hidden units           [4, 64, 2, 32, 32]\n",
      "Activation function               sigmoid\n",
      "Dropout                               0.1\n",
      "L1                                0.00001\n",
      "L2                                    0.0\n",
      "Batch size                             32\n",
      "Optimizer                         RMSprop\n",
      "Learning rate                       0.001\n",
      "Name: 82, dtype: object\n",
      "NN5Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 6s 14ms/step - loss: 1.4037 - accuracy: 0.2640 - categorical_accuracy: 0.2640 - mean_directional_accuracy: 0.5047 - val_loss: 1.3944 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "174/178 [============================>.] - ETA: 0s - loss: 1.3987 - accuracy: 0.2581 - categorical_accuracy: 0.2581 - mean_directional_accuracy: 0.5092Restoring model weights from the end of the best epoch: 1.\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3989 - accuracy: 0.2585 - categorical_accuracy: 0.2585 - mean_directional_accuracy: 0.5104 - val_loss: 1.3956 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.3944 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.394366979598999, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 1 \n",
      "\n",
      "Model time: 0.14688371866941452 minutes\n",
      "\n",
      "Total time: 47.43015744164586 minutes\n",
      "\n",
      "\n",
      "Model  95  out of  111\n",
      "Model type                         FeedForward\n",
      "Hidden layers                                5\n",
      "Hidden units           [64, 256, 128, 256, 16]\n",
      "Activation function                       relu\n",
      "Dropout                                    0.1\n",
      "L1                                       100.0\n",
      "L2                                        0.01\n",
      "Batch size                                  16\n",
      "Optimizer                              RMSprop\n",
      "Learning rate                             0.01\n",
      "Name: 83, dtype: object\n",
      "NN5Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 9s 14ms/step - loss: 48904.6250 - accuracy: 0.2626 - categorical_accuracy: 0.2626 - mean_directional_accuracy: 0.5047 - val_loss: 40026.0430 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "351/356 [============================>.] - ETA: 0s - loss: 43070.9531 - accuracy: 0.2520 - categorical_accuracy: 0.2520 - mean_directional_accuracy: 0.4922Restoring model weights from the end of the best epoch: 1.\n",
      "356/356 [==============================] - 4s 11ms/step - loss: 43077.7539 - accuracy: 0.2529 - categorical_accuracy: 0.2529 - mean_directional_accuracy: 0.4932 - val_loss: 40395.9961 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2: early stopping\n",
      "479/479 [==============================] - 2s 3ms/step - loss: 40026.0430 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 40026.04296875, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 1 \n",
      "\n",
      "Model time: 0.3147120922803879 minutes\n",
      "\n",
      "Total time: 47.74495268240571 minutes\n",
      "\n",
      "\n",
      "Model  96  out of  111\n",
      "Model type                FeedForward\n",
      "Hidden layers                       4\n",
      "Hidden units           [2, 128, 2, 2]\n",
      "Activation function            linear\n",
      "Dropout                           0.8\n",
      "L1                               0.01\n",
      "L2                            0.00001\n",
      "Batch size                        256\n",
      "Optimizer                        Adam\n",
      "Learning rate                    0.01\n",
      "Name: 84, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 4s 52ms/step - loss: 2.1335 - accuracy: 0.2631 - categorical_accuracy: 0.2631 - mean_directional_accuracy: 0.5081 - val_loss: 1.8143 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.7050 - accuracy: 0.2654 - categorical_accuracy: 0.2654 - mean_directional_accuracy: 0.5109 - val_loss: 1.6029 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5481 - accuracy: 0.2691 - categorical_accuracy: 0.2691 - mean_directional_accuracy: 0.5083 - val_loss: 1.5079 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4858 - accuracy: 0.2648 - categorical_accuracy: 0.2648 - mean_directional_accuracy: 0.5102 - val_loss: 1.4711 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.4587 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5095 - val_loss: 1.4516 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.4434 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5107 - val_loss: 1.4400 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.4329 - accuracy: 0.2669 - categorical_accuracy: 0.2669 - mean_directional_accuracy: 0.5105 - val_loss: 1.4320 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4258 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4256 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4199 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4205 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4160 - accuracy: 0.2659 - categorical_accuracy: 0.2659 - mean_directional_accuracy: 0.5105 - val_loss: 1.4164 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4120 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4159 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4097 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4113 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4087 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4103 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.4062 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4089 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.4052 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4066 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4039 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4059 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4023 - accuracy: 0.2631 - categorical_accuracy: 0.2631 - mean_directional_accuracy: 0.5049 - val_loss: 1.4046 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "13/23 [===============>..............] - ETA: 0s - loss: 1.4023 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5132Restoring model weights from the end of the best epoch: 17.\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4016 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4048 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18: early stopping\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.4046 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.4046223163604736, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 17 \n",
      "\n",
      "Model time: 0.16949442774057388 minutes\n",
      "\n",
      "Total time: 47.91454087942839 minutes\n",
      "\n",
      "\n",
      "Model  97  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                  [32]\n",
      "Activation function         linear\n",
      "Dropout                        0.0\n",
      "L1                            0.01\n",
      "L2                             1.0\n",
      "Batch size                     256\n",
      "Optimizer                  RMSprop\n",
      "Learning rate               0.0001\n",
      "Name: 85, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 3s 45ms/step - loss: 64.2425 - accuracy: 0.2564 - categorical_accuracy: 0.2564 - mean_directional_accuracy: 0.5026 - val_loss: 62.7404 - val_accuracy: 0.2599 - val_categorical_accuracy: 0.2599 - val_mean_directional_accuracy: 0.5056\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 61.7207 - accuracy: 0.2564 - categorical_accuracy: 0.2564 - mean_directional_accuracy: 0.5030 - val_loss: 60.4600 - val_accuracy: 0.2599 - val_categorical_accuracy: 0.2599 - val_mean_directional_accuracy: 0.5050\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 59.4875 - accuracy: 0.2580 - categorical_accuracy: 0.2580 - mean_directional_accuracy: 0.5032 - val_loss: 58.2734 - val_accuracy: 0.2584 - val_categorical_accuracy: 0.2584 - val_mean_directional_accuracy: 0.5026\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 57.3266 - accuracy: 0.2587 - categorical_accuracy: 0.2587 - mean_directional_accuracy: 0.5042 - val_loss: 56.1467 - val_accuracy: 0.2569 - val_categorical_accuracy: 0.2569 - val_mean_directional_accuracy: 0.5005\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 55.2229 - accuracy: 0.2590 - categorical_accuracy: 0.2590 - mean_directional_accuracy: 0.5039 - val_loss: 54.0764 - val_accuracy: 0.2572 - val_categorical_accuracy: 0.2572 - val_mean_directional_accuracy: 0.5005\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 53.1751 - accuracy: 0.2590 - categorical_accuracy: 0.2590 - mean_directional_accuracy: 0.5032 - val_loss: 52.0609 - val_accuracy: 0.2573 - val_categorical_accuracy: 0.2573 - val_mean_directional_accuracy: 0.5005\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 51.1821 - accuracy: 0.2594 - categorical_accuracy: 0.2594 - mean_directional_accuracy: 0.5030 - val_loss: 50.0999 - val_accuracy: 0.2560 - val_categorical_accuracy: 0.2560 - val_mean_directional_accuracy: 0.4988\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 49.2436 - accuracy: 0.2599 - categorical_accuracy: 0.2599 - mean_directional_accuracy: 0.5019 - val_loss: 48.1929 - val_accuracy: 0.2563 - val_categorical_accuracy: 0.2563 - val_mean_directional_accuracy: 0.4997\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 47.3582 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.5018 - val_loss: 46.3380 - val_accuracy: 0.2559 - val_categorical_accuracy: 0.2559 - val_mean_directional_accuracy: 0.4993\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 45.5252 - accuracy: 0.2626 - categorical_accuracy: 0.2626 - mean_directional_accuracy: 0.5028 - val_loss: 44.5354 - val_accuracy: 0.2564 - val_categorical_accuracy: 0.2564 - val_mean_directional_accuracy: 0.5005\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 43.7445 - accuracy: 0.2624 - categorical_accuracy: 0.2624 - mean_directional_accuracy: 0.5026 - val_loss: 42.7848 - val_accuracy: 0.2550 - val_categorical_accuracy: 0.2550 - val_mean_directional_accuracy: 0.4983\n",
      "Epoch 12/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 42.0150 - accuracy: 0.2622 - categorical_accuracy: 0.2622 - mean_directional_accuracy: 0.5023 - val_loss: 41.0843 - val_accuracy: 0.2556 - val_categorical_accuracy: 0.2556 - val_mean_directional_accuracy: 0.4970\n",
      "Epoch 13/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 40.3367 - accuracy: 0.2604 - categorical_accuracy: 0.2604 - mean_directional_accuracy: 0.5014 - val_loss: 39.4350 - val_accuracy: 0.2546 - val_categorical_accuracy: 0.2546 - val_mean_directional_accuracy: 0.4980\n",
      "Epoch 14/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 38.7082 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.5019 - val_loss: 37.8339 - val_accuracy: 0.2547 - val_categorical_accuracy: 0.2547 - val_mean_directional_accuracy: 0.4975\n",
      "Epoch 15/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 37.1279 - accuracy: 0.2615 - categorical_accuracy: 0.2615 - mean_directional_accuracy: 0.5030 - val_loss: 36.2814 - val_accuracy: 0.2555 - val_categorical_accuracy: 0.2555 - val_mean_directional_accuracy: 0.4980\n",
      "Epoch 16/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 35.5962 - accuracy: 0.2638 - categorical_accuracy: 0.2638 - mean_directional_accuracy: 0.5039 - val_loss: 34.7766 - val_accuracy: 0.2556 - val_categorical_accuracy: 0.2556 - val_mean_directional_accuracy: 0.4982\n",
      "Epoch 17/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 34.1123 - accuracy: 0.2610 - categorical_accuracy: 0.2610 - mean_directional_accuracy: 0.5021 - val_loss: 33.3197 - val_accuracy: 0.2553 - val_categorical_accuracy: 0.2553 - val_mean_directional_accuracy: 0.4978\n",
      "Epoch 18/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 32.6750 - accuracy: 0.2617 - categorical_accuracy: 0.2617 - mean_directional_accuracy: 0.5018 - val_loss: 31.9079 - val_accuracy: 0.2559 - val_categorical_accuracy: 0.2559 - val_mean_directional_accuracy: 0.4967\n",
      "Epoch 19/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 31.2835 - accuracy: 0.2592 - categorical_accuracy: 0.2592 - mean_directional_accuracy: 0.5019 - val_loss: 30.5427 - val_accuracy: 0.2553 - val_categorical_accuracy: 0.2553 - val_mean_directional_accuracy: 0.4962\n",
      "Epoch 20/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 29.9378 - accuracy: 0.2599 - categorical_accuracy: 0.2599 - mean_directional_accuracy: 0.5028 - val_loss: 29.2217 - val_accuracy: 0.2553 - val_categorical_accuracy: 0.2553 - val_mean_directional_accuracy: 0.4961\n",
      "Epoch 21/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 28.6361 - accuracy: 0.2594 - categorical_accuracy: 0.2594 - mean_directional_accuracy: 0.5028 - val_loss: 27.9453 - val_accuracy: 0.2560 - val_categorical_accuracy: 0.2560 - val_mean_directional_accuracy: 0.4960\n",
      "Epoch 22/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 27.3784 - accuracy: 0.2590 - categorical_accuracy: 0.2590 - mean_directional_accuracy: 0.5016 - val_loss: 26.7116 - val_accuracy: 0.2547 - val_categorical_accuracy: 0.2547 - val_mean_directional_accuracy: 0.4954\n",
      "Epoch 23/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 26.1634 - accuracy: 0.2592 - categorical_accuracy: 0.2592 - mean_directional_accuracy: 0.5032 - val_loss: 25.5203 - val_accuracy: 0.2533 - val_categorical_accuracy: 0.2533 - val_mean_directional_accuracy: 0.4936\n",
      "Epoch 24/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 24.9903 - accuracy: 0.2596 - categorical_accuracy: 0.2596 - mean_directional_accuracy: 0.5035 - val_loss: 24.3705 - val_accuracy: 0.2547 - val_categorical_accuracy: 0.2547 - val_mean_directional_accuracy: 0.4950\n",
      "Epoch 25/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 23.8586 - accuracy: 0.2589 - categorical_accuracy: 0.2589 - mean_directional_accuracy: 0.5005 - val_loss: 23.2620 - val_accuracy: 0.2531 - val_categorical_accuracy: 0.2531 - val_mean_directional_accuracy: 0.4948\n",
      "Epoch 26/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 22.7677 - accuracy: 0.2569 - categorical_accuracy: 0.2569 - mean_directional_accuracy: 0.4998 - val_loss: 22.1934 - val_accuracy: 0.2521 - val_categorical_accuracy: 0.2521 - val_mean_directional_accuracy: 0.4937\n",
      "Epoch 27/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 21.7164 - accuracy: 0.2564 - categorical_accuracy: 0.2564 - mean_directional_accuracy: 0.4993 - val_loss: 21.1640 - val_accuracy: 0.2518 - val_categorical_accuracy: 0.2518 - val_mean_directional_accuracy: 0.4939\n",
      "Epoch 28/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 20.7042 - accuracy: 0.2564 - categorical_accuracy: 0.2564 - mean_directional_accuracy: 0.4989 - val_loss: 20.1734 - val_accuracy: 0.2530 - val_categorical_accuracy: 0.2530 - val_mean_directional_accuracy: 0.4943\n",
      "Epoch 29/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 19.7299 - accuracy: 0.2543 - categorical_accuracy: 0.2543 - mean_directional_accuracy: 0.4963 - val_loss: 19.2198 - val_accuracy: 0.2540 - val_categorical_accuracy: 0.2540 - val_mean_directional_accuracy: 0.4943\n",
      "Epoch 30/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 18.7934 - accuracy: 0.2559 - categorical_accuracy: 0.2559 - mean_directional_accuracy: 0.4986 - val_loss: 18.3043 - val_accuracy: 0.2540 - val_categorical_accuracy: 0.2540 - val_mean_directional_accuracy: 0.4943\n",
      "Epoch 31/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 17.8935 - accuracy: 0.2541 - categorical_accuracy: 0.2541 - mean_directional_accuracy: 0.4967 - val_loss: 17.4245 - val_accuracy: 0.2535 - val_categorical_accuracy: 0.2535 - val_mean_directional_accuracy: 0.4924\n",
      "Epoch 32/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 17.0295 - accuracy: 0.2538 - categorical_accuracy: 0.2538 - mean_directional_accuracy: 0.4951 - val_loss: 16.5799 - val_accuracy: 0.2522 - val_categorical_accuracy: 0.2522 - val_mean_directional_accuracy: 0.4913\n",
      "Epoch 33/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 16.2003 - accuracy: 0.2532 - categorical_accuracy: 0.2532 - mean_directional_accuracy: 0.4963 - val_loss: 15.7697 - val_accuracy: 0.2510 - val_categorical_accuracy: 0.2510 - val_mean_directional_accuracy: 0.4915\n",
      "Epoch 34/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 15.4050 - accuracy: 0.2536 - categorical_accuracy: 0.2536 - mean_directional_accuracy: 0.4946 - val_loss: 14.9928 - val_accuracy: 0.2520 - val_categorical_accuracy: 0.2520 - val_mean_directional_accuracy: 0.4933\n",
      "Epoch 35/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 14.6436 - accuracy: 0.2547 - categorical_accuracy: 0.2547 - mean_directional_accuracy: 0.4937 - val_loss: 14.2501 - val_accuracy: 0.2505 - val_categorical_accuracy: 0.2505 - val_mean_directional_accuracy: 0.4906\n",
      "Epoch 36/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 13.9149 - accuracy: 0.2561 - categorical_accuracy: 0.2561 - mean_directional_accuracy: 0.4923 - val_loss: 13.5395 - val_accuracy: 0.2510 - val_categorical_accuracy: 0.2510 - val_mean_directional_accuracy: 0.4918\n",
      "Epoch 37/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 13.2182 - accuracy: 0.2552 - categorical_accuracy: 0.2552 - mean_directional_accuracy: 0.4902 - val_loss: 12.8599 - val_accuracy: 0.2491 - val_categorical_accuracy: 0.2491 - val_mean_directional_accuracy: 0.4906\n",
      "Epoch 38/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 12.5524 - accuracy: 0.2559 - categorical_accuracy: 0.2559 - mean_directional_accuracy: 0.4917 - val_loss: 12.2114 - val_accuracy: 0.2474 - val_categorical_accuracy: 0.2474 - val_mean_directional_accuracy: 0.4888\n",
      "Epoch 39/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 11.9172 - accuracy: 0.2545 - categorical_accuracy: 0.2545 - mean_directional_accuracy: 0.4919 - val_loss: 11.5929 - val_accuracy: 0.2465 - val_categorical_accuracy: 0.2465 - val_mean_directional_accuracy: 0.4875\n",
      "Epoch 40/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 11.3115 - accuracy: 0.2550 - categorical_accuracy: 0.2550 - mean_directional_accuracy: 0.4891 - val_loss: 11.0021 - val_accuracy: 0.2471 - val_categorical_accuracy: 0.2471 - val_mean_directional_accuracy: 0.4884\n",
      "Epoch 41/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 10.7342 - accuracy: 0.2562 - categorical_accuracy: 0.2562 - mean_directional_accuracy: 0.4909 - val_loss: 10.4403 - val_accuracy: 0.2473 - val_categorical_accuracy: 0.2473 - val_mean_directional_accuracy: 0.4892\n",
      "Epoch 42/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 10.1852 - accuracy: 0.2557 - categorical_accuracy: 0.2557 - mean_directional_accuracy: 0.4912 - val_loss: 9.9064 - val_accuracy: 0.2454 - val_categorical_accuracy: 0.2454 - val_mean_directional_accuracy: 0.4883\n",
      "Epoch 43/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 9.6636 - accuracy: 0.2547 - categorical_accuracy: 0.2547 - mean_directional_accuracy: 0.4900 - val_loss: 9.3995 - val_accuracy: 0.2461 - val_categorical_accuracy: 0.2461 - val_mean_directional_accuracy: 0.4881\n",
      "Epoch 44/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 9.1688 - accuracy: 0.2562 - categorical_accuracy: 0.2562 - mean_directional_accuracy: 0.4940 - val_loss: 8.9190 - val_accuracy: 0.2456 - val_categorical_accuracy: 0.2456 - val_mean_directional_accuracy: 0.4877\n",
      "Epoch 45/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 8.7001 - accuracy: 0.2554 - categorical_accuracy: 0.2554 - mean_directional_accuracy: 0.4967 - val_loss: 8.4643 - val_accuracy: 0.2462 - val_categorical_accuracy: 0.2462 - val_mean_directional_accuracy: 0.4872\n",
      "Epoch 46/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 8.2569 - accuracy: 0.2554 - categorical_accuracy: 0.2554 - mean_directional_accuracy: 0.4963 - val_loss: 8.0341 - val_accuracy: 0.2457 - val_categorical_accuracy: 0.2457 - val_mean_directional_accuracy: 0.4870\n",
      "Epoch 47/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 7.8379 - accuracy: 0.2550 - categorical_accuracy: 0.2550 - mean_directional_accuracy: 0.4970 - val_loss: 7.6282 - val_accuracy: 0.2445 - val_categorical_accuracy: 0.2445 - val_mean_directional_accuracy: 0.4840\n",
      "Epoch 48/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 7.4426 - accuracy: 0.2573 - categorical_accuracy: 0.2573 - mean_directional_accuracy: 0.4979 - val_loss: 7.2453 - val_accuracy: 0.2431 - val_categorical_accuracy: 0.2431 - val_mean_directional_accuracy: 0.4829\n",
      "Epoch 49/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 7.0702 - accuracy: 0.2620 - categorical_accuracy: 0.2620 - mean_directional_accuracy: 0.5000 - val_loss: 6.8848 - val_accuracy: 0.2419 - val_categorical_accuracy: 0.2419 - val_mean_directional_accuracy: 0.4823\n",
      "Epoch 50/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 6.7198 - accuracy: 0.2622 - categorical_accuracy: 0.2622 - mean_directional_accuracy: 0.5018 - val_loss: 6.5464 - val_accuracy: 0.2414 - val_categorical_accuracy: 0.2414 - val_mean_directional_accuracy: 0.4819\n",
      "Epoch 51/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 6.3905 - accuracy: 0.2604 - categorical_accuracy: 0.2604 - mean_directional_accuracy: 0.4993 - val_loss: 6.2275 - val_accuracy: 0.2416 - val_categorical_accuracy: 0.2416 - val_mean_directional_accuracy: 0.4843\n",
      "Epoch 52/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 6.0816 - accuracy: 0.2597 - categorical_accuracy: 0.2597 - mean_directional_accuracy: 0.4996 - val_loss: 5.9289 - val_accuracy: 0.2407 - val_categorical_accuracy: 0.2407 - val_mean_directional_accuracy: 0.4846\n",
      "Epoch 53/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 5.7920 - accuracy: 0.2594 - categorical_accuracy: 0.2594 - mean_directional_accuracy: 0.4996 - val_loss: 5.6491 - val_accuracy: 0.2390 - val_categorical_accuracy: 0.2390 - val_mean_directional_accuracy: 0.4837\n",
      "Epoch 54/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 5.5211 - accuracy: 0.2626 - categorical_accuracy: 0.2626 - mean_directional_accuracy: 0.5000 - val_loss: 5.3884 - val_accuracy: 0.2398 - val_categorical_accuracy: 0.2398 - val_mean_directional_accuracy: 0.4860\n",
      "Epoch 55/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 5.2682 - accuracy: 0.2633 - categorical_accuracy: 0.2633 - mean_directional_accuracy: 0.5026 - val_loss: 5.1457 - val_accuracy: 0.2354 - val_categorical_accuracy: 0.2354 - val_mean_directional_accuracy: 0.4776\n",
      "Epoch 56/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 5.0327 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5067 - val_loss: 4.9181 - val_accuracy: 0.2347 - val_categorical_accuracy: 0.2347 - val_mean_directional_accuracy: 0.4799\n",
      "Epoch 57/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 4.8132 - accuracy: 0.2689 - categorical_accuracy: 0.2689 - mean_directional_accuracy: 0.5051 - val_loss: 4.7066 - val_accuracy: 0.2346 - val_categorical_accuracy: 0.2346 - val_mean_directional_accuracy: 0.4791\n",
      "Epoch 58/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 4.6094 - accuracy: 0.2668 - categorical_accuracy: 0.2668 - mean_directional_accuracy: 0.5068 - val_loss: 4.5104 - val_accuracy: 0.2340 - val_categorical_accuracy: 0.2340 - val_mean_directional_accuracy: 0.4768\n",
      "Epoch 59/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 4.4206 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5123 - val_loss: 4.3296 - val_accuracy: 0.2303 - val_categorical_accuracy: 0.2303 - val_mean_directional_accuracy: 0.4708\n",
      "Epoch 60/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 4.2463 - accuracy: 0.2669 - categorical_accuracy: 0.2669 - mean_directional_accuracy: 0.5112 - val_loss: 4.1623 - val_accuracy: 0.2315 - val_categorical_accuracy: 0.2315 - val_mean_directional_accuracy: 0.4683\n",
      "Epoch 61/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 4.0852 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5118 - val_loss: 4.0088 - val_accuracy: 0.2294 - val_categorical_accuracy: 0.2294 - val_mean_directional_accuracy: 0.4622\n",
      "Epoch 62/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 3.9370 - accuracy: 0.2694 - categorical_accuracy: 0.2694 - mean_directional_accuracy: 0.5133 - val_loss: 3.8656 - val_accuracy: 0.2350 - val_categorical_accuracy: 0.2350 - val_mean_directional_accuracy: 0.4725\n",
      "Epoch 63/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 3.8002 - accuracy: 0.2691 - categorical_accuracy: 0.2691 - mean_directional_accuracy: 0.5167 - val_loss: 3.7349 - val_accuracy: 0.2326 - val_categorical_accuracy: 0.2326 - val_mean_directional_accuracy: 0.4635\n",
      "Epoch 64/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 3.6747 - accuracy: 0.2712 - categorical_accuracy: 0.2712 - mean_directional_accuracy: 0.5200 - val_loss: 3.6148 - val_accuracy: 0.2332 - val_categorical_accuracy: 0.2332 - val_mean_directional_accuracy: 0.4641\n",
      "Epoch 65/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.5593 - accuracy: 0.2750 - categorical_accuracy: 0.2750 - mean_directional_accuracy: 0.5211 - val_loss: 3.5051 - val_accuracy: 0.2351 - val_categorical_accuracy: 0.2351 - val_mean_directional_accuracy: 0.4663\n",
      "Epoch 66/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.4538 - accuracy: 0.2752 - categorical_accuracy: 0.2752 - mean_directional_accuracy: 0.5213 - val_loss: 3.4043 - val_accuracy: 0.2340 - val_categorical_accuracy: 0.2340 - val_mean_directional_accuracy: 0.4676\n",
      "Epoch 67/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 3.3574 - accuracy: 0.2803 - categorical_accuracy: 0.2803 - mean_directional_accuracy: 0.5248 - val_loss: 3.3124 - val_accuracy: 0.2324 - val_categorical_accuracy: 0.2324 - val_mean_directional_accuracy: 0.4658\n",
      "Epoch 68/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.2695 - accuracy: 0.2780 - categorical_accuracy: 0.2780 - mean_directional_accuracy: 0.5227 - val_loss: 3.2294 - val_accuracy: 0.2341 - val_categorical_accuracy: 0.2341 - val_mean_directional_accuracy: 0.4667\n",
      "Epoch 69/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 3.1896 - accuracy: 0.2792 - categorical_accuracy: 0.2792 - mean_directional_accuracy: 0.5255 - val_loss: 3.1532 - val_accuracy: 0.2328 - val_categorical_accuracy: 0.2328 - val_mean_directional_accuracy: 0.4631\n",
      "Epoch 70/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 3.1170 - accuracy: 0.2750 - categorical_accuracy: 0.2750 - mean_directional_accuracy: 0.5202 - val_loss: 3.0845 - val_accuracy: 0.2325 - val_categorical_accuracy: 0.2325 - val_mean_directional_accuracy: 0.4623\n",
      "Epoch 71/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 3.0510 - accuracy: 0.2759 - categorical_accuracy: 0.2759 - mean_directional_accuracy: 0.5205 - val_loss: 3.0220 - val_accuracy: 0.2333 - val_categorical_accuracy: 0.2333 - val_mean_directional_accuracy: 0.4645\n",
      "Epoch 72/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 2.9911 - accuracy: 0.2754 - categorical_accuracy: 0.2754 - mean_directional_accuracy: 0.5205 - val_loss: 2.9650 - val_accuracy: 0.2351 - val_categorical_accuracy: 0.2351 - val_mean_directional_accuracy: 0.4671\n",
      "Epoch 73/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 2.9361 - accuracy: 0.2761 - categorical_accuracy: 0.2761 - mean_directional_accuracy: 0.5218 - val_loss: 2.9123 - val_accuracy: 0.2330 - val_categorical_accuracy: 0.2330 - val_mean_directional_accuracy: 0.4635\n",
      "Epoch 74/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2.8856 - accuracy: 0.2734 - categorical_accuracy: 0.2734 - mean_directional_accuracy: 0.5191 - val_loss: 2.8637 - val_accuracy: 0.2315 - val_categorical_accuracy: 0.2315 - val_mean_directional_accuracy: 0.4616\n",
      "Epoch 75/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 2.8388 - accuracy: 0.2740 - categorical_accuracy: 0.2740 - mean_directional_accuracy: 0.5184 - val_loss: 2.8187 - val_accuracy: 0.2324 - val_categorical_accuracy: 0.2324 - val_mean_directional_accuracy: 0.4627\n",
      "Epoch 76/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.7955 - accuracy: 0.2719 - categorical_accuracy: 0.2719 - mean_directional_accuracy: 0.5163 - val_loss: 2.7769 - val_accuracy: 0.2315 - val_categorical_accuracy: 0.2315 - val_mean_directional_accuracy: 0.4611\n",
      "Epoch 77/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.7548 - accuracy: 0.2724 - categorical_accuracy: 0.2724 - mean_directional_accuracy: 0.5170 - val_loss: 2.7378 - val_accuracy: 0.2315 - val_categorical_accuracy: 0.2315 - val_mean_directional_accuracy: 0.4611\n",
      "Epoch 78/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 2.7156 - accuracy: 0.2733 - categorical_accuracy: 0.2733 - mean_directional_accuracy: 0.5183 - val_loss: 2.6987 - val_accuracy: 0.2299 - val_categorical_accuracy: 0.2299 - val_mean_directional_accuracy: 0.4592\n",
      "Epoch 79/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.6771 - accuracy: 0.2710 - categorical_accuracy: 0.2710 - mean_directional_accuracy: 0.5155 - val_loss: 2.6606 - val_accuracy: 0.2300 - val_categorical_accuracy: 0.2300 - val_mean_directional_accuracy: 0.4597\n",
      "Epoch 80/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.6393 - accuracy: 0.2719 - categorical_accuracy: 0.2719 - mean_directional_accuracy: 0.5155 - val_loss: 2.6228 - val_accuracy: 0.2303 - val_categorical_accuracy: 0.2303 - val_mean_directional_accuracy: 0.4590\n",
      "Epoch 81/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.6023 - accuracy: 0.2701 - categorical_accuracy: 0.2701 - mean_directional_accuracy: 0.5142 - val_loss: 2.5865 - val_accuracy: 0.2326 - val_categorical_accuracy: 0.2326 - val_mean_directional_accuracy: 0.4627\n",
      "Epoch 82/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2.5659 - accuracy: 0.2733 - categorical_accuracy: 0.2733 - mean_directional_accuracy: 0.5162 - val_loss: 2.5503 - val_accuracy: 0.2311 - val_categorical_accuracy: 0.2311 - val_mean_directional_accuracy: 0.4607\n",
      "Epoch 83/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 2.5302 - accuracy: 0.2712 - categorical_accuracy: 0.2712 - mean_directional_accuracy: 0.5156 - val_loss: 2.5152 - val_accuracy: 0.2312 - val_categorical_accuracy: 0.2312 - val_mean_directional_accuracy: 0.4609\n",
      "Epoch 84/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.4954 - accuracy: 0.2698 - categorical_accuracy: 0.2698 - mean_directional_accuracy: 0.5139 - val_loss: 2.4805 - val_accuracy: 0.2299 - val_categorical_accuracy: 0.2299 - val_mean_directional_accuracy: 0.4592\n",
      "Epoch 85/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2.4611 - accuracy: 0.2708 - categorical_accuracy: 0.2708 - mean_directional_accuracy: 0.5151 - val_loss: 2.4471 - val_accuracy: 0.2338 - val_categorical_accuracy: 0.2338 - val_mean_directional_accuracy: 0.4648\n",
      "Epoch 86/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2.4276 - accuracy: 0.2708 - categorical_accuracy: 0.2708 - mean_directional_accuracy: 0.5151 - val_loss: 2.4138 - val_accuracy: 0.2311 - val_categorical_accuracy: 0.2311 - val_mean_directional_accuracy: 0.4611\n",
      "Epoch 87/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 2.3948 - accuracy: 0.2705 - categorical_accuracy: 0.2705 - mean_directional_accuracy: 0.5153 - val_loss: 2.3811 - val_accuracy: 0.2323 - val_categorical_accuracy: 0.2323 - val_mean_directional_accuracy: 0.4618\n",
      "Epoch 88/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 2.3627 - accuracy: 0.2694 - categorical_accuracy: 0.2694 - mean_directional_accuracy: 0.5133 - val_loss: 2.3489 - val_accuracy: 0.2289 - val_categorical_accuracy: 0.2289 - val_mean_directional_accuracy: 0.4581\n",
      "Epoch 89/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2.3311 - accuracy: 0.2694 - categorical_accuracy: 0.2694 - mean_directional_accuracy: 0.5126 - val_loss: 2.3180 - val_accuracy: 0.2302 - val_categorical_accuracy: 0.2302 - val_mean_directional_accuracy: 0.4597\n",
      "Epoch 90/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 2.3004 - accuracy: 0.2684 - categorical_accuracy: 0.2684 - mean_directional_accuracy: 0.5109 - val_loss: 2.2876 - val_accuracy: 0.2302 - val_categorical_accuracy: 0.2302 - val_mean_directional_accuracy: 0.4592\n",
      "Epoch 91/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.2703 - accuracy: 0.2694 - categorical_accuracy: 0.2694 - mean_directional_accuracy: 0.5128 - val_loss: 2.2578 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 92/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.2409 - accuracy: 0.2682 - categorical_accuracy: 0.2682 - mean_directional_accuracy: 0.5118 - val_loss: 2.2287 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 93/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.2121 - accuracy: 0.2680 - categorical_accuracy: 0.2680 - mean_directional_accuracy: 0.5111 - val_loss: 2.2001 - val_accuracy: 0.2290 - val_categorical_accuracy: 0.2290 - val_mean_directional_accuracy: 0.4582\n",
      "Epoch 94/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.1839 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5112 - val_loss: 2.1721 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 95/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 2.1564 - accuracy: 0.2680 - categorical_accuracy: 0.2680 - mean_directional_accuracy: 0.5111 - val_loss: 2.1451 - val_accuracy: 0.2295 - val_categorical_accuracy: 0.2295 - val_mean_directional_accuracy: 0.4590\n",
      "Epoch 96/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2.1295 - accuracy: 0.2680 - categorical_accuracy: 0.2680 - mean_directional_accuracy: 0.5112 - val_loss: 2.1183 - val_accuracy: 0.2291 - val_categorical_accuracy: 0.2291 - val_mean_directional_accuracy: 0.4581\n",
      "Epoch 97/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 2.1031 - accuracy: 0.2685 - categorical_accuracy: 0.2685 - mean_directional_accuracy: 0.5118 - val_loss: 2.0926 - val_accuracy: 0.2293 - val_categorical_accuracy: 0.2293 - val_mean_directional_accuracy: 0.4585\n",
      "Epoch 98/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.0775 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5104 - val_loss: 2.0671 - val_accuracy: 0.2293 - val_categorical_accuracy: 0.2293 - val_mean_directional_accuracy: 0.4585\n",
      "Epoch 99/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.0525 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5105 - val_loss: 2.0427 - val_accuracy: 0.2286 - val_categorical_accuracy: 0.2286 - val_mean_directional_accuracy: 0.4579\n",
      "Epoch 100/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2.0280 - accuracy: 0.2684 - categorical_accuracy: 0.2684 - mean_directional_accuracy: 0.5116 - val_loss: 2.0188 - val_accuracy: 0.2293 - val_categorical_accuracy: 0.2293 - val_mean_directional_accuracy: 0.4588\n",
      "Epoch 101/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 2.0042 - accuracy: 0.2685 - categorical_accuracy: 0.2685 - mean_directional_accuracy: 0.5121 - val_loss: 1.9950 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 102/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.9810 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.9722 - val_accuracy: 0.2291 - val_categorical_accuracy: 0.2291 - val_mean_directional_accuracy: 0.4588\n",
      "Epoch 103/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.9583 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5104 - val_loss: 1.9494 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 104/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.9361 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5107 - val_loss: 1.9276 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 105/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.9146 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5104 - val_loss: 1.9061 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 106/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.8935 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8855 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 107/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.8731 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8651 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 108/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.8531 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8457 - val_accuracy: 0.2289 - val_categorical_accuracy: 0.2289 - val_mean_directional_accuracy: 0.4586\n",
      "Epoch 109/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.8337 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8264 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 110/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.8148 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.8078 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 111/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.7964 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7899 - val_accuracy: 0.2289 - val_categorical_accuracy: 0.2289 - val_mean_directional_accuracy: 0.4586\n",
      "Epoch 112/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.7785 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7723 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 113/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7612 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7551 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 114/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7443 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7385 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 115/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.7280 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7220 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 116/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.7121 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7066 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 117/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.6968 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6917 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 118/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6820 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6769 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 119/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.6676 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6630 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 120/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.6538 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6493 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 121/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.6405 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6362 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 122/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6276 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6237 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 123/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6152 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6115 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 124/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.6033 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5999 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 125/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5920 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5888 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 126/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5810 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5779 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 127/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.5705 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5676 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 128/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5604 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5578 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 129/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5507 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5484 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 130/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.5415 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5395 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 131/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5327 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5307 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 132/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.5242 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5224 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 133/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.5160 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5146 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 134/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.5082 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5068 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 135/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5007 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4995 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 136/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4936 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4926 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 137/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4868 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4859 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 138/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4803 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4796 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 139/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4741 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4735 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 140/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.4682 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4677 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 141/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4625 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4622 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 142/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4572 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4570 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 143/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.4521 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4521 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 144/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4473 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4474 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 145/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4427 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4430 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 146/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4384 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4387 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 147/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4343 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4348 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 148/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4304 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4311 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 149/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.4268 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4275 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 150/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4233 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4242 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 151/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.4201 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4211 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 152/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.4170 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4181 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 153/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4142 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4153 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 154/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4115 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4128 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 155/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.4090 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4103 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 156/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.4067 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4081 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 157/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4045 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4061 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 158/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4026 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4042 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 159/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4008 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4025 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 160/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.3992 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4010 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 161/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3977 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3996 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 162/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3964 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3983 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 163/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3952 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3972 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 164/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.3941 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3962 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 165/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3931 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3952 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 166/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.3922 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3943 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 167/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3913 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3936 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 168/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.3906 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3929 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 169/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3900 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3923 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 170/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.3894 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3918 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 171/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3890 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3914 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 172/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3886 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3911 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 173/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3883 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3909 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 174/300\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 1.3881 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3907 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 175/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3880 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3907 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 176/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3879 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3906 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 177/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3879 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3906 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 178/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3879 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3906 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 179/300\n",
      "14/23 [=================>............] - ETA: 0s - loss: 1.3870 - accuracy: 0.2776 - categorical_accuracy: 0.2776 - mean_directional_accuracy: 0.5179Restoring model weights from the end of the best epoch: 178.\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3879 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3906 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 179: early stopping\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.3906 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3905924558639526, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 178 \n",
      "\n",
      "Model time: 0.9102186374366283 minutes\n",
      "\n",
      "Total time: 48.82484348863363 minutes\n",
      "\n",
      "\n",
      "Model  98  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                 [256]\n",
      "Activation function           tanh\n",
      "Dropout                        0.4\n",
      "L1                             0.0\n",
      "L2                            10.0\n",
      "Batch size                      32\n",
      "Optimizer                     Adam\n",
      "Learning rate                0.001\n",
      "Name: 86, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 4s 13ms/step - loss: 414.2272 - accuracy: 0.2601 - categorical_accuracy: 0.2601 - mean_directional_accuracy: 0.5033 - val_loss: 4.8946 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8126 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3878 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "176/178 [============================>.] - ETA: 0s - loss: 1.3854 - accuracy: 0.2676 - categorical_accuracy: 0.2676 - mean_directional_accuracy: 0.5110Restoring model weights from the end of the best epoch: 2.\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.3854 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3878 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.3878 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3878382444381714, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 2 \n",
      "\n",
      "Model time: 0.14148743450641632 minutes\n",
      "\n",
      "Total time: 48.96639757975936 minutes\n",
      "\n",
      "\n",
      "Model  99  out of  111\n",
      "Model type                FeedForward\n",
      "Hidden layers                       4\n",
      "Hidden units           [64, 2, 1, 16]\n",
      "Activation function            linear\n",
      "Dropout                           0.6\n",
      "L1                                1.0\n",
      "L2                             0.0001\n",
      "Batch size                         64\n",
      "Optimizer                     RMSprop\n",
      "Learning rate                   0.001\n",
      "Name: 87, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 5s 19ms/step - loss: 454.4576 - accuracy: 0.2568 - categorical_accuracy: 0.2568 - mean_directional_accuracy: 0.5040 - val_loss: 162.5342 - val_accuracy: 0.2748 - val_categorical_accuracy: 0.2748 - val_mean_directional_accuracy: 0.5251\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 57.8175 - accuracy: 0.2601 - categorical_accuracy: 0.2601 - mean_directional_accuracy: 0.5107 - val_loss: 22.0230 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 17.7510 - accuracy: 0.2622 - categorical_accuracy: 0.2622 - mean_directional_accuracy: 0.5061 - val_loss: 14.0798 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 12.0673 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5114 - val_loss: 10.4170 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 9.0859 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 7.9887 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 7.5071 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 7.2166 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 7.0669 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 6.9530 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - ETA: 0s - loss: 6.9722 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105Restoring model weights from the end of the best epoch: 7.\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 6.9722 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 6.9976 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8: early stopping\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 6.9530 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 6.9530158042907715, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 7 \n",
      "\n",
      "Model time: 0.24616596102714539 minutes\n",
      "\n",
      "Total time: 49.21264688298106 minutes\n",
      "\n",
      "\n",
      "Model  100  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    3\n",
      "Hidden units           [1, 16, 64]\n",
      "Activation function           tanh\n",
      "Dropout                        0.5\n",
      "L1                          0.0001\n",
      "L2                         0.00001\n",
      "Batch size                      32\n",
      "Optimizer                  RMSprop\n",
      "Learning rate                0.001\n",
      "Name: 88, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 6s 13ms/step - loss: 1.4264 - accuracy: 0.2503 - categorical_accuracy: 0.2503 - mean_directional_accuracy: 0.4986 - val_loss: 1.4066 - val_accuracy: 0.2694 - val_categorical_accuracy: 0.2694 - val_mean_directional_accuracy: 0.5162\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.4122 - accuracy: 0.2597 - categorical_accuracy: 0.2597 - mean_directional_accuracy: 0.5037 - val_loss: 1.4041 - val_accuracy: 0.2792 - val_categorical_accuracy: 0.2792 - val_mean_directional_accuracy: 0.5326\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.4090 - accuracy: 0.2548 - categorical_accuracy: 0.2548 - mean_directional_accuracy: 0.4965 - val_loss: 1.4026 - val_accuracy: 0.2529 - val_categorical_accuracy: 0.2529 - val_mean_directional_accuracy: 0.4973\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.4036 - accuracy: 0.2615 - categorical_accuracy: 0.2615 - mean_directional_accuracy: 0.5012 - val_loss: 1.3989 - val_accuracy: 0.2410 - val_categorical_accuracy: 0.2410 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3988 - accuracy: 0.2778 - categorical_accuracy: 0.2778 - mean_directional_accuracy: 0.4977 - val_loss: 1.3981 - val_accuracy: 0.2441 - val_categorical_accuracy: 0.2441 - val_mean_directional_accuracy: 0.4572\n",
      "Epoch 6/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3919 - accuracy: 0.2836 - categorical_accuracy: 0.2836 - mean_directional_accuracy: 0.5097 - val_loss: 1.3966 - val_accuracy: 0.2466 - val_categorical_accuracy: 0.2466 - val_mean_directional_accuracy: 0.4609\n",
      "Epoch 7/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.3933 - accuracy: 0.2736 - categorical_accuracy: 0.2736 - mean_directional_accuracy: 0.5004 - val_loss: 1.3932 - val_accuracy: 0.2617 - val_categorical_accuracy: 0.2617 - val_mean_directional_accuracy: 0.4739\n",
      "Epoch 8/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.3909 - accuracy: 0.2792 - categorical_accuracy: 0.2792 - mean_directional_accuracy: 0.5107 - val_loss: 1.3910 - val_accuracy: 0.2642 - val_categorical_accuracy: 0.2642 - val_mean_directional_accuracy: 0.4727\n",
      "Epoch 9/300\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3865 - accuracy: 0.2759 - categorical_accuracy: 0.2759 - mean_directional_accuracy: 0.4981 - val_loss: 1.3891 - val_accuracy: 0.2668 - val_categorical_accuracy: 0.2668 - val_mean_directional_accuracy: 0.4774\n",
      "Epoch 10/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.3871 - accuracy: 0.2864 - categorical_accuracy: 0.2864 - mean_directional_accuracy: 0.5135 - val_loss: 1.3862 - val_accuracy: 0.2843 - val_categorical_accuracy: 0.2843 - val_mean_directional_accuracy: 0.5014\n",
      "Epoch 11/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.3869 - accuracy: 0.2799 - categorical_accuracy: 0.2799 - mean_directional_accuracy: 0.5035 - val_loss: 1.3854 - val_accuracy: 0.2782 - val_categorical_accuracy: 0.2782 - val_mean_directional_accuracy: 0.4881\n",
      "Epoch 12/300\n",
      "162/178 [==========================>...] - ETA: 0s - loss: 1.3854 - accuracy: 0.2924 - categorical_accuracy: 0.2924 - mean_directional_accuracy: 0.5149Restoring model weights from the end of the best epoch: 11.\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 1.3861 - accuracy: 0.2900 - categorical_accuracy: 0.2900 - mean_directional_accuracy: 0.5125 - val_loss: 1.3873 - val_accuracy: 0.2696 - val_categorical_accuracy: 0.2696 - val_mean_directional_accuracy: 0.4774\n",
      "Epoch 12: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.3854 - accuracy: 0.2782 - categorical_accuracy: 0.2782 - mean_directional_accuracy: 0.4881\n",
      "{'loss': 1.3854068517684937, 'accuracy': 0.2781837284564972, 'categorical_accuracy': 0.2781837284564972, 'mean_directional_accuracy': 0.48812630772590637} \n",
      " 11 \n",
      "\n",
      "Model time: 0.3670360669493675 minutes\n",
      "\n",
      "Total time: 49.579788234084845 minutes\n",
      "\n",
      "\n",
      "Model  101  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                   [2]\n",
      "Activation function           tanh\n",
      "Dropout                        0.3\n",
      "L1                           100.0\n",
      "L2                         0.00001\n",
      "Batch size                     128\n",
      "Optimizer                     Adam\n",
      "Learning rate                0.001\n",
      "Name: 89, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 3s 27ms/step - loss: 2957.0330 - accuracy: 0.2538 - categorical_accuracy: 0.2538 - mean_directional_accuracy: 0.4923 - val_loss: 2283.9500 - val_accuracy: 0.2623 - val_categorical_accuracy: 0.2623 - val_mean_directional_accuracy: 0.4916\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 1786.4731 - accuracy: 0.2471 - categorical_accuracy: 0.2471 - mean_directional_accuracy: 0.4923 - val_loss: 1315.9226 - val_accuracy: 0.2581 - val_categorical_accuracy: 0.2581 - val_mean_directional_accuracy: 0.4911\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 988.9228 - accuracy: 0.2531 - categorical_accuracy: 0.2531 - mean_directional_accuracy: 0.5097 - val_loss: 701.2511 - val_accuracy: 0.2458 - val_categorical_accuracy: 0.2458 - val_mean_directional_accuracy: 0.4853\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 545.8109 - accuracy: 0.2575 - categorical_accuracy: 0.2575 - mean_directional_accuracy: 0.5063 - val_loss: 427.8662 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 402.5201 - accuracy: 0.2657 - categorical_accuracy: 0.2657 - mean_directional_accuracy: 0.5091 - val_loss: 380.2827 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 362.3818 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 343.4481 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 326.0748 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 307.4558 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 292.0234 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 275.6310 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 260.3886 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 243.9406 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 229.1447 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 214.1748 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 202.6755 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 190.7401 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 179.8965 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 168.3711 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 157.4344 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 145.6175 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 134.9510 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 123.2971 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 112.4589 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 100.8963 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 89.9623 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 78.2751 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 67.4288 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 55.7545 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 48.2004 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 40.8351 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 34.3756 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 27.3111 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 21.3106 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 15.9662 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 11.6603 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 8.0180 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 6.2972 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.7264 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 5.5590 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.6750 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 5.5381 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.5080 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 5.5344 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.4054 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 5.5346 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.3319 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27/300\n",
      "30/45 [===================>..........] - ETA: 0s - loss: 5.5400 - accuracy: 0.2768 - categorical_accuracy: 0.2768 - mean_directional_accuracy: 0.5198Restoring model weights from the end of the best epoch: 26.\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 5.5453 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5.5128 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27: early stopping\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 5.3319 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 5.331891059875488, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 26 \n",
      "\n",
      "Model time: 0.3838437460362911 minutes\n",
      "\n",
      "Total time: 49.96369882300496 minutes\n",
      "\n",
      "\n",
      "Model  102  out of  111\n",
      "Model type                   FeedForward\n",
      "Hidden layers                          4\n",
      "Hidden units           [2, 256, 256, 64]\n",
      "Activation function                 relu\n",
      "Dropout                              0.4\n",
      "L1                                   0.1\n",
      "L2                                  10.0\n",
      "Batch size                            64\n",
      "Optimizer                        RMSprop\n",
      "Learning rate                       0.01\n",
      "Name: 90, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 6s 25ms/step - loss: 146.1265 - accuracy: 0.2650 - categorical_accuracy: 0.2650 - mean_directional_accuracy: 0.5198 - val_loss: 63.6590 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 63.6552 - accuracy: 0.2641 - categorical_accuracy: 0.2641 - mean_directional_accuracy: 0.5081 - val_loss: 63.6586 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 63.6551 - accuracy: 0.2655 - categorical_accuracy: 0.2655 - mean_directional_accuracy: 0.5033 - val_loss: 63.6564 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "85/89 [===========================>..] - ETA: 0s - loss: 63.6548 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5151Restoring model weights from the end of the best epoch: 3.\n",
      "89/89 [==============================] - 1s 12ms/step - loss: 63.6551 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5126 - val_loss: 63.6582 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4: early stopping\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 63.6564 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 63.65640640258789, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 3 \n",
      "\n",
      "Model time: 0.17477669194340706 minutes\n",
      "\n",
      "Total time: 50.138585574924946 minutes\n",
      "\n",
      "\n",
      "Model  103  out of  111\n",
      "Model type                   FeedForward\n",
      "Hidden layers                          5\n",
      "Hidden units           [64, 8, 64, 4, 2]\n",
      "Activation function              sigmoid\n",
      "Dropout                              0.6\n",
      "L1                                  10.0\n",
      "L2                               0.00001\n",
      "Batch size                            16\n",
      "Optimizer                           Adam\n",
      "Learning rate                       0.01\n",
      "Name: 91, dtype: object\n",
      "NN5Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 7s 11ms/step - loss: 422.7645 - accuracy: 0.2459 - categorical_accuracy: 0.2459 - mean_directional_accuracy: 0.4870 - val_loss: 150.0320 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "342/356 [===========================>..] - ETA: 0s - loss: 155.9015 - accuracy: 0.2562 - categorical_accuracy: 0.2562 - mean_directional_accuracy: 0.4932Restoring model weights from the end of the best epoch: 1.\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 155.9168 - accuracy: 0.2573 - categorical_accuracy: 0.2573 - mean_directional_accuracy: 0.4935 - val_loss: 156.2814 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2: early stopping\n",
      "479/479 [==============================] - 1s 3ms/step - loss: 150.0320 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 150.03195190429688, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 1 \n",
      "\n",
      "Model time: 0.19758928567171097 minutes\n",
      "\n",
      "Total time: 50.33625817671418 minutes\n",
      "\n",
      "\n",
      "Model  104  out of  111\n",
      "Model type                  FeedForward\n",
      "Hidden layers                         4\n",
      "Hidden units           [256, 2, 32, 32]\n",
      "Activation function                relu\n",
      "Dropout                             0.1\n",
      "L1                                100.0\n",
      "L2                                 10.0\n",
      "Batch size                          128\n",
      "Optimizer                       RMSprop\n",
      "Learning rate                     0.001\n",
      "Name: 92, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 5s 43ms/step - loss: 169504.0625 - accuracy: 0.2636 - categorical_accuracy: 0.2636 - mean_directional_accuracy: 0.5005 - val_loss: 93199.0000 - val_accuracy: 0.2306 - val_categorical_accuracy: 0.2306 - val_mean_directional_accuracy: 0.4614\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 51016.2852 - accuracy: 0.2701 - categorical_accuracy: 0.2701 - mean_directional_accuracy: 0.5144 - val_loss: 18976.4551 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 11195.4561 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 8250.3086 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 6896.1807 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 5597.2520 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 4665.6450 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3807.4080 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 3230.8240 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2744.2517 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 2537.0481 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2413.8142 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 2334.1709 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2268.5371 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 2260.7043 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2260.8613 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 1s 13ms/step - loss: 2251.2407 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2241.4973 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "38/45 [========================>.....] - ETA: 0s - loss: 2251.1335 - accuracy: 0.2679 - categorical_accuracy: 0.2679 - mean_directional_accuracy: 0.5105Restoring model weights from the end of the best epoch: 10.\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 2251.0295 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 2260.6958 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11: early stopping\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 2241.4973 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 2241.497314453125, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 10 \n",
      "\n",
      "Model time: 0.19431819766759872 minutes\n",
      "\n",
      "Total time: 50.53068473935127 minutes\n",
      "\n",
      "\n",
      "Model  105  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    2\n",
      "Hidden units              [2, 128]\n",
      "Activation function           tanh\n",
      "Dropout                        0.7\n",
      "L1                            0.01\n",
      "L2                             0.0\n",
      "Batch size                      64\n",
      "Optimizer                     Adam\n",
      "Learning rate               0.0001\n",
      "Name: 93, dtype: object\n",
      "NN2Layer\n",
      "Epoch 1/300\n",
      "89/89 [==============================] - 4s 18ms/step - loss: 2.4982 - accuracy: 0.2532 - categorical_accuracy: 0.2532 - mean_directional_accuracy: 0.4977 - val_loss: 2.4284 - val_accuracy: 0.2508 - val_categorical_accuracy: 0.2508 - val_mean_directional_accuracy: 0.5025\n",
      "Epoch 2/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.4369 - accuracy: 0.2561 - categorical_accuracy: 0.2561 - mean_directional_accuracy: 0.5061 - val_loss: 2.3713 - val_accuracy: 0.2534 - val_categorical_accuracy: 0.2534 - val_mean_directional_accuracy: 0.4922\n",
      "Epoch 3/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 2.3713 - accuracy: 0.2634 - categorical_accuracy: 0.2634 - mean_directional_accuracy: 0.4974 - val_loss: 2.3157 - val_accuracy: 0.2473 - val_categorical_accuracy: 0.2473 - val_mean_directional_accuracy: 0.4798\n",
      "Epoch 4/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 2.3123 - accuracy: 0.2496 - categorical_accuracy: 0.2496 - mean_directional_accuracy: 0.4970 - val_loss: 2.2613 - val_accuracy: 0.2469 - val_categorical_accuracy: 0.2469 - val_mean_directional_accuracy: 0.4776\n",
      "Epoch 5/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 2.2519 - accuracy: 0.2592 - categorical_accuracy: 0.2592 - mean_directional_accuracy: 0.5000 - val_loss: 2.2086 - val_accuracy: 0.2452 - val_categorical_accuracy: 0.2452 - val_mean_directional_accuracy: 0.4751\n",
      "Epoch 6/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 2.1968 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5111 - val_loss: 2.1568 - val_accuracy: 0.2439 - val_categorical_accuracy: 0.2439 - val_mean_directional_accuracy: 0.4763\n",
      "Epoch 7/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 2.1423 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.5016 - val_loss: 2.1067 - val_accuracy: 0.2428 - val_categorical_accuracy: 0.2428 - val_mean_directional_accuracy: 0.4753\n",
      "Epoch 8/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 2.0924 - accuracy: 0.2589 - categorical_accuracy: 0.2589 - mean_directional_accuracy: 0.4989 - val_loss: 2.0576 - val_accuracy: 0.2396 - val_categorical_accuracy: 0.2396 - val_mean_directional_accuracy: 0.4729\n",
      "Epoch 9/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 2.0398 - accuracy: 0.2599 - categorical_accuracy: 0.2599 - mean_directional_accuracy: 0.5077 - val_loss: 2.0103 - val_accuracy: 0.2389 - val_categorical_accuracy: 0.2389 - val_mean_directional_accuracy: 0.4733\n",
      "Epoch 10/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9930 - accuracy: 0.2604 - categorical_accuracy: 0.2604 - mean_directional_accuracy: 0.5044 - val_loss: 1.9652 - val_accuracy: 0.2390 - val_categorical_accuracy: 0.2390 - val_mean_directional_accuracy: 0.4730\n",
      "Epoch 11/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.9488 - accuracy: 0.2590 - categorical_accuracy: 0.2590 - mean_directional_accuracy: 0.5058 - val_loss: 1.9216 - val_accuracy: 0.2370 - val_categorical_accuracy: 0.2370 - val_mean_directional_accuracy: 0.4670\n",
      "Epoch 12/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.9041 - accuracy: 0.2648 - categorical_accuracy: 0.2648 - mean_directional_accuracy: 0.5148 - val_loss: 1.8794 - val_accuracy: 0.2379 - val_categorical_accuracy: 0.2379 - val_mean_directional_accuracy: 0.4678\n",
      "Epoch 13/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.8605 - accuracy: 0.2590 - categorical_accuracy: 0.2590 - mean_directional_accuracy: 0.5086 - val_loss: 1.8390 - val_accuracy: 0.2364 - val_categorical_accuracy: 0.2364 - val_mean_directional_accuracy: 0.4648\n",
      "Epoch 14/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.8200 - accuracy: 0.2629 - categorical_accuracy: 0.2629 - mean_directional_accuracy: 0.5091 - val_loss: 1.8003 - val_accuracy: 0.2349 - val_categorical_accuracy: 0.2349 - val_mean_directional_accuracy: 0.4626\n",
      "Epoch 15/300\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.7815 - accuracy: 0.2647 - categorical_accuracy: 0.2647 - mean_directional_accuracy: 0.5102 - val_loss: 1.7637 - val_accuracy: 0.2336 - val_categorical_accuracy: 0.2336 - val_mean_directional_accuracy: 0.4609\n",
      "Epoch 16/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.7445 - accuracy: 0.2626 - categorical_accuracy: 0.2626 - mean_directional_accuracy: 0.5012 - val_loss: 1.7283 - val_accuracy: 0.2316 - val_categorical_accuracy: 0.2316 - val_mean_directional_accuracy: 0.4598\n",
      "Epoch 17/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.7097 - accuracy: 0.2650 - categorical_accuracy: 0.2650 - mean_directional_accuracy: 0.5112 - val_loss: 1.6951 - val_accuracy: 0.2320 - val_categorical_accuracy: 0.2320 - val_mean_directional_accuracy: 0.4616\n",
      "Epoch 18/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.6772 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5128 - val_loss: 1.6642 - val_accuracy: 0.2312 - val_categorical_accuracy: 0.2312 - val_mean_directional_accuracy: 0.4611\n",
      "Epoch 19/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6481 - accuracy: 0.2615 - categorical_accuracy: 0.2615 - mean_directional_accuracy: 0.5093 - val_loss: 1.6352 - val_accuracy: 0.2313 - val_categorical_accuracy: 0.2313 - val_mean_directional_accuracy: 0.4603\n",
      "Epoch 20/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.6198 - accuracy: 0.2645 - categorical_accuracy: 0.2645 - mean_directional_accuracy: 0.5095 - val_loss: 1.6080 - val_accuracy: 0.2313 - val_categorical_accuracy: 0.2313 - val_mean_directional_accuracy: 0.4607\n",
      "Epoch 21/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.5944 - accuracy: 0.2624 - categorical_accuracy: 0.2624 - mean_directional_accuracy: 0.5042 - val_loss: 1.5828 - val_accuracy: 0.2308 - val_categorical_accuracy: 0.2308 - val_mean_directional_accuracy: 0.4610\n",
      "Epoch 22/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.5687 - accuracy: 0.2684 - categorical_accuracy: 0.2684 - mean_directional_accuracy: 0.5133 - val_loss: 1.5598 - val_accuracy: 0.2299 - val_categorical_accuracy: 0.2299 - val_mean_directional_accuracy: 0.4596\n",
      "Epoch 23/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.5473 - accuracy: 0.2634 - categorical_accuracy: 0.2634 - mean_directional_accuracy: 0.5119 - val_loss: 1.5385 - val_accuracy: 0.2299 - val_categorical_accuracy: 0.2299 - val_mean_directional_accuracy: 0.4599\n",
      "Epoch 24/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.5264 - accuracy: 0.2629 - categorical_accuracy: 0.2629 - mean_directional_accuracy: 0.5063 - val_loss: 1.5182 - val_accuracy: 0.2293 - val_categorical_accuracy: 0.2293 - val_mean_directional_accuracy: 0.4596\n",
      "Epoch 25/300\n",
      "89/89 [==============================] - 4s 51ms/step - loss: 1.5069 - accuracy: 0.2661 - categorical_accuracy: 0.2661 - mean_directional_accuracy: 0.5079 - val_loss: 1.4999 - val_accuracy: 0.2294 - val_categorical_accuracy: 0.2294 - val_mean_directional_accuracy: 0.4596\n",
      "Epoch 26/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4891 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5137 - val_loss: 1.4828 - val_accuracy: 0.2286 - val_categorical_accuracy: 0.2286 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.4731 - accuracy: 0.2694 - categorical_accuracy: 0.2694 - mean_directional_accuracy: 0.5126 - val_loss: 1.4676 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 28/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4588 - accuracy: 0.2682 - categorical_accuracy: 0.2682 - mean_directional_accuracy: 0.5091 - val_loss: 1.4543 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 29/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4464 - accuracy: 0.2699 - categorical_accuracy: 0.2699 - mean_directional_accuracy: 0.5125 - val_loss: 1.4424 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 30/300\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 1.4353 - accuracy: 0.2692 - categorical_accuracy: 0.2692 - mean_directional_accuracy: 0.5118 - val_loss: 1.4319 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 31/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4254 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5111 - val_loss: 1.4228 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 32/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4168 - accuracy: 0.2680 - categorical_accuracy: 0.2680 - mean_directional_accuracy: 0.5111 - val_loss: 1.4149 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 33/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.4095 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4082 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.4034 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4028 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3987 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3987 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3949 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3954 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3919 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3927 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 1.3896 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3908 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3878 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3893 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3866 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3884 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3859 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3880 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3857 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3879 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3855 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3878 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.3854 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3878 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.3853 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3877 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "86/89 [===========================>..] - ETA: 0s - loss: 1.3853 - accuracy: 0.2671 - categorical_accuracy: 0.2671 - mean_directional_accuracy: 0.5107Restoring model weights from the end of the best epoch: 45.\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 1.3853 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3877 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46: early stopping\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 1.3877 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3877105712890625, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 45 \n",
      "\n",
      "Model time: 0.7288272194564342 minutes\n",
      "\n",
      "Total time: 51.25961731374264 minutes\n",
      "\n",
      "\n",
      "Model  106  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    1\n",
      "Hidden units                  [32]\n",
      "Activation function           tanh\n",
      "Dropout                        0.4\n",
      "L1                           0.001\n",
      "L2                            0.01\n",
      "Batch size                      16\n",
      "Optimizer                     Adam\n",
      "Learning rate                0.001\n",
      "Name: 94, dtype: object\n",
      "NN1Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 5s 9ms/step - loss: 2.0570 - accuracy: 0.2770 - categorical_accuracy: 0.2770 - mean_directional_accuracy: 0.5037 - val_loss: 1.7388 - val_accuracy: 0.2968 - val_categorical_accuracy: 0.2968 - val_mean_directional_accuracy: 0.5241\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.6550 - accuracy: 0.3181 - categorical_accuracy: 0.3181 - mean_directional_accuracy: 0.5369 - val_loss: 1.5840 - val_accuracy: 0.3031 - val_categorical_accuracy: 0.3031 - val_mean_directional_accuracy: 0.5243\n",
      "Epoch 3/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.5288 - accuracy: 0.3260 - categorical_accuracy: 0.3260 - mean_directional_accuracy: 0.5404 - val_loss: 1.4960 - val_accuracy: 0.3103 - val_categorical_accuracy: 0.3103 - val_mean_directional_accuracy: 0.5414\n",
      "Epoch 4/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4590 - accuracy: 0.3286 - categorical_accuracy: 0.3286 - mean_directional_accuracy: 0.5499 - val_loss: 1.4459 - val_accuracy: 0.3111 - val_categorical_accuracy: 0.3111 - val_mean_directional_accuracy: 0.5386\n",
      "Epoch 5/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 1.4237 - accuracy: 0.3254 - categorical_accuracy: 0.3254 - mean_directional_accuracy: 0.5455 - val_loss: 1.4184 - val_accuracy: 0.3108 - val_categorical_accuracy: 0.3108 - val_mean_directional_accuracy: 0.5465\n",
      "Epoch 6/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.4006 - accuracy: 0.3260 - categorical_accuracy: 0.3260 - mean_directional_accuracy: 0.5407 - val_loss: 1.4024 - val_accuracy: 0.3173 - val_categorical_accuracy: 0.3173 - val_mean_directional_accuracy: 0.5548\n",
      "Epoch 7/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3901 - accuracy: 0.3314 - categorical_accuracy: 0.3314 - mean_directional_accuracy: 0.5474 - val_loss: 1.3988 - val_accuracy: 0.3009 - val_categorical_accuracy: 0.3009 - val_mean_directional_accuracy: 0.5304\n",
      "Epoch 8/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 1.3862 - accuracy: 0.3302 - categorical_accuracy: 0.3302 - mean_directional_accuracy: 0.5585 - val_loss: 1.3940 - val_accuracy: 0.3049 - val_categorical_accuracy: 0.3049 - val_mean_directional_accuracy: 0.5300\n",
      "Epoch 9/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 1.3847 - accuracy: 0.3277 - categorical_accuracy: 0.3277 - mean_directional_accuracy: 0.5483 - val_loss: 1.3932 - val_accuracy: 0.3062 - val_categorical_accuracy: 0.3062 - val_mean_directional_accuracy: 0.5348\n",
      "Epoch 10/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 1.3821 - accuracy: 0.3244 - categorical_accuracy: 0.3244 - mean_directional_accuracy: 0.5458 - val_loss: 1.3908 - val_accuracy: 0.3086 - val_categorical_accuracy: 0.3086 - val_mean_directional_accuracy: 0.5470\n",
      "Epoch 11/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 1.3799 - accuracy: 0.3284 - categorical_accuracy: 0.3284 - mean_directional_accuracy: 0.5534 - val_loss: 1.3901 - val_accuracy: 0.3065 - val_categorical_accuracy: 0.3065 - val_mean_directional_accuracy: 0.5295\n",
      "Epoch 12/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 1.3797 - accuracy: 0.3240 - categorical_accuracy: 0.3240 - mean_directional_accuracy: 0.5411 - val_loss: 1.3854 - val_accuracy: 0.3168 - val_categorical_accuracy: 0.3168 - val_mean_directional_accuracy: 0.5364\n",
      "Epoch 13/300\n",
      "355/356 [============================>.] - ETA: 0s - loss: 1.3779 - accuracy: 0.3308 - categorical_accuracy: 0.3308 - mean_directional_accuracy: 0.5479Restoring model weights from the end of the best epoch: 12.\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 1.3778 - accuracy: 0.3307 - categorical_accuracy: 0.3307 - mean_directional_accuracy: 0.5474 - val_loss: 1.3856 - val_accuracy: 0.3125 - val_categorical_accuracy: 0.3125 - val_mean_directional_accuracy: 0.5339\n",
      "Epoch 13: early stopping\n",
      "479/479 [==============================] - 1s 2ms/step - loss: 1.3854 - accuracy: 0.3168 - categorical_accuracy: 0.3168 - mean_directional_accuracy: 0.5364\n",
      "{'loss': 1.385399341583252, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} \n",
      " 12 \n",
      "\n",
      "Model time: 0.7392472214996815 minutes\n",
      "\n",
      "Total time: 51.998947869986296 minutes\n",
      "\n",
      "\n",
      "Model  107  out of  111\n",
      "Model type                    FeedForward\n",
      "Hidden layers                           5\n",
      "Hidden units           [1, 16, 64, 8, 32]\n",
      "Activation function                  relu\n",
      "Dropout                               0.1\n",
      "L1                                   0.01\n",
      "L2                                  0.001\n",
      "Batch size                             32\n",
      "Optimizer                            Adam\n",
      "Learning rate                       0.001\n",
      "Name: 95, dtype: object\n",
      "NN5Layer\n",
      "Epoch 1/300\n",
      "178/178 [==============================] - 6s 14ms/step - loss: 3.1099 - accuracy: 0.2606 - categorical_accuracy: 0.2606 - mean_directional_accuracy: 0.5026 - val_loss: 1.9688 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.5723 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4109 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.3937 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3913 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.3880 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3907 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.3878 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3906 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "177/178 [============================>.] - ETA: 0s - loss: 1.3878 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5109Restoring model weights from the end of the best epoch: 5.\n",
      "178/178 [==============================] - 1s 8ms/step - loss: 1.3878 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3910 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6: early stopping\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.3906 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3906307220458984, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 5 \n",
      "\n",
      "Model time: 0.2466692440211773 minutes\n",
      "\n",
      "Total time: 52.24566712230444 minutes\n",
      "\n",
      "\n",
      "Model  108  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    3\n",
      "Hidden units           [256, 2, 1]\n",
      "Activation function        sigmoid\n",
      "Dropout                        0.7\n",
      "L1                         0.00001\n",
      "L2                             0.1\n",
      "Batch size                      16\n",
      "Optimizer                  RMSprop\n",
      "Learning rate               0.0001\n",
      "Name: 96, dtype: object\n",
      "NN3Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 7s 11ms/step - loss: 14.8029 - accuracy: 0.2345 - categorical_accuracy: 0.2345 - mean_directional_accuracy: 0.4942 - val_loss: 8.8470 - val_accuracy: 0.2602 - val_categorical_accuracy: 0.2602 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 5.6005 - accuracy: 0.2466 - categorical_accuracy: 0.2466 - mean_directional_accuracy: 0.5176 - val_loss: 3.1660 - val_accuracy: 0.2602 - val_categorical_accuracy: 0.2602 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 3/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 2.3074 - accuracy: 0.2483 - categorical_accuracy: 0.2483 - mean_directional_accuracy: 0.5061 - val_loss: 1.7778 - val_accuracy: 0.2602 - val_categorical_accuracy: 0.2602 - val_mean_directional_accuracy: 0.5416\n",
      "Epoch 4/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.7928 - accuracy: 0.2352 - categorical_accuracy: 0.2352 - mean_directional_accuracy: 0.4937 - val_loss: 1.6982 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.7351 - accuracy: 0.2446 - categorical_accuracy: 0.2446 - mean_directional_accuracy: 0.5072 - val_loss: 1.6637 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.7024 - accuracy: 0.2408 - categorical_accuracy: 0.2408 - mean_directional_accuracy: 0.5032 - val_loss: 1.6376 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 7/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.6752 - accuracy: 0.2348 - categorical_accuracy: 0.2348 - mean_directional_accuracy: 0.4956 - val_loss: 1.6156 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 8/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.6477 - accuracy: 0.2403 - categorical_accuracy: 0.2403 - mean_directional_accuracy: 0.5032 - val_loss: 1.5958 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 9/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.6305 - accuracy: 0.2352 - categorical_accuracy: 0.2352 - mean_directional_accuracy: 0.5018 - val_loss: 1.5772 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.6091 - accuracy: 0.2327 - categorical_accuracy: 0.2327 - mean_directional_accuracy: 0.5004 - val_loss: 1.5601 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.5832 - accuracy: 0.2408 - categorical_accuracy: 0.2408 - mean_directional_accuracy: 0.5102 - val_loss: 1.5445 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.5635 - accuracy: 0.2452 - categorical_accuracy: 0.2452 - mean_directional_accuracy: 0.5061 - val_loss: 1.5302 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.5458 - accuracy: 0.2543 - categorical_accuracy: 0.2543 - mean_directional_accuracy: 0.5098 - val_loss: 1.5168 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.5242 - accuracy: 0.2608 - categorical_accuracy: 0.2608 - mean_directional_accuracy: 0.5102 - val_loss: 1.5044 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.5128 - accuracy: 0.2541 - categorical_accuracy: 0.2541 - mean_directional_accuracy: 0.5105 - val_loss: 1.4929 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.5017 - accuracy: 0.2501 - categorical_accuracy: 0.2501 - mean_directional_accuracy: 0.5105 - val_loss: 1.4827 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.4914 - accuracy: 0.2429 - categorical_accuracy: 0.2429 - mean_directional_accuracy: 0.5105 - val_loss: 1.4732 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.4754 - accuracy: 0.2520 - categorical_accuracy: 0.2520 - mean_directional_accuracy: 0.5105 - val_loss: 1.4642 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.4650 - accuracy: 0.2499 - categorical_accuracy: 0.2499 - mean_directional_accuracy: 0.5105 - val_loss: 1.4564 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.4565 - accuracy: 0.2527 - categorical_accuracy: 0.2527 - mean_directional_accuracy: 0.5105 - val_loss: 1.4493 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.4466 - accuracy: 0.2594 - categorical_accuracy: 0.2594 - mean_directional_accuracy: 0.5105 - val_loss: 1.4428 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.4424 - accuracy: 0.2503 - categorical_accuracy: 0.2503 - mean_directional_accuracy: 0.5105 - val_loss: 1.4367 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.4345 - accuracy: 0.2497 - categorical_accuracy: 0.2497 - mean_directional_accuracy: 0.5105 - val_loss: 1.4311 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.4306 - accuracy: 0.2404 - categorical_accuracy: 0.2404 - mean_directional_accuracy: 0.5105 - val_loss: 1.4260 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 25/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.4235 - accuracy: 0.2485 - categorical_accuracy: 0.2485 - mean_directional_accuracy: 0.5105 - val_loss: 1.4214 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.4175 - accuracy: 0.2582 - categorical_accuracy: 0.2582 - mean_directional_accuracy: 0.5105 - val_loss: 1.4173 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.4131 - accuracy: 0.2504 - categorical_accuracy: 0.2504 - mean_directional_accuracy: 0.5105 - val_loss: 1.4134 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 28/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.4098 - accuracy: 0.2522 - categorical_accuracy: 0.2522 - mean_directional_accuracy: 0.5105 - val_loss: 1.4098 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 29/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.4052 - accuracy: 0.2513 - categorical_accuracy: 0.2513 - mean_directional_accuracy: 0.5105 - val_loss: 1.4066 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 30/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.4028 - accuracy: 0.2452 - categorical_accuracy: 0.2452 - mean_directional_accuracy: 0.5105 - val_loss: 1.4037 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 31/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3995 - accuracy: 0.2441 - categorical_accuracy: 0.2441 - mean_directional_accuracy: 0.5105 - val_loss: 1.4009 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 32/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3965 - accuracy: 0.2545 - categorical_accuracy: 0.2545 - mean_directional_accuracy: 0.5105 - val_loss: 1.3987 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 33/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3946 - accuracy: 0.2501 - categorical_accuracy: 0.2501 - mean_directional_accuracy: 0.5105 - val_loss: 1.3966 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3925 - accuracy: 0.2497 - categorical_accuracy: 0.2497 - mean_directional_accuracy: 0.5105 - val_loss: 1.3948 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3902 - accuracy: 0.2531 - categorical_accuracy: 0.2531 - mean_directional_accuracy: 0.5105 - val_loss: 1.3933 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3891 - accuracy: 0.2506 - categorical_accuracy: 0.2506 - mean_directional_accuracy: 0.5105 - val_loss: 1.3921 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3878 - accuracy: 0.2515 - categorical_accuracy: 0.2515 - mean_directional_accuracy: 0.5105 - val_loss: 1.3911 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3867 - accuracy: 0.2532 - categorical_accuracy: 0.2532 - mean_directional_accuracy: 0.5105 - val_loss: 1.3903 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3867 - accuracy: 0.2459 - categorical_accuracy: 0.2459 - mean_directional_accuracy: 0.5105 - val_loss: 1.3899 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "356/356 [==============================] - 3s 10ms/step - loss: 1.3862 - accuracy: 0.2557 - categorical_accuracy: 0.2557 - mean_directional_accuracy: 0.5105 - val_loss: 1.3896 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3860 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3895 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3855 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3895 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3856 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3894 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3857 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3893 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3857 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3892 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3856 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3891 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3856 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3890 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3854 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3890 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3853 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3889 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3889 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "356/356 [==============================] - 3s 9ms/step - loss: 1.3852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3888 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "348/356 [============================>.] - ETA: 0s - loss: 1.3852 - accuracy: 0.2680 - categorical_accuracy: 0.2680 - mean_directional_accuracy: 0.5104Restoring model weights from the end of the best epoch: 51.\n",
      "356/356 [==============================] - 3s 8ms/step - loss: 1.3852 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3889 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52: early stopping\n",
      "479/479 [==============================] - 2s 3ms/step - loss: 1.3888 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.388848900794983, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 51 \n",
      "\n",
      "Model time: 2.7704021520912647 minutes\n",
      "\n",
      "Total time: 55.01615358144045 minutes\n",
      "\n",
      "\n",
      "Model  109  out of  111\n",
      "Model type                FeedForward\n",
      "Hidden layers                       4\n",
      "Hidden units           [8, 64, 32, 8]\n",
      "Activation function            linear\n",
      "Dropout                           0.3\n",
      "L1                                0.0\n",
      "L2                                0.1\n",
      "Batch size                        256\n",
      "Optimizer                        Adam\n",
      "Learning rate                   0.001\n",
      "Name: 97, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "23/23 [==============================] - 4s 50ms/step - loss: 10.0096 - accuracy: 0.2569 - categorical_accuracy: 0.2569 - mean_directional_accuracy: 0.4979 - val_loss: 8.7029 - val_accuracy: 0.2456 - val_categorical_accuracy: 0.2456 - val_mean_directional_accuracy: 0.4830\n",
      "Epoch 2/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 8.0273 - accuracy: 0.2585 - categorical_accuracy: 0.2585 - mean_directional_accuracy: 0.5037 - val_loss: 7.1294 - val_accuracy: 0.2564 - val_categorical_accuracy: 0.2564 - val_mean_directional_accuracy: 0.5004\n",
      "Epoch 3/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 6.5523 - accuracy: 0.2520 - categorical_accuracy: 0.2520 - mean_directional_accuracy: 0.4995 - val_loss: 5.8723 - val_accuracy: 0.2338 - val_categorical_accuracy: 0.2338 - val_mean_directional_accuracy: 0.4582\n",
      "Epoch 4/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 5.4004 - accuracy: 0.2510 - categorical_accuracy: 0.2510 - mean_directional_accuracy: 0.4998 - val_loss: 4.8791 - val_accuracy: 0.2338 - val_categorical_accuracy: 0.2338 - val_mean_directional_accuracy: 0.4546\n",
      "Epoch 5/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 4.5060 - accuracy: 0.2569 - categorical_accuracy: 0.2569 - mean_directional_accuracy: 0.5026 - val_loss: 4.1012 - val_accuracy: 0.2304 - val_categorical_accuracy: 0.2304 - val_mean_directional_accuracy: 0.4525\n",
      "Epoch 6/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 3.8061 - accuracy: 0.2585 - categorical_accuracy: 0.2585 - mean_directional_accuracy: 0.4921 - val_loss: 3.4969 - val_accuracy: 0.2278 - val_categorical_accuracy: 0.2278 - val_mean_directional_accuracy: 0.4546\n",
      "Epoch 7/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 3.2673 - accuracy: 0.2589 - categorical_accuracy: 0.2589 - mean_directional_accuracy: 0.5018 - val_loss: 3.0276 - val_accuracy: 0.2290 - val_categorical_accuracy: 0.2290 - val_mean_directional_accuracy: 0.4582\n",
      "Epoch 8/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 2.8491 - accuracy: 0.2662 - categorical_accuracy: 0.2662 - mean_directional_accuracy: 0.5118 - val_loss: 2.6651 - val_accuracy: 0.2285 - val_categorical_accuracy: 0.2285 - val_mean_directional_accuracy: 0.4581\n",
      "Epoch 9/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.5269 - accuracy: 0.2603 - categorical_accuracy: 0.2603 - mean_directional_accuracy: 0.5035 - val_loss: 2.3863 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 10/300\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 2.2788 - accuracy: 0.2652 - categorical_accuracy: 0.2652 - mean_directional_accuracy: 0.5083 - val_loss: 2.1709 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 11/300\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 2.0877 - accuracy: 0.2677 - categorical_accuracy: 0.2677 - mean_directional_accuracy: 0.5105 - val_loss: 2.0053 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 12/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.9404 - accuracy: 0.2673 - categorical_accuracy: 0.2673 - mean_directional_accuracy: 0.5098 - val_loss: 1.8777 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 13/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.8271 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7793 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 14/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.7395 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.7030 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 15/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.6716 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.6437 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 16/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6185 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5976 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 17/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.5769 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5606 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 18/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5440 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5318 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 19/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.5176 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.5083 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 20/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4965 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4892 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 21/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4791 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4737 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 22/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4649 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4611 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 23/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4534 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4507 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 24/300\n",
      "23/23 [==============================] - 0s 22ms/step - loss: 1.4436 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4418 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 25/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4354 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4343 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 26/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.4285 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4280 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 27/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4226 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4226 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 28/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.4175 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4183 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 29/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.4133 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4143 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 30/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4095 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4110 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 31/300\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.4062 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4077 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 32/300\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.4035 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4055 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 33/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.4010 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4032 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 34/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3989 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.4013 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 35/300\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 1.3970 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3997 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 36/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3955 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3981 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 37/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3940 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3968 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 38/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.3929 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3955 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 39/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.3918 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3947 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 40/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.3909 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3937 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 41/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3901 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3928 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 42/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3894 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3923 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 43/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3888 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3919 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 44/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3883 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3912 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 45/300\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3878 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3909 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 46/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3874 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3904 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 47/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.3871 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3899 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 48/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3868 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3899 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 49/300\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 1.3866 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3896 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 50/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3864 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3894 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 51/300\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.3862 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3892 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 52/300\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 1.3860 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3890 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 53/300\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3859 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3890 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 54/300\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 1.3857 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3890 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55/300\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 1.3856 - accuracy: 0.2676 - categorical_accuracy: 0.2676 - mean_directional_accuracy: 0.5103Restoring model weights from the end of the best epoch: 54.\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 1.3856 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3890 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 55: early stopping\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 1.3890 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3890141248703003, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 54 \n",
      "\n",
      "Model time: 0.396452359855175 minutes\n",
      "\n",
      "Total time: 55.41268928349018 minutes\n",
      "\n",
      "\n",
      "Model  110  out of  111\n",
      "Model type             FeedForward\n",
      "Hidden layers                    2\n",
      "Hidden units                [1, 8]\n",
      "Activation function           tanh\n",
      "Dropout                        0.1\n",
      "L1                           100.0\n",
      "L2                            10.0\n",
      "Batch size                      16\n",
      "Optimizer                     Adam\n",
      "Learning rate                0.001\n",
      "Name: 98, dtype: object\n",
      "NN2Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 6s 9ms/step - loss: 1033.3940 - accuracy: 0.2666 - categorical_accuracy: 0.2666 - mean_directional_accuracy: 0.5133 - val_loss: 306.2576 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 116.9497 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 13.1371 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 3/300\n",
      "356/356 [==============================] - 2s 7ms/step - loss: 4.9386 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.2972 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 4.0731 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.1371 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 5/300\n",
      "356/356 [==============================] - 2s 6ms/step - loss: 4.0535 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 3.9550 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6/300\n",
      "354/356 [============================>.] - ETA: 0s - loss: 4.0464 - accuracy: 0.2678 - categorical_accuracy: 0.2678 - mean_directional_accuracy: 0.5109Restoring model weights from the end of the best epoch: 5.\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 4.0461 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 4.2225 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 6: early stopping\n",
      "479/479 [==============================] - 2s 3ms/step - loss: 3.9550 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 3.955024242401123, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 5 \n",
      "\n",
      "Model time: 0.3313055634498596 minutes\n",
      "\n",
      "Total time: 55.74414951354265 minutes\n",
      "\n",
      "\n",
      "Model  111  out of  111\n",
      "Model type                FeedForward\n",
      "Hidden layers                       4\n",
      "Hidden units           [128, 2, 1, 1]\n",
      "Activation function              relu\n",
      "Dropout                           0.9\n",
      "L1                            0.00001\n",
      "L2                            0.00001\n",
      "Batch size                         16\n",
      "Optimizer                     RMSprop\n",
      "Learning rate                   0.001\n",
      "Name: 99, dtype: object\n",
      "NN4Layer\n",
      "Epoch 1/300\n",
      "356/356 [==============================] - 8s 11ms/step - loss: 1.3879 - accuracy: 0.2643 - categorical_accuracy: 0.2643 - mean_directional_accuracy: 0.5112 - val_loss: 1.3874 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2/300\n",
      "356/356 [==============================] - ETA: 0s - loss: 1.3855 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105Restoring model weights from the end of the best epoch: 1.\n",
      "356/356 [==============================] - 3s 7ms/step - loss: 1.3855 - accuracy: 0.2675 - categorical_accuracy: 0.2675 - mean_directional_accuracy: 0.5105 - val_loss: 1.3879 - val_accuracy: 0.2287 - val_categorical_accuracy: 0.2287 - val_mean_directional_accuracy: 0.4584\n",
      "Epoch 2: early stopping\n",
      "479/479 [==============================] - 2s 3ms/step - loss: 1.3874 - accuracy: 0.2287 - categorical_accuracy: 0.2287 - mean_directional_accuracy: 0.4584\n",
      "{'loss': 1.3873709440231323, 'accuracy': 0.22873173654079437, 'categorical_accuracy': 0.22873173654079437, 'mean_directional_accuracy': 0.45837682485580444} \n",
      " 1 \n",
      "\n",
      "Model time: 0.20715900510549545 minutes\n",
      "\n",
      "Total time: 55.9513751976192 minutes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option(\"display.precision\", 5)\n",
    "np.set_printoptions(precision=5, suppress=False)\n",
    "\n",
    "runningDate = datetime.now().strftime(\"%d-%m-%Y_%H%M\")\n",
    "\n",
    "\n",
    "#Comment these out to continue from last point\n",
    "MSEMatrix = pd.DataFrame()\n",
    "if not classification:\n",
    "  MSEMatrix = pd.concat((MSEMatrix, NaiveZeroGrid, NaiveMeanGrid, NaiveFirmMeanGrid, NaiveFiveYearGrid), axis=0)\n",
    "\n",
    "\n",
    "#Random grid search\n",
    "seed = 183\n",
    "import random\n",
    "random.seed(a=seed)\n",
    "numberOfLassoRidgeModels = 10\n",
    "numberOfNNModels = 100  #176 models took 40 min   #120 Class4 models took 267min\n",
    "\n",
    "randomNNGrid = random.sample(list(itertools.product(\n",
    "    *NNParams.values())), k=numberOfNNModels)\n",
    "randomNNGrid = pd.DataFrame(randomNNGrid, columns=NNParams.keys())\n",
    "\n",
    "randomNNGrid[\"Hidden units\"] = randomNNGrid[\"Hidden layers\"].apply(lambda n: np.random.choice(hiddenUnits, size=n, replace=True))\n",
    "\n",
    "#randomLinearGrid = linearGrid.drop(0).sample(n=numberOfLassoRidgeModels, random_state=seed)\n",
    "randomLinearGrid = linearGrid.sample(n=numberOfLassoRidgeModels, random_state=seed)\n",
    "#randomNNGrid = NNGrid.sample(n=numberOfNNModels, random_state=seed)\n",
    "HPGrid = pd.concat([StandardLinearGrid, randomLinearGrid, randomNNGrid])\n",
    "\n",
    "HPsize = np.shape(HPGrid)[0]\n",
    "HPRange = range(0, HPsize)\n",
    "\n",
    "startTime = time.time() / 60\n",
    "for n in HPRange:\n",
    "  \n",
    "  modelStartTime = time.time() / 60\n",
    "  print(\"Model \", str(n+1), \" out of \", str(HPsize))\n",
    "\n",
    "  HP = HPGrid.iloc[n]; print(HP)\n",
    "  CV = NNfunction(*HP, runningDate)\n",
    "\n",
    "  modelEndTime = time.time() / 60\n",
    "  modelTrainingDuration = modelEndTime - modelStartTime\n",
    "  totalTrainingDuration = modelEndTime - startTime\n",
    "\n",
    "  print(\"Model time: \" + str(modelTrainingDuration) + \" minutes\\n\")\n",
    "  print(\"Total time: \" + str(totalTrainingDuration) + \" minutes\\n\\n\")\n",
    "\n",
    "\n",
    "  newMSE = HPGrid.iloc[[n]]\n",
    "  trainResults = CV[0] | {\n",
    "      \"Epochs\": CV[1],\n",
    "      \"Training Time (minutes)\": modelTrainingDuration,\n",
    "      \"ModelPointer\": CV[2]\n",
    "      }\n",
    "\n",
    "  trainResults = pd.DataFrame(trainResults, index=newMSE.index)\n",
    "  newMSE = pd.concat([newMSE, trainResults], axis=1)\n",
    "  MSEMatrix = pd.concat((MSEMatrix, newMSE), axis=0)\n",
    "\n",
    "finalResults = MSEMatrix\n",
    "\n",
    "\n",
    "if classification:\n",
    "  formattedMSEMatrix = finalResults.sort_values(by=[\"accuracy\"], axis = 0, ascending=False)\n",
    "else: \n",
    "  formattedMSEMatrix = finalResults.sort_values(by=[\"mean_absolute_error\", \"mean_squared_error\"], axis = 0)\n",
    "\n",
    "\n",
    "beep()\n",
    "formattedMSEMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model type</th>\n",
       "      <th>Hidden layers</th>\n",
       "      <th>Hidden units</th>\n",
       "      <th>Activation function</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>categorical_accuracy</th>\n",
       "      <th>mean_directional_accuracy</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Training Time (minutes)</th>\n",
       "      <th>ModelPointer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>RegularizedLinear</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>32</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.38181</td>\n",
       "      <td>0.32320</td>\n",
       "      <td>0.32320</td>\n",
       "      <td>0.55819</td>\n",
       "      <td>82</td>\n",
       "      <td>1.62009</td>\n",
       "      <td>&lt;keras.engine.functional.Functional object at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>FeedForward</td>\n",
       "      <td>1</td>\n",
       "      <td>[32]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>16</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.38540</td>\n",
       "      <td>0.31681</td>\n",
       "      <td>0.31681</td>\n",
       "      <td>0.53640</td>\n",
       "      <td>12</td>\n",
       "      <td>0.73925</td>\n",
       "      <td>&lt;keras.engine.functional.Functional object at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FeedForward</td>\n",
       "      <td>3</td>\n",
       "      <td>[8, 1, 4]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>32</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1.37443</td>\n",
       "      <td>0.30585</td>\n",
       "      <td>0.30585</td>\n",
       "      <td>0.52349</td>\n",
       "      <td>1</td>\n",
       "      <td>0.12178</td>\n",
       "      <td>&lt;keras.engine.functional.Functional object at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>FeedForward</td>\n",
       "      <td>2</td>\n",
       "      <td>[4, 2]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>256</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>1.38921</td>\n",
       "      <td>0.30532</td>\n",
       "      <td>0.30532</td>\n",
       "      <td>0.52401</td>\n",
       "      <td>2</td>\n",
       "      <td>0.08282</td>\n",
       "      <td>&lt;keras.engine.functional.Functional object at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>RegularizedLinear</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>16</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.37947</td>\n",
       "      <td>0.30441</td>\n",
       "      <td>0.30441</td>\n",
       "      <td>0.53510</td>\n",
       "      <td>33</td>\n",
       "      <td>1.27879</td>\n",
       "      <td>&lt;keras.engine.functional.Functional object at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FeedForward</td>\n",
       "      <td>1</td>\n",
       "      <td>[32]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>32</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.38800</td>\n",
       "      <td>0.22873</td>\n",
       "      <td>0.22873</td>\n",
       "      <td>0.45838</td>\n",
       "      <td>6</td>\n",
       "      <td>0.19428</td>\n",
       "      <td>&lt;keras.engine.functional.Functional object at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>FeedForward</td>\n",
       "      <td>1</td>\n",
       "      <td>[256]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>64</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.61970</td>\n",
       "      <td>0.22873</td>\n",
       "      <td>0.22873</td>\n",
       "      <td>0.45838</td>\n",
       "      <td>18</td>\n",
       "      <td>0.38992</td>\n",
       "      <td>&lt;keras.engine.functional.Functional object at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FeedForward</td>\n",
       "      <td>5</td>\n",
       "      <td>[64, 1, 128, 128, 1]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>32</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.39185</td>\n",
       "      <td>0.22873</td>\n",
       "      <td>0.22873</td>\n",
       "      <td>0.45838</td>\n",
       "      <td>71</td>\n",
       "      <td>2.20990</td>\n",
       "      <td>&lt;keras.engine.functional.Functional object at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>FeedForward</td>\n",
       "      <td>4</td>\n",
       "      <td>[128, 2, 1, 1]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>16</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.38737</td>\n",
       "      <td>0.22873</td>\n",
       "      <td>0.22873</td>\n",
       "      <td>0.45838</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20716</td>\n",
       "      <td>&lt;keras.engine.functional.Functional object at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>RegularizedLinear</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>32</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>35.29091</td>\n",
       "      <td>0.22808</td>\n",
       "      <td>0.22808</td>\n",
       "      <td>0.45707</td>\n",
       "      <td>1</td>\n",
       "      <td>0.08355</td>\n",
       "      <td>&lt;keras.engine.functional.Functional object at ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model type  Hidden layers          Hidden units  \\\n",
       "1359  RegularizedLinear              0                     0   \n",
       "94          FeedForward              1                  [32]   \n",
       "28          FeedForward              3             [8, 1, 4]   \n",
       "76          FeedForward              2                [4, 2]   \n",
       "123   RegularizedLinear              0                     0   \n",
       "...                 ...            ...                   ...   \n",
       "11          FeedForward              1                  [32]   \n",
       "43          FeedForward              1                 [256]   \n",
       "12          FeedForward              5  [64, 1, 128, 128, 1]   \n",
       "99          FeedForward              4        [128, 2, 1, 1]   \n",
       "458   RegularizedLinear              0                     0   \n",
       "\n",
       "     Activation function  Dropout         L1         L2  Batch size Optimizer  \\\n",
       "1359                None      0.0    0.01000    0.00000          32      Adam   \n",
       "94                  tanh      0.4    0.00100    0.01000          16      Adam   \n",
       "28                  tanh      0.0    0.00001    0.00000          32      Adam   \n",
       "76                  tanh      0.1    0.00100    0.00001         256   RMSprop   \n",
       "123                 None      0.0    0.00000    0.10000          16      Adam   \n",
       "...                  ...      ...        ...        ...         ...       ...   \n",
       "11                  tanh      0.0    0.00000    1.00000          32      Adam   \n",
       "43                  relu      0.9    0.10000  100.00000          64   RMSprop   \n",
       "12                  relu      0.9    0.01000    0.10000          32      Adam   \n",
       "99                  relu      0.9    0.00001    0.00001          16   RMSprop   \n",
       "458                 None      0.0  100.00000    0.00100          32   RMSprop   \n",
       "\n",
       "      Learning rate      loss  accuracy  categorical_accuracy  \\\n",
       "1359         0.0001   1.38181   0.32320               0.32320   \n",
       "94           0.0010   1.38540   0.31681               0.31681   \n",
       "28           0.0100   1.37443   0.30585               0.30585   \n",
       "76           0.0100   1.38921   0.30532               0.30532   \n",
       "123          0.0001   1.37947   0.30441               0.30441   \n",
       "...             ...       ...       ...                   ...   \n",
       "11           0.0010   1.38800   0.22873               0.22873   \n",
       "43           0.0001   1.61970   0.22873               0.22873   \n",
       "12           0.0001   1.39185   0.22873               0.22873   \n",
       "99           0.0010   1.38737   0.22873               0.22873   \n",
       "458          0.0010  35.29091   0.22808               0.22808   \n",
       "\n",
       "      mean_directional_accuracy  Epochs  Training Time (minutes)  \\\n",
       "1359                    0.55819      82                  1.62009   \n",
       "94                      0.53640      12                  0.73925   \n",
       "28                      0.52349       1                  0.12178   \n",
       "76                      0.52401       2                  0.08282   \n",
       "123                     0.53510      33                  1.27879   \n",
       "...                         ...     ...                      ...   \n",
       "11                      0.45838       6                  0.19428   \n",
       "43                      0.45838      18                  0.38992   \n",
       "12                      0.45838      71                  2.20990   \n",
       "99                      0.45838       1                  0.20716   \n",
       "458                     0.45707       1                  0.08355   \n",
       "\n",
       "                                           ModelPointer  \n",
       "1359  <keras.engine.functional.Functional object at ...  \n",
       "94    <keras.engine.functional.Functional object at ...  \n",
       "28    <keras.engine.functional.Functional object at ...  \n",
       "76    <keras.engine.functional.Functional object at ...  \n",
       "123   <keras.engine.functional.Functional object at ...  \n",
       "...                                                 ...  \n",
       "11    <keras.engine.functional.Functional object at ...  \n",
       "43    <keras.engine.functional.Functional object at ...  \n",
       "12    <keras.engine.functional.Functional object at ...  \n",
       "99    <keras.engine.functional.Functional object at ...  \n",
       "458   <keras.engine.functional.Functional object at ...  \n",
       "\n",
       "[111 rows x 17 columns]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formattedMSEMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in zip(yVal, xVal): print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledData\n",
    "#pd.concat((yVal, xVal.iloc[:,39:57]), axis=1)\n",
    "pd.concat((yTrain, finalData.iloc[:,69:87]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN4Layer\n",
      "Epoch 1/300\n",
      "168/168 [==============================] - 4s 13ms/step - loss: 14.3140 - mean_absolute_error: 14.3140 - mean_squared_error: 473.9607 - val_loss: 13.8154 - val_mean_absolute_error: 13.8154 - val_mean_squared_error: 451.9902\n",
      "Epoch 2/300\n",
      "165/168 [============================>.] - ETA: 0s - loss: 14.3340 - mean_absolute_error: 14.3340 - mean_squared_error: 472.3824Restoring model weights from the end of the best epoch: 1.\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 14.3264 - mean_absolute_error: 14.3264 - mean_squared_error: 474.1860 - val_loss: 13.8154 - val_mean_absolute_error: 13.8154 - val_mean_squared_error: 451.9900\n",
      "Epoch 2: early stopping\n",
      "155/155 [==============================] - 1s 4ms/step - loss: 13.8154 - mean_absolute_error: 13.8154 - mean_squared_error: 451.9902\n",
      "{'loss': 13.815396308898926, 'mean_absolute_error': 13.815396308898926, 'mean_squared_error': 451.9902038574219} \n",
      " 1 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'loss': 13.815396308898926,\n",
       "  'mean_absolute_error': 13.815396308898926,\n",
       "  'mean_squared_error': 451.9902038574219},\n",
       " 1,\n",
       " <keras.engine.functional.Functional at 0x2ed1e375a50>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - 13.7826/zeroForecastMAE\n",
    "\n",
    "\n",
    "# #Gu et al model NN3\n",
    "# NNfunction(modelType=\"FeedForward\", layers=3, units=[32, 16, 8], activation=\"relu\",\n",
    "#            dropout=0.2, L1=0.1, L2=0,\n",
    "#            batch_size=64, optimizer=\"Adam\", learning_rate=0.000001,\n",
    "#            runningDate=datetime.now().strftime(\"%d-%m-%Y_%H%M\"), loss=\"mse\")\n",
    "\n",
    "# #Gu et al model NN4\n",
    "# NNfunction(modelType=\"FeedForward\", layers=4\n",
    "# , units=[32, 16, 8, 4], activation=\"relu\",\n",
    "#            dropout=0.2, L1=0.1, L2=0,\n",
    "#            batch_size=64, optimizer=\"Adam\", learning_rate=0.000001,\n",
    "#            runningDate=datetime.now().strftime(\"%d-%m-%Y_%H%M\"), loss=\"mse\")\n",
    "\n",
    "\n",
    "# NNfunction(modelType=\"FeedForward\", layers=3, units=[32, 16, 8], activation=\"relu\",\n",
    "#            dropout=0.2, L1=0.1, L2=0,\n",
    "#            batch_size=64, optimizer=\"Adam\", learning_rate=0.000001,\n",
    "#            runningDate=datetime.now().strftime(\"%d-%m-%Y_%H%M\"), loss=\"mae\")\n",
    "\n",
    "# NNfunction(modelType=\"FeedForward\", layers=3, units=[32, 16, 8], activation=\"relu\",\n",
    "#            dropout=0, L1=0.1, L2=0,\n",
    "#            batch_size=64, optimizer=\"Adam\", learning_rate=0.000001,\n",
    "#            runningDate=datetime.now().strftime(\"%d-%m-%Y_%H%M\"), loss=\"mae\")\n",
    "\n",
    "NNfunction(modelType=\"FeedForward\", layers=4, units=[64, 32, 16, 8], activation=\"relu\",\n",
    "           dropout=0.690677223466582, L1=0, L2=0,\n",
    "           batch_size=32, optimizer=\"Adam\", learning_rate=0.000001,\n",
    "           runningDate=datetime.now().strftime(\"%d-%m-%Y_%H%M\"), loss=\"mae\")\n",
    "\n",
    "\n",
    "\n",
    "# NNfunction(modelType=\"FeedForward\", layers=1, units=1, activation=\"relu\",\n",
    "#            dropout=0, L1=0.0001, L2=0,\n",
    "#            batch_size=64, optimizer=\"Adam\", learning_rate=0.00001,\n",
    "#            runningDate=datetime.now().strftime(\"%d-%m-%Y_%H%M\"))\n",
    "# Model  77  out of  105\n",
    "# Model type             FeedForward\n",
    "# Hidden layers                    1\n",
    "# Hidden units                     1\n",
    "# Activation function        sigmoid\n",
    "# Dropout                        0.1\n",
    "# L1                             0.1\n",
    "# L2                             1.0\n",
    "# Batch size                       8\n",
    "# Optimizer                     Adam\n",
    "# Learning rate              0.00001\n",
    "#mean_absolute_error: 13.7826\n",
    "\n",
    "\n",
    "# NNfunction(modelType=\"FeedForward\", layers=2, units=8, activation=\"sigmoid\",\n",
    "#            dropout=0.4, L1=10.0, L2=100.0,\n",
    "#            batch_size=16, optimizer=\"RMSprop\", learning_rate=0.00001,\n",
    "#            runningDate=datetime.now().strftime(\"%d-%m-%Y_%H%M\"))\n",
    "# Model  87  out of  105\n",
    "# Model type             FeedForward\n",
    "# Hidden layers                    2\n",
    "# Hidden units                     8\n",
    "# Activation function        sigmoid\n",
    "# Dropout                        0.4\n",
    "# L1                            10.0\n",
    "# L2                           100.0\n",
    "# Batch size                      16\n",
    "# Optimizer                  RMSprop\n",
    "# Learning rate              0.00001\n",
    "#mean_absolute_error: 13.7952\n",
    "\n",
    "\n",
    "\n",
    "# NNfunction(modelType=\"FeedForward\", layers=1, units=1, activation=\"sigmoid\",\n",
    "#            dropout=0.1, L1=0.1, L2=1.0,\n",
    "#            batch_size=8, optimizer=\"Adam\", learning_rate=0.00001,\n",
    "#            runningDate=datetime.now().strftime(\"%d-%m-%Y_%H%M\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Results/['4Classes']/Ordinary/BestKerasModel1\\assets\n",
      "INFO:tensorflow:Assets written to: Results/['4Classes']/Ordinary/BestKerasModel2\\assets\n",
      "INFO:tensorflow:Assets written to: Results/['4Classes']/Ordinary/BestKerasModel3\\assets\n",
      "INFO:tensorflow:Assets written to: Results/['4Classes']/Ordinary/BestKerasModel4\\assets\n",
      "INFO:tensorflow:Assets written to: Results/['4Classes']/Ordinary/BestKerasModel5\\assets\n",
      "INFO:tensorflow:Assets written to: Results/['4Classes']/Ordinary/KerasLinearModel\\assets\n"
     ]
    }
   ],
   "source": [
    "#MSEMatrix\n",
    "\n",
    "kerasModelResults = formattedMSEMatrix.dropna(subset = [\"ModelPointer\"])\n",
    "kerasModelResults = kerasModelResults.reset_index()\n",
    "#bestParams = finalResults.iloc[index][:-3]\n",
    "#bestModel = finalResults(*bestParams)[1]\n",
    "#bestParams = finalResults[finalResults['MSE']==finalResults['MSE'].min()]\n",
    "\n",
    "if classification:\n",
    "    index = kerasModelResults['accuracy'].idxmax()\n",
    "else: \n",
    "    index = kerasModelResults['mean_absolute_error'].idxmin()\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    bModel = kerasModelResults.iloc[i][\"ModelPointer\"]\n",
    "    modelPath = f\"Results/{target}/{dataSample}/BestKerasModel\" + str(i + 1)\n",
    "    bModel.save(modelPath)\n",
    "\n",
    "\n",
    "OLSModel = kerasModelResults[kerasModelResults[\"Model type\"] == \"StandardLinear\"][\"ModelPointer\"].iloc[0]\n",
    "\n",
    "modelPath = f\"Results/{target}/{dataSample}/KerasLinearModel\"\n",
    "OLSModel.save(modelPath)\n",
    "\n",
    "\n",
    "# # xTrainValiPooled = scaled_X[:validationSize,:]\n",
    "# # yTrainValiPooled = Y[:validationSize]\n",
    "\n",
    "# #xTrainValiPooled = scaled_X[:validationSize,:]\n",
    "# #xTrainValiPooled = X.loc[split!=\"Test\", :]\n",
    "# xTrainValiPooled = dummiesClean.loc[split!=\"Test\", :]\n",
    "# #yTrainValiPooled = Y[:validationSize]\n",
    "# yTrainValiPooled = Y[split!=\"Test\"]\n",
    "\n",
    "# #bestModel = bestParams[\"ModelPointer\"]\n",
    "# bestModel = kerasModelResults.iloc[index][\"ModelPointer\"]\n",
    "\n",
    "# callback = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# history = bestModel.fit(x=xTrainValiPooled, y=yTrainValiPooled,\n",
    "#                         batch_size=32, epochs=100,\n",
    "#                         callbacks=[callback], verbose=2, validation_split=0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #bestModel.save('Results/BestKerasModel10')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 1s 3ms/step\n",
      "(array([0.2244, 0.2742, 0.2705, 0.2309], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2135, 0.2772, 0.2871, 0.2223], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1918, 0.2861, 0.2937, 0.2284], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2105, 0.272 , 0.2791, 0.2384], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1983, 0.2992, 0.3016, 0.2009], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2013, 0.2808, 0.2938, 0.2241], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2056, 0.2902, 0.2763, 0.228 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2007, 0.2817, 0.2868, 0.2308], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2013, 0.2542, 0.2788, 0.2656], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1925, 0.2536, 0.2535, 0.3003], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2136, 0.2578, 0.2592, 0.2693], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1993, 0.2933, 0.2922, 0.2152], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1885, 0.2885, 0.2929, 0.2302], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1841, 0.2893, 0.281 , 0.2455], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1933, 0.2866, 0.2861, 0.2339], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1765, 0.2887, 0.2796, 0.2552], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2072, 0.2779, 0.2752, 0.2397], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2162, 0.2802, 0.2753, 0.2283], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.247 , 0.2614, 0.2478, 0.2437], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1981, 0.2904, 0.2953, 0.2162], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1921, 0.2739, 0.2882, 0.2458], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2088, 0.2694, 0.276 , 0.2458], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.306 , 0.2942, 0.1924, 0.2075], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1938, 0.2822, 0.3024, 0.2217], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1965, 0.2882, 0.2879, 0.2274], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1919, 0.2902, 0.2877, 0.2303], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.194 , 0.2871, 0.2956, 0.2234], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2326, 0.2577, 0.2566, 0.2531], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1882, 0.2916, 0.2975, 0.2228], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2371, 0.2206, 0.2405, 0.3017], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2198, 0.2733, 0.2713, 0.2355], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1955, 0.2879, 0.2811, 0.2355], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.24  , 0.2682, 0.2676, 0.2242], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2142, 0.2708, 0.2809, 0.2341], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.203 , 0.278 , 0.281 , 0.2381], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1893, 0.2942, 0.3036, 0.2129], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1974, 0.2932, 0.2989, 0.2105], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2071, 0.2696, 0.2751, 0.2482], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3243, 0.2048, 0.1741, 0.2968], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2035, 0.2664, 0.2649, 0.2651], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2352, 0.2433, 0.2109, 0.3106], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2222, 0.276 , 0.2746, 0.2271], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.286 , 0.2324, 0.2296, 0.2521], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1917, 0.2902, 0.304 , 0.214 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1888, 0.2883, 0.3087, 0.2142], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2016, 0.2828, 0.2915, 0.2241], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.216 , 0.2691, 0.2782, 0.2367], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2146, 0.2792, 0.276 , 0.2301], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2001, 0.2845, 0.2835, 0.2319], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2346, 0.273 , 0.2516, 0.2408], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2308, 0.255 , 0.2559, 0.2583], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2401, 0.2709, 0.2541, 0.2349], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2953, 0.226 , 0.2139, 0.2648], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2857, 0.2455, 0.2311, 0.2378], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2565, 0.2057, 0.2336, 0.3042], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2593, 0.2125, 0.2324, 0.2958], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.246 , 0.18  , 0.1689, 0.4051], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2428, 0.2485, 0.2691, 0.2396], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2707, 0.3038, 0.1927, 0.2328], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1729, 0.2622, 0.2811, 0.2839], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2018, 0.2778, 0.2909, 0.2296], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2985, 0.2173, 0.2255, 0.2587], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3005, 0.2159, 0.2255, 0.2581], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3098, 0.2214, 0.2201, 0.2486], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2336, 0.2541, 0.2583, 0.254 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2685, 0.2493, 0.2244, 0.2578], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2401, 0.2507, 0.2437, 0.2655], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2155, 0.2869, 0.2688, 0.2287], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2325, 0.2777, 0.254 , 0.2357], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2755, 0.2177, 0.2145, 0.2923], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2872, 0.2177, 0.2126, 0.2825], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1958, 0.2774, 0.3042, 0.2227], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2135, 0.274 , 0.2829, 0.2296], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2124, 0.2704, 0.2705, 0.2467], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2123, 0.2661, 0.2811, 0.2406], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.19  , 0.2808, 0.2826, 0.2466], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1981, 0.2796, 0.2839, 0.2384], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2228, 0.2655, 0.2716, 0.2401], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2752, 0.2506, 0.2421, 0.2321], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2033, 0.2874, 0.2767, 0.2326], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2038, 0.2722, 0.2822, 0.2418], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1937, 0.2788, 0.2959, 0.2317], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2004, 0.2901, 0.2875, 0.222 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.182 , 0.2836, 0.3011, 0.2333], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1872, 0.2875, 0.3002, 0.2251], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2165, 0.2929, 0.2701, 0.2205], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1901, 0.2806, 0.2895, 0.2399], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2301, 0.2667, 0.2728, 0.2304], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2537, 0.2271, 0.246 , 0.2733], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3399, 0.1966, 0.1795, 0.2841], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2274, 0.1724, 0.2195, 0.3808], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2802, 0.167 , 0.1666, 0.3862], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1884, 0.2815, 0.3079, 0.2221], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.215 , 0.2951, 0.2884, 0.2015], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2029, 0.2808, 0.2717, 0.2446], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2045, 0.2714, 0.2913, 0.2328], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2094, 0.2825, 0.2689, 0.2392], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2318, 0.2697, 0.2614, 0.237 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2353, 0.2661, 0.2571, 0.2414], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2061, 0.2689, 0.2726, 0.2524], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2097, 0.2736, 0.2697, 0.247 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2049, 0.276 , 0.2709, 0.2482], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2343, 0.2477, 0.2631, 0.2549], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2359, 0.2701, 0.2625, 0.2315], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2659, 0.2422, 0.2421, 0.2498], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2563, 0.1943, 0.2231, 0.3263], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2426, 0.2161, 0.2172, 0.3241], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1654, 0.2049, 0.2478, 0.3819], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1662, 0.2101, 0.2509, 0.3729], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2743, 0.2265, 0.2163, 0.2828], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2056, 0.2171, 0.2481, 0.3293], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1741, 0.2301, 0.2597, 0.3361], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2339, 0.2291, 0.2438, 0.2931], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2501, 0.2387, 0.24  , 0.2712], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2345, 0.2353, 0.2426, 0.2876], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2105, 0.2083, 0.2283, 0.3529], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2166, 0.2626, 0.2691, 0.2517], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1969, 0.2685, 0.3043, 0.2303], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1896, 0.2667, 0.3004, 0.2433], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2   , 0.2759, 0.2949, 0.2292], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2057, 0.276 , 0.285 , 0.2333], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2164, 0.272 , 0.2853, 0.2263], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1873, 0.2693, 0.3055, 0.238 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1925, 0.2851, 0.2929, 0.2295], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1906, 0.2812, 0.2936, 0.2347], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1983, 0.2794, 0.2944, 0.2279], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1989, 0.2777, 0.2962, 0.2272], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1686, 0.2816, 0.2845, 0.2653], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1773, 0.2866, 0.2976, 0.2384], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1749, 0.2915, 0.3006, 0.233 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1578, 0.2867, 0.306 , 0.2495], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1616, 0.2879, 0.2929, 0.2576], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1607, 0.3024, 0.3062, 0.2307], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1469, 0.2921, 0.3101, 0.2508], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1516, 0.2981, 0.3136, 0.2366], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1474, 0.2842, 0.3185, 0.2499], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.18  , 0.27  , 0.2726, 0.2774], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2013, 0.2829, 0.2896, 0.2262], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2337, 0.2383, 0.2544, 0.2736], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1769, 0.3037, 0.3179, 0.2014], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1821, 0.3044, 0.3025, 0.211 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2086, 0.2748, 0.2903, 0.2264], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1984, 0.2854, 0.2928, 0.2234], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1911, 0.2879, 0.2972, 0.2238], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2946, 0.2524, 0.2458, 0.2072], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2005, 0.2831, 0.2869, 0.2295], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2198, 0.2814, 0.2637, 0.2351], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2543, 0.2929, 0.2229, 0.2299], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2359, 0.2631, 0.2602, 0.2408], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1895, 0.2756, 0.3038, 0.2312], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2145, 0.2742, 0.2787, 0.2325], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2049, 0.2739, 0.2953, 0.2259], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1947, 0.2901, 0.2925, 0.2228], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2011, 0.2789, 0.287 , 0.233 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.227 , 0.2692, 0.2787, 0.2252], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3097, 0.2411, 0.2274, 0.2218], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2035, 0.2886, 0.2975, 0.2105], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3488, 0.222 , 0.2031, 0.2261], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2454, 0.1661, 0.1732, 0.4153], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2837, 0.2486, 0.2292, 0.2385], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2268, 0.251 , 0.2591, 0.263 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.196 , 0.281 , 0.2939, 0.2291], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2175, 0.2795, 0.2872, 0.2157], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2124, 0.2761, 0.2665, 0.245 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2056, 0.2777, 0.289 , 0.2277], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2348, 0.2781, 0.2559, 0.2312], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.201 , 0.2779, 0.2878, 0.2332], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2392, 0.2546, 0.2597, 0.2465], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2547, 0.2653, 0.2473, 0.2327], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.304 , 0.237 , 0.2309, 0.2282], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2629, 0.2437, 0.2571, 0.2363], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2328, 0.2625, 0.2632, 0.2415], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.6075, 0.1435, 0.0933, 0.1557], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2374, 0.2448, 0.2563, 0.2614], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1852, 0.2974, 0.3058, 0.2116], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1969, 0.2907, 0.3015, 0.2109], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2014, 0.2904, 0.2959, 0.2124], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2288, 0.256 , 0.2828, 0.2323], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2127, 0.279 , 0.2901, 0.2182], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2459, 0.2561, 0.252 , 0.2461], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2641, 0.248 , 0.2489, 0.239 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2332, 0.2622, 0.2775, 0.2271], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2338, 0.2512, 0.2763, 0.2387], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2363, 0.2522, 0.2697, 0.2418], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1844, 0.2505, 0.2825, 0.2827], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1757, 0.2539, 0.2977, 0.2728], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2148, 0.2753, 0.2596, 0.2503], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2034, 0.2768, 0.2846, 0.2352], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1861, 0.256 , 0.3085, 0.2494], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2093, 0.2736, 0.2817, 0.2354], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2169, 0.284 , 0.2758, 0.2233], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2464, 0.2294, 0.2374, 0.2867], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1961, 0.286 , 0.302 , 0.2159], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2016, 0.2803, 0.2949, 0.2232], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2289, 0.263 , 0.2706, 0.2375], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1886, 0.2842, 0.3065, 0.2206], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3591, 0.2173, 0.2337, 0.1899], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2439, 0.2324, 0.2621, 0.2615], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2654, 0.2369, 0.2558, 0.2418], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1935, 0.2882, 0.2881, 0.2302], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1862, 0.2809, 0.287 , 0.2458], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2099, 0.2868, 0.2819, 0.2214], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2311, 0.2894, 0.2576, 0.2219], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2862, 0.2263, 0.2209, 0.2667], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3028, 0.2282, 0.2076, 0.2614], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.5052, 0.1941, 0.1228, 0.178 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1938, 0.2568, 0.2834, 0.266 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1977, 0.2496, 0.2821, 0.2707], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.205 , 0.2631, 0.2823, 0.2496], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.269 , 0.2741, 0.2453, 0.2116], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1959, 0.2934, 0.2836, 0.2271], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2118, 0.2757, 0.2753, 0.2372], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1975, 0.2787, 0.2975, 0.2262], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2705, 0.2831, 0.2501, 0.1964], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2131, 0.2696, 0.2805, 0.2368], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.205 , 0.2878, 0.2895, 0.2178], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3426, 0.2368, 0.2079, 0.2127], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2124, 0.2733, 0.268 , 0.2462], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2042, 0.29  , 0.2814, 0.2245], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.219 , 0.272 , 0.2747, 0.2343], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1934, 0.2927, 0.3001, 0.2139], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2203, 0.2403, 0.2748, 0.2646], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2081, 0.2181, 0.2682, 0.3056], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1958, 0.221 , 0.2885, 0.2946], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2316, 0.2195, 0.2587, 0.2902], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2085, 0.2109, 0.2559, 0.3248], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1863, 0.2753, 0.307 , 0.2314], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2086, 0.2743, 0.2833, 0.2338], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1841, 0.3028, 0.2972, 0.2159], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1851, 0.2976, 0.3081, 0.2092], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1863, 0.2923, 0.3067, 0.2147], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1936, 0.2994, 0.2926, 0.2144], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2222, 0.2883, 0.2813, 0.2082], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2175, 0.2601, 0.2772, 0.2452], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2559, 0.2552, 0.2551, 0.2338], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1689, 0.268 , 0.3159, 0.2472], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.241 , 0.2515, 0.2568, 0.2508], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2134, 0.2652, 0.2813, 0.24  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2175, 0.2571, 0.2682, 0.2573], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2496, 0.2538, 0.2537, 0.2429], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2502, 0.2598, 0.227 , 0.263 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2459, 0.2556, 0.2681, 0.2305], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2378, 0.2293, 0.2504, 0.2825], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2903, 0.2208, 0.2239, 0.265 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2327, 0.2513, 0.2655, 0.2505], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2813, 0.2313, 0.2438, 0.2436], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3855, 0.1966, 0.1891, 0.2288], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2291, 0.2841, 0.2595, 0.2273], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2852, 0.2661, 0.2217, 0.227 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2071, 0.2769, 0.2763, 0.2397], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1844, 0.2982, 0.2943, 0.2231], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2399, 0.223 , 0.2324, 0.3047], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3008, 0.2124, 0.2152, 0.2716], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3056, 0.2232, 0.2057, 0.2655], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1914, 0.2985, 0.2843, 0.2258], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2142, 0.2746, 0.2726, 0.2386], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2258, 0.2768, 0.2697, 0.2278], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1876, 0.2776, 0.2892, 0.2456], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2858, 0.2834, 0.2227, 0.2081], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.168 , 0.2218, 0.2725, 0.3377], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2028, 0.2776, 0.286 , 0.2336], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2674, 0.2345, 0.246 , 0.252 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3451, 0.2158, 0.2097, 0.2294], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2488, 0.2318, 0.2435, 0.2759], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3491, 0.2228, 0.1912, 0.2369], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1972, 0.2939, 0.2804, 0.2285], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1937, 0.2922, 0.2852, 0.2288], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3317, 0.2163, 0.1862, 0.2658], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.212 , 0.265 , 0.2684, 0.2546], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2043, 0.2789, 0.2839, 0.2329], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2117, 0.279 , 0.2818, 0.2275], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2037, 0.2492, 0.2631, 0.284 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2005, 0.2512, 0.2698, 0.2785], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1889, 0.2562, 0.2541, 0.3008], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2524, 0.248 , 0.2322, 0.2674], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4171, 0.1937, 0.1468, 0.2424], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2217, 0.2861, 0.2778, 0.2144], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.223 , 0.2692, 0.2758, 0.2321], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2186, 0.2753, 0.2808, 0.2254], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.232 , 0.279 , 0.2646, 0.2243], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.222 , 0.2573, 0.2656, 0.2551], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2002, 0.2759, 0.287 , 0.237 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1949, 0.2753, 0.2779, 0.252 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1906, 0.2816, 0.2977, 0.23  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1891, 0.2862, 0.2849, 0.2398], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2402, 0.27  , 0.2575, 0.2323], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2188, 0.2661, 0.287 , 0.2281], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1978, 0.2704, 0.276 , 0.2559], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2118, 0.2575, 0.2633, 0.2673], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2003, 0.2538, 0.2638, 0.2821], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3984, 0.2406, 0.1613, 0.1997], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2504, 0.2657, 0.2265, 0.2574], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2553, 0.2384, 0.2303, 0.2759], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.244 , 0.2392, 0.242 , 0.2747], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.208 , 0.2646, 0.2889, 0.2385], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.251 , 0.2323, 0.2339, 0.2828], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1902, 0.2753, 0.2929, 0.2416], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1929, 0.2743, 0.2942, 0.2387], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2244, 0.258 , 0.2753, 0.2423], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2409, 0.2491, 0.2587, 0.2512], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2501, 0.2582, 0.2544, 0.2373], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2218, 0.2572, 0.2635, 0.2576], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3223, 0.2146, 0.2   , 0.263 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3629, 0.2228, 0.1947, 0.2195], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3195, 0.2203, 0.2046, 0.2556], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4599, 0.1399, 0.1021, 0.2981], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.5716, 0.156 , 0.1073, 0.1652], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2479, 0.2602, 0.2575, 0.2345], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2097, 0.2827, 0.2615, 0.2461], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2297, 0.268 , 0.248 , 0.2543], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1886, 0.2978, 0.298 , 0.2156], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1756, 0.2895, 0.3229, 0.212 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1852, 0.2658, 0.2982, 0.2508], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2059, 0.276 , 0.2761, 0.242 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.203 , 0.2629, 0.2687, 0.2654], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4583, 0.219 , 0.1409, 0.1819], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1964, 0.2826, 0.2892, 0.2318], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.218 , 0.272 , 0.2746, 0.2354], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2126, 0.2767, 0.2729, 0.2378], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2041, 0.2826, 0.2712, 0.2422], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2083, 0.2756, 0.2702, 0.246 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2112, 0.2773, 0.2716, 0.2399], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2139, 0.2723, 0.2683, 0.2455], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2564, 0.2424, 0.2605, 0.2407], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3054, 0.2328, 0.2243, 0.2375], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2633, 0.2309, 0.2401, 0.2657], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4581, 0.2084, 0.1477, 0.1858], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2799, 0.2336, 0.2263, 0.2603], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2633, 0.2551, 0.2382, 0.2434], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.214 , 0.2487, 0.2838, 0.2535], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2106, 0.2563, 0.2627, 0.2704], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2812, 0.241 , 0.2254, 0.2524], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2921, 0.2446, 0.2326, 0.2307], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2433, 0.2504, 0.2701, 0.2362], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2308, 0.2536, 0.2481, 0.2675], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2471, 0.2669, 0.2588, 0.2271], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2075, 0.2727, 0.2941, 0.2258], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2515, 0.2596, 0.2667, 0.2222], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3206, 0.2607, 0.2376, 0.1811], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2302, 0.2719, 0.2568, 0.2412], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2828, 0.2491, 0.2297, 0.2384], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2164, 0.2755, 0.284 , 0.224 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2108, 0.2741, 0.289 , 0.2261], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2051, 0.2805, 0.289 , 0.2255], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2051, 0.2743, 0.2923, 0.2283], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2165, 0.2683, 0.2768, 0.2384], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2473, 0.2578, 0.2647, 0.2303], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2576, 0.2513, 0.2455, 0.2456], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1994, 0.2886, 0.2881, 0.2239], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2044, 0.2949, 0.2854, 0.2152], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1965, 0.2909, 0.2822, 0.2304], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2014, 0.2885, 0.2901, 0.2199], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2256, 0.3007, 0.273 , 0.2007], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2651, 0.2859, 0.2501, 0.1988], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2351, 0.2948, 0.277 , 0.1931], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2418, 0.2632, 0.2546, 0.2405], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2658, 0.2416, 0.2384, 0.2542], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3125, 0.2335, 0.2369, 0.2171], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2443, 0.223 , 0.2274, 0.3053], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2921, 0.2307, 0.2121, 0.2651], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2492, 0.2509, 0.2331, 0.2668], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2971, 0.2329, 0.1969, 0.2731], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1767, 0.2103, 0.2807, 0.3322], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1768, 0.224 , 0.2867, 0.3124], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1811, 0.2175, 0.2762, 0.3251], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.191 , 0.2243, 0.2696, 0.3151], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1981, 0.2236, 0.2709, 0.3075], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2136, 0.2054, 0.2431, 0.3379], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2302, 0.2279, 0.2719, 0.27  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2022, 0.2826, 0.2888, 0.2264], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1937, 0.2985, 0.2961, 0.2117], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1954, 0.3009, 0.2965, 0.2072], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1891, 0.3063, 0.3009, 0.2037], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1923, 0.3024, 0.3018, 0.2035], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1997, 0.2985, 0.2904, 0.2114], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1958, 0.301 , 0.2938, 0.2094], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1921, 0.3008, 0.2944, 0.2127], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2012, 0.2924, 0.2874, 0.219 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2068, 0.2887, 0.2846, 0.2199], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.204 , 0.2866, 0.2847, 0.2247], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2005, 0.2887, 0.2836, 0.2271], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2498, 0.2562, 0.2556, 0.2384], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2048, 0.2801, 0.2678, 0.2473], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2095, 0.2764, 0.271 , 0.243 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.217 , 0.2809, 0.2705, 0.2317], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2192, 0.2691, 0.2804, 0.2313], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.236 , 0.2531, 0.2688, 0.2421], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2147, 0.2627, 0.2751, 0.2475], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1902, 0.2986, 0.2988, 0.2124], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3105, 0.1882, 0.2126, 0.2887], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3105, 0.2246, 0.2018, 0.263 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.254 , 0.2048, 0.2261, 0.3151], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2113, 0.2737, 0.2801, 0.235 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2237, 0.2605, 0.273 , 0.2427], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.25  , 0.2489, 0.2548, 0.2462], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3131, 0.1832, 0.1901, 0.3136], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1859, 0.3046, 0.2963, 0.2132], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2002, 0.2912, 0.2786, 0.23  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2126, 0.2678, 0.2661, 0.2535], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2152, 0.2681, 0.2753, 0.2413], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2137, 0.2743, 0.2683, 0.2437], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2182, 0.2631, 0.2696, 0.2492], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2118, 0.2698, 0.2681, 0.2502], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2172, 0.2695, 0.2666, 0.2468], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2048, 0.2652, 0.2673, 0.2627], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2525, 0.2694, 0.2548, 0.2233], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2336, 0.2763, 0.2678, 0.2223], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1775, 0.2953, 0.2756, 0.2516], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1711, 0.3015, 0.2891, 0.2383], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2295, 0.2406, 0.251 , 0.279 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2043, 0.2321, 0.2668, 0.2967], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2632, 0.2643, 0.23  , 0.2426], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2089, 0.2467, 0.2616, 0.2827], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2276, 0.2487, 0.2613, 0.2623], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2646, 0.2564, 0.2243, 0.2546], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.213 , 0.2858, 0.2678, 0.2334], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2442, 0.2655, 0.2437, 0.2467], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2741, 0.2669, 0.2318, 0.2272], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3251, 0.1987, 0.1759, 0.3003], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3653, 0.2059, 0.1925, 0.2364], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2993, 0.2507, 0.2206, 0.2294], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2632, 0.2612, 0.2417, 0.2339], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4219, 0.1881, 0.1382, 0.2518], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2511, 0.2573, 0.257 , 0.2346], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.203 , 0.256 , 0.2768, 0.2642], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1926, 0.2667, 0.2875, 0.2532], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2063, 0.2676, 0.255 , 0.2711], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.231 , 0.2648, 0.2671, 0.2371], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2171, 0.256 , 0.2512, 0.2757], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2123, 0.2686, 0.2595, 0.2596], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2022, 0.2672, 0.2709, 0.2597], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1999, 0.2742, 0.2769, 0.249 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2137, 0.2794, 0.268 , 0.2389], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1971, 0.2833, 0.2733, 0.2464], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2044, 0.2757, 0.2885, 0.2314], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2145, 0.268 , 0.2607, 0.2569], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2136, 0.2749, 0.2676, 0.2439], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2441, 0.2652, 0.2502, 0.2406], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3508, 0.1734, 0.1734, 0.3024], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2087, 0.2861, 0.2949, 0.2104], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1882, 0.2872, 0.2858, 0.2388], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1916, 0.2723, 0.2948, 0.2413], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2157, 0.2819, 0.2743, 0.2281], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2012, 0.2806, 0.2986, 0.2196], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2029, 0.2804, 0.287 , 0.2298], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2126, 0.2858, 0.2734, 0.2282], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2144, 0.2657, 0.263 , 0.257 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2278, 0.2622, 0.2546, 0.2554], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3066, 0.2378, 0.217 , 0.2386], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4049, 0.1536, 0.1395, 0.302 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.323 , 0.1633, 0.1938, 0.3199], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3126, 0.1517, 0.1632, 0.3725], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2695, 0.1532, 0.201 , 0.3764], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3027, 0.2132, 0.1844, 0.2997], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.287 , 0.24  , 0.2322, 0.2408], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2736, 0.236 , 0.221 , 0.2694], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2709, 0.2351, 0.2264, 0.2676], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2758, 0.2392, 0.2244, 0.2606], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3313, 0.1637, 0.1685, 0.3366], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1958, 0.2111, 0.2476, 0.3455], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2194, 0.1928, 0.2189, 0.3689], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2287, 0.1955, 0.223 , 0.3528], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5262, 0.1423, 0.1138, 0.2177], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5193, 0.161 , 0.1089, 0.2108], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1964, 0.2634, 0.2656, 0.2745], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2034, 0.2658, 0.2803, 0.2504], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1674, 0.256 , 0.306 , 0.2706], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1672, 0.2483, 0.298 , 0.2865], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2035, 0.2629, 0.2677, 0.266 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1884, 0.3008, 0.2949, 0.216 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1824, 0.3036, 0.2995, 0.2145], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2   , 0.3054, 0.3036, 0.191 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1363, 0.2712, 0.321 , 0.2714], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2283, 0.2488, 0.2657, 0.2572], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2256, 0.2579, 0.262 , 0.2545], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2227, 0.2501, 0.2647, 0.2625], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2291, 0.2632, 0.2645, 0.2432], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2077, 0.2852, 0.2661, 0.241 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2122, 0.2603, 0.2484, 0.279 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2332, 0.2671, 0.2466, 0.253 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2457, 0.2384, 0.232 , 0.2839], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2351, 0.2552, 0.2462, 0.2635], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2189, 0.2627, 0.2569, 0.2615], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2256, 0.2411, 0.2348, 0.2985], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2155, 0.2423, 0.2547, 0.2875], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2752, 0.1526, 0.1546, 0.4177], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2451, 0.1689, 0.1264, 0.4596], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2421, 0.1787, 0.1324, 0.4468], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2582, 0.2382, 0.2424, 0.2613], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2333, 0.2391, 0.2283, 0.2992], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2621, 0.2405, 0.2437, 0.2537], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2649, 0.248 , 0.2472, 0.2399], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2278, 0.2516, 0.268 , 0.2526], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2611, 0.2236, 0.2358, 0.2795], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2827, 0.2247, 0.2359, 0.2566], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2978, 0.2243, 0.2108, 0.2671], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2829, 0.2418, 0.2303, 0.2449], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2371, 0.2509, 0.2527, 0.2594], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1828, 0.2942, 0.3079, 0.215 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1712, 0.2997, 0.3134, 0.2156], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.189 , 0.2891, 0.2896, 0.2323], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1783, 0.3033, 0.3087, 0.2097], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2074, 0.28  , 0.2858, 0.2268], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2094, 0.2747, 0.2859, 0.23  ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2472, 0.2361, 0.2283, 0.2885], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.28  , 0.2336, 0.2317, 0.2547], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2478, 0.2661, 0.2481, 0.238 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2868, 0.2326, 0.2101, 0.2705], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3135, 0.2452, 0.2064, 0.2349], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3243, 0.1999, 0.1948, 0.281 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3871, 0.1982, 0.1907, 0.224 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2565, 0.251 , 0.2431, 0.2493], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2447, 0.2581, 0.2609, 0.2363], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2385, 0.2472, 0.2711, 0.2432], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2334, 0.2613, 0.2554, 0.2499], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2364, 0.2535, 0.2664, 0.2437], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.233 , 0.268 , 0.2598, 0.2392], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2133, 0.2757, 0.2803, 0.2307], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2208, 0.2703, 0.2702, 0.2387], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2208, 0.2737, 0.2772, 0.2283], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2123, 0.2649, 0.2899, 0.2329], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.286 , 0.2342, 0.2353, 0.2445], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2996, 0.2546, 0.2416, 0.2042], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2196, 0.2609, 0.2777, 0.2418], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3288, 0.1929, 0.1987, 0.2797], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4857, 0.1485, 0.1151, 0.2507], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4718, 0.1649, 0.1237, 0.2396], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3779, 0.1667, 0.2186, 0.2368], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3091, 0.2237, 0.2152, 0.252 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3057, 0.2307, 0.2202, 0.2434], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2109, 0.2842, 0.2734, 0.2314], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2159, 0.2783, 0.2709, 0.2348], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2244, 0.2786, 0.2758, 0.2211], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1968, 0.2697, 0.2894, 0.2441], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2028, 0.2703, 0.2861, 0.2407], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2227, 0.2726, 0.2786, 0.226 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2344, 0.2476, 0.2628, 0.2552], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2039, 0.2888, 0.2881, 0.2191], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2006, 0.2888, 0.2946, 0.216 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2034, 0.2836, 0.2868, 0.2262], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.225 , 0.2781, 0.2738, 0.2232], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2213, 0.2818, 0.2761, 0.2207], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2285, 0.2684, 0.2776, 0.2256], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2391, 0.2681, 0.2567, 0.2362], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3011, 0.2817, 0.223 , 0.1942], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3916, 0.2378, 0.1854, 0.1852], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2209, 0.2663, 0.2879, 0.2248], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2396, 0.2631, 0.2733, 0.224 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.257 , 0.2712, 0.2475, 0.2243], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2564, 0.2659, 0.2516, 0.2261], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2378, 0.2131, 0.2651, 0.2841], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2243, 0.2369, 0.2429, 0.296 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2791, 0.2341, 0.2201, 0.2667], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2525, 0.237 , 0.2553, 0.2552], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2611, 0.2488, 0.2224, 0.2676], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2405, 0.263 , 0.2361, 0.2603], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1672, 0.2803, 0.2944, 0.2582], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1841, 0.255 , 0.2646, 0.2963], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1808, 0.286 , 0.2965, 0.2367], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3144, 0.2023, 0.1845, 0.2988], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3335, 0.2268, 0.2017, 0.238 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2283, 0.2595, 0.274 , 0.2382], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2429, 0.2705, 0.2689, 0.2177], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2222, 0.2781, 0.2679, 0.2318], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2089, 0.2799, 0.2877, 0.2235], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2797, 0.2023, 0.2248, 0.2932], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2946, 0.2186, 0.2222, 0.2647], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2651, 0.2284, 0.25  , 0.2564], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2639, 0.2295, 0.2485, 0.258 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2141, 0.2474, 0.2777, 0.2609], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2291, 0.2501, 0.2657, 0.2551], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3192, 0.2173, 0.2142, 0.2493], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2318, 0.2864, 0.2749, 0.2069], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2606, 0.2781, 0.2513, 0.21  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2706, 0.2647, 0.244 , 0.2207], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1784, 0.3014, 0.3052, 0.215 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1682, 0.2923, 0.3001, 0.2394], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1753, 0.3005, 0.3048, 0.2194], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2194, 0.2691, 0.2752, 0.2363], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2881, 0.2523, 0.2367, 0.2229], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2038, 0.2994, 0.27  , 0.2268], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2561, 0.2429, 0.2314, 0.2696], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2187, 0.2741, 0.2664, 0.2408], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1834, 0.3048, 0.296 , 0.2158], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1868, 0.2988, 0.3041, 0.2103], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2049, 0.2709, 0.2742, 0.25  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2235, 0.2834, 0.281 , 0.2122], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2622, 0.2529, 0.2389, 0.246 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2809, 0.2686, 0.2449, 0.2056], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1965, 0.2771, 0.2847, 0.2417], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2769, 0.2535, 0.2421, 0.2274], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2021, 0.2752, 0.2817, 0.2411], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1924, 0.2761, 0.2802, 0.2514], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1864, 0.285 , 0.2891, 0.2394], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1792, 0.2839, 0.3013, 0.2356], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2513, 0.2862, 0.2507, 0.2118], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2073, 0.2692, 0.2587, 0.2648], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2081, 0.2641, 0.2528, 0.275 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2259, 0.273 , 0.2532, 0.248 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2702, 0.2255, 0.2326, 0.2717], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2203, 0.2297, 0.2658, 0.2841], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2453, 0.2407, 0.238 , 0.276 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1943, 0.2297, 0.2492, 0.3269], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2296, 0.2305, 0.2326, 0.3072], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3048, 0.2488, 0.1544, 0.292 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2166, 0.2788, 0.2724, 0.2322], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1758, 0.226 , 0.2397, 0.3586], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1853, 0.247 , 0.2503, 0.3174], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.196 , 0.2777, 0.2932, 0.2331], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.185 , 0.2902, 0.2993, 0.2255], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1785, 0.3002, 0.3003, 0.2209], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1804, 0.2937, 0.2997, 0.2261], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2819, 0.2396, 0.237 , 0.2415], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1908, 0.2858, 0.2905, 0.2329], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2087, 0.2749, 0.2788, 0.2376], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2037, 0.281 , 0.2844, 0.2309], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2066, 0.2931, 0.283 , 0.2173], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1869, 0.2917, 0.2958, 0.2256], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.226 , 0.2834, 0.2674, 0.2231], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2162, 0.2786, 0.2678, 0.2374], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1854, 0.2876, 0.2925, 0.2345], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1855, 0.2921, 0.2893, 0.2331], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2152, 0.2921, 0.2851, 0.2076], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1781, 0.3051, 0.3072, 0.2096], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1734, 0.3019, 0.3063, 0.2183], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.175 , 0.3026, 0.3052, 0.2171], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1954, 0.2838, 0.2899, 0.2309], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2168, 0.2843, 0.2782, 0.2208], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2011, 0.2894, 0.2732, 0.2363], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1868, 0.2987, 0.297 , 0.2175], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1955, 0.2971, 0.2864, 0.2211], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2323, 0.2598, 0.2672, 0.2407], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1981, 0.2434, 0.2843, 0.2743], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2157, 0.2648, 0.2872, 0.2323], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2604, 0.2426, 0.2186, 0.2784], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2571, 0.2099, 0.2218, 0.3112], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3625, 0.1644, 0.1714, 0.3017], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4263, 0.1165, 0.1171, 0.3401], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4849, 0.0938, 0.1066, 0.3147], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4825, 0.0872, 0.1011, 0.3293], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4168, 0.1202, 0.1411, 0.3219], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.158 , 0.2316, 0.2874, 0.3231], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1656, 0.271 , 0.2908, 0.2726], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1918, 0.2752, 0.282 , 0.251 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1805, 0.279 , 0.2849, 0.2556], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1954, 0.2773, 0.2811, 0.2462], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1667, 0.2794, 0.2893, 0.2646], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2324, 0.1371, 0.1734, 0.4571], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2586, 0.1398, 0.1661, 0.4355], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.5538, 0.1404, 0.0999, 0.2058], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2663, 0.1269, 0.1642, 0.4426], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2093, 0.1415, 0.1943, 0.4548], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.253 , 0.168 , 0.1977, 0.3812], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1851, 0.2549, 0.279 , 0.2811], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1893, 0.2696, 0.2904, 0.2507], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1848, 0.2538, 0.2827, 0.2787], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1744, 0.2757, 0.2906, 0.2593], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2055, 0.2626, 0.2682, 0.2636], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2314, 0.2346, 0.2465, 0.2874], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1942, 0.2913, 0.2865, 0.228 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2059, 0.2956, 0.2803, 0.2182], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2022, 0.2597, 0.297 , 0.241 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1836, 0.2685, 0.2862, 0.2617], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.224 , 0.2711, 0.264 , 0.241 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2339, 0.2684, 0.2541, 0.2435], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.229 , 0.2535, 0.2573, 0.2601], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2465, 0.256 , 0.2326, 0.2649], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2572, 0.2561, 0.2299, 0.2567], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2145, 0.244 , 0.2458, 0.2957], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2137, 0.2422, 0.2481, 0.296 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2264, 0.2253, 0.2603, 0.2881], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2407, 0.235 , 0.2452, 0.2791], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2288, 0.2335, 0.2492, 0.2884], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2386, 0.2352, 0.2507, 0.2755], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.247 , 0.2326, 0.2401, 0.2803], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2366, 0.2245, 0.2503, 0.2886], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2414, 0.247 , 0.2423, 0.2693], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2393, 0.239 , 0.2467, 0.275 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2934, 0.2095, 0.2017, 0.2954], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2883, 0.2225, 0.1838, 0.3055], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2903, 0.2437, 0.2078, 0.2581], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2715, 0.251 , 0.2105, 0.267 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.5302, 0.1812, 0.1155, 0.1732], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.5353, 0.1693, 0.0992, 0.1961], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2144, 0.2785, 0.261 , 0.2461], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2122, 0.2792, 0.2773, 0.2313], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2157, 0.268 , 0.261 , 0.2553], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.22  , 0.2688, 0.2611, 0.2501], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2236, 0.2694, 0.2731, 0.2339], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2922, 0.2013, 0.2531, 0.2534], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4084, 0.191 , 0.1558, 0.2448], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3731, 0.2122, 0.1778, 0.2368], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3227, 0.2078, 0.1739, 0.2956], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3535, 0.2096, 0.1631, 0.2738], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3893, 0.1907, 0.1781, 0.2419], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4523, 0.1961, 0.1386, 0.2129], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.408 , 0.2029, 0.1342, 0.2549], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3741, 0.1965, 0.1248, 0.3045], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3766, 0.2239, 0.1507, 0.2488], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4219, 0.164 , 0.1093, 0.3048], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1985, 0.272 , 0.2936, 0.236 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2064, 0.2674, 0.2851, 0.2411], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2125, 0.2664, 0.2819, 0.2393], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2152, 0.2592, 0.2696, 0.256 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2227, 0.259 , 0.2655, 0.2528], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2263, 0.2509, 0.2742, 0.2486], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2143, 0.2707, 0.2673, 0.2477], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1785, 0.2852, 0.2808, 0.2555], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1861, 0.2863, 0.2879, 0.2398], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2367, 0.2524, 0.2488, 0.2621], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2609, 0.2624, 0.2498, 0.2269], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2168, 0.2708, 0.2513, 0.2611], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1433, 0.2578, 0.3095, 0.2895], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2518, 0.2252, 0.2313, 0.2916], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2781, 0.2159, 0.2356, 0.2704], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2243, 0.2352, 0.2482, 0.2923], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1929, 0.232 , 0.2814, 0.2938], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2357, 0.2549, 0.2596, 0.2499], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2355, 0.2544, 0.2588, 0.2514], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2437, 0.258 , 0.2637, 0.2346], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2191, 0.2544, 0.266 , 0.2605], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2674, 0.231 , 0.2363, 0.2653], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3391, 0.1885, 0.1944, 0.278 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2726, 0.2495, 0.2547, 0.2232], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2942, 0.2491, 0.2383, 0.2184], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1861, 0.2952, 0.2877, 0.2309], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1916, 0.3033, 0.2919, 0.2133], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1815, 0.2944, 0.295 , 0.2291], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1877, 0.2944, 0.2975, 0.2204], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2214, 0.2849, 0.2749, 0.2188], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2199, 0.2968, 0.2798, 0.2035], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2006, 0.2829, 0.2959, 0.2206], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2359, 0.2523, 0.2626, 0.2492], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1948, 0.283 , 0.2946, 0.2276], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2183, 0.2862, 0.2706, 0.2249], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2526, 0.2392, 0.2564, 0.2517], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2555, 0.2552, 0.256 , 0.2333], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2383, 0.2699, 0.2595, 0.2322], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4365, 0.194 , 0.1457, 0.2238], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2203, 0.2624, 0.2839, 0.2333], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2142, 0.2686, 0.2865, 0.2307], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2137, 0.2934, 0.2696, 0.2233], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2521, 0.2767, 0.2477, 0.2235], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1969, 0.2866, 0.2876, 0.2289], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2348, 0.2744, 0.2533, 0.2374], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2264, 0.2833, 0.2707, 0.2196], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2704, 0.2887, 0.2429, 0.1981], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2222, 0.266 , 0.267 , 0.2448], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2245, 0.2608, 0.272 , 0.2427], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2253, 0.264 , 0.2635, 0.2473], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2371, 0.2393, 0.256 , 0.2676], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2651, 0.2607, 0.2448, 0.2294], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.171 , 0.3078, 0.3124, 0.2088], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1685, 0.3082, 0.3138, 0.2095], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1877, 0.296 , 0.2996, 0.2167], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1872, 0.3053, 0.3013, 0.2062], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2109, 0.2739, 0.2684, 0.2468], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1855, 0.2853, 0.2809, 0.2483], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1886, 0.2756, 0.2963, 0.2395], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2639, 0.2269, 0.2583, 0.2509], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2821, 0.2211, 0.2341, 0.2627], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2441, 0.21  , 0.2221, 0.3238], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2652, 0.2046, 0.213 , 0.3172], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3017, 0.1875, 0.2119, 0.2989], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2981, 0.1931, 0.1924, 0.3164], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2692, 0.1944, 0.2151, 0.3213], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2704, 0.2269, 0.2164, 0.2864], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2213, 0.2123, 0.2373, 0.3291], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3805, 0.2065, 0.1866, 0.2264], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2575, 0.2485, 0.2395, 0.2546], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3022, 0.2053, 0.2056, 0.2869], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3462, 0.2086, 0.2002, 0.245 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2114, 0.2749, 0.2736, 0.2402], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2206, 0.2645, 0.2687, 0.2461], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2313, 0.2684, 0.2666, 0.2338], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2343, 0.2666, 0.2621, 0.2369], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2405, 0.2658, 0.2553, 0.2384], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2444, 0.2567, 0.2517, 0.2473], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3113, 0.2438, 0.223 , 0.2219], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.221 , 0.2742, 0.27  , 0.2348], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2227, 0.2783, 0.2741, 0.2249], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2224, 0.2807, 0.2706, 0.2263], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2021, 0.2759, 0.2931, 0.2289], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2022, 0.2806, 0.2961, 0.2211], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3199, 0.1867, 0.1977, 0.2957], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2682, 0.1778, 0.2056, 0.3484], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2757, 0.1952, 0.2161, 0.313 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3164, 0.203 , 0.2013, 0.2794], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3033, 0.191 , 0.1956, 0.3101], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2749, 0.2082, 0.2115, 0.3053], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2049, 0.2901, 0.2935, 0.2115], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2105, 0.2951, 0.2854, 0.2091], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2009, 0.2851, 0.2851, 0.2289], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2103, 0.2802, 0.28  , 0.2294], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3739, 0.2671, 0.1778, 0.1812], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3458, 0.2631, 0.1782, 0.2129], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2609, 0.2603, 0.2361, 0.2427], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.259 , 0.2615, 0.2316, 0.2479], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.363 , 0.2713, 0.1628, 0.2029], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1827, 0.2877, 0.2943, 0.2354], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2   , 0.2884, 0.3009, 0.2107], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2199, 0.2694, 0.2659, 0.2448], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2057, 0.2754, 0.2677, 0.2512], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.206 , 0.2792, 0.2486, 0.2661], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2091, 0.2779, 0.2784, 0.2346], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2291, 0.2579, 0.25  , 0.2631], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.169 , 0.2518, 0.2708, 0.3084], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1946, 0.2938, 0.283 , 0.2286], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2262, 0.2382, 0.2597, 0.2759], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1834, 0.2331, 0.2678, 0.3157], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2309, 0.2181, 0.2249, 0.3261], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1739, 0.2469, 0.2384, 0.3408], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1354, 0.2336, 0.2352, 0.3958], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1719, 0.229 , 0.2539, 0.3452], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1902, 0.2811, 0.2766, 0.2522], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1693, 0.2562, 0.2713, 0.3032], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1794, 0.2622, 0.2823, 0.276 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2376, 0.2483, 0.2451, 0.269 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3009, 0.2149, 0.2003, 0.2838], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2995, 0.2115, 0.2106, 0.2783], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3175, 0.1918, 0.2033, 0.2874], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2604, 0.1912, 0.238 , 0.3104], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3677, 0.2162, 0.1951, 0.221 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2304, 0.2469, 0.2685, 0.2542], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1893, 0.294 , 0.294 , 0.2227], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2236, 0.2657, 0.2834, 0.2273], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2122, 0.2648, 0.2751, 0.2479], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1958, 0.2765, 0.2933, 0.2345], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3116, 0.1887, 0.2061, 0.2935], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2453, 0.2208, 0.2681, 0.2659], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2829, 0.2093, 0.2471, 0.2608], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2813, 0.2057, 0.2139, 0.2991], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2696, 0.1964, 0.1875, 0.3464], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1931, 0.2652, 0.2845, 0.2572], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.205 , 0.2714, 0.2599, 0.2637], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2017, 0.2705, 0.2687, 0.2591], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2264, 0.2394, 0.2702, 0.2641], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.232 , 0.2515, 0.2712, 0.2452], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3112, 0.2113, 0.2142, 0.2632], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2753, 0.1782, 0.236 , 0.3104], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2992, 0.1986, 0.219 , 0.2832], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2639, 0.2149, 0.2053, 0.3159], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.26  , 0.2546, 0.2452, 0.2401], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2464, 0.2721, 0.2581, 0.2234], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2481, 0.2608, 0.257 , 0.2342], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2703, 0.2413, 0.2306, 0.2579], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2161, 0.2543, 0.2737, 0.2559], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1908, 0.2566, 0.2679, 0.2848], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1962, 0.2486, 0.2672, 0.288 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2284, 0.2453, 0.2511, 0.2752], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2145, 0.2369, 0.2574, 0.2913], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2027, 0.2567, 0.2535, 0.2872], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2086, 0.2437, 0.2579, 0.2899], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2222, 0.2561, 0.2579, 0.2638], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.239 , 0.2402, 0.231 , 0.2899], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.253 , 0.2472, 0.2405, 0.2593], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2106, 0.2657, 0.2834, 0.2404], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2147, 0.2677, 0.2948, 0.2228], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2657, 0.2537, 0.2512, 0.2294], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2099, 0.2861, 0.2723, 0.2317], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2798, 0.2277, 0.2343, 0.2583], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2587, 0.2413, 0.2453, 0.2547], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2899, 0.2442, 0.2375, 0.2284], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2584, 0.2402, 0.2533, 0.2482], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2402, 0.2709, 0.2562, 0.2327], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2184, 0.289 , 0.272 , 0.2206], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2156, 0.2968, 0.2846, 0.203 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2321, 0.2674, 0.2623, 0.2381], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4378, 0.1908, 0.1535, 0.2179], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2781, 0.2339, 0.2104, 0.2776], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2563, 0.2294, 0.221 , 0.2933], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2943, 0.2176, 0.2086, 0.2795], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2898, 0.2218, 0.2116, 0.2768], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2878, 0.1975, 0.1995, 0.3153], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3647, 0.1603, 0.1662, 0.3087], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2451, 0.2564, 0.2652, 0.2333], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2203, 0.2423, 0.2665, 0.2709], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2168, 0.2704, 0.2803, 0.2326], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2486, 0.2744, 0.2603, 0.2167], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2477, 0.2762, 0.2559, 0.2202], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2476, 0.2594, 0.2649, 0.228 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.213 , 0.2559, 0.2709, 0.2602], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2182, 0.2729, 0.261 , 0.248 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2442, 0.2606, 0.2511, 0.2442], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3022, 0.2058, 0.2044, 0.2876], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2085, 0.2781, 0.2811, 0.2323], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1946, 0.291 , 0.2921, 0.2222], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2379, 0.2522, 0.2484, 0.2614], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2193, 0.2809, 0.2656, 0.2341], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2192, 0.2779, 0.2658, 0.2371], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2109, 0.2832, 0.2715, 0.2344], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2066, 0.2688, 0.2833, 0.2413], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2643, 0.2591, 0.242 , 0.2346], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2696, 0.2623, 0.2407, 0.2275], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2172, 0.2795, 0.2678, 0.2355], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2071, 0.2744, 0.29  , 0.2284], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.202 , 0.2927, 0.2831, 0.2222], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2828, 0.2396, 0.2418, 0.2357], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3955, 0.206 , 0.1413, 0.2572], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2964, 0.2039, 0.2151, 0.2846], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3028, 0.1997, 0.2103, 0.2872], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3539, 0.1888, 0.1947, 0.2626], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2131, 0.2972, 0.2832, 0.2065], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2275, 0.2765, 0.2633, 0.2328], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2462, 0.2731, 0.2576, 0.2231], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2159, 0.2058, 0.256 , 0.3223], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2549, 0.287 , 0.1964, 0.2618], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4609, 0.1844, 0.1447, 0.2101], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4092, 0.2062, 0.1789, 0.2057], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3526, 0.1854, 0.1881, 0.2739], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3378, 0.1284, 0.0895, 0.4443], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4417, 0.1122, 0.0771, 0.369 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2405, 0.1249, 0.1616, 0.4731], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1865, 0.2954, 0.2942, 0.2239], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.188 , 0.2939, 0.293 , 0.2251], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1727, 0.2942, 0.3042, 0.2289], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1931, 0.2985, 0.3003, 0.2082], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3671, 0.1963, 0.163 , 0.2736], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2982, 0.2117, 0.196 , 0.2941], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2224, 0.2033, 0.2536, 0.3207], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2114, 0.1966, 0.2321, 0.3599], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4136, 0.1937, 0.1748, 0.2179], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4359, 0.1865, 0.1635, 0.2141], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.5017, 0.1768, 0.1332, 0.1882], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3385, 0.176 , 0.1706, 0.3149], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2647, 0.2804, 0.2384, 0.2165], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2624, 0.2328, 0.2553, 0.2495], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.243 , 0.2646, 0.2634, 0.229 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2265, 0.267 , 0.2727, 0.2339], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2167, 0.2681, 0.2811, 0.234 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2227, 0.2783, 0.2825, 0.2165], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4118, 0.1762, 0.1647, 0.2473], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1939, 0.2681, 0.2931, 0.2449], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.182 , 0.2822, 0.3159, 0.22  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2183, 0.2811, 0.2809, 0.2196], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2434, 0.2568, 0.2512, 0.2486], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2423, 0.2381, 0.2627, 0.2569], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2067, 0.2943, 0.2827, 0.2163], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2011, 0.29  , 0.2858, 0.223 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.231 , 0.188 , 0.1953, 0.3858], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2551, 0.2536, 0.2694, 0.222 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1899, 0.2719, 0.296 , 0.2421], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.274 , 0.1999, 0.2063, 0.3198], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2951, 0.1982, 0.206 , 0.3007], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3302, 0.2031, 0.2048, 0.262 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3242, 0.2155, 0.2189, 0.2414], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1831, 0.2585, 0.2791, 0.2793], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1975, 0.2523, 0.2689, 0.2814], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2036, 0.2325, 0.2679, 0.296 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2114, 0.2475, 0.2696, 0.2715], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1988, 0.2666, 0.2728, 0.2618], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2323, 0.2633, 0.2734, 0.231 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2308, 0.2707, 0.2892, 0.2093], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3  , 0.249, 0.232, 0.219], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4009, 0.2378, 0.1634, 0.1979], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2308, 0.2588, 0.2498, 0.2606], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2436, 0.242 , 0.2397, 0.2748], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4249, 0.1406, 0.1404, 0.2941], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4862, 0.1258, 0.099 , 0.2889], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2712, 0.2606, 0.2371, 0.2311], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3011, 0.2514, 0.236 , 0.2115], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2563, 0.2238, 0.2641, 0.2558], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.213 , 0.2912, 0.2729, 0.223 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2301, 0.2812, 0.2694, 0.2193], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2109, 0.2934, 0.2771, 0.2186], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2113, 0.2862, 0.2774, 0.2251], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2151, 0.2765, 0.2653, 0.2432], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2378, 0.2775, 0.263 , 0.2217], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2593, 0.267 , 0.2386, 0.2351], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2389, 0.2682, 0.2561, 0.2369], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2417, 0.2437, 0.2485, 0.2661], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2493, 0.2511, 0.2439, 0.2558], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2582, 0.2485, 0.2242, 0.2692], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3292, 0.1943, 0.2058, 0.2707], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2078, 0.289 , 0.2767, 0.2265], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1977, 0.2943, 0.2801, 0.2279], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1917, 0.294 , 0.2805, 0.2338], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1941, 0.296 , 0.2814, 0.2285], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2129, 0.3019, 0.2801, 0.2051], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2632, 0.1702, 0.2204, 0.3462], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.286 , 0.2173, 0.2481, 0.2486], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3008, 0.1981, 0.19  , 0.3112], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2817, 0.2306, 0.2456, 0.242 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3909, 0.1571, 0.2065, 0.2456], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2413, 0.1461, 0.2182, 0.3945], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4062, 0.2064, 0.1426, 0.2448], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.248 , 0.2309, 0.2342, 0.2868], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3367, 0.1816, 0.1883, 0.2934], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4726, 0.1167, 0.1309, 0.2797], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2276, 0.2627, 0.2472, 0.2624], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2206, 0.2682, 0.2533, 0.2579], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2034, 0.2748, 0.2805, 0.2413], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2003, 0.2794, 0.2856, 0.2346], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2397, 0.2542, 0.2366, 0.2695], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.192 , 0.2975, 0.2913, 0.2191], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.217 , 0.2943, 0.2907, 0.198 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2489, 0.2707, 0.2509, 0.2295], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2443, 0.2469, 0.2533, 0.2555], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2081, 0.2843, 0.285 , 0.2226], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2273, 0.2751, 0.2777, 0.2199], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3062, 0.1973, 0.1964, 0.3002], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3361, 0.2231, 0.2116, 0.2292], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2655, 0.234 , 0.2605, 0.2401], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2116, 0.2981, 0.2771, 0.2133], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2004, 0.2849, 0.2788, 0.2359], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2121, 0.2893, 0.2763, 0.2222], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2004, 0.2857, 0.2966, 0.2173], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2164, 0.2962, 0.2827, 0.2047], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2441, 0.2112, 0.2427, 0.302 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2253, 0.2211, 0.2466, 0.307 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2391, 0.2334, 0.2458, 0.2817], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2542, 0.2036, 0.2373, 0.3049], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.292 , 0.1787, 0.205 , 0.3244], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3477, 0.2082, 0.1614, 0.2828], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4321, 0.2025, 0.0992, 0.2662], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1776, 0.2622, 0.2669, 0.2933], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1971, 0.2809, 0.2652, 0.2568], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1719, 0.2606, 0.2654, 0.3021], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2256, 0.2732, 0.2651, 0.2361], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2456, 0.2533, 0.2529, 0.2482], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2017, 0.2772, 0.2682, 0.2529], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2867, 0.2344, 0.2102, 0.2686], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1989, 0.2799, 0.29  , 0.2312], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2275, 0.2599, 0.2702, 0.2424], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2195, 0.2648, 0.2897, 0.226 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1886, 0.2774, 0.2963, 0.2377], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2184, 0.2837, 0.2754, 0.2225], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.204 , 0.2678, 0.2788, 0.2494], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.216 , 0.2605, 0.2696, 0.2539], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2373, 0.2596, 0.2672, 0.2358], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2367, 0.2671, 0.2637, 0.2326], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1926, 0.2909, 0.3005, 0.216 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1896, 0.2909, 0.2936, 0.226 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1935, 0.2819, 0.2945, 0.2301], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1958, 0.294 , 0.2928, 0.2174], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1872, 0.2899, 0.3046, 0.2183], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1938, 0.293 , 0.2911, 0.2221], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1892, 0.2966, 0.3015, 0.2126], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.24  , 0.2673, 0.2686, 0.2241], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2022, 0.2961, 0.2928, 0.2089], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.204 , 0.2805, 0.285 , 0.2304], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1854, 0.2907, 0.3067, 0.2172], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2452, 0.279 , 0.2626, 0.2132], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2282, 0.2743, 0.2639, 0.2337], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.291 , 0.2677, 0.2281, 0.2133], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2652, 0.2461, 0.2334, 0.2554], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2459, 0.2435, 0.2373, 0.2733], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2372, 0.2437, 0.2492, 0.2698], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2324, 0.2676, 0.2492, 0.2508], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2982, 0.2571, 0.221 , 0.2236], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2969, 0.2509, 0.2051, 0.2471], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2143, 0.2734, 0.2658, 0.2465], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2015, 0.2702, 0.2739, 0.2544], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2111, 0.2659, 0.2658, 0.2571], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2735, 0.2103, 0.2385, 0.2777], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4226, 0.1554, 0.1737, 0.2482], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4165, 0.1742, 0.1852, 0.2241], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4185, 0.1462, 0.1436, 0.2918], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3621, 0.1731, 0.1724, 0.2924], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3302, 0.1617, 0.1917, 0.3164], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3157, 0.1906, 0.1986, 0.295 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2926, 0.1959, 0.22  , 0.2914], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2868, 0.2267, 0.2355, 0.251 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2758, 0.2286, 0.239 , 0.2567], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2696, 0.2279, 0.2476, 0.2549], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.186 , 0.2706, 0.2895, 0.2539], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1794, 0.284 , 0.2827, 0.2538], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1806, 0.2714, 0.2908, 0.2572], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1816, 0.267 , 0.2884, 0.263 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1855, 0.2839, 0.2891, 0.2415], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2547, 0.2188, 0.2406, 0.286 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2913, 0.2259, 0.2413, 0.2414], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1831, 0.2743, 0.2959, 0.2467], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2182, 0.2668, 0.2764, 0.2386], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1856, 0.297 , 0.3008, 0.2166], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.228 , 0.2618, 0.256 , 0.2542], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2258, 0.274 , 0.262 , 0.2381], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2408, 0.2519, 0.2541, 0.2532], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2464, 0.2678, 0.2505, 0.2353], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2421, 0.2581, 0.2499, 0.2499], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2633, 0.2383, 0.2283, 0.27  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2693, 0.2305, 0.2401, 0.2601], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1931, 0.1682, 0.2129, 0.4258], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1795, 0.1705, 0.2141, 0.4359], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1587, 0.2144, 0.2455, 0.3814], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3032, 0.2063, 0.2044, 0.2862], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2165, 0.2412, 0.2392, 0.303 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2442, 0.2268, 0.2225, 0.3065], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2163, 0.1986, 0.2145, 0.3706], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2183, 0.1793, 0.1995, 0.4028], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.21  , 0.1963, 0.208 , 0.3857], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2101, 0.2783, 0.2884, 0.2232], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2312, 0.2753, 0.2744, 0.2191], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.253 , 0.2716, 0.2618, 0.2136], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3054, 0.1585, 0.1834, 0.3527], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2406, 0.2503, 0.2589, 0.2502], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.256 , 0.2288, 0.2158, 0.2994], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2603, 0.23  , 0.231 , 0.2787], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2769, 0.2417, 0.2399, 0.2415], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2115, 0.2857, 0.266 , 0.2368], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1841, 0.2962, 0.3068, 0.2129], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2095, 0.2903, 0.2892, 0.211 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2956, 0.241 , 0.2277, 0.2358], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2986, 0.2533, 0.2286, 0.2195], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2755, 0.2349, 0.2353, 0.2543], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2179, 0.2632, 0.2857, 0.2333], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2809, 0.2268, 0.2319, 0.2604], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3502, 0.2193, 0.2032, 0.2273], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3843, 0.1703, 0.1748, 0.2705], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3692, 0.1818, 0.1835, 0.2655], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2367, 0.2854, 0.2659, 0.2121], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1753, 0.2576, 0.2913, 0.2758], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1872, 0.2805, 0.3022, 0.2301], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2237, 0.288 , 0.2718, 0.2165], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1977, 0.2703, 0.3052, 0.2268], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2472, 0.2437, 0.2372, 0.2719], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2963, 0.2207, 0.2364, 0.2466], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2696, 0.2352, 0.2474, 0.2478], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3158, 0.2402, 0.2314, 0.2125], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2359, 0.2471, 0.2537, 0.2633], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2649, 0.248 , 0.241 , 0.2462], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2588, 0.2526, 0.2474, 0.2412], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3129, 0.2511, 0.219 , 0.217 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2209, 0.2367, 0.264 , 0.2784], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2459, 0.2351, 0.2492, 0.2697], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2254, 0.2276, 0.2494, 0.2976], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2284, 0.2261, 0.2583, 0.2873], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2732, 0.2064, 0.2088, 0.3116], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.192 , 0.2888, 0.2894, 0.2298], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1903, 0.294 , 0.294 , 0.2217], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1881, 0.2899, 0.3085, 0.2134], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1925, 0.2827, 0.2998, 0.225 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2339, 0.2842, 0.2728, 0.2091], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1838, 0.2705, 0.3051, 0.2407], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1989, 0.2676, 0.2742, 0.2594], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2041, 0.2792, 0.2931, 0.2235], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2114, 0.2699, 0.2797, 0.239 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2251, 0.2738, 0.2681, 0.233 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2886, 0.2271, 0.2485, 0.2358], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3596, 0.179 , 0.1801, 0.2813], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2054, 0.2631, 0.2993, 0.2322], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2214, 0.2666, 0.2788, 0.2332], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2193, 0.2692, 0.2784, 0.2331], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2197, 0.2625, 0.2749, 0.243 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2098, 0.2638, 0.2677, 0.2586], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2516, 0.2316, 0.237 , 0.2798], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2966, 0.2251, 0.2373, 0.241 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2962, 0.2065, 0.2586, 0.2386], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2308, 0.228 , 0.2572, 0.284 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2182, 0.2391, 0.2614, 0.2814], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2363, 0.2402, 0.2584, 0.2651], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2437, 0.2557, 0.2628, 0.2378], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2394, 0.2625, 0.2557, 0.2424], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2533, 0.2489, 0.2578, 0.2399], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.215 , 0.2533, 0.2514, 0.2803], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2532, 0.2539, 0.2457, 0.2472], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2718, 0.2218, 0.2002, 0.3062], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2529, 0.2395, 0.233 , 0.2746], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2206, 0.2586, 0.2445, 0.2763], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2119, 0.2713, 0.2516, 0.2652], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1979, 0.2889, 0.2532, 0.26  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1989, 0.2989, 0.2506, 0.2516], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2174, 0.2923, 0.2461, 0.2443], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2082, 0.2969, 0.2813, 0.2135], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2228, 0.2586, 0.2771, 0.2416], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2308, 0.2607, 0.2691, 0.2394], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1925, 0.271 , 0.2887, 0.2478], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2247, 0.2751, 0.2737, 0.2265], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.298 , 0.2453, 0.2074, 0.2493], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2116, 0.2483, 0.2574, 0.2827], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.197 , 0.2885, 0.2826, 0.2319], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2123, 0.2835, 0.2686, 0.2356], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2262, 0.2675, 0.2549, 0.2514], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2127, 0.2693, 0.2593, 0.2587], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2465, 0.2256, 0.2268, 0.3012], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2365, 0.2515, 0.2703, 0.2417], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2097, 0.2735, 0.2914, 0.2254], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1832, 0.2943, 0.2982, 0.2243], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4891, 0.1516, 0.1236, 0.2357], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3985, 0.152 , 0.1493, 0.3003], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3847, 0.1643, 0.1422, 0.3089], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3838, 0.1604, 0.1394, 0.3164], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4326, 0.1619, 0.1478, 0.2577], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2195, 0.2702, 0.2605, 0.2498], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2887, 0.2107, 0.2043, 0.2963], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.314 , 0.2419, 0.2195, 0.2246], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2365, 0.2599, 0.2592, 0.2444], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2376, 0.2644, 0.2676, 0.2304], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2536, 0.2628, 0.2514, 0.2322], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2543, 0.2445, 0.2605, 0.2407], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2513, 0.258 , 0.2488, 0.242 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2455, 0.2488, 0.2563, 0.2494], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.214 , 0.2861, 0.2755, 0.2244], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2332, 0.293 , 0.2657, 0.2081], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2521, 0.2448, 0.2531, 0.25  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2432, 0.2525, 0.2566, 0.2477], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3243, 0.2114, 0.199 , 0.2653], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2401, 0.2736, 0.2619, 0.2243], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2174, 0.2666, 0.2622, 0.2539], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2208, 0.2444, 0.2569, 0.2779], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2357, 0.2428, 0.2352, 0.2863], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2665, 0.2615, 0.241 , 0.231 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2128, 0.2487, 0.2528, 0.2857], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2509, 0.2487, 0.237 , 0.2634], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2438, 0.247 , 0.2524, 0.2568], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.198 , 0.2718, 0.2844, 0.2458], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2894, 0.228 , 0.2271, 0.2555], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2051, 0.2843, 0.2833, 0.2273], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1953, 0.2829, 0.2981, 0.2237], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2516, 0.2577, 0.2614, 0.2293], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2205, 0.2527, 0.2539, 0.2729], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2305, 0.2804, 0.2886, 0.2006], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2104, 0.261 , 0.297 , 0.2316], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2789, 0.2492, 0.2403, 0.2316], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1833, 0.3023, 0.2908, 0.2236], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1845, 0.2988, 0.2922, 0.2245], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1979, 0.2951, 0.2862, 0.2208], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2009, 0.3036, 0.2871, 0.2084], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1875, 0.2886, 0.2824, 0.2415], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1848, 0.2478, 0.2467, 0.3207], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2564, 0.2628, 0.2241, 0.2567], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4806, 0.1982, 0.1079, 0.2133], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2235, 0.2465, 0.2323, 0.2976], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2659, 0.2391, 0.2203, 0.2747], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2649, 0.2505, 0.2228, 0.2619], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1837, 0.3047, 0.2963, 0.2153], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1864, 0.3004, 0.3014, 0.2117], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2138, 0.2974, 0.2811, 0.2077], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2706, 0.2357, 0.2352, 0.2585], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2169, 0.2456, 0.2552, 0.2822], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3654, 0.2306, 0.1852, 0.2187], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2521, 0.2681, 0.2461, 0.2336], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2794, 0.2544, 0.2275, 0.2387], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2069, 0.2811, 0.2842, 0.2278], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2681, 0.2525, 0.2365, 0.2429], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2315, 0.2724, 0.2619, 0.2342], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2296, 0.2737, 0.2721, 0.2245], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2382, 0.272 , 0.2586, 0.2313], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2349, 0.2745, 0.2647, 0.2259], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2641, 0.252 , 0.2577, 0.2263], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3235, 0.2023, 0.2067, 0.2675], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2104, 0.2552, 0.2527, 0.2817], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2108, 0.279 , 0.2888, 0.2214], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2154, 0.285 , 0.2878, 0.2119], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2413, 0.282 , 0.2751, 0.2016], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2642, 0.2872, 0.2536, 0.195 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2205, 0.2807, 0.2536, 0.2452], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2916, 0.2542, 0.221 , 0.2332], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3181, 0.2104, 0.2006, 0.2709], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3094, 0.2102, 0.2037, 0.2767], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1984, 0.2735, 0.2884, 0.2396], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1969, 0.2895, 0.2878, 0.2258], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2446, 0.2581, 0.2628, 0.2345], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2315, 0.2494, 0.2652, 0.2539], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2264, 0.2539, 0.2516, 0.2681], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2277, 0.2324, 0.2463, 0.2936], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2305, 0.2466, 0.257 , 0.266 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2613, 0.2498, 0.2478, 0.2411], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2578, 0.2311, 0.2372, 0.2739], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.192 , 0.2867, 0.2842, 0.2372], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2061, 0.2733, 0.2611, 0.2594], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2279, 0.2543, 0.2402, 0.2775], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2337, 0.2889, 0.2506, 0.2268], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2372, 0.2728, 0.2392, 0.2508], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2093, 0.2778, 0.2658, 0.2471], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2358, 0.2387, 0.2404, 0.285 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2327, 0.2484, 0.2436, 0.2752], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2207, 0.249 , 0.2516, 0.2787], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2161, 0.2638, 0.2578, 0.2623], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2232, 0.2531, 0.2603, 0.2634], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2686, 0.2274, 0.2173, 0.2867], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2963, 0.2378, 0.2191, 0.2469], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.211 , 0.2806, 0.2703, 0.238 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2358, 0.1737, 0.1887, 0.4017], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2654, 0.1925, 0.2171, 0.325 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2111, 0.263 , 0.281 , 0.2449], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2189, 0.2671, 0.2866, 0.2274], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.22  , 0.2764, 0.2881, 0.2155], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2533, 0.2517, 0.236 , 0.2589], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2981, 0.2325, 0.214 , 0.2554], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2355, 0.2508, 0.2471, 0.2666], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2663, 0.2374, 0.2255, 0.2709], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3049, 0.2187, 0.2321, 0.2443], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2633, 0.201 , 0.2068, 0.329 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3139, 0.2162, 0.2076, 0.2623], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2639, 0.2842, 0.2591, 0.1928], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2628, 0.257 , 0.2417, 0.2385], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2161, 0.2688, 0.2731, 0.242 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.173 , 0.2624, 0.3068, 0.2578], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2307, 0.251 , 0.2538, 0.2645], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2202, 0.251 , 0.2687, 0.2602], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2456, 0.2578, 0.2528, 0.2438], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2135, 0.2467, 0.2498, 0.2899], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.24  , 0.2365, 0.2276, 0.2959], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2588, 0.2257, 0.2286, 0.2869], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2706, 0.1912, 0.218 , 0.3202], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4208, 0.1362, 0.1381, 0.3049], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3871, 0.2733, 0.1772, 0.1624], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2285, 0.2497, 0.2457, 0.2762], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2616, 0.2445, 0.2376, 0.2563], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1717, 0.2996, 0.2986, 0.2301], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1867, 0.2996, 0.2981, 0.2156], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2417, 0.2193, 0.2422, 0.2968], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3689, 0.1969, 0.1877, 0.2464], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2098, 0.259 , 0.252 , 0.2792], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.26  , 0.2681, 0.2407, 0.2311], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.266 , 0.2345, 0.2191, 0.2804], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4192, 0.1531, 0.1572, 0.2705], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3384, 0.186 , 0.2022, 0.2734], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3686, 0.1892, 0.2011, 0.2411], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3282, 0.189 , 0.2207, 0.2621], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2251, 0.2762, 0.2686, 0.2301], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2149, 0.2594, 0.2578, 0.2679], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.285, 0.265, 0.257, 0.193], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2529, 0.2485, 0.2648, 0.2338], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2265, 0.2643, 0.264 , 0.2453], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.217 , 0.2642, 0.2655, 0.2533], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2218, 0.2749, 0.2664, 0.2369], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1999, 0.2938, 0.287 , 0.2193], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1942, 0.2885, 0.2871, 0.2303], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2025, 0.2879, 0.2862, 0.2234], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1977, 0.2893, 0.2908, 0.2222], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2528, 0.2244, 0.2563, 0.2664], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2843, 0.2196, 0.232 , 0.2641], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1955, 0.2748, 0.2714, 0.2582], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1942, 0.2693, 0.28  , 0.2565], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2012, 0.2868, 0.2634, 0.2486], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2311, 0.2684, 0.2426, 0.2579], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2374, 0.2595, 0.2647, 0.2383], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2554, 0.2403, 0.2478, 0.2565], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3116, 0.2259, 0.22  , 0.2425], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3407, 0.2409, 0.192 , 0.2263], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2723, 0.2306, 0.2324, 0.2647], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2709, 0.2241, 0.2287, 0.2763], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2112, 0.2658, 0.272 , 0.251 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2342, 0.2528, 0.2464, 0.2665], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2477, 0.2407, 0.256 , 0.2557], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2119, 0.2241, 0.2651, 0.2989], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2182, 0.2375, 0.2689, 0.2754], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1868, 0.2222, 0.2509, 0.3401], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2102, 0.2281, 0.2404, 0.3212], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.253 , 0.2516, 0.2531, 0.2423], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2153, 0.2265, 0.2662, 0.2921], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1936, 0.2429, 0.252 , 0.3115], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2273, 0.2804, 0.2658, 0.2265], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2335, 0.2739, 0.2643, 0.2282], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2279, 0.2685, 0.2569, 0.2467], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2555, 0.2679, 0.2452, 0.2314], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2639, 0.2383, 0.2445, 0.2534], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1906, 0.2902, 0.2874, 0.2318], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1871, 0.2835, 0.3032, 0.2263], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1837, 0.2907, 0.3067, 0.2189], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2719, 0.2305, 0.244 , 0.2535], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3235, 0.2139, 0.2099, 0.2527], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2008, 0.2886, 0.294 , 0.2166], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2242, 0.2574, 0.2799, 0.2385], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2328, 0.2678, 0.259 , 0.2404], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2057, 0.2571, 0.2816, 0.2556], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1751, 0.2857, 0.3115, 0.2277], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1995, 0.2859, 0.278 , 0.2366], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2254, 0.2469, 0.2546, 0.273 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2907, 0.2145, 0.2149, 0.2799], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1772, 0.2918, 0.305 , 0.226 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2054, 0.291 , 0.2855, 0.2181], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1927, 0.2915, 0.2911, 0.2247], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2334, 0.2473, 0.2582, 0.2611], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2122, 0.2435, 0.2616, 0.2827], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2037, 0.2564, 0.275 , 0.2649], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2251, 0.2501, 0.2565, 0.2683], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2721, 0.2478, 0.2082, 0.2719], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1934, 0.2565, 0.247 , 0.3031], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2125, 0.2651, 0.2378, 0.2846], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2097, 0.2469, 0.2498, 0.2936], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2257, 0.2275, 0.2432, 0.3037], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2375, 0.2523, 0.2516, 0.2587], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2269, 0.2632, 0.2678, 0.242 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.325 , 0.2198, 0.1971, 0.2581], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2534, 0.189 , 0.2156, 0.342 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2701, 0.1938, 0.1973, 0.3389], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2464, 0.2094, 0.2177, 0.3265], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3146, 0.1991, 0.1677, 0.3186], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.223 , 0.1984, 0.1987, 0.3799], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3272, 0.1688, 0.1783, 0.3257], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2647, 0.2453, 0.235 , 0.255 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2048, 0.2832, 0.2865, 0.2255], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2361, 0.2485, 0.2556, 0.2599], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2554, 0.2571, 0.2485, 0.2389], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2415, 0.2475, 0.2357, 0.2752], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.247 , 0.2563, 0.2486, 0.2481], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2275, 0.2517, 0.2506, 0.2702], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2308, 0.2496, 0.2589, 0.2607], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2206, 0.258 , 0.2561, 0.2653], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2843, 0.2136, 0.2134, 0.2887], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3118, 0.1972, 0.2099, 0.2811], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.5854, 0.1605, 0.1028, 0.1513], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2435, 0.2031, 0.2219, 0.3315], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1831, 0.2806, 0.3106, 0.2256], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2873, 0.2243, 0.2275, 0.2608], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2008, 0.1872, 0.2291, 0.383 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1999, 0.2083, 0.2445, 0.3473], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2165, 0.2193, 0.232 , 0.3322], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2397, 0.2342, 0.2394, 0.2868], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3064, 0.2169, 0.2222, 0.2544], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2978, 0.2219, 0.2089, 0.2714], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.29  , 0.2338, 0.2319, 0.2443], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2395, 0.2542, 0.2694, 0.2369], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2713, 0.2463, 0.2382, 0.2442], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2394, 0.2284, 0.2427, 0.2895], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.271 , 0.2345, 0.2256, 0.269 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2736, 0.2296, 0.237 , 0.2599], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3694, 0.1754, 0.1752, 0.28  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2993, 0.2291, 0.2334, 0.2381], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2675, 0.2202, 0.2445, 0.2677], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2315, 0.2555, 0.2582, 0.2548], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2143, 0.2475, 0.2653, 0.2729], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2131, 0.256 , 0.267 , 0.2639], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2159, 0.2687, 0.2762, 0.2391], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2948, 0.2322, 0.2308, 0.2423], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1932, 0.2863, 0.3023, 0.2182], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2528, 0.2237, 0.2223, 0.3011], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2525, 0.2134, 0.2353, 0.2987], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2339, 0.231 , 0.229 , 0.3062], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2678, 0.2647, 0.2372, 0.2302], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2433, 0.2464, 0.2094, 0.301 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4295, 0.2317, 0.1365, 0.2023], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.186 , 0.2839, 0.2973, 0.2329], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2179, 0.2672, 0.2704, 0.2444], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2213, 0.2663, 0.2783, 0.2342], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2084, 0.2704, 0.2967, 0.2245], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2281, 0.2764, 0.2734, 0.2221], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2223, 0.2734, 0.2738, 0.2305], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2338, 0.2602, 0.2678, 0.2382], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.219 , 0.2679, 0.2888, 0.2243], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2329, 0.2781, 0.2677, 0.2212], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2312, 0.2626, 0.263 , 0.2432], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2355, 0.2556, 0.2732, 0.2357], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.5132, 0.1742, 0.124 , 0.1886], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2098, 0.1945, 0.2358, 0.3599], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2733, 0.1851, 0.1672, 0.3745], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2464, 0.2075, 0.2218, 0.3244], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2658, 0.1739, 0.1984, 0.3619], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2072, 0.2105, 0.2291, 0.3533], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.293 , 0.1907, 0.2032, 0.3131], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3509, 0.1691, 0.1929, 0.2871], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1914, 0.2862, 0.298 , 0.2245], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2134, 0.2785, 0.2727, 0.2355], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1946, 0.2851, 0.2799, 0.2405], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2444, 0.2559, 0.2487, 0.2511], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.215 , 0.267 , 0.27  , 0.2479], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2288, 0.2545, 0.2719, 0.2448], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1907, 0.245 , 0.2637, 0.3006], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1833, 0.2242, 0.2445, 0.348 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3465, 0.1793, 0.1877, 0.2865], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4903, 0.0988, 0.1207, 0.2902], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1806, 0.2517, 0.3007, 0.2671], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2327, 0.2474, 0.2553, 0.2647], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.174 , 0.3041, 0.3062, 0.2157], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1852, 0.2944, 0.2987, 0.2217], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1821, 0.3045, 0.306 , 0.2073], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.176 , 0.2915, 0.3114, 0.2211], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1732, 0.2974, 0.3017, 0.2278], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1716, 0.3025, 0.3074, 0.2185], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2464, 0.2634, 0.2531, 0.2372], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1988, 0.2742, 0.2876, 0.2394], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.249 , 0.2428, 0.2605, 0.2476], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2263, 0.2619, 0.2651, 0.2467], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2702, 0.2267, 0.2318, 0.2713], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2746, 0.251 , 0.2325, 0.2419], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.271 , 0.2331, 0.2173, 0.2787], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3202, 0.2282, 0.2103, 0.2412], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2371, 0.2388, 0.2481, 0.276 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2823, 0.2201, 0.2196, 0.278 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2662, 0.2456, 0.232 , 0.2562], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.348 , 0.257 , 0.2084, 0.1866], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2712, 0.2681, 0.2459, 0.2148], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2523, 0.2768, 0.2579, 0.213 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2709, 0.2671, 0.2353, 0.2268], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2637, 0.2658, 0.2505, 0.2199], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2044, 0.2343, 0.2787, 0.2826], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2996, 0.2254, 0.2087, 0.2662], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2807, 0.207 , 0.235 , 0.2773], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3169, 0.2244, 0.2197, 0.239 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2558, 0.2423, 0.2409, 0.261 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2069, 0.2316, 0.2675, 0.294 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2926, 0.2041, 0.2131, 0.2902], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2948, 0.2182, 0.2192, 0.2678], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.17  , 0.3094, 0.2993, 0.2213], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1798, 0.3098, 0.2962, 0.2143], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1761, 0.2951, 0.2941, 0.2346], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1704, 0.2981, 0.3033, 0.2281], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2053, 0.308 , 0.2851, 0.2016], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2128, 0.284 , 0.2654, 0.2379], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2019, 0.2778, 0.2798, 0.2405], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2016, 0.2875, 0.2924, 0.2184], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2212, 0.2788, 0.2787, 0.2213], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1927, 0.2948, 0.2974, 0.2151], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2289, 0.272 , 0.2666, 0.2324], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2042, 0.2923, 0.2846, 0.2189], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2381, 0.2041, 0.2201, 0.3377], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3199, 0.2283, 0.2134, 0.2384], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3636, 0.2177, 0.1878, 0.2309], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5662, 0.1489, 0.0853, 0.1995], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3353, 0.132 , 0.1558, 0.3769], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2715, 0.1509, 0.1848, 0.3929], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2357, 0.2446, 0.2614, 0.2583], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.319 , 0.2141, 0.2021, 0.2647], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1735, 0.2572, 0.3113, 0.258 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2587, 0.201 , 0.2269, 0.3134], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2581, 0.218 , 0.2359, 0.288 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.193 , 0.1973, 0.2585, 0.3512], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2329, 0.1933, 0.2435, 0.3303], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2509, 0.1944, 0.2395, 0.3152], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2942, 0.1694, 0.1827, 0.3536], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3391, 0.1853, 0.1877, 0.2879], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3301, 0.1802, 0.1827, 0.307 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3281, 0.1928, 0.1974, 0.2817], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3349, 0.1956, 0.1949, 0.2745], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2236, 0.278 , 0.277 , 0.2215], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.5428, 0.1886, 0.1187, 0.15  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.216 , 0.2675, 0.2587, 0.2579], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2336, 0.2731, 0.2616, 0.2317], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.245 , 0.2553, 0.2405, 0.2591], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2399, 0.2487, 0.2627, 0.2487], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4813, 0.1447, 0.1307, 0.2433], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1797, 0.2635, 0.2901, 0.2667], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1694, 0.2723, 0.2924, 0.2659], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1755, 0.2621, 0.2888, 0.2736], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1848, 0.2575, 0.2852, 0.2725], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1708, 0.2656, 0.2972, 0.2664], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2596, 0.2401, 0.2331, 0.2672], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.21  , 0.2681, 0.2743, 0.2476], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1773, 0.2559, 0.2911, 0.2756], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4273, 0.2379, 0.1576, 0.1772], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2793, 0.2261, 0.2143, 0.2804], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3954, 0.2127, 0.1599, 0.2319], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5844, 0.1505, 0.0775, 0.1876], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5808, 0.1571, 0.0792, 0.183 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.6879, 0.0775, 0.0601, 0.1745], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2471, 0.2488, 0.27  , 0.234 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2029, 0.2698, 0.2656, 0.2616], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2059, 0.2647, 0.2793, 0.25  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2466, 0.2553, 0.2425, 0.2556], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2289, 0.2617, 0.2418, 0.2675], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2269, 0.2588, 0.2466, 0.2677], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2668, 0.2531, 0.2326, 0.2475], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2596, 0.2473, 0.2598, 0.2333], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2132, 0.2726, 0.2934, 0.2208], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3234, 0.1647, 0.1716, 0.3403], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1842, 0.2962, 0.2899, 0.2297], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2019, 0.2889, 0.2955, 0.2137], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2725, 0.254 , 0.2461, 0.2273], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2239, 0.23  , 0.2706, 0.2754], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2466, 0.2569, 0.2575, 0.239 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.241 , 0.248 , 0.2655, 0.2456], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2343, 0.2476, 0.2725, 0.2456], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2591, 0.2455, 0.2574, 0.238 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.231 , 0.2337, 0.2534, 0.282 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2465, 0.2502, 0.2509, 0.2524], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2555, 0.2423, 0.245 , 0.2572], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2203, 0.2462, 0.2722, 0.2613], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2085, 0.251 , 0.2663, 0.2742], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2303, 0.2448, 0.2436, 0.2814], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2435, 0.2624, 0.2633, 0.2308], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2282, 0.2507, 0.2619, 0.2592], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2649, 0.238 , 0.2442, 0.2529], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.253 , 0.2531, 0.2642, 0.2297], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2326, 0.2599, 0.2764, 0.2311], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2313, 0.2435, 0.2624, 0.2628], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2739, 0.2415, 0.2347, 0.2499], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2478, 0.2704, 0.281 , 0.2008], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2207, 0.2728, 0.2774, 0.2292], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2147, 0.2694, 0.2818, 0.2341], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2377, 0.2724, 0.2713, 0.2186], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2944, 0.2454, 0.2067, 0.2536], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2115, 0.2409, 0.2401, 0.3075], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3429, 0.2292, 0.1955, 0.2324], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5169, 0.1611, 0.1055, 0.2164], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2818, 0.2215, 0.2107, 0.286 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3667, 0.2414, 0.2015, 0.1905], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2665, 0.2196, 0.2156, 0.2983], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.312 , 0.2562, 0.2303, 0.2015], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3189, 0.2222, 0.1898, 0.2691], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1893, 0.2608, 0.2819, 0.268 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1981, 0.2801, 0.2954, 0.2263], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1962, 0.2895, 0.297 , 0.2172], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1903, 0.2904, 0.3006, 0.2187], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2419, 0.2611, 0.2704, 0.2266], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2256, 0.2747, 0.281 , 0.2187], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3078, 0.2008, 0.2016, 0.2898], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1881, 0.1902, 0.2553, 0.3664], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2175, 0.2841, 0.2602, 0.2382], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2215, 0.2671, 0.256 , 0.2554], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2861, 0.2158, 0.2063, 0.2918], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3089, 0.2509, 0.2344, 0.2058], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3542, 0.1882, 0.159 , 0.2985], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.5222, 0.1381, 0.0916, 0.2481], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.23  , 0.2433, 0.2582, 0.2685], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3031, 0.2092, 0.2235, 0.2642], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3226, 0.2269, 0.2169, 0.2335], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.262 , 0.2532, 0.2453, 0.2394], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2406, 0.2402, 0.2448, 0.2745], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1799, 0.3077, 0.2946, 0.2178], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1875, 0.3056, 0.294 , 0.2129], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1968, 0.3028, 0.2779, 0.2224], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.195 , 0.29  , 0.2838, 0.2312], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1898, 0.3023, 0.287 , 0.2208], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2283, 0.2881, 0.2686, 0.215 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2285, 0.2433, 0.2807, 0.2474], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2105, 0.2729, 0.2975, 0.2192], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2255, 0.2728, 0.2817, 0.22  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1967, 0.2519, 0.2916, 0.2598], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2886, 0.2522, 0.2474, 0.2118], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3279, 0.2511, 0.2171, 0.204 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4585, 0.2099, 0.1608, 0.1709], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2161, 0.2796, 0.2661, 0.2381], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2859, 0.2523, 0.2422, 0.2196], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2026, 0.2654, 0.2535, 0.2785], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2845, 0.2116, 0.2137, 0.2902], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3121, 0.1892, 0.2128, 0.2859], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2905, 0.2033, 0.2153, 0.2908], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2036, 0.1978, 0.1946, 0.404 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1979, 0.2152, 0.2148, 0.3722], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2308, 0.2036, 0.2003, 0.3653], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2223, 0.2067, 0.2082, 0.3629], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2003, 0.2548, 0.2784, 0.2664], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2088, 0.2301, 0.2614, 0.2997], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1767, 0.2385, 0.2728, 0.312 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1813, 0.2472, 0.2782, 0.2933], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2184, 0.2463, 0.2578, 0.2774], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2396, 0.2561, 0.2485, 0.2558], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1954, 0.2719, 0.2761, 0.2566], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2057, 0.2828, 0.2828, 0.2287], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1819, 0.2942, 0.2851, 0.2388], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1799, 0.3077, 0.2808, 0.2316], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1732, 0.3042, 0.2818, 0.2408], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1694, 0.3052, 0.2746, 0.2509], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1673, 0.3037, 0.278 , 0.251 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1783, 0.3111, 0.2748, 0.2357], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1701, 0.3044, 0.286 , 0.2395], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1982, 0.2803, 0.293 , 0.2284], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2002, 0.2794, 0.2965, 0.2238], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2099, 0.2793, 0.2829, 0.228 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2214, 0.2738, 0.2823, 0.2225], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2374, 0.2401, 0.2627, 0.2597], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2185, 0.2426, 0.2539, 0.2849], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2393, 0.2581, 0.2508, 0.2519], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2576, 0.2472, 0.2443, 0.2509], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2729, 0.2529, 0.2333, 0.2408], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2928, 0.2199, 0.2243, 0.263 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.326 , 0.24  , 0.2116, 0.2223], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1855, 0.3059, 0.3013, 0.2073], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1989, 0.2947, 0.2954, 0.2109], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2397, 0.2908, 0.2695, 0.2   ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1751, 0.2766, 0.3133, 0.2351], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2032, 0.2693, 0.2808, 0.2467], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1824, 0.2805, 0.2884, 0.2487], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2312, 0.2693, 0.2591, 0.2404], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2256, 0.2532, 0.2603, 0.2609], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2114, 0.2538, 0.2528, 0.282 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2309, 0.2516, 0.2441, 0.2734], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2324, 0.2597, 0.2521, 0.2559], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2884, 0.2365, 0.2363, 0.2388], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2206, 0.2628, 0.262 , 0.2546], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1991, 0.2626, 0.2745, 0.2637], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2266, 0.244 , 0.2462, 0.2832], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2056, 0.2282, 0.2442, 0.3219], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2118, 0.2533, 0.2578, 0.2772], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2197, 0.2395, 0.243 , 0.2979], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2639, 0.2363, 0.2356, 0.2642], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2232, 0.2413, 0.255 , 0.2805], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3249, 0.1873, 0.1879, 0.2998], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1704, 0.2998, 0.3084, 0.2214], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1851, 0.3009, 0.2941, 0.2198], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1802, 0.3041, 0.2975, 0.2182], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1696, 0.307 , 0.3065, 0.2169], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1879, 0.2968, 0.2896, 0.2256], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2183, 0.2862, 0.2729, 0.2226], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1901, 0.3011, 0.3011, 0.2078], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1748, 0.3008, 0.2971, 0.2272], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1917, 0.2936, 0.2847, 0.2299], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2104, 0.194 , 0.2245, 0.3711], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.213 , 0.235 , 0.2656, 0.2864], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2287, 0.249 , 0.2643, 0.2581], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3603, 0.2444, 0.2041, 0.1911], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4513, 0.1873, 0.1518, 0.2096], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3941, 0.1961, 0.1592, 0.2507], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4198, 0.1393, 0.1265, 0.3144], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5727, 0.1147, 0.076 , 0.2365], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2422, 0.2339, 0.2413, 0.2825], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2258, 0.2457, 0.2426, 0.2858], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2383, 0.2761, 0.2611, 0.2245], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2864, 0.2309, 0.1936, 0.2891], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2809, 0.2636, 0.2097, 0.2457], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2222, 0.2143, 0.1894, 0.374 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.225 , 0.1822, 0.1593, 0.4335], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2315, 0.2097, 0.1971, 0.3617], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2076, 0.2784, 0.2859, 0.2281], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1936, 0.2831, 0.2949, 0.2284], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2492, 0.2664, 0.2735, 0.211 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1874, 0.2621, 0.294 , 0.2566], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2346, 0.2932, 0.2575, 0.2147], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2241, 0.2855, 0.2586, 0.2318], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2421, 0.1415, 0.1627, 0.4537], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3525, 0.1419, 0.1603, 0.3453], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3468, 0.1902, 0.1695, 0.2936], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2396, 0.1987, 0.2318, 0.33  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2679, 0.1945, 0.2311, 0.3065], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2987, 0.1983, 0.2205, 0.2825], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2971, 0.1904, 0.2199, 0.2926], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3314, 0.1775, 0.1882, 0.3029], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3387, 0.1874, 0.1927, 0.2812], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3763, 0.1685, 0.1411, 0.3141], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3705, 0.226 , 0.1804, 0.2232], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2387, 0.26  , 0.2664, 0.2349], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2684, 0.2367, 0.2376, 0.2574], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1796, 0.3016, 0.3036, 0.2152], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.18  , 0.3026, 0.2999, 0.2176], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1674, 0.2915, 0.3031, 0.238 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1948, 0.2924, 0.2938, 0.219 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.181 , 0.3024, 0.3006, 0.216 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1708, 0.3073, 0.305 , 0.2169], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1671, 0.3047, 0.3147, 0.2135], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.177 , 0.3071, 0.3124, 0.2034], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1925, 0.2912, 0.2941, 0.2222], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1911, 0.2825, 0.3056, 0.2207], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2433, 0.254 , 0.2408, 0.2619], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2462, 0.2668, 0.2364, 0.2506], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2689, 0.2388, 0.2169, 0.2754], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2737, 0.2444, 0.2193, 0.2626], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2507, 0.2394, 0.2488, 0.2611], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1786, 0.2389, 0.2678, 0.3147], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2248, 0.2481, 0.2614, 0.2657], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2323, 0.2533, 0.2529, 0.2615], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3063, 0.2067, 0.1927, 0.2942], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3046, 0.2254, 0.2142, 0.2558], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1766, 0.234 , 0.2687, 0.3207], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.248 , 0.2695, 0.2502, 0.2322], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2099, 0.2482, 0.2593, 0.2826], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2428, 0.2459, 0.2599, 0.2514], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1848, 0.2937, 0.3128, 0.2086], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2645, 0.2073, 0.2496, 0.2786], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2213, 0.2755, 0.2661, 0.237 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2928, 0.2786, 0.2124, 0.2163], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1232, 0.2889, 0.3083, 0.2796], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1296, 0.1667, 0.1753, 0.5284], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1328, 0.1634, 0.1831, 0.5207], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1375, 0.1767, 0.1824, 0.5034], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2068, 0.2655, 0.2871, 0.2405], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2023, 0.2775, 0.2921, 0.228 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2213, 0.2686, 0.2896, 0.2205], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2069, 0.2798, 0.2998, 0.2135], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2063, 0.2769, 0.293 , 0.2239], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.262 , 0.2352, 0.2243, 0.2785], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2646, 0.1566, 0.2063, 0.3726], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3486, 0.2147, 0.2009, 0.2359], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2251, 0.2527, 0.2706, 0.2515], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2059, 0.2926, 0.2955, 0.206 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2479, 0.253 , 0.2493, 0.2498], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2209, 0.2678, 0.2747, 0.2366], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1992, 0.2789, 0.2937, 0.2281], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.194 , 0.2906, 0.2853, 0.2302], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.211 , 0.2779, 0.2695, 0.2416], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2514, 0.2423, 0.2372, 0.2692], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2924, 0.2516, 0.2282, 0.2278], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1974, 0.2832, 0.2795, 0.2399], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2085, 0.2789, 0.2681, 0.2445], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2001, 0.269 , 0.2799, 0.2511], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2089, 0.2696, 0.2711, 0.2504], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.207 , 0.272 , 0.2682, 0.2528], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2019, 0.2694, 0.2674, 0.2612], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2285, 0.2499, 0.254 , 0.2676], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2129, 0.1797, 0.1879, 0.4195], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1919, 0.2196, 0.2211, 0.3673], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2149, 0.2625, 0.2868, 0.2358], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2478, 0.2622, 0.2439, 0.246 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2465, 0.251 , 0.2588, 0.2437], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3401, 0.2267, 0.2015, 0.2317], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2722, 0.2182, 0.2106, 0.299 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2902, 0.2327, 0.2164, 0.2607], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2292, 0.2604, 0.2728, 0.2377], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2092, 0.279 , 0.2859, 0.2259], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3134, 0.2467, 0.2221, 0.2178], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.221 , 0.2763, 0.2872, 0.2155], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2118, 0.2661, 0.2782, 0.2438], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2276, 0.252 , 0.2655, 0.255 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.222 , 0.25  , 0.2608, 0.2672], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2364, 0.2564, 0.2642, 0.243 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2149, 0.2658, 0.278 , 0.2413], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2119, 0.2597, 0.2644, 0.2639], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3115, 0.2146, 0.1814, 0.2925], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2173, 0.2763, 0.2647, 0.2417], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.219 , 0.2653, 0.2788, 0.237 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.25  , 0.2722, 0.2313, 0.2464], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2174, 0.2226, 0.2535, 0.3066], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2631, 0.2081, 0.2161, 0.3127], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2239, 0.2014, 0.2376, 0.3371], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2262, 0.206 , 0.21  , 0.3579], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1913, 0.2078, 0.2524, 0.3485], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2298, 0.2261, 0.2152, 0.3289], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2023, 0.2324, 0.2347, 0.3306], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2303, 0.2381, 0.236 , 0.2955], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2802, 0.2324, 0.216 , 0.2714], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3151, 0.2136, 0.1861, 0.2852], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3246, 0.2096, 0.1977, 0.2681], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2918, 0.1575, 0.1673, 0.3834], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3625, 0.1565, 0.1518, 0.3292], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3392, 0.1378, 0.1564, 0.3666], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.203 , 0.2736, 0.2791, 0.2443], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2302, 0.2655, 0.2814, 0.2228], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2415, 0.2633, 0.2628, 0.2324], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2317, 0.2312, 0.2339, 0.3032], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1947, 0.2973, 0.3025, 0.2055], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1979, 0.2919, 0.2968, 0.2134], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3821, 0.1488, 0.1527, 0.3164], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3672, 0.1535, 0.1478, 0.3315], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3394, 0.161 , 0.1533, 0.3463], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.376 , 0.1564, 0.1471, 0.3205], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3669, 0.1622, 0.1369, 0.334 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2278, 0.2648, 0.2547, 0.2527], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2416, 0.2113, 0.2096, 0.3375], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2689, 0.239 , 0.2218, 0.2703], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.276 , 0.2515, 0.2165, 0.2559], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.24  , 0.2644, 0.2597, 0.2358], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2636, 0.2752, 0.2573, 0.2039], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4069, 0.2342, 0.2038, 0.155 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2305, 0.2643, 0.2677, 0.2375], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2338, 0.2659, 0.278 , 0.2222], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1999, 0.2853, 0.2991, 0.2157], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1988, 0.2944, 0.2987, 0.2081], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2313, 0.2867, 0.2858, 0.1963], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3085, 0.2471, 0.2361, 0.2084], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2712, 0.2369, 0.2329, 0.2591], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2526, 0.2452, 0.2479, 0.2544], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2386, 0.2502, 0.2341, 0.2771], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2272, 0.2638, 0.2672, 0.2418], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2266, 0.2568, 0.2675, 0.2491], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2373, 0.2711, 0.2694, 0.2222], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2425, 0.2559, 0.2488, 0.2528], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2811, 0.2307, 0.2309, 0.2573], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3103, 0.2138, 0.2157, 0.2602], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3097, 0.2239, 0.2185, 0.248 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.344 , 0.2369, 0.222 , 0.197 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1973, 0.281 , 0.2853, 0.2363], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2197, 0.2756, 0.2715, 0.2332], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1996, 0.2803, 0.284 , 0.2361], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2483, 0.2521, 0.2536, 0.246 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2487, 0.2506, 0.247 , 0.2537], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.271 , 0.2444, 0.243 , 0.2416], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2662, 0.2477, 0.241 , 0.2451], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2808, 0.2124, 0.2133, 0.2936], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2413, 0.2452, 0.246 , 0.2675], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2535, 0.2271, 0.2286, 0.2908], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3171, 0.143 , 0.1688, 0.3711], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2699, 0.1598, 0.1931, 0.3772], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2579, 0.198 , 0.2296, 0.3145], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2872, 0.1963, 0.227 , 0.2894], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.271 , 0.185 , 0.2107, 0.3333], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.248 , 0.2156, 0.2213, 0.3151], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3071, 0.251 , 0.2337, 0.2082], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5628, 0.1636, 0.1201, 0.1534], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2034, 0.2411, 0.2544, 0.3011], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.285 , 0.2277, 0.2367, 0.2505], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1963, 0.2974, 0.2894, 0.2168], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.218 , 0.2762, 0.2693, 0.2365], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2062, 0.2929, 0.2757, 0.2252], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4046, 0.1241, 0.1373, 0.334 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3518, 0.2392, 0.1987, 0.2102], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2128, 0.2956, 0.2976, 0.1941], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2073, 0.2933, 0.292 , 0.2075], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2228, 0.2683, 0.2694, 0.2395], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2136, 0.2827, 0.2808, 0.2229], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2235, 0.2163, 0.2501, 0.3101], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2273, 0.2736, 0.2651, 0.2341], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2725, 0.2424, 0.2366, 0.2486], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2535, 0.2524, 0.2649, 0.2292], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2112, 0.2718, 0.286 , 0.231 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2056, 0.2757, 0.2786, 0.2402], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1903, 0.2711, 0.2682, 0.2704], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.207 , 0.2853, 0.2674, 0.2402], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1935, 0.2753, 0.2693, 0.2619], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1883, 0.2795, 0.2752, 0.2569], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2441, 0.2521, 0.2476, 0.2561], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2506, 0.2308, 0.2244, 0.2941], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2881, 0.2115, 0.222 , 0.2784], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2662, 0.1856, 0.1911, 0.3571], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2616, 0.188 , 0.2007, 0.3497], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2582, 0.1822, 0.2002, 0.3594], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2706, 0.1796, 0.1859, 0.3638], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.5527, 0.1453, 0.1028, 0.1992], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.5319, 0.1471, 0.0834, 0.2376], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.223 , 0.252 , 0.2766, 0.2484], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2216, 0.2588, 0.2805, 0.2391], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4066, 0.1382, 0.1421, 0.3131], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3608, 0.1545, 0.1656, 0.3191], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2741, 0.2278, 0.2467, 0.2514], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2984, 0.2011, 0.2096, 0.2909], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2832, 0.2425, 0.2399, 0.2344], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2447, 0.2526, 0.2584, 0.2443], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2117, 0.2729, 0.2803, 0.2352], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2139, 0.2684, 0.2781, 0.2396], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2067, 0.2662, 0.283 , 0.2442], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2167, 0.2718, 0.2883, 0.2232], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2667, 0.2513, 0.2588, 0.2233], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.6862, 0.0701, 0.0509, 0.1928], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.6175, 0.113 , 0.0767, 0.1928], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1858, 0.2866, 0.2859, 0.2416], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2268, 0.2656, 0.2688, 0.2387], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2892, 0.2468, 0.2302, 0.2338], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2807, 0.2205, 0.2201, 0.2787], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.241 , 0.2188, 0.2164, 0.3239], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1965, 0.2851, 0.2916, 0.2268], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.193 , 0.2886, 0.2961, 0.2223], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2329, 0.2515, 0.2381, 0.2775], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2235, 0.2452, 0.2436, 0.2877], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2239, 0.2605, 0.248 , 0.2677], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2345, 0.2538, 0.2613, 0.2504], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3286, 0.2425, 0.1964, 0.2325], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1896, 0.2415, 0.2549, 0.3141], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2037, 0.2677, 0.2752, 0.2534], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2471, 0.2495, 0.2586, 0.2448], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2397, 0.229 , 0.2809, 0.2505], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2795, 0.2425, 0.2383, 0.2396], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.201 , 0.2658, 0.2862, 0.247 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2746, 0.1831, 0.2165, 0.3258], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3184, 0.2494, 0.213 , 0.2192], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3164, 0.258 , 0.2097, 0.2159], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2662, 0.2536, 0.2099, 0.2703], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1897, 0.3002, 0.3022, 0.2078], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2835, 0.215 , 0.2215, 0.2801], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1954, 0.277 , 0.2882, 0.2393], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.279 , 0.2269, 0.2243, 0.2699], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3325, 0.2102, 0.2038, 0.2534], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3634, 0.177 , 0.138 , 0.3216], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4748, 0.1499, 0.1034, 0.2719], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2618, 0.2361, 0.2503, 0.2519], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2739, 0.2085, 0.2335, 0.284 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.248 , 0.2279, 0.2302, 0.2939], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2028, 0.2776, 0.2995, 0.2201], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2521, 0.2364, 0.2453, 0.2661], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2016, 0.2511, 0.2749, 0.2724], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.336 , 0.2166, 0.1812, 0.2662], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3018, 0.2202, 0.2188, 0.2592], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2798, 0.2359, 0.2281, 0.2563], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.227 , 0.2397, 0.2576, 0.2757], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2269, 0.2373, 0.2533, 0.2825], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2288, 0.271 , 0.2779, 0.2224], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1883, 0.2341, 0.2529, 0.3247], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2128, 0.2824, 0.2811, 0.2237], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.199 , 0.2683, 0.2823, 0.2504], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1889, 0.2821, 0.2866, 0.2424], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2483, 0.1914, 0.2221, 0.3382], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1959, 0.3016, 0.2978, 0.2047], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1879, 0.302 , 0.3068, 0.2032], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1787, 0.2931, 0.3188, 0.2094], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2723, 0.2482, 0.2412, 0.2383], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.267 , 0.2484, 0.2373, 0.2473], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2483, 0.2506, 0.2488, 0.2523], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.219 , 0.2651, 0.2699, 0.2459], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2181, 0.2733, 0.27  , 0.2385], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2314, 0.2093, 0.2519, 0.3073], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2842, 0.2293, 0.1901, 0.2965], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4301, 0.1261, 0.1142, 0.3295], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4188, 0.1264, 0.1306, 0.3242], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3851, 0.1644, 0.1544, 0.2961], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3936, 0.1285, 0.1335, 0.3444], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.175 , 0.3088, 0.2858, 0.2304], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4342, 0.1468, 0.144 , 0.275 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.5356, 0.1699, 0.1126, 0.1819], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3689, 0.1974, 0.2004, 0.2333], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2307, 0.2701, 0.2774, 0.2219], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4382, 0.1127, 0.1454, 0.3037], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1971, 0.3025, 0.2865, 0.2139], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2041, 0.281 , 0.2982, 0.2168], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2165, 0.2927, 0.2729, 0.2179], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1954, 0.2828, 0.3002, 0.2217], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2555, 0.2692, 0.2569, 0.2185], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2336, 0.275 , 0.2545, 0.2369], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4181, 0.1416, 0.1444, 0.296 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.272 , 0.2119, 0.2329, 0.2832], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2537, 0.2161, 0.2265, 0.3037], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2271, 0.2492, 0.279 , 0.2447], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3081, 0.2329, 0.1793, 0.2798], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4009, 0.1842, 0.1458, 0.2691], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3783, 0.2292, 0.1593, 0.2333], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2632, 0.2459, 0.2445, 0.2464], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2611, 0.2531, 0.2468, 0.239 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3478, 0.2422, 0.1805, 0.2295], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3458, 0.2438, 0.1644, 0.246 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3617, 0.2328, 0.1484, 0.2571], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4438, 0.2216, 0.1299, 0.2047], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2258, 0.2211, 0.2616, 0.2914], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4762, 0.143 , 0.0957, 0.2851], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4334, 0.1473, 0.1394, 0.28  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2954, 0.2482, 0.2019, 0.2546], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2189, 0.1993, 0.1684, 0.4133], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3358, 0.213 , 0.1418, 0.3094], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2037, 0.1734, 0.1976, 0.4252], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1976, 0.1816, 0.194 , 0.4268], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2931, 0.2423, 0.2247, 0.2399], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2165, 0.2653, 0.2699, 0.2482], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1798, 0.312 , 0.3012, 0.207 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1806, 0.309 , 0.3031, 0.2072], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1776, 0.308 , 0.3068, 0.2076], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1601, 0.2735, 0.2905, 0.276 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1598, 0.2843, 0.2888, 0.2671], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3016, 0.2136, 0.2401, 0.2446], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2771, 0.2367, 0.2457, 0.2405], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3223, 0.2038, 0.2093, 0.2645], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3585, 0.1995, 0.2006, 0.2415], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3303, 0.1898, 0.1997, 0.2802], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2155, 0.2831, 0.258 , 0.2434], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3276, 0.257 , 0.1902, 0.2252], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2618, 0.2603, 0.2287, 0.2491], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2355, 0.2566, 0.2705, 0.2375], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2265, 0.2809, 0.2651, 0.2275], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2556, 0.2312, 0.2386, 0.2745], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.29  , 0.2118, 0.2155, 0.2827], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.629 , 0.0779, 0.0487, 0.2444], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2306, 0.2604, 0.2564, 0.2525], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2322, 0.2627, 0.2625, 0.2427], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2788, 0.2431, 0.2371, 0.241 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3279, 0.1588, 0.1931, 0.3202], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.223 , 0.2627, 0.2667, 0.2477], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2132, 0.2593, 0.2715, 0.256 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2107, 0.2794, 0.3   , 0.21  ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2154, 0.2826, 0.2757, 0.2262], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3627, 0.1809, 0.1976, 0.2589], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2493, 0.2485, 0.26  , 0.2423], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2722, 0.2533, 0.2389, 0.2356], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2716, 0.2522, 0.2386, 0.2376], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3058, 0.0962, 0.1343, 0.4637], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3669, 0.1957, 0.2266, 0.2109], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2073, 0.2683, 0.2868, 0.2377], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2022, 0.2775, 0.2783, 0.242 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2931, 0.243 , 0.2453, 0.2186], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2512, 0.2046, 0.22  , 0.3242], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1747, 0.1976, 0.1906, 0.4371], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2028, 0.1895, 0.2012, 0.4065], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2191, 0.2037, 0.188 , 0.3892], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.209 , 0.2428, 0.2715, 0.2766], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2071, 0.2494, 0.2679, 0.2755], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1802, 0.2063, 0.2308, 0.3827], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2069, 0.2005, 0.2112, 0.3814], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3367, 0.223 , 0.2056, 0.2346], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2006, 0.2832, 0.2903, 0.2258], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.189 , 0.2735, 0.281 , 0.2566], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2594, 0.2096, 0.2101, 0.321 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2355, 0.2105, 0.2115, 0.3425], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2501, 0.2027, 0.2304, 0.3168], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2926, 0.2154, 0.2316, 0.2605], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2078, 0.2834, 0.2902, 0.2186], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1893, 0.2965, 0.3037, 0.2105], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1738, 0.291 , 0.311 , 0.2242], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1742, 0.2908, 0.302 , 0.233 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1665, 0.2858, 0.3051, 0.2425], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2806, 0.2774, 0.2403, 0.2017], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3105, 0.268 , 0.2144, 0.2071], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2786, 0.2285, 0.2236, 0.2693], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2795, 0.2198, 0.2264, 0.2744], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2833, 0.2324, 0.2293, 0.255 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.302 , 0.2077, 0.2061, 0.2841], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3126, 0.2204, 0.2278, 0.2392], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3074, 0.2483, 0.2165, 0.2277], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1905, 0.3091, 0.2976, 0.2028], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2163, 0.2834, 0.2876, 0.2127], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2083, 0.2888, 0.2753, 0.2277], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1746, 0.3132, 0.3042, 0.2079], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1748, 0.3105, 0.3041, 0.2106], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2697, 0.2447, 0.2254, 0.2602], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2061, 0.2671, 0.2894, 0.2374], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.21  , 0.2722, 0.2748, 0.2431], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2114, 0.2743, 0.2866, 0.2278], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2011, 0.2688, 0.2878, 0.2422], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2815, 0.2078, 0.2205, 0.2903], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4008, 0.2127, 0.1263, 0.2602], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2109, 0.2552, 0.2549, 0.279 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2073, 0.2595, 0.2677, 0.2655], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1729, 0.3017, 0.3167, 0.2087], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2075, 0.2947, 0.2967, 0.2011], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2593, 0.2604, 0.2572, 0.2231], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3122, 0.2488, 0.2171, 0.222 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.437 , 0.1689, 0.1514, 0.2427], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.258 , 0.2559, 0.2393, 0.2468], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2927, 0.2334, 0.2182, 0.2558], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2791, 0.239 , 0.232 , 0.25  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3341, 0.2011, 0.2124, 0.2524], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3168, 0.2019, 0.2141, 0.2672], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2447, 0.258 , 0.2484, 0.2489], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2333, 0.2726, 0.254 , 0.2402], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2163, 0.2664, 0.2663, 0.251 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2029, 0.279 , 0.2814, 0.2367], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2115, 0.2833, 0.2713, 0.2338], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2195, 0.2618, 0.2604, 0.2582], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2218, 0.2556, 0.2612, 0.2614], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2373, 0.2499, 0.268 , 0.2448], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2153, 0.2685, 0.2628, 0.2533], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.221 , 0.2681, 0.2787, 0.2322], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.238 , 0.2623, 0.2574, 0.2423], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2577, 0.2471, 0.2455, 0.2498], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2663, 0.2454, 0.2465, 0.2419], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2675, 0.2452, 0.2469, 0.2404], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2623, 0.259 , 0.2527, 0.2259], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2186, 0.2517, 0.2597, 0.2701], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.23  , 0.2533, 0.266 , 0.2507], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2472, 0.268 , 0.2602, 0.2246], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2442, 0.2399, 0.2444, 0.2715], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2261, 0.2574, 0.2603, 0.2562], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2388, 0.2529, 0.2524, 0.2559], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2339, 0.2527, 0.2602, 0.2532], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1937, 0.2562, 0.2809, 0.2692], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2125, 0.2433, 0.2639, 0.2804], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2349, 0.2594, 0.2733, 0.2324], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2583, 0.2471, 0.2487, 0.2459], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2125, 0.2599, 0.2714, 0.2561], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2478, 0.2624, 0.2597, 0.23  ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2774, 0.2492, 0.2328, 0.2406], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2786, 0.2293, 0.2472, 0.2449], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2617, 0.2427, 0.2492, 0.2464], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3287, 0.1938, 0.1968, 0.2808], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3112, 0.2261, 0.2142, 0.2484], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1751, 0.2651, 0.2962, 0.2636], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2015, 0.2839, 0.2754, 0.2391], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2988, 0.2324, 0.2343, 0.2344], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3024, 0.2292, 0.2218, 0.2466], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3296, 0.2064, 0.2132, 0.2508], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3446, 0.1886, 0.1881, 0.2787], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3236, 0.184 , 0.2085, 0.2839], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3204, 0.1679, 0.2003, 0.3113], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3329, 0.1762, 0.226 , 0.2649], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3605, 0.186 , 0.1901, 0.2634], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2446, 0.2468, 0.2577, 0.2509], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2614, 0.2523, 0.2529, 0.2334], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2296, 0.2774, 0.2727, 0.2203], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.218 , 0.2711, 0.2778, 0.2331], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2735, 0.2467, 0.2425, 0.2373], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2622, 0.2464, 0.2407, 0.2507], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2325, 0.2801, 0.269 , 0.2184], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2198, 0.2606, 0.2727, 0.247 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2164, 0.2804, 0.2822, 0.2211], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1955, 0.2802, 0.282 , 0.2424], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2096, 0.2678, 0.2632, 0.2595], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2025, 0.2492, 0.2827, 0.2656], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2092, 0.2518, 0.2789, 0.26  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.217 , 0.2683, 0.2567, 0.2579], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2181, 0.2515, 0.2687, 0.2617], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2309, 0.2525, 0.2649, 0.2516], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2461, 0.2554, 0.2651, 0.2333], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2655, 0.2568, 0.2478, 0.2299], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.282 , 0.2371, 0.2295, 0.2514], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2657, 0.2283, 0.245 , 0.2611], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2454, 0.2643, 0.2483, 0.242 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.336 , 0.2378, 0.2172, 0.209 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3622, 0.2507, 0.1883, 0.1988], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2945, 0.2315, 0.2313, 0.2426], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2948, 0.2341, 0.2294, 0.2417], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3114, 0.2356, 0.2225, 0.2305], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3152, 0.2371, 0.2269, 0.2208], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2838, 0.2415, 0.2263, 0.2484], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2832, 0.2272, 0.2366, 0.2529], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2797, 0.2392, 0.2356, 0.2455], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2143, 0.2566, 0.2706, 0.2586], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.22  , 0.2717, 0.2623, 0.246 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2101, 0.2641, 0.2726, 0.2532], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2302, 0.2773, 0.2588, 0.2336], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2034, 0.2654, 0.2737, 0.2575], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2131, 0.276 , 0.2766, 0.2343], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2602, 0.2337, 0.2423, 0.2638], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2557, 0.2407, 0.2484, 0.2552], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2398, 0.2479, 0.2661, 0.2462], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2334, 0.2683, 0.2616, 0.2367], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2314, 0.2573, 0.262 , 0.2493], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2592, 0.2481, 0.2439, 0.2488], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2703, 0.2295, 0.2253, 0.2749], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3012, 0.2267, 0.2148, 0.2574], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2209, 0.2443, 0.2637, 0.2711], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2809, 0.2436, 0.2224, 0.2531], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2704, 0.2256, 0.2456, 0.2584], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2741, 0.2324, 0.2362, 0.2573], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.262 , 0.2391, 0.2465, 0.2524], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.275 , 0.2425, 0.2366, 0.2459], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2631, 0.2465, 0.2375, 0.2528], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3183, 0.2385, 0.2088, 0.2344], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2668, 0.2296, 0.2413, 0.2623], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2754, 0.2278, 0.2218, 0.275 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.23  , 0.2549, 0.247 , 0.2681], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2916, 0.2013, 0.2182, 0.2888], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3693, 0.1904, 0.1764, 0.2638], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2995, 0.1932, 0.2034, 0.304 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2772, 0.2148, 0.2309, 0.2771], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2863, 0.2144, 0.2198, 0.2795], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2044, 0.2832, 0.2839, 0.2286], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2886, 0.2346, 0.2303, 0.2464], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3443, 0.1993, 0.2205, 0.2359], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3097, 0.2094, 0.2259, 0.255 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3052, 0.1934, 0.2048, 0.2966], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2014, 0.2265, 0.2439, 0.3282], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2429, 0.2619, 0.2658, 0.2294], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2261, 0.27  , 0.2584, 0.2454], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2256, 0.2884, 0.2569, 0.229 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.298 , 0.2557, 0.2255, 0.2208], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2112, 0.2525, 0.2936, 0.2428], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2088, 0.2632, 0.2824, 0.2456], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2074, 0.2741, 0.2818, 0.2366], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3215, 0.1877, 0.2104, 0.2804], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2198, 0.2525, 0.2914, 0.2364], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2417, 0.248 , 0.2482, 0.2622], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2351, 0.2494, 0.2555, 0.26  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2982, 0.2291, 0.2279, 0.2448], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2194, 0.2341, 0.2639, 0.2826], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2116, 0.2638, 0.2822, 0.2425], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2423, 0.2537, 0.2631, 0.2408], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4075, 0.2434, 0.1484, 0.2007], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2098, 0.2576, 0.2784, 0.2541], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1999, 0.2629, 0.2732, 0.2639], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2202, 0.2808, 0.2743, 0.2247], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1991, 0.2883, 0.3207, 0.1918], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1896, 0.2936, 0.3246, 0.1921], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2135, 0.295 , 0.3136, 0.1778], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.5139, 0.1268, 0.0917, 0.2676], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2774, 0.2068, 0.2224, 0.2935], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2727, 0.2328, 0.2408, 0.2536], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2309, 0.2563, 0.2649, 0.2478], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2522, 0.2251, 0.2679, 0.2548], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2744, 0.2082, 0.2459, 0.2715], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2656, 0.1947, 0.2199, 0.3199], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2602, 0.1979, 0.2289, 0.313 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2471, 0.2148, 0.2129, 0.3252], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2194, 0.2123, 0.2223, 0.3459], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.243 , 0.2014, 0.222 , 0.3336], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2138, 0.2367, 0.2256, 0.3238], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.148 , 0.2456, 0.305 , 0.3014], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2517, 0.2233, 0.2387, 0.2863], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2225, 0.264 , 0.2645, 0.249 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2722, 0.2243, 0.2131, 0.2904], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.257 , 0.2214, 0.2193, 0.3024], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2022, 0.2273, 0.206 , 0.3645], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2876, 0.2419, 0.2318, 0.2386], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2985, 0.246 , 0.213 , 0.2426], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2905, 0.2412, 0.2247, 0.2435], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.276 , 0.2544, 0.2433, 0.2264], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3079, 0.2333, 0.2235, 0.2353], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3307, 0.2198, 0.2167, 0.2328], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3008, 0.2278, 0.2224, 0.249 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2741, 0.245 , 0.2339, 0.247 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1704, 0.2993, 0.3069, 0.2234], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3204, 0.2032, 0.1946, 0.2819], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3011, 0.1795, 0.1866, 0.3329], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2792, 0.1911, 0.209 , 0.3207], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2044, 0.2621, 0.2776, 0.2559], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2254, 0.2457, 0.2674, 0.2616], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2643, 0.2249, 0.2397, 0.2711], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2573, 0.2243, 0.2285, 0.2899], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2522, 0.2482, 0.2476, 0.252 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1985, 0.2805, 0.2751, 0.2459], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1945, 0.2797, 0.2824, 0.2435], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2059, 0.2821, 0.2743, 0.2377], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2873, 0.2729, 0.2375, 0.2023], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3493, 0.2043, 0.2   , 0.2463], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3753, 0.2286, 0.2121, 0.1841], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3611, 0.202 , 0.2036, 0.2333], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.209 , 0.2822, 0.2783, 0.2304], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1856, 0.2865, 0.2927, 0.2352], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1857, 0.3074, 0.2854, 0.2215], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1903, 0.3025, 0.2795, 0.2276], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1875, 0.3058, 0.2865, 0.2202], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2455, 0.2622, 0.25  , 0.2422], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3401, 0.2167, 0.1975, 0.2458], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2382, 0.2269, 0.2218, 0.3131], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3083, 0.1975, 0.2027, 0.2915], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2912, 0.232 , 0.2289, 0.248 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3094, 0.2337, 0.2261, 0.2308], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2974, 0.2237, 0.2496, 0.2293], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3896, 0.1698, 0.1626, 0.278 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2556, 0.25  , 0.2554, 0.239 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2622, 0.2085, 0.2291, 0.3002], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2644, 0.221 , 0.2407, 0.274 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1958, 0.2428, 0.2767, 0.2847], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2921, 0.2374, 0.2335, 0.237 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2871, 0.2225, 0.2326, 0.2578], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2742, 0.2377, 0.2487, 0.2394], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2675, 0.2394, 0.2376, 0.2555], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.207 , 0.2754, 0.2927, 0.2248], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2182, 0.2741, 0.2863, 0.2213], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2442, 0.2398, 0.2888, 0.2272], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.179 , 0.3088, 0.2893, 0.2229], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1932, 0.299 , 0.2933, 0.2146], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.195 , 0.2922, 0.2889, 0.224 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1822, 0.2976, 0.302 , 0.2182], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1861, 0.2881, 0.295 , 0.2309], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1921, 0.2963, 0.2845, 0.2272], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1953, 0.2851, 0.2969, 0.2227], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2102, 0.2894, 0.2727, 0.2276], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2092, 0.2809, 0.2723, 0.2377], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2266, 0.2704, 0.2546, 0.2484], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2243, 0.2747, 0.2619, 0.2391], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2023, 0.2862, 0.2926, 0.2189], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2055, 0.2808, 0.2857, 0.228 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2278, 0.2745, 0.2752, 0.2225], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.196 , 0.2878, 0.2819, 0.2343], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.196 , 0.2925, 0.2841, 0.2273], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2018, 0.2725, 0.2817, 0.244 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1703, 0.2732, 0.3164, 0.24  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2166, 0.2653, 0.2857, 0.2324], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.365 , 0.2066, 0.1937, 0.2347], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.323 , 0.1969, 0.2135, 0.2666], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3059, 0.2055, 0.2217, 0.267 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.301 , 0.2112, 0.2181, 0.2698], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3492, 0.2019, 0.2122, 0.2368], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2464, 0.2   , 0.2354, 0.3182], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2695, 0.1982, 0.224 , 0.3083], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2189, 0.2626, 0.2528, 0.2657], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2568, 0.2753, 0.2521, 0.2158], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2333, 0.235 , 0.2422, 0.2894], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3648, 0.236 , 0.202 , 0.1973], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2101, 0.2749, 0.281 , 0.2341], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2931, 0.252 , 0.2091, 0.2457], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2567, 0.2476, 0.1963, 0.2995], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2997, 0.234 , 0.1449, 0.3214], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1872, 0.2015, 0.2315, 0.3798], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3288, 0.214 , 0.1447, 0.3125], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3147, 0.2191, 0.1605, 0.3056], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4594, 0.1019, 0.1065, 0.3322], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2935, 0.2285, 0.209 , 0.2691], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.26  , 0.232 , 0.2243, 0.2837], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2732, 0.2354, 0.2341, 0.2572], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2478, 0.2522, 0.2476, 0.2524], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2919, 0.2262, 0.2246, 0.2572], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2736, 0.2382, 0.2456, 0.2426], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3653, 0.1226, 0.1433, 0.3688], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4556, 0.097 , 0.1168, 0.3306], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.5064, 0.0977, 0.1062, 0.2896], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3835, 0.1265, 0.1487, 0.3413], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3938, 0.1555, 0.1651, 0.2856], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5306, 0.111 , 0.1145, 0.244 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2586, 0.2494, 0.2556, 0.2364], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2617, 0.2617, 0.2532, 0.2234], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2385, 0.2616, 0.2835, 0.2164], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2124, 0.2723, 0.2828, 0.2325], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2369, 0.2723, 0.2536, 0.2372], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2677, 0.2724, 0.2588, 0.2011], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2437, 0.2587, 0.2477, 0.2499], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2767, 0.2178, 0.224 , 0.2815], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1999, 0.2963, 0.2768, 0.2269], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2369, 0.2513, 0.2455, 0.2663], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2618, 0.2476, 0.2351, 0.2555], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2506, 0.25  , 0.2568, 0.2426], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2187, 0.2918, 0.2858, 0.2037], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3245, 0.223 , 0.2009, 0.2516], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4077, 0.1861, 0.1739, 0.2323], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2891, 0.2323, 0.229 , 0.2497], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3473, 0.219 , 0.2114, 0.2223], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3796, 0.1428, 0.1788, 0.2987], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3452, 0.1517, 0.1701, 0.333 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.311 , 0.1702, 0.188 , 0.3308], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.401 , 0.166 , 0.1214, 0.3116], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4017, 0.1473, 0.1113, 0.3398], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1816, 0.2476, 0.2654, 0.3054], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2026, 0.2705, 0.2724, 0.2545], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1904, 0.2776, 0.2851, 0.2469], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2001, 0.2301, 0.2448, 0.325 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3099, 0.2268, 0.189 , 0.2743], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.299 , 0.2312, 0.1985, 0.2713], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2489, 0.2747, 0.2429, 0.2335], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2177, 0.2856, 0.2564, 0.2403], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.256 , 0.2304, 0.2427, 0.271 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2361, 0.2537, 0.2668, 0.2433], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2064, 0.2565, 0.2758, 0.2614], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.19  , 0.2732, 0.2814, 0.2554], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1841, 0.2692, 0.3047, 0.242 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2277, 0.2695, 0.2616, 0.2413], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2301, 0.2622, 0.2669, 0.2408], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3859, 0.1499, 0.1698, 0.2944], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4125, 0.1285, 0.1247, 0.3344], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4137, 0.1495, 0.1653, 0.2715], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2629, 0.2313, 0.2362, 0.2696], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2327, 0.2589, 0.2612, 0.2472], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.274 , 0.2374, 0.2355, 0.253 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3343, 0.2299, 0.2058, 0.2299], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3115, 0.2352, 0.2086, 0.2448], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3255, 0.1572, 0.171 , 0.3463], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2196, 0.267 , 0.2645, 0.2489], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2214, 0.2626, 0.2669, 0.2492], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2193, 0.2677, 0.2763, 0.2367], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2213, 0.2635, 0.2742, 0.2411], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3025, 0.2143, 0.2198, 0.2635], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3193, 0.1813, 0.1971, 0.3022], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3744, 0.1676, 0.1835, 0.2746], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3052, 0.1912, 0.1936, 0.31  ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3078, 0.1831, 0.1921, 0.3169], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3073, 0.2005, 0.2076, 0.2846], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3781, 0.1711, 0.17  , 0.2808], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.493 , 0.1408, 0.1032, 0.2631], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4986, 0.1233, 0.1235, 0.2546], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1658, 0.2914, 0.313 , 0.2298], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1769, 0.2839, 0.3053, 0.2338], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2746, 0.2342, 0.2251, 0.2661], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3226, 0.2431, 0.214 , 0.2203], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2686, 0.2631, 0.2362, 0.2321], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2127, 0.2541, 0.2481, 0.2851], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1679, 0.2257, 0.2344, 0.3719], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3657, 0.2444, 0.1874, 0.2025], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.273 , 0.2334, 0.1872, 0.3064], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2667, 0.2227, 0.1847, 0.326 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4702, 0.1215, 0.1185, 0.2898], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2528, 0.2367, 0.2463, 0.2643], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2539, 0.251 , 0.2526, 0.2425], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2252, 0.241 , 0.2569, 0.2768], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2406, 0.2663, 0.2608, 0.2323], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2346, 0.238 , 0.2499, 0.2775], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2327, 0.2597, 0.2697, 0.2379], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2024, 0.2616, 0.2666, 0.2694], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2116, 0.2771, 0.2717, 0.2396], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2066, 0.2783, 0.2795, 0.2355], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2233, 0.274 , 0.2685, 0.2342], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2655, 0.2567, 0.2448, 0.233 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1891, 0.2896, 0.2953, 0.226 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2271, 0.2343, 0.2659, 0.2727], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2242, 0.2723, 0.281 , 0.2225], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2471, 0.2644, 0.2498, 0.2388], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2564, 0.2328, 0.2483, 0.2625], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2483, 0.2204, 0.2238, 0.3075], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1942, 0.2775, 0.2824, 0.2459], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1978, 0.279 , 0.2903, 0.2328], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1959, 0.2761, 0.2937, 0.2342], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1998, 0.2834, 0.2818, 0.235 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1974, 0.2731, 0.2865, 0.243 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1798, 0.2621, 0.3101, 0.248 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1807, 0.2748, 0.3079, 0.2365], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2325, 0.2696, 0.2665, 0.2314], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2246, 0.2708, 0.2847, 0.2199], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2293, 0.2773, 0.2646, 0.2288], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2556, 0.2729, 0.2576, 0.2139], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2141, 0.2241, 0.2523, 0.3095], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3574, 0.1184, 0.1261, 0.3981], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2439, 0.1902, 0.2209, 0.345 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2711, 0.2172, 0.2199, 0.2917], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.186 , 0.2745, 0.2871, 0.2525], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1878, 0.2822, 0.2845, 0.2455], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2018, 0.266 , 0.2746, 0.2576], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2115, 0.2608, 0.266 , 0.2616], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1854, 0.2828, 0.2842, 0.2476], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.18  , 0.2837, 0.2869, 0.2493], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.194 , 0.2796, 0.277 , 0.2493], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2021, 0.2848, 0.2832, 0.2299], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2858, 0.2392, 0.2191, 0.256 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.236 , 0.2355, 0.2373, 0.2912], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2304, 0.2272, 0.2436, 0.2988], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2257, 0.2432, 0.2399, 0.2912], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2243, 0.2554, 0.2465, 0.2738], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2647, 0.1805, 0.1914, 0.3634], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1797, 0.2935, 0.3023, 0.2244], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.211 , 0.2827, 0.2753, 0.2311], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.248 , 0.2885, 0.262 , 0.2015], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.5219, 0.1902, 0.1295, 0.1585], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2868, 0.2426, 0.2364, 0.2342], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2327, 0.2628, 0.2596, 0.2449], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2264, 0.2403, 0.2611, 0.2721], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2193, 0.2763, 0.2772, 0.2272], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2148, 0.2666, 0.28  , 0.2387], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2188, 0.2632, 0.2668, 0.2511], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3154, 0.2416, 0.2241, 0.2189], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3199, 0.2114, 0.2169, 0.2518], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2643, 0.235 , 0.2382, 0.2624], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2558, 0.2379, 0.2493, 0.257 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2141, 0.2693, 0.2863, 0.2302], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2006, 0.2745, 0.2859, 0.239 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.224 , 0.251 , 0.2669, 0.2581], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3072, 0.1911, 0.2194, 0.2823], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2458, 0.2558, 0.2477, 0.2507], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1971, 0.2812, 0.2762, 0.2455], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1964, 0.2911, 0.2764, 0.236 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2036, 0.2937, 0.2762, 0.2264], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1985, 0.2908, 0.2822, 0.2284], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.31  , 0.2135, 0.1976, 0.2789], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4231, 0.165 , 0.1404, 0.2715], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3066, 0.2055, 0.2166, 0.2713], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.231 , 0.2292, 0.2326, 0.3072], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2564, 0.2142, 0.2169, 0.3125], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2034, 0.2322, 0.2308, 0.3337], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2327, 0.2608, 0.2559, 0.2505], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2388, 0.2341, 0.2185, 0.3086], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2351, 0.2584, 0.2572, 0.2493], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2654, 0.2409, 0.2467, 0.2471], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2199, 0.2156, 0.2317, 0.3328], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2112, 0.2722, 0.2855, 0.2311], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2211, 0.2646, 0.2776, 0.2366], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2339, 0.2656, 0.2602, 0.2404], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2065, 0.2747, 0.2803, 0.2385], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2056, 0.2826, 0.2879, 0.224 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1989, 0.2822, 0.3   , 0.2188], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2043, 0.2814, 0.277 , 0.2372], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2527, 0.2531, 0.2533, 0.2409], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2684, 0.2418, 0.2356, 0.2542], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2968, 0.2314, 0.22  , 0.2517], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2568, 0.2556, 0.2512, 0.2364], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2703, 0.2401, 0.234 , 0.2556], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2735, 0.2345, 0.2337, 0.2583], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2679, 0.2402, 0.2447, 0.2472], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2644, 0.2498, 0.2444, 0.2414], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2538, 0.2587, 0.2489, 0.2387], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2905, 0.192 , 0.2001, 0.3173], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.284 , 0.2026, 0.2297, 0.2837], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2446, 0.2427, 0.2543, 0.2583], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2856, 0.2371, 0.2273, 0.25  ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1822, 0.2849, 0.3006, 0.2323], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2256, 0.2315, 0.2511, 0.2918], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2367, 0.27  , 0.256 , 0.2373], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2942, 0.2264, 0.2286, 0.2508], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2766, 0.2385, 0.2337, 0.2512], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.331 , 0.1952, 0.1916, 0.2822], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.321 , 0.2072, 0.2122, 0.2596], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2145, 0.2565, 0.289 , 0.2399], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2328, 0.2552, 0.2646, 0.2474], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2281, 0.2438, 0.2491, 0.2789], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2226, 0.2004, 0.2297, 0.3472], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1721, 0.211 , 0.2347, 0.3822], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1958, 0.2417, 0.2585, 0.304 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2201, 0.2466, 0.2677, 0.2656], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2157, 0.2648, 0.2815, 0.238 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1979, 0.2619, 0.2666, 0.2736], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2331, 0.2345, 0.2551, 0.2772], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2575, 0.253 , 0.2492, 0.2403], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2506, 0.2412, 0.2537, 0.2545], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2403, 0.2572, 0.2622, 0.2403], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2376, 0.2529, 0.259 , 0.2505], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2336, 0.2656, 0.259 , 0.2417], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2605, 0.2273, 0.2451, 0.2671], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2992, 0.221 , 0.227 , 0.2528], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2486, 0.2548, 0.2562, 0.2404], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2075, 0.2669, 0.2674, 0.2582], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2108, 0.2686, 0.278 , 0.2426], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2939, 0.2388, 0.2141, 0.2531], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2613, 0.2343, 0.216 , 0.2884], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2283, 0.2625, 0.2442, 0.2649], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2438, 0.2569, 0.2547, 0.2446], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1886, 0.2759, 0.294 , 0.2415], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2561, 0.2331, 0.2558, 0.255 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.238 , 0.2464, 0.2467, 0.2689], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1942, 0.2818, 0.3216, 0.2025], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2063, 0.2608, 0.2633, 0.2695], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2237, 0.2774, 0.2598, 0.2391], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2119, 0.2761, 0.2447, 0.2674], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2008, 0.2842, 0.2772, 0.2379], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1751, 0.3028, 0.2668, 0.2553], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2053, 0.2732, 0.2897, 0.2318], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3136, 0.2175, 0.2148, 0.254 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2648, 0.2515, 0.2438, 0.2399], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3093, 0.2128, 0.2154, 0.2625], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3624, 0.1583, 0.1672, 0.3121], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.303 , 0.189 , 0.2285, 0.2795], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2836, 0.1837, 0.2255, 0.3071], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2263, 0.2647, 0.2624, 0.2466], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2436, 0.246 , 0.255 , 0.2554], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2086, 0.2612, 0.2638, 0.2664], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1809, 0.2685, 0.285 , 0.2656], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1925, 0.2876, 0.2965, 0.2235], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2067, 0.2585, 0.259 , 0.2757], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1862, 0.2664, 0.2791, 0.2684], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2251, 0.2428, 0.2587, 0.2733], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2058, 0.2294, 0.2608, 0.3039], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2139, 0.2495, 0.2611, 0.2756], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2248, 0.2687, 0.259 , 0.2475], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2149, 0.2572, 0.2482, 0.2796], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2649, 0.256 , 0.2504, 0.2287], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2917, 0.2565, 0.239 , 0.2128], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3578, 0.1821, 0.196 , 0.2641], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3421, 0.1844, 0.1855, 0.2879], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2522, 0.247 , 0.2386, 0.2622], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2132, 0.2434, 0.2501, 0.2933], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2203, 0.2626, 0.2799, 0.2372], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2149, 0.2565, 0.2814, 0.2472], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2233, 0.2363, 0.2703, 0.2701], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2703, 0.2518, 0.2443, 0.2336], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2106, 0.2598, 0.2833, 0.2462], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2059, 0.2603, 0.2785, 0.2553], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2139, 0.266 , 0.2888, 0.2314], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2387, 0.277 , 0.2754, 0.2089], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2446, 0.2321, 0.2414, 0.2818], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2182, 0.2581, 0.2773, 0.2464], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2232, 0.2549, 0.2662, 0.2557], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2276, 0.2481, 0.2585, 0.2658], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2399, 0.2516, 0.2777, 0.2309], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2449, 0.2489, 0.254 , 0.2522], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4602, 0.2272, 0.1445, 0.1681], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2247, 0.2635, 0.2808, 0.231 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3027, 0.2392, 0.2316, 0.2266], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4113, 0.1668, 0.1815, 0.2403], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3886, 0.1669, 0.1715, 0.273 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4467, 0.1468, 0.0957, 0.3109], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5472, 0.1032, 0.0719, 0.2777], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3433, 0.2109, 0.2388, 0.2071], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2959, 0.2363, 0.2405, 0.2273], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3581, 0.1709, 0.2009, 0.2702], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3049, 0.1961, 0.2241, 0.2749], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2553, 0.2323, 0.2346, 0.2779], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3632, 0.1604, 0.1637, 0.3127], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3489, 0.1688, 0.174 , 0.3082], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2132, 0.2714, 0.2898, 0.2256], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2082, 0.2847, 0.2662, 0.2409], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5044, 0.1824, 0.0886, 0.2246], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1797, 0.2067, 0.2114, 0.4022], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4161, 0.1102, 0.1345, 0.3392], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3999, 0.0955, 0.1146, 0.39  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4114, 0.1216, 0.1418, 0.3252], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2123, 0.2853, 0.2806, 0.2218], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2656, 0.2272, 0.2465, 0.2607], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2634, 0.2355, 0.2444, 0.2567], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2919, 0.201 , 0.2269, 0.2803], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.314 , 0.2215, 0.1998, 0.2646], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2842, 0.2292, 0.2171, 0.2695], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3197, 0.2108, 0.202 , 0.2675], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2436, 0.2498, 0.2443, 0.2624], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2356, 0.2431, 0.2322, 0.2891], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2331, 0.2608, 0.2444, 0.2617], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2939, 0.2197, 0.2072, 0.2792], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.276 , 0.2564, 0.2174, 0.2502], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2532, 0.2065, 0.2267, 0.3136], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2387, 0.2442, 0.2487, 0.2684], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2937, 0.1664, 0.1805, 0.3594], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2279, 0.1798, 0.1585, 0.4338], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4181, 0.2039, 0.1893, 0.1887], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2949, 0.1431, 0.1756, 0.3864], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.6583, 0.1195, 0.0734, 0.1487], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2483, 0.2763, 0.2612, 0.2142], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2549, 0.2597, 0.2493, 0.2361], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.209 , 0.2603, 0.2815, 0.2491], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1597, 0.2564, 0.2954, 0.2885], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1819, 0.2684, 0.2876, 0.2621], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2151, 0.2562, 0.2812, 0.2475], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2304, 0.2575, 0.2658, 0.2462], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2927, 0.2309, 0.2235, 0.2529], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2975, 0.2401, 0.2366, 0.2258], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2596, 0.2637, 0.2469, 0.2298], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2643, 0.2498, 0.2399, 0.246 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3024, 0.2203, 0.2079, 0.2693], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3254, 0.2163, 0.2125, 0.2458], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2772, 0.2355, 0.2324, 0.2549], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2594, 0.2426, 0.2424, 0.2557], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2423, 0.2443, 0.2537, 0.2597], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2707, 0.2337, 0.2237, 0.2718], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2398, 0.2451, 0.2555, 0.2596], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.273 , 0.2379, 0.2254, 0.2637], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.225 , 0.2763, 0.2705, 0.2281], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2728, 0.2407, 0.246 , 0.2405], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2726, 0.2409, 0.2476, 0.239 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3196, 0.1956, 0.1912, 0.2936], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2974, 0.2036, 0.2158, 0.2832], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2611, 0.2274, 0.2322, 0.2793], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2501, 0.2272, 0.2466, 0.2762], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2378, 0.2373, 0.2615, 0.2634], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2646, 0.229 , 0.2498, 0.2566], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2648, 0.2301, 0.2401, 0.265 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3017, 0.2071, 0.217 , 0.2742], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2285, 0.2379, 0.2478, 0.2858], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2523, 0.2198, 0.2395, 0.2884], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2479, 0.2378, 0.2584, 0.2559], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2438, 0.2406, 0.2704, 0.2452], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2469, 0.2458, 0.2527, 0.2546], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2727, 0.2444, 0.2514, 0.2315], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1696, 0.2808, 0.3025, 0.247 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3559, 0.169 , 0.1966, 0.2785], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2775, 0.1928, 0.2036, 0.3261], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2739, 0.1554, 0.1786, 0.3921], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3319, 0.1508, 0.1779, 0.3393], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3205, 0.161 , 0.2088, 0.3098], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2662, 0.2443, 0.2419, 0.2476], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.147 , 0.205 , 0.2595, 0.3885], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.239 , 0.2476, 0.2554, 0.258 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.197 , 0.242 , 0.2801, 0.2808], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2115, 0.2753, 0.2854, 0.2278], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2204, 0.2639, 0.2676, 0.2481], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2346, 0.2737, 0.2618, 0.2299], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3158, 0.2127, 0.2287, 0.2427], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3352, 0.2386, 0.21  , 0.2162], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3141, 0.2209, 0.2074, 0.2577], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3086, 0.2209, 0.2264, 0.2441], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.329 , 0.2254, 0.2127, 0.2329], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2966, 0.2256, 0.2362, 0.2416], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3103, 0.247 , 0.2364, 0.2063], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3016, 0.217 , 0.2267, 0.2547], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2582, 0.2396, 0.2468, 0.2554], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2534, 0.2624, 0.249 , 0.2352], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2553, 0.2469, 0.2373, 0.2604], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2853, 0.2424, 0.2399, 0.2323], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1986, 0.2256, 0.2465, 0.3294], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2056, 0.2402, 0.2509, 0.3033], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2089, 0.2748, 0.2707, 0.2456], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.213 , 0.2449, 0.2518, 0.2903], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2148, 0.2578, 0.2514, 0.2761], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3049, 0.2304, 0.2128, 0.2519], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2128, 0.2656, 0.2591, 0.2624], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.236 , 0.2583, 0.2597, 0.246 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2191, 0.2699, 0.2659, 0.2451], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2115, 0.2674, 0.2577, 0.2635], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2965, 0.2195, 0.2176, 0.2664], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3462, 0.2376, 0.2129, 0.2033], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2868, 0.2542, 0.2115, 0.2475], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2563, 0.2382, 0.2243, 0.2811], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2588, 0.2452, 0.2467, 0.2493], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2098, 0.261 , 0.2642, 0.265 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2459, 0.2653, 0.2523, 0.2365], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2039, 0.2644, 0.2753, 0.2563], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2143, 0.2761, 0.2367, 0.2728], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1921, 0.2767, 0.2185, 0.3128], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2071, 0.2409, 0.2202, 0.3318], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2179, 0.2288, 0.2148, 0.3386], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2026, 0.2735, 0.2808, 0.2431], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2188, 0.2657, 0.2679, 0.2476], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.242 , 0.2809, 0.255 , 0.2222], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2167, 0.2901, 0.2835, 0.2098], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2286, 0.2902, 0.2467, 0.2344], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3624, 0.1926, 0.1997, 0.2453], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3094, 0.1782, 0.2089, 0.3035], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3187, 0.169 , 0.1939, 0.3184], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2723, 0.2331, 0.2405, 0.254 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2237, 0.2624, 0.2709, 0.243 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2518, 0.2454, 0.2247, 0.2781], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2459, 0.2369, 0.2201, 0.2972], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2871, 0.2366, 0.2321, 0.2442], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3076, 0.2178, 0.2014, 0.2733], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2828, 0.2323, 0.2163, 0.2686], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2259, 0.2564, 0.2641, 0.2536], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2323, 0.2539, 0.2613, 0.2525], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2565, 0.2373, 0.2324, 0.2738], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.6197, 0.0842, 0.0774, 0.2186], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2488, 0.2189, 0.2496, 0.2827], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2448, 0.2422, 0.248 , 0.265 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2458, 0.2447, 0.2397, 0.2698], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2531, 0.2237, 0.247 , 0.2762], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2559, 0.2457, 0.244 , 0.2544], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2328, 0.239 , 0.2636, 0.2645], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2767, 0.2207, 0.2196, 0.2829], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3071, 0.2045, 0.2029, 0.2855], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.29  , 0.202 , 0.224 , 0.2839], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1853, 0.2895, 0.2858, 0.2394], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1835, 0.2907, 0.3005, 0.2254], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1965, 0.2909, 0.2949, 0.2177], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1892, 0.295 , 0.2938, 0.2221], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2387, 0.262 , 0.263 , 0.2364], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2324, 0.2714, 0.2628, 0.2334], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2617, 0.2311, 0.2441, 0.263 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2522, 0.2251, 0.2255, 0.2972], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2447, 0.2459, 0.2598, 0.2496], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3202, 0.1752, 0.1973, 0.3073], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2638, 0.2015, 0.2059, 0.3288], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2833, 0.208 , 0.2327, 0.276 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3219, 0.201 , 0.2185, 0.2586], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2793, 0.2312, 0.2358, 0.2537], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2572, 0.2391, 0.2515, 0.2522], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2931, 0.2244, 0.2307, 0.2518], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1905, 0.2351, 0.2932, 0.2812], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.266 , 0.1116, 0.1486, 0.4738], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3065, 0.1271, 0.1726, 0.3938], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3262, 0.1328, 0.1481, 0.393 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3772, 0.1854, 0.1887, 0.2487], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2394, 0.2024, 0.2237, 0.3345], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2363, 0.2746, 0.2615, 0.2276], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3321, 0.2042, 0.2076, 0.2561], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2415, 0.2366, 0.2449, 0.277 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2489, 0.213 , 0.2406, 0.2975], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2485, 0.2349, 0.2277, 0.2889], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.236 , 0.2354, 0.2374, 0.2912], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2324, 0.2415, 0.2452, 0.2808], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2309, 0.2428, 0.2377, 0.2886], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2354, 0.2324, 0.2599, 0.2724], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2396, 0.2452, 0.2691, 0.246 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2302, 0.2473, 0.2753, 0.2472], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2136, 0.2599, 0.269 , 0.2575], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2742, 0.2268, 0.2203, 0.2787], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3095, 0.2276, 0.2115, 0.2514], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2068, 0.2805, 0.2803, 0.2325], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2154, 0.2677, 0.2731, 0.2439], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2309, 0.2554, 0.2655, 0.2482], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1951, 0.2951, 0.3036, 0.2061], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2774, 0.2591, 0.2312, 0.2322], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2734, 0.2507, 0.2504, 0.2254], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3144, 0.2293, 0.2156, 0.2407], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4717, 0.1388, 0.1429, 0.2466], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1677, 0.3022, 0.322 , 0.2081], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3292, 0.2351, 0.2099, 0.2257], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.301 , 0.2506, 0.2227, 0.2257], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2201, 0.2753, 0.2697, 0.235 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2211, 0.2781, 0.2633, 0.2375], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3007, 0.2291, 0.2096, 0.2605], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2632, 0.1316, 0.1478, 0.4573], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2497, 0.1827, 0.1864, 0.3812], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2897, 0.1446, 0.1512, 0.4144], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2199, 0.1599, 0.1619, 0.4583], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2293, 0.2288, 0.273 , 0.2689], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2068, 0.2929, 0.2846, 0.2158], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4254, 0.1407, 0.1572, 0.2768], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1928, 0.2729, 0.28  , 0.2543], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3315, 0.2093, 0.2053, 0.2539], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2871, 0.2099, 0.2414, 0.2616], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3376, 0.184 , 0.1878, 0.2906], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3413, 0.1805, 0.198 , 0.2802], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2756, 0.2189, 0.2354, 0.2701], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.284 , 0.2134, 0.2203, 0.2823], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2385, 0.2775, 0.2569, 0.2271], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2491, 0.2574, 0.2447, 0.2488], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3196, 0.2579, 0.2202, 0.2023], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2912, 0.2045, 0.213 , 0.2913], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2258, 0.2474, 0.2362, 0.2906], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2162, 0.2673, 0.2603, 0.2562], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2005, 0.2604, 0.2812, 0.2578], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2076, 0.2826, 0.279 , 0.2308], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2027, 0.2651, 0.2747, 0.2574], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.213 , 0.2819, 0.2805, 0.2247], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2606, 0.2506, 0.2478, 0.241 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2718, 0.2444, 0.2328, 0.251 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2572, 0.2325, 0.2423, 0.268 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2692, 0.2356, 0.2342, 0.261 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2173, 0.2591, 0.279 , 0.2445], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.261 , 0.2682, 0.2605, 0.2103], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2645, 0.2411, 0.236 , 0.2583], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2158, 0.2693, 0.2615, 0.2534], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2152, 0.2731, 0.2649, 0.2468], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2007, 0.2699, 0.2582, 0.2712], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.217 , 0.2842, 0.2784, 0.2203], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2335, 0.2335, 0.2467, 0.2864], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2683, 0.2258, 0.2283, 0.2776], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.214 , 0.2456, 0.2582, 0.2822], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.238 , 0.2382, 0.2373, 0.2865], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2501, 0.2261, 0.226 , 0.2978], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2428, 0.2271, 0.2189, 0.3112], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2333, 0.2106, 0.1868, 0.3693], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1773, 0.1908, 0.1897, 0.4422], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2397, 0.2637, 0.2597, 0.237 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2192, 0.286 , 0.2695, 0.2254], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.209 , 0.2928, 0.2774, 0.2207], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2423, 0.2546, 0.247 , 0.2562], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2177, 0.247 , 0.2454, 0.29  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2276, 0.2323, 0.247 , 0.2931], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.285 , 0.2389, 0.2336, 0.2426], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2876, 0.2453, 0.2348, 0.2323], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5429, 0.1583, 0.1146, 0.1842], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3303, 0.2054, 0.2014, 0.2629], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.418, 0.173, 0.152, 0.257], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4828, 0.1614, 0.1427, 0.2131], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2488, 0.2347, 0.2575, 0.2591], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2519, 0.2425, 0.2495, 0.2561], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2542, 0.2241, 0.2414, 0.2804], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1845, 0.2809, 0.2928, 0.2418], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2306, 0.2784, 0.2714, 0.2196], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2425, 0.2595, 0.2652, 0.2329], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2495, 0.2802, 0.257 , 0.2132], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2279, 0.283 , 0.2709, 0.2182], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.247 , 0.2678, 0.2551, 0.2302], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2216, 0.2573, 0.2702, 0.2509], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2474, 0.2326, 0.2463, 0.2737], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2362, 0.2278, 0.25  , 0.2861], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2436, 0.2631, 0.249 , 0.2443], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2201, 0.2439, 0.2513, 0.2847], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2304, 0.2539, 0.2563, 0.2594], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2292, 0.2345, 0.253 , 0.2834], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2833, 0.2356, 0.2247, 0.2564], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2039, 0.2762, 0.284 , 0.2358], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1999, 0.2921, 0.275 , 0.233 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3411, 0.1732, 0.1844, 0.3013], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3335, 0.1786, 0.1961, 0.2918], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2787, 0.192 , 0.2175, 0.3118], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2805, 0.1933, 0.1943, 0.332 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.6327, 0.1132, 0.0798, 0.1742], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.315 , 0.1943, 0.1987, 0.292 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3052, 0.208 , 0.2235, 0.2633], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2419, 0.2471, 0.2651, 0.2459], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2629, 0.2642, 0.2464, 0.2265], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3096, 0.2182, 0.2113, 0.2608], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2663, 0.2361, 0.247 , 0.2506], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2451, 0.2713, 0.2642, 0.2194], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.248 , 0.253 , 0.2628, 0.2362], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2578, 0.252 , 0.2523, 0.238 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2414, 0.259 , 0.2608, 0.2388], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2925, 0.2396, 0.2337, 0.2342], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2239, 0.2501, 0.268 , 0.258 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.237 , 0.2592, 0.2578, 0.2461], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2543, 0.2656, 0.2506, 0.2295], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3285, 0.2037, 0.1913, 0.2766], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2971, 0.221 , 0.2319, 0.25  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.361 , 0.1881, 0.1782, 0.2727], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2825, 0.2329, 0.2347, 0.2499], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2868, 0.2426, 0.2357, 0.2349], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.292 , 0.2302, 0.2354, 0.2424], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3599, 0.1984, 0.1826, 0.2591], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.282 , 0.2328, 0.2324, 0.2528], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3401, 0.1897, 0.1939, 0.2763], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.335 , 0.2012, 0.2231, 0.2407], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2492, 0.239 , 0.256 , 0.2557], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2612, 0.2437, 0.2502, 0.2448], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2812, 0.2224, 0.2418, 0.2546], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2708, 0.2456, 0.2587, 0.2248], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2283, 0.2557, 0.2677, 0.2483], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2635, 0.236 , 0.2374, 0.2631], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2654, 0.2648, 0.2403, 0.2295], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.28  , 0.2005, 0.2132, 0.3063], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2718, 0.2203, 0.2231, 0.2848], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2475, 0.2328, 0.2468, 0.2728], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2522, 0.2323, 0.2394, 0.2761], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2437, 0.2364, 0.2544, 0.2655], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3034, 0.2132, 0.2161, 0.2673], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2319, 0.2808, 0.2682, 0.2191], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2362, 0.2653, 0.2672, 0.2313], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2431, 0.2733, 0.263 , 0.2205], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2743, 0.2403, 0.2459, 0.2396], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2879, 0.2254, 0.2266, 0.2601], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3001, 0.2175, 0.214 , 0.2684], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3121, 0.2102, 0.2172, 0.2605], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2108, 0.2696, 0.3121, 0.2075], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3746, 0.2677, 0.1834, 0.1742], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3703, 0.2182, 0.2045, 0.207 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4853, 0.1782, 0.1456, 0.1909], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2565, 0.2462, 0.2557, 0.2417], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2505, 0.2555, 0.2642, 0.2298], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2313, 0.2617, 0.2733, 0.2337], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.232 , 0.2597, 0.2699, 0.2384], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1864, 0.2453, 0.2898, 0.2785], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2089, 0.2554, 0.2744, 0.2613], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.247 , 0.2525, 0.2595, 0.2409], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2896, 0.2399, 0.2376, 0.2328], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.306 , 0.2309, 0.2138, 0.2493], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2431, 0.2612, 0.2489, 0.2468], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2535, 0.2665, 0.2534, 0.2266], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3139, 0.2077, 0.2153, 0.2631], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.321 , 0.2152, 0.217 , 0.2468], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2126, 0.2478, 0.2157, 0.3239], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2703, 0.2524, 0.2406, 0.2366], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3244, 0.2074, 0.2075, 0.2607], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2997, 0.2263, 0.2346, 0.2394], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2861, 0.2379, 0.2345, 0.2415], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2152, 0.2451, 0.26  , 0.2797], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2128, 0.2501, 0.2568, 0.2803], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2667, 0.2492, 0.2516, 0.2325], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3162, 0.221 , 0.2126, 0.2502], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2178, 0.2807, 0.2696, 0.2319], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2361, 0.2585, 0.2652, 0.2402], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3573, 0.1952, 0.1762, 0.2713], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3428, 0.2079, 0.1926, 0.2567], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2448, 0.2403, 0.2474, 0.2675], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.265 , 0.2345, 0.2499, 0.2507], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2616, 0.2411, 0.2394, 0.2578], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2908, 0.2351, 0.2277, 0.2464], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3464, 0.1975, 0.1952, 0.2608], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2233, 0.2627, 0.2713, 0.2427], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2227, 0.2732, 0.2729, 0.2312], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2618, 0.2471, 0.2515, 0.2396], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2514, 0.2552, 0.252 , 0.2415], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2084, 0.2675, 0.2795, 0.2446], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.217 , 0.2767, 0.2797, 0.2266], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2485, 0.2538, 0.2458, 0.2518], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2813, 0.2258, 0.2318, 0.261 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2333, 0.2398, 0.2579, 0.269 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2427, 0.256 , 0.2569, 0.2445], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2857, 0.2365, 0.2311, 0.2467], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2054, 0.2853, 0.2631, 0.2462], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2955, 0.2231, 0.2047, 0.2767], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.255 , 0.203 , 0.2077, 0.3343], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2653, 0.205 , 0.2194, 0.3103], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3235, 0.1818, 0.1892, 0.3055], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2826, 0.2352, 0.2487, 0.2335], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3183, 0.2242, 0.236 , 0.2215], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2449, 0.2495, 0.2456, 0.2601], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2403, 0.2493, 0.2537, 0.2567], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3161, 0.2023, 0.2248, 0.2568], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3081, 0.2167, 0.2257, 0.2495], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2698, 0.2183, 0.2363, 0.2756], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2268, 0.2551, 0.2656, 0.2525], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2179, 0.2615, 0.265 , 0.2556], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2875, 0.2353, 0.2315, 0.2458], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2866, 0.2374, 0.2312, 0.2449], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2692, 0.2254, 0.2473, 0.2581], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2759, 0.2407, 0.2406, 0.2427], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2566, 0.2327, 0.2485, 0.2622], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2855, 0.2204, 0.2268, 0.2672], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2587, 0.2296, 0.2347, 0.2771], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2196, 0.2655, 0.265 , 0.2499], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2347, 0.2713, 0.2646, 0.2294], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2323, 0.2539, 0.2716, 0.2421], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.218 , 0.2323, 0.2464, 0.3033], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.253 , 0.2156, 0.2244, 0.307 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2461, 0.2387, 0.2568, 0.2584], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.244 , 0.2434, 0.2459, 0.2668], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2383, 0.2552, 0.2637, 0.2428], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2416, 0.2407, 0.2586, 0.2591], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2533, 0.2511, 0.2594, 0.2361], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2574, 0.2347, 0.2578, 0.25  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2821, 0.2455, 0.2526, 0.2197], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4479, 0.1852, 0.1652, 0.2017], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4847, 0.2146, 0.1359, 0.1648], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5366, 0.1868, 0.12  , 0.1565], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2746, 0.2311, 0.2244, 0.2699], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.211 , 0.2509, 0.2604, 0.2777], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2124, 0.2664, 0.275 , 0.2462], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2063, 0.2624, 0.2686, 0.2626], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2115, 0.2528, 0.2785, 0.2572], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2396, 0.2631, 0.2551, 0.2422], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2597, 0.2492, 0.2447, 0.2464], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2628, 0.2488, 0.2468, 0.2416], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2387, 0.2629, 0.2547, 0.2437], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2147, 0.2636, 0.2751, 0.2467], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2471, 0.2582, 0.2473, 0.2474], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2744, 0.2392, 0.2495, 0.2369], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2821, 0.2482, 0.2483, 0.2214], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2668, 0.2348, 0.2431, 0.2554], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2748, 0.2449, 0.2369, 0.2434], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.304 , 0.2374, 0.2254, 0.2332], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.268 , 0.2386, 0.2479, 0.2455], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2945, 0.2311, 0.2175, 0.257 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.284 , 0.2518, 0.2374, 0.2268], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2661, 0.2399, 0.2393, 0.2546], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2502, 0.2581, 0.2474, 0.2443], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2679, 0.2572, 0.2536, 0.2213], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2626, 0.249 , 0.2462, 0.2421], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2842, 0.2196, 0.221 , 0.2752], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2536, 0.2364, 0.2334, 0.2766], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2073, 0.2788, 0.2775, 0.2364], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2121, 0.2597, 0.2776, 0.2506], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2994, 0.1878, 0.2034, 0.3094], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2784, 0.2495, 0.2282, 0.2439], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.227 , 0.2763, 0.2733, 0.2234], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2432, 0.2381, 0.2579, 0.2608], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2748, 0.2298, 0.2164, 0.279 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2708, 0.2523, 0.2438, 0.2331], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2467, 0.2644, 0.2667, 0.2222], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2343, 0.2517, 0.247 , 0.267 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.211 , 0.2709, 0.2776, 0.2406], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2501, 0.2163, 0.2397, 0.2939], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.204 , 0.2513, 0.253 , 0.2918], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2472, 0.2399, 0.2559, 0.257 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2833, 0.2452, 0.2435, 0.228 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.267 , 0.237 , 0.2463, 0.2496], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2675, 0.2453, 0.2502, 0.2369], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.276 , 0.2499, 0.238 , 0.2361], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2256, 0.2471, 0.2602, 0.2671], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2365, 0.2625, 0.2483, 0.2527], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2833, 0.2287, 0.2283, 0.2597], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2819, 0.2349, 0.2372, 0.246 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2642, 0.2104, 0.2214, 0.304 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2945, 0.237 , 0.2415, 0.227 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3028, 0.2411, 0.2105, 0.2456], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2224, 0.2423, 0.2596, 0.2757], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1968, 0.1957, 0.2282, 0.3793], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2438, 0.2516, 0.239 , 0.2656], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2324, 0.2771, 0.2665, 0.224 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2636, 0.2352, 0.2471, 0.2541], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.236 , 0.2698, 0.2692, 0.225 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2557, 0.2595, 0.2589, 0.2259], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2539, 0.2409, 0.2449, 0.2603], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3027, 0.2087, 0.2085, 0.2801], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2815, 0.2406, 0.2395, 0.2384], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3183, 0.19  , 0.2159, 0.2758], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2936, 0.2281, 0.2242, 0.2541], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2375, 0.2595, 0.2517, 0.2512], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2639, 0.2377, 0.2492, 0.2492], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2287, 0.2444, 0.2554, 0.2715], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.262 , 0.2412, 0.2429, 0.2539], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2329, 0.2212, 0.2502, 0.2958], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2207, 0.2359, 0.2492, 0.2942], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2392, 0.2442, 0.2555, 0.2611], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2268, 0.256 , 0.2803, 0.2369], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2475, 0.2433, 0.255 , 0.2542], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2851, 0.2525, 0.238 , 0.2245], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2457, 0.2246, 0.2366, 0.2931], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2657, 0.2348, 0.2367, 0.2629], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3019, 0.2443, 0.226 , 0.2278], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2652, 0.2064, 0.2106, 0.3178], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2688, 0.2334, 0.2481, 0.2497], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3695, 0.1802, 0.1731, 0.2773], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2434, 0.2388, 0.2482, 0.2696], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2896, 0.2135, 0.2201, 0.2767], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2323, 0.2435, 0.2465, 0.2777], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2634, 0.2364, 0.2412, 0.259 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2555, 0.2348, 0.2372, 0.2726], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2688, 0.2435, 0.2417, 0.246 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2322, 0.2157, 0.2475, 0.3046], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2541, 0.2166, 0.235 , 0.2943], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3776, 0.1892, 0.1628, 0.2705], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2721, 0.2405, 0.2465, 0.2409], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2754, 0.2503, 0.2437, 0.2306], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2828, 0.2344, 0.238 , 0.2448], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2926, 0.2384, 0.2347, 0.2343], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3315, 0.2051, 0.2206, 0.2429], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.281 , 0.2178, 0.228 , 0.2732], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2837, 0.2559, 0.2364, 0.224 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3155, 0.2194, 0.209 , 0.2562], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2236, 0.2367, 0.2563, 0.2834], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2522, 0.2549, 0.2529, 0.2399], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2213, 0.249 , 0.2568, 0.273 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2137, 0.2399, 0.2489, 0.2974], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2234, 0.2847, 0.2775, 0.2145], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.223 , 0.2705, 0.2762, 0.2303], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2606, 0.2391, 0.2217, 0.2786], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2566, 0.2463, 0.2434, 0.2537], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2362, 0.2689, 0.2526, 0.2423], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2494, 0.2514, 0.2577, 0.2415], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2388, 0.255 , 0.2546, 0.2516], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2414, 0.255 , 0.2551, 0.2485], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2513, 0.2229, 0.2533, 0.2725], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2643, 0.2234, 0.2349, 0.2774], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2452, 0.2361, 0.2644, 0.2543], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2632, 0.2382, 0.2526, 0.2461], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2246, 0.2511, 0.2671, 0.2572], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2473, 0.2547, 0.2423, 0.2558], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2147, 0.2671, 0.2769, 0.2413], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2551, 0.2451, 0.241 , 0.2587], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2201, 0.251 , 0.2552, 0.2737], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2339, 0.2369, 0.2235, 0.3058], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2257, 0.2435, 0.2428, 0.288 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2485, 0.2622, 0.253 , 0.2363], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2327, 0.2558, 0.2451, 0.2664], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2512, 0.257 , 0.2529, 0.2389], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2487, 0.2686, 0.2495, 0.2332], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2508, 0.259 , 0.2478, 0.2424], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.292 , 0.2259, 0.2235, 0.2587], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2586, 0.2523, 0.2444, 0.2447], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2551, 0.2505, 0.2477, 0.2467], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2434, 0.2703, 0.2537, 0.2326], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2205, 0.254 , 0.272 , 0.2534], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2285, 0.2635, 0.263 , 0.245 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2039, 0.2181, 0.2437, 0.3343], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2392, 0.2458, 0.2612, 0.2538], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2312, 0.2337, 0.2398, 0.2952], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2464, 0.2513, 0.2428, 0.2596], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2251, 0.2622, 0.2612, 0.2515], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2381, 0.273 , 0.2669, 0.222 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2992, 0.2251, 0.2149, 0.2608], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2762, 0.2266, 0.2251, 0.2722], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2875, 0.2232, 0.2133, 0.2759], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2817, 0.2183, 0.2244, 0.2756], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2626, 0.2322, 0.2414, 0.2638], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3404, 0.1969, 0.2028, 0.2599], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2562, 0.2279, 0.2379, 0.278 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2053, 0.2413, 0.2568, 0.2967], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1891, 0.2584, 0.2744, 0.2781], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2665, 0.2442, 0.2494, 0.24  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.258 , 0.2313, 0.2512, 0.2595], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2912, 0.2154, 0.2253, 0.268 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2533, 0.2493, 0.2636, 0.2339], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2904, 0.2223, 0.215 , 0.2723], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2647, 0.2271, 0.2272, 0.2809], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2864, 0.2327, 0.2372, 0.2437], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3183, 0.2075, 0.2099, 0.2643], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2341, 0.2608, 0.2588, 0.2462], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2414, 0.234 , 0.2621, 0.2625], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2414, 0.2305, 0.2632, 0.2649], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2479, 0.2415, 0.2536, 0.257 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2678, 0.2415, 0.249 , 0.2417], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2639, 0.2359, 0.2435, 0.2567], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2401, 0.2411, 0.2445, 0.2743], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2208, 0.2639, 0.2501, 0.2653], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2057, 0.2654, 0.2599, 0.2689], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.225 , 0.2599, 0.2529, 0.2621], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2373, 0.254 , 0.2532, 0.2555], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2136, 0.247 , 0.2691, 0.2703], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2208, 0.2585, 0.2554, 0.2654], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2147, 0.2561, 0.2802, 0.2489], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2292, 0.2352, 0.2533, 0.2823], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2294, 0.2471, 0.2597, 0.2638], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2596, 0.2483, 0.2538, 0.2384], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3556, 0.1718, 0.1857, 0.2869], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.323 , 0.1931, 0.2106, 0.2733], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2314, 0.2585, 0.2624, 0.2478], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2295, 0.2458, 0.2505, 0.2742], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2158, 0.2556, 0.2645, 0.2641], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2187, 0.2704, 0.2734, 0.2374], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.221 , 0.2594, 0.264 , 0.2556], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.216 , 0.2733, 0.2818, 0.229 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2259, 0.277 , 0.2659, 0.2311], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2477, 0.2588, 0.2652, 0.2283], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2716, 0.2478, 0.2379, 0.2427], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2636, 0.267 , 0.254 , 0.2154], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2446, 0.2443, 0.2508, 0.2603], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2779, 0.2306, 0.2303, 0.2612], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2966, 0.2191, 0.228 , 0.2563], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2789, 0.2181, 0.217 , 0.286 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2451, 0.2383, 0.2318, 0.2848], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2861, 0.2216, 0.218 , 0.2742], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3061, 0.1871, 0.2051, 0.3016], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2815, 0.2246, 0.2267, 0.2673], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2733, 0.241 , 0.2436, 0.2422], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2795, 0.2395, 0.2375, 0.2435], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2087, 0.2679, 0.2672, 0.2562], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.192 , 0.2565, 0.2653, 0.2863], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2096, 0.2784, 0.2811, 0.2309], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2168, 0.2637, 0.2623, 0.2572], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3245, 0.2389, 0.2142, 0.2224], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3024, 0.199 , 0.2116, 0.287 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3232, 0.2103, 0.2099, 0.2566], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.452 , 0.21  , 0.1584, 0.1797], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2456, 0.2708, 0.2678, 0.2157], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2383, 0.2601, 0.2732, 0.2284], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2459, 0.262 , 0.266 , 0.226 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.258 , 0.2327, 0.2491, 0.2602], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2719, 0.2335, 0.2365, 0.2581], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2302, 0.2307, 0.2491, 0.2901], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2469, 0.23  , 0.2336, 0.2896], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2618, 0.218 , 0.226 , 0.2942], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2455, 0.2282, 0.2485, 0.2777], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3232, 0.2028, 0.1997, 0.2744], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2661, 0.2333, 0.2303, 0.2703], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2443, 0.2343, 0.2479, 0.2735], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2537, 0.2423, 0.2486, 0.2554], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.23  , 0.2516, 0.2801, 0.2383], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3284, 0.1935, 0.2127, 0.2654], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3637, 0.197 , 0.1817, 0.2577], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2499, 0.2321, 0.2543, 0.2637], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3862, 0.1849, 0.1864, 0.2425], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2692, 0.2039, 0.2332, 0.2936], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2687, 0.2238, 0.2388, 0.2688], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2454, 0.2301, 0.2416, 0.2829], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2533, 0.2343, 0.2407, 0.2716], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2332, 0.2713, 0.2509, 0.2447], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2241, 0.2771, 0.2662, 0.2326], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.275 , 0.2355, 0.1964, 0.2931], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.218 , 0.2585, 0.218 , 0.3055], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2823, 0.2459, 0.2362, 0.2356], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3036, 0.2343, 0.2228, 0.2394], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2621, 0.2376, 0.2492, 0.2512], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2316, 0.2636, 0.2584, 0.2464], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2331, 0.2475, 0.2551, 0.2643], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2266, 0.2662, 0.2701, 0.2371], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2433, 0.2669, 0.2412, 0.2486], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2846, 0.2236, 0.2273, 0.2646], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3332, 0.1841, 0.1896, 0.2931], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2285, 0.2673, 0.261 , 0.2432], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2094, 0.2555, 0.2819, 0.2533], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2351, 0.2558, 0.2585, 0.2506], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2313, 0.2531, 0.2629, 0.2527], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2155, 0.2521, 0.2626, 0.2698], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1926, 0.2851, 0.3022, 0.2201], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2549, 0.2546, 0.2526, 0.2378], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2333, 0.2533, 0.2498, 0.2636], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2378, 0.2593, 0.265 , 0.2379], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2695, 0.2453, 0.2377, 0.2475], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2757, 0.2408, 0.2447, 0.2388], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.27  , 0.2317, 0.2357, 0.2627], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2681, 0.2355, 0.239 , 0.2574], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3481, 0.1848, 0.1956, 0.2716], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4686, 0.1954, 0.1503, 0.1857], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2372, 0.277 , 0.2577, 0.2281], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2208, 0.2558, 0.2552, 0.2681], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2312, 0.2798, 0.256 , 0.2329], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2437, 0.2487, 0.2634, 0.2442], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2475, 0.2634, 0.2548, 0.2343], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2234, 0.256 , 0.2618, 0.2588], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2275, 0.2766, 0.2647, 0.2312], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2809, 0.2444, 0.2389, 0.2358], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2186, 0.2556, 0.2561, 0.2696], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2012, 0.2744, 0.2889, 0.2355], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2422, 0.2656, 0.2497, 0.2424], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2182, 0.2726, 0.2677, 0.2415], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2329, 0.2788, 0.261 , 0.2273], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2448, 0.2491, 0.2484, 0.2577], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2439, 0.262 , 0.2607, 0.2334], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.245 , 0.2463, 0.2378, 0.2708], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2108, 0.2594, 0.2431, 0.2866], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2256, 0.2692, 0.2377, 0.2675], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2294, 0.2559, 0.2384, 0.2763], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2949, 0.2249, 0.2377, 0.2426], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2644, 0.2305, 0.25  , 0.2551], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2737, 0.2381, 0.2361, 0.2521], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.401 , 0.1672, 0.1662, 0.2656], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2781, 0.2348, 0.2439, 0.2432], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2742, 0.2479, 0.2461, 0.2318], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2797, 0.2427, 0.2398, 0.2378], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2987, 0.2293, 0.2332, 0.2389], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2707, 0.238 , 0.2434, 0.2479], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.21  , 0.2801, 0.2734, 0.2365], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1971, 0.2955, 0.2734, 0.234 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.265 , 0.2449, 0.2483, 0.2419], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2498, 0.2619, 0.2535, 0.2349], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2924, 0.2429, 0.2301, 0.2346], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3188, 0.237 , 0.2134, 0.2307], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3179, 0.2278, 0.1988, 0.2555], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3299, 0.2314, 0.214 , 0.2247], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.369 , 0.1871, 0.1844, 0.2595], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2703, 0.227 , 0.2215, 0.2812], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3368, 0.2127, 0.2093, 0.2412], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2388, 0.2432, 0.2465, 0.2715], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2374, 0.2405, 0.2184, 0.3037], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2727, 0.2198, 0.2246, 0.283 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3531, 0.1843, 0.1774, 0.2852], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2502, 0.1912, 0.2062, 0.3524], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2211, 0.2071, 0.2348, 0.337 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3071, 0.2209, 0.2254, 0.2466], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2446, 0.2389, 0.2428, 0.2738], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2134, 0.2202, 0.2601, 0.3063], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2544, 0.2227, 0.2251, 0.2978], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2998, 0.2234, 0.2216, 0.2552], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2831, 0.2086, 0.2109, 0.2974], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2818, 0.2173, 0.2467, 0.2542], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2434, 0.2103, 0.2252, 0.321 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3095, 0.2168, 0.2129, 0.2609], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2983, 0.2222, 0.2238, 0.2558], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.267 , 0.2253, 0.2305, 0.2771], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.258 , 0.2474, 0.2527, 0.2419], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3073, 0.1875, 0.1998, 0.3054], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2545, 0.239 , 0.2602, 0.2462], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2812, 0.2246, 0.2299, 0.2643], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2274, 0.267 , 0.2695, 0.2361], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2456, 0.2396, 0.254 , 0.2609], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.247 , 0.2135, 0.2717, 0.2677], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3193, 0.2005, 0.2058, 0.2743], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2471, 0.2313, 0.2618, 0.2598], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.228 , 0.2474, 0.2549, 0.2696], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3061, 0.1943, 0.1972, 0.3024], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2897, 0.2287, 0.2218, 0.2599], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.288 , 0.2334, 0.2289, 0.2497], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2716, 0.2404, 0.227 , 0.261 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2832, 0.231 , 0.2168, 0.269 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.274 , 0.2274, 0.239 , 0.2597], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.295 , 0.2148, 0.2234, 0.2667], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2274, 0.2641, 0.2698, 0.2387], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2616, 0.247 , 0.2518, 0.2396], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2444, 0.2613, 0.267 , 0.2274], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2897, 0.2352, 0.2236, 0.2516], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2843, 0.2301, 0.2392, 0.2464], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1897, 0.296 , 0.2913, 0.223 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2129, 0.2829, 0.2833, 0.2208], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2743, 0.2448, 0.2373, 0.2436], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2861, 0.2676, 0.2448, 0.2015], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2567, 0.2515, 0.2546, 0.2372], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2293, 0.2356, 0.2842, 0.2509], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.185 , 0.2732, 0.3063, 0.2355], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2438, 0.2301, 0.2629, 0.2632], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2367, 0.2433, 0.2595, 0.2605], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2152, 0.2467, 0.278 , 0.2601], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2143, 0.2422, 0.2755, 0.2679], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2147, 0.2424, 0.2666, 0.2763], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2233, 0.2537, 0.2867, 0.2364], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1995, 0.2619, 0.2994, 0.2392], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.198 , 0.2667, 0.2959, 0.2394], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2029, 0.2713, 0.2906, 0.2351], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1967, 0.2816, 0.2929, 0.2288], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2302, 0.2535, 0.2593, 0.257 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1901, 0.2399, 0.275 , 0.2949], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2106, 0.2356, 0.2411, 0.3127], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2367, 0.2477, 0.2455, 0.2701], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2281, 0.2485, 0.2436, 0.2799], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2669, 0.2066, 0.2135, 0.3131], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3217, 0.22  , 0.2049, 0.2534], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2275, 0.2751, 0.2409, 0.2565], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4591, 0.2053, 0.1404, 0.1952], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2396, 0.2186, 0.2247, 0.317 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2013, 0.2645, 0.2929, 0.2413], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1905, 0.2703, 0.2755, 0.2637], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2008, 0.2571, 0.266 , 0.2761], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1904, 0.2646, 0.2833, 0.2617], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1755, 0.2736, 0.2945, 0.2564], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1689, 0.2695, 0.292 , 0.2697], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2782, 0.2289, 0.2146, 0.2782], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2104, 0.2813, 0.2884, 0.2198], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2044, 0.3083, 0.3   , 0.1873], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2162, 0.3086, 0.2814, 0.1938], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1971, 0.2981, 0.295 , 0.2098], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1958, 0.2917, 0.2925, 0.22  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2036, 0.2888, 0.2892, 0.2184], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2117, 0.2755, 0.2822, 0.2305], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2054, 0.2866, 0.2856, 0.2223], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2019, 0.2935, 0.2797, 0.2249], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2641, 0.1769, 0.2224, 0.3366], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2214, 0.1538, 0.197 , 0.4278], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1713, 0.1772, 0.2347, 0.4168], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2553, 0.2279, 0.2265, 0.2903], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1903, 0.2021, 0.2441, 0.3634], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1997, 0.1964, 0.2318, 0.3721], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2402, 0.1945, 0.2284, 0.337 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.281 , 0.2344, 0.2376, 0.247 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3202, 0.2143, 0.21  , 0.2554], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2944, 0.2395, 0.2271, 0.239 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.206 , 0.2517, 0.2879, 0.2544], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2558, 0.2514, 0.241 , 0.2518], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2426, 0.2408, 0.2428, 0.2738], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2511, 0.2362, 0.2399, 0.2728], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2688, 0.2574, 0.2429, 0.2309], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.23  , 0.261 , 0.2539, 0.2551], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2539, 0.2113, 0.2603, 0.2745], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2484, 0.1423, 0.1695, 0.4398], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2293, 0.1398, 0.1968, 0.434 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2211, 0.2577, 0.2792, 0.2419], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2596, 0.239 , 0.2352, 0.2663], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2543, 0.2378, 0.2443, 0.2636], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.238 , 0.2499, 0.2612, 0.2508], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2212, 0.2548, 0.2731, 0.2509], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2387, 0.2634, 0.2625, 0.2354], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2829, 0.2637, 0.2445, 0.2089], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2471, 0.2717, 0.2491, 0.2322], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2762, 0.2046, 0.2028, 0.3164], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3064, 0.2248, 0.2205, 0.2483], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2327, 0.2156, 0.2053, 0.3464], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2395, 0.2621, 0.2576, 0.2408], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2674, 0.2156, 0.2383, 0.2787], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2541, 0.2223, 0.2432, 0.2805], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2659, 0.2457, 0.2343, 0.2542], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2744, 0.2442, 0.2356, 0.2458], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.298 , 0.2094, 0.2201, 0.2725], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2162, 0.2342, 0.2457, 0.3038], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.273 , 0.2334, 0.227 , 0.2667], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2482, 0.2529, 0.2538, 0.2451], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2398, 0.2577, 0.2606, 0.2419], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3126, 0.2239, 0.2153, 0.2483], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3634, 0.1755, 0.1712, 0.2899], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3362, 0.1962, 0.1907, 0.277 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2313, 0.2686, 0.2735, 0.2266], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.234 , 0.2655, 0.2581, 0.2424], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2454, 0.2561, 0.2457, 0.2528], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3024, 0.216 , 0.2324, 0.2492], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2995, 0.2011, 0.2344, 0.265 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2731, 0.2092, 0.2486, 0.2691], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2441, 0.2273, 0.2584, 0.2702], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2873, 0.1981, 0.2083, 0.3063], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4085, 0.151 , 0.1592, 0.2813], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2707, 0.2429, 0.2284, 0.258 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2587, 0.2363, 0.2375, 0.2675], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2551, 0.2461, 0.2511, 0.2477], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.303 , 0.2018, 0.2123, 0.2829], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2148, 0.2665, 0.2787, 0.24  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2236, 0.2639, 0.272 , 0.2405], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2356, 0.27  , 0.2608, 0.2336], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2087, 0.2517, 0.2719, 0.2677], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2189, 0.2734, 0.2678, 0.2399], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3415, 0.1966, 0.2032, 0.2587], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2868, 0.218 , 0.2229, 0.2723], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2518, 0.2428, 0.2569, 0.2485], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2526, 0.2552, 0.2458, 0.2464], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3085, 0.2148, 0.2193, 0.2574], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3019, 0.2198, 0.2319, 0.2464], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3672, 0.1628, 0.1697, 0.3003], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2859, 0.2346, 0.226 , 0.2534], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2983, 0.2261, 0.222 , 0.2536], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2442, 0.2643, 0.2551, 0.2364], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2833, 0.2349, 0.2237, 0.2582], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3179, 0.2157, 0.1953, 0.2712], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2709, 0.2307, 0.2352, 0.2632], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2914, 0.2219, 0.2228, 0.2638], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2315, 0.2253, 0.2414, 0.3017], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2232, 0.2379, 0.2494, 0.2895], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2433, 0.2421, 0.2394, 0.2751], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2066, 0.244 , 0.2548, 0.2945], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1999, 0.2699, 0.2737, 0.2564], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2089, 0.2565, 0.2591, 0.2755], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2591, 0.2267, 0.2308, 0.2834], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2525, 0.2299, 0.2596, 0.258 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2573, 0.2415, 0.2523, 0.2489], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2179, 0.2517, 0.2662, 0.2642], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2657, 0.2354, 0.253 , 0.2459], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2269, 0.2841, 0.2638, 0.2253], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2385, 0.2552, 0.271 , 0.2353], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2651, 0.2661, 0.275 , 0.1938], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2656, 0.2275, 0.2469, 0.26  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.186 , 0.1351, 0.1866, 0.4922], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1843, 0.1518, 0.1951, 0.4688], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2407, 0.2639, 0.2658, 0.2296], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2383, 0.2646, 0.2598, 0.2372], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3283, 0.1995, 0.203 , 0.2692], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2609, 0.2226, 0.2301, 0.2864], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2271, 0.2603, 0.2743, 0.2383], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2314, 0.2697, 0.2594, 0.2395], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2238, 0.2453, 0.254 , 0.2769], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2612, 0.2269, 0.2396, 0.2723], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3049, 0.2083, 0.2183, 0.2685], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2207, 0.2314, 0.2605, 0.2874], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1713, 0.2128, 0.26  , 0.356 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3255, 0.2296, 0.2105, 0.2344], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.25  , 0.243 , 0.2615, 0.2455], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2677, 0.2395, 0.2297, 0.263 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2976, 0.1981, 0.2039, 0.3004], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2967, 0.2285, 0.2298, 0.245 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2413, 0.216 , 0.2352, 0.3075], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2238, 0.2527, 0.2546, 0.2689], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2223, 0.2728, 0.2624, 0.2425], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2047, 0.2641, 0.2414, 0.2897], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3145, 0.2184, 0.223 , 0.2442], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2522, 0.2259, 0.2075, 0.3144], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3158, 0.2339, 0.2131, 0.2371], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2917, 0.2285, 0.2278, 0.252 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2667, 0.2359, 0.2399, 0.2575], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2752, 0.2388, 0.2314, 0.2546], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2707, 0.235 , 0.2289, 0.2654], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2754, 0.2353, 0.2415, 0.2477], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2849, 0.239 , 0.2291, 0.2469], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2966, 0.2102, 0.2089, 0.2843], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.324 , 0.2095, 0.2211, 0.2455], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2316, 0.2532, 0.2643, 0.2509], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2444, 0.2717, 0.2606, 0.2234], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2157, 0.2536, 0.2607, 0.27  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2545, 0.2438, 0.2405, 0.2612], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3047, 0.1969, 0.192 , 0.3064], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2527, 0.2478, 0.2489, 0.2507], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2568, 0.244 , 0.2442, 0.255 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2056, 0.2589, 0.2642, 0.2713], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2247, 0.2868, 0.2712, 0.2174], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1996, 0.245 , 0.2432, 0.3123], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1961, 0.2689, 0.2524, 0.2826], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.255 , 0.2552, 0.2529, 0.2369], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3181, 0.2032, 0.1974, 0.2813], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3315, 0.1884, 0.1915, 0.2886], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2885, 0.2063, 0.2317, 0.2735], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2844, 0.2256, 0.2391, 0.2508], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2862, 0.2282, 0.2189, 0.2667], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2856, 0.2292, 0.2295, 0.2558], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2999, 0.2034, 0.2078, 0.289 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2854, 0.2127, 0.2149, 0.287 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3118, 0.1944, 0.1991, 0.2947], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3013, 0.2165, 0.2159, 0.2663], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3245, 0.2006, 0.2164, 0.2585], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2732, 0.2274, 0.23  , 0.2694], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2651, 0.2594, 0.2493, 0.2262], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2496, 0.2473, 0.2446, 0.2586], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2588, 0.2478, 0.2456, 0.2478], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2538, 0.2444, 0.2617, 0.2401], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3549, 0.1972, 0.1975, 0.2504], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2865, 0.2288, 0.2391, 0.2456], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2832, 0.2398, 0.2372, 0.2398], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2323, 0.2687, 0.2694, 0.2296], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2384, 0.2597, 0.2686, 0.2333], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2327, 0.23  , 0.2499, 0.2875], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2819, 0.2176, 0.2305, 0.27  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2657, 0.2395, 0.2492, 0.2456], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.269 , 0.2347, 0.2447, 0.2516], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2329, 0.2793, 0.2546, 0.2332], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2172, 0.2751, 0.2635, 0.2442], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2968, 0.2148, 0.2227, 0.2657], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2916, 0.2448, 0.2448, 0.2188], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2939, 0.2108, 0.2226, 0.2727], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2586, 0.2361, 0.2364, 0.2689], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2879, 0.2154, 0.2166, 0.2802], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3144, 0.2193, 0.217 , 0.2494], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3246, 0.1919, 0.1872, 0.2962], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3374, 0.2   , 0.1993, 0.2633], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3692, 0.1623, 0.1672, 0.3012], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3073, 0.2088, 0.2217, 0.2621], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3421, 0.173 , 0.1894, 0.2954], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3136, 0.1911, 0.2283, 0.267 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2784, 0.2245, 0.2281, 0.269 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2295, 0.2507, 0.2682, 0.2516], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2665, 0.2446, 0.238 , 0.2509], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2241, 0.2986, 0.2596, 0.2177], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2606, 0.2439, 0.2355, 0.26  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2839, 0.2368, 0.2253, 0.254 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2025, 0.2719, 0.2734, 0.2521], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3276, 0.2171, 0.2036, 0.2517], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2293, 0.2501, 0.2581, 0.2624], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2806, 0.242 , 0.2331, 0.2443], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3202, 0.1885, 0.1876, 0.3037], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3386, 0.2016, 0.1951, 0.2647], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3131, 0.2136, 0.2049, 0.2684], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.253 , 0.246 , 0.2391, 0.2619], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2319, 0.2329, 0.2479, 0.2873], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2519, 0.2499, 0.2441, 0.2541], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2412, 0.2424, 0.2477, 0.2687], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2319, 0.2664, 0.2722, 0.2295], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.233 , 0.2602, 0.2698, 0.237 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2328, 0.2523, 0.2579, 0.2569], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3081, 0.2495, 0.2261, 0.2163], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2613, 0.2333, 0.2239, 0.2815], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2661, 0.255 , 0.254 , 0.2248], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.293 , 0.2382, 0.2221, 0.2467], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2444, 0.2479, 0.2591, 0.2486], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2604, 0.2503, 0.241 , 0.2483], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.247 , 0.2382, 0.2571, 0.2578], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2624, 0.2465, 0.2352, 0.2559], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2432, 0.2518, 0.2543, 0.2507], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2071, 0.2635, 0.292 , 0.2373], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2541, 0.2587, 0.2434, 0.2438], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2763, 0.2549, 0.243 , 0.2257], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2306, 0.2588, 0.2751, 0.2355], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2344, 0.2585, 0.2691, 0.238 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2253, 0.2582, 0.2673, 0.2492], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2378, 0.2582, 0.2662, 0.2377], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3163, 0.1932, 0.2144, 0.2761], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2401, 0.2624, 0.2585, 0.239 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2225, 0.2447, 0.2678, 0.2649], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2506, 0.2397, 0.2529, 0.2568], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2982, 0.2257, 0.236 , 0.2401], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3271, 0.2339, 0.2171, 0.2219], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3284, 0.2265, 0.216 , 0.2291], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3019, 0.2321, 0.2241, 0.2419], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3282, 0.2299, 0.2053, 0.2366], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3402, 0.1581, 0.1865, 0.3151], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3422, 0.1921, 0.1997, 0.266 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3254, 0.1855, 0.2147, 0.2743], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3013, 0.1964, 0.2298, 0.2725], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.251 , 0.233 , 0.2592, 0.2568], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2633, 0.2418, 0.2439, 0.251 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2174, 0.2481, 0.2739, 0.2605], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2087, 0.2703, 0.2824, 0.2386], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2004, 0.2723, 0.2751, 0.2522], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2285, 0.2759, 0.2715, 0.2241], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2111, 0.2581, 0.27  , 0.2608], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2241, 0.258 , 0.2542, 0.2637], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2172, 0.2583, 0.268 , 0.2565], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2647, 0.2643, 0.2474, 0.2235], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2129, 0.2945, 0.272 , 0.2207], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2037, 0.2741, 0.2674, 0.2549], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2848, 0.2577, 0.2425, 0.215 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.261 , 0.2535, 0.2621, 0.2233], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2459, 0.2584, 0.2491, 0.2466], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2501, 0.2492, 0.2579, 0.2428], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2789, 0.2399, 0.2474, 0.2339], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3035, 0.2186, 0.2223, 0.2556], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3376, 0.1978, 0.192 , 0.2726], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3543, 0.2079, 0.1911, 0.2467], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2685, 0.2359, 0.2391, 0.2565], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2491, 0.2631, 0.2541, 0.2337], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.344 , 0.226 , 0.1964, 0.2335], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.277 , 0.1788, 0.1896, 0.3545], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2851, 0.2218, 0.23  , 0.263 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2276, 0.2598, 0.2721, 0.2405], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2372, 0.256 , 0.2702, 0.2366], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2596, 0.2671, 0.2463, 0.227 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2547, 0.2561, 0.2484, 0.2409], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2523, 0.2328, 0.2334, 0.2815], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.257 , 0.244 , 0.2447, 0.2543], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2547, 0.2326, 0.2544, 0.2583], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2247, 0.2765, 0.272 , 0.2268], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2569, 0.2539, 0.2409, 0.2484], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2406, 0.2564, 0.259 , 0.244 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2342, 0.2588, 0.2554, 0.2517], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2547, 0.2516, 0.2333, 0.2605], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2945, 0.2247, 0.241 , 0.2398], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2424, 0.2263, 0.2622, 0.2691], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2662, 0.2229, 0.233 , 0.2778], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.256 , 0.2238, 0.2424, 0.2778], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2647, 0.2418, 0.2622, 0.2313], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2524, 0.246 , 0.2465, 0.2552], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2568, 0.26  , 0.2512, 0.2319], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2215, 0.2612, 0.2254, 0.292 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2338, 0.2729, 0.2396, 0.2537], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2268, 0.2491, 0.2299, 0.2942], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2211, 0.2491, 0.2395, 0.2904], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2105, 0.2791, 0.2497, 0.2606], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1994, 0.2648, 0.2428, 0.293 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2468, 0.264 , 0.2412, 0.248 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2878, 0.2436, 0.2164, 0.2523], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2527, 0.249 , 0.2386, 0.2597], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2435, 0.2648, 0.2502, 0.2415], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2169, 0.2619, 0.2606, 0.2605], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2047, 0.265 , 0.2724, 0.2579], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2154, 0.2822, 0.2746, 0.2278], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2494, 0.2607, 0.2588, 0.2311], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3157, 0.2405, 0.2139, 0.2299], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3352, 0.2075, 0.1941, 0.2633], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.218 , 0.2703, 0.2793, 0.2324], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2322, 0.2635, 0.2672, 0.2371], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2261, 0.271 , 0.2712, 0.2316], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3242, 0.1821, 0.2074, 0.2863], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1872, 0.2176, 0.2368, 0.3583], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2187, 0.2619, 0.2244, 0.295 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.191 , 0.2264, 0.2148, 0.3678], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2265, 0.2415, 0.2046, 0.3274], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3059, 0.2217, 0.236 , 0.2364], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3142, 0.223 , 0.2286, 0.2342], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2665, 0.2419, 0.2544, 0.2372], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2676, 0.2257, 0.2419, 0.2648], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.265 , 0.2433, 0.2539, 0.2377], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2621, 0.2375, 0.2536, 0.2468], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3545, 0.1842, 0.2111, 0.2501], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3   , 0.2027, 0.2092, 0.2881], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.283 , 0.2092, 0.2267, 0.281 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2754, 0.231 , 0.2536, 0.2399], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2967, 0.232 , 0.2372, 0.2341], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.272 , 0.2189, 0.2233, 0.2859], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2218, 0.2479, 0.2546, 0.2757], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2434, 0.262 , 0.2688, 0.2258], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2233, 0.2725, 0.2746, 0.2297], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2355, 0.2634, 0.2708, 0.2303], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2296, 0.2759, 0.2771, 0.2173], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2315, 0.2717, 0.2589, 0.2379], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.231 , 0.2652, 0.2558, 0.2479], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2592, 0.2324, 0.236 , 0.2724], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.235 , 0.2511, 0.2641, 0.2497], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2508, 0.2349, 0.2547, 0.2596], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2405, 0.2574, 0.255 , 0.247 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2026, 0.2796, 0.281 , 0.2368], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2154, 0.2751, 0.2747, 0.2348], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2064, 0.2822, 0.281 , 0.2304], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2583, 0.2234, 0.2256, 0.2927], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2456, 0.2408, 0.2391, 0.2746], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.239 , 0.2281, 0.2367, 0.2962], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2442, 0.2297, 0.2428, 0.2832], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2836, 0.211 , 0.218 , 0.2875], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2955, 0.231 , 0.2242, 0.2493], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2642, 0.2294, 0.2316, 0.2749], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2498, 0.276 , 0.2526, 0.2215], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2618, 0.2583, 0.2708, 0.2091], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1978, 0.2607, 0.2787, 0.2628], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.37  , 0.2125, 0.1922, 0.2253], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2237, 0.263 , 0.2708, 0.2425], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2126, 0.28  , 0.2721, 0.2353], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2237, 0.2596, 0.2734, 0.2433], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2015, 0.2601, 0.2712, 0.2673], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2085, 0.2737, 0.273 , 0.2448], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2034, 0.269 , 0.2687, 0.2589], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2252, 0.272 , 0.2473, 0.2554], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2348, 0.2604, 0.2476, 0.2572], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2242, 0.2694, 0.2576, 0.2488], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2618, 0.2425, 0.2347, 0.2611], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2555, 0.2583, 0.2507, 0.2355], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2861, 0.2496, 0.2409, 0.2233], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3057, 0.2244, 0.2226, 0.2473], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2293, 0.2702, 0.2732, 0.2274], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.237 , 0.2768, 0.2639, 0.2223], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2744, 0.239 , 0.2353, 0.2513], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2196, 0.2695, 0.2852, 0.2257], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2686, 0.2339, 0.2277, 0.2698], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2699, 0.2427, 0.2411, 0.2463], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2661, 0.2431, 0.245 , 0.2458], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2489, 0.2539, 0.2398, 0.2574], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2953, 0.2356, 0.2183, 0.2508], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2754, 0.2366, 0.2408, 0.2472], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.241 , 0.2559, 0.2669, 0.2362], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2041, 0.258 , 0.2832, 0.2548], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2254, 0.2751, 0.2661, 0.2334], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2133, 0.2583, 0.2763, 0.2521], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2096, 0.2807, 0.2889, 0.2207], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2546, 0.2483, 0.2425, 0.2546], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2319, 0.2442, 0.261 , 0.2629], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2603, 0.2827, 0.2695, 0.1876], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3915, 0.1896, 0.1595, 0.2594], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2053, 0.2728, 0.2726, 0.2493], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2107, 0.2673, 0.2572, 0.2647], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.257 , 0.2777, 0.2465, 0.2188], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2053, 0.2766, 0.2672, 0.2509], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2215, 0.2877, 0.2627, 0.228 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4799, 0.2097, 0.1414, 0.169 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1997, 0.2627, 0.2769, 0.2607], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2951, 0.1947, 0.2041, 0.3061], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2113, 0.2707, 0.2826, 0.2353], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2169, 0.2777, 0.2726, 0.2328], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2041, 0.2573, 0.2683, 0.2704], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.231 , 0.2684, 0.2595, 0.2412], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2696, 0.2299, 0.2482, 0.2523], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2794, 0.2365, 0.2446, 0.2395], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3013, 0.2163, 0.2111, 0.2714], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2796, 0.207 , 0.2374, 0.276 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2772, 0.2164, 0.2193, 0.2871], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2623, 0.2478, 0.257 , 0.2329], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2216, 0.2414, 0.2581, 0.2789], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2471, 0.264 , 0.2499, 0.239 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2137, 0.2628, 0.269 , 0.2545], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2214, 0.2709, 0.2626, 0.2452], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2006, 0.2853, 0.2986, 0.2155], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2299, 0.28  , 0.2654, 0.2246], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2013, 0.2752, 0.2849, 0.2386], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2295, 0.2738, 0.283 , 0.2137], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2426, 0.2476, 0.2656, 0.2442], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2901, 0.2139, 0.2368, 0.2592], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2923, 0.2423, 0.2351, 0.2303], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4275, 0.1975, 0.1732, 0.2018], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3627, 0.1714, 0.1775, 0.2883], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2396, 0.2571, 0.2647, 0.2386], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2015, 0.2765, 0.2726, 0.2494], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2445, 0.2405, 0.2167, 0.2983], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2426, 0.2337, 0.2086, 0.3151], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2152, 0.2537, 0.2235, 0.3076], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2922, 0.1982, 0.2145, 0.2951], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.272 , 0.2101, 0.2257, 0.2922], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2662, 0.2371, 0.2576, 0.2392], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2701, 0.2413, 0.2354, 0.2533], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2879, 0.22  , 0.2319, 0.2602], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2675, 0.2276, 0.2404, 0.2645], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2343, 0.2308, 0.2497, 0.2852], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2371, 0.2332, 0.2575, 0.2722], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2655, 0.2372, 0.2434, 0.2539], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2482, 0.2524, 0.2418, 0.2577], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2683, 0.2497, 0.23  , 0.252 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2144, 0.2336, 0.2714, 0.2806], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2957, 0.2094, 0.2087, 0.2863], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3073, 0.1965, 0.206 , 0.2901], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.275 , 0.211 , 0.2124, 0.3016], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4499, 0.218 , 0.1594, 0.1726], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2465, 0.2474, 0.2524, 0.2537], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3275, 0.1971, 0.1967, 0.2787], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.269 , 0.2266, 0.2362, 0.2682], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2746, 0.2277, 0.2416, 0.2561], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4057, 0.2435, 0.1602, 0.1906], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2539, 0.2403, 0.2438, 0.2619], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2517, 0.228 , 0.2464, 0.2738], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2964, 0.2204, 0.2316, 0.2517], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3493, 0.1767, 0.1997, 0.2743], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2975, 0.22  , 0.2247, 0.2577], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2917, 0.2194, 0.2274, 0.2615], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2887, 0.2153, 0.2231, 0.2729], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3289, 0.2061, 0.2096, 0.2555], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2745, 0.2543, 0.2384, 0.2328], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3134, 0.2056, 0.2025, 0.2785], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2906, 0.2205, 0.2357, 0.2533], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2651, 0.2074, 0.2168, 0.3107], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2008, 0.257 , 0.2841, 0.2581], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3447, 0.182 , 0.1903, 0.283 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3372, 0.1778, 0.196 , 0.289 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3578, 0.1799, 0.1936, 0.2688], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3421, 0.1835, 0.1992, 0.2752], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3439, 0.1858, 0.216 , 0.2542], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3319, 0.2056, 0.2269, 0.2357], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2994, 0.2054, 0.2342, 0.261 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.307 , 0.2079, 0.2341, 0.251 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2933, 0.2196, 0.2428, 0.2443], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2309, 0.2601, 0.2579, 0.2511], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2639, 0.2381, 0.2425, 0.2555], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2511, 0.2514, 0.2598, 0.2377], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2565, 0.2473, 0.2459, 0.2503], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.256 , 0.2197, 0.2346, 0.2896], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2596, 0.2293, 0.2349, 0.2762], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.231 , 0.234 , 0.2564, 0.2786], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2545, 0.2185, 0.2364, 0.2906], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2673, 0.2423, 0.2413, 0.2491], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2936, 0.2291, 0.2355, 0.2417], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2257, 0.2664, 0.2622, 0.2457], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2304, 0.2659, 0.2726, 0.2311], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2409, 0.2584, 0.2562, 0.2446], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2752, 0.2473, 0.2368, 0.2407], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3388, 0.2414, 0.2049, 0.2149], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2146, 0.2538, 0.2666, 0.265 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.28  , 0.2423, 0.2417, 0.2359], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2784, 0.223 , 0.2338, 0.2649], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2461, 0.2218, 0.2419, 0.2902], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2859, 0.2186, 0.2229, 0.2727], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2637, 0.2361, 0.2364, 0.2638], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2556, 0.2506, 0.2567, 0.2371], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2214, 0.2675, 0.2628, 0.2483], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3099, 0.2054, 0.2133, 0.2713], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3263, 0.1916, 0.1891, 0.293 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2764, 0.2211, 0.2245, 0.2779], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2108, 0.2667, 0.2682, 0.2543], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2653, 0.2623, 0.2402, 0.2322], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3352, 0.2534, 0.2092, 0.2021], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3423, 0.2169, 0.1966, 0.2442], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2725, 0.2224, 0.2121, 0.2929], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2825, 0.2175, 0.2133, 0.2867], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2329, 0.2797, 0.2414, 0.2459], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2331, 0.2674, 0.2323, 0.2672], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3637, 0.2059, 0.1976, 0.2328], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2934, 0.2163, 0.2211, 0.2691], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2662, 0.1953, 0.1924, 0.346 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3286, 0.1994, 0.1853, 0.2867], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3135, 0.2063, 0.2001, 0.2801], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2457, 0.233 , 0.2373, 0.284 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2681, 0.2562, 0.245 , 0.2307], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2772, 0.223 , 0.2183, 0.2815], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2347, 0.2105, 0.2582, 0.2967], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2498, 0.2539, 0.2513, 0.245 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2121, 0.2651, 0.2649, 0.2579], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1968, 0.2623, 0.255 , 0.2859], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2516, 0.2382, 0.2306, 0.2796], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2107, 0.2436, 0.2585, 0.2871], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2526, 0.2465, 0.2266, 0.2744], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2817, 0.2229, 0.249 , 0.2465], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3686, 0.1935, 0.1923, 0.2456], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4032, 0.1622, 0.165 , 0.2696], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3063, 0.1942, 0.2148, 0.2847], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.31  , 0.1856, 0.2255, 0.2789], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2995, 0.1985, 0.2445, 0.2575], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2649, 0.2526, 0.2502, 0.2323], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2574, 0.2589, 0.2489, 0.2348], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2511, 0.248 , 0.2579, 0.243 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2514, 0.2448, 0.2438, 0.2601], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2926, 0.2193, 0.2379, 0.2502], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2893, 0.2163, 0.2377, 0.2567], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2737, 0.2175, 0.2469, 0.2619], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3878, 0.1621, 0.1773, 0.2728], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2666, 0.2389, 0.2335, 0.261 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.271 , 0.2497, 0.2408, 0.2384], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2717, 0.2473, 0.2423, 0.2387], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2774, 0.2196, 0.2312, 0.2718], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2676, 0.2284, 0.24  , 0.264 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2834, 0.2357, 0.2318, 0.2491], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1929, 0.2807, 0.2973, 0.229 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2306, 0.2579, 0.2555, 0.256 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2367, 0.2482, 0.2586, 0.2564], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2595, 0.2439, 0.2596, 0.237 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2382, 0.2589, 0.2785, 0.2243], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2353, 0.2526, 0.2616, 0.2504], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2572, 0.2314, 0.2285, 0.283 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2472, 0.2419, 0.2555, 0.2553], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2591, 0.2557, 0.248 , 0.2372], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2446, 0.2535, 0.257 , 0.245 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.241 , 0.2565, 0.2667, 0.2358], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2624, 0.2529, 0.2373, 0.2474], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2421, 0.2696, 0.2616, 0.2267], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2426, 0.2714, 0.2599, 0.226 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2936, 0.2315, 0.2312, 0.2436], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2784, 0.2379, 0.2382, 0.2455], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.227 , 0.2809, 0.2613, 0.2308], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2901, 0.234 , 0.225 , 0.2509], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2502, 0.2273, 0.2482, 0.2743], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2553, 0.2455, 0.2647, 0.2345], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2289, 0.27  , 0.262 , 0.2391], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2634, 0.2608, 0.2526, 0.2231], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2899, 0.2496, 0.2279, 0.2325], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2213, 0.2697, 0.2713, 0.2377], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.221 , 0.2717, 0.2619, 0.2454], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4549, 0.1152, 0.1312, 0.2988], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3575, 0.1635, 0.1774, 0.3016], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2653, 0.2357, 0.2414, 0.2576], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2878, 0.2063, 0.2186, 0.2873], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2768, 0.2218, 0.2281, 0.2733], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2709, 0.2131, 0.2317, 0.2843], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2753, 0.22  , 0.2281, 0.2766], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2695, 0.2518, 0.2332, 0.2454], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3586, 0.1842, 0.1719, 0.2853], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3007, 0.2107, 0.2034, 0.2852], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2369, 0.276 , 0.2532, 0.2338], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2652, 0.2649, 0.2417, 0.2282], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3187, 0.2149, 0.2146, 0.2518], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.261 , 0.2309, 0.2245, 0.2836], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2357, 0.2658, 0.2587, 0.2399], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.307 , 0.2006, 0.2176, 0.2748], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2989, 0.2165, 0.2246, 0.2601], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.315 , 0.2225, 0.2182, 0.2444], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2852, 0.2319, 0.234 , 0.2489], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2143, 0.2645, 0.2773, 0.2439], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2195, 0.2556, 0.2737, 0.2513], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.287 , 0.2369, 0.2302, 0.2459], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2542, 0.2457, 0.2508, 0.2492], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2562, 0.2339, 0.2456, 0.2642], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.248 , 0.2413, 0.2416, 0.2691], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3069, 0.2176, 0.2079, 0.2676], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2684, 0.21  , 0.2276, 0.2939], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.266 , 0.2331, 0.2403, 0.2606], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2684, 0.2267, 0.224 , 0.2809], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3225, 0.1906, 0.2059, 0.281 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.286 , 0.2177, 0.2242, 0.272 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2586, 0.2394, 0.247 , 0.255 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2615, 0.2423, 0.2269, 0.2693], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2742, 0.2179, 0.2252, 0.2827], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.242 , 0.2497, 0.25  , 0.2583], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2454, 0.2537, 0.2334, 0.2675], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2652, 0.2861, 0.2591, 0.1896], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2923, 0.2341, 0.2311, 0.2424], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2571, 0.2385, 0.2362, 0.2682], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2865, 0.2201, 0.249 , 0.2444], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2824, 0.2483, 0.2328, 0.2366], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2733, 0.2451, 0.2421, 0.2395], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.254 , 0.2488, 0.2519, 0.2453], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2446, 0.2454, 0.2479, 0.2621], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2721, 0.2131, 0.2131, 0.3017], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2788, 0.1935, 0.2047, 0.323 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2609, 0.2421, 0.2477, 0.2494], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2682, 0.2425, 0.2482, 0.2411], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2623, 0.2303, 0.2485, 0.2589], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2444, 0.2565, 0.2536, 0.2455], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2869, 0.248 , 0.2294, 0.2357], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3012, 0.2192, 0.2099, 0.2697], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2389, 0.2374, 0.2571, 0.2666], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.273 , 0.2422, 0.2431, 0.2418], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2238, 0.2637, 0.2637, 0.2488], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2876, 0.2415, 0.2452, 0.2257], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2787, 0.2305, 0.2431, 0.2477], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2738, 0.2513, 0.2401, 0.2347], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2851, 0.2116, 0.234 , 0.2694], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2981, 0.2202, 0.22  , 0.2618], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.273 , 0.2294, 0.248 , 0.2495], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2847, 0.245 , 0.2508, 0.2194], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2717, 0.2483, 0.2474, 0.2325], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2535, 0.2297, 0.238 , 0.2789], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3249, 0.2261, 0.215 , 0.234 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4295, 0.1766, 0.1445, 0.2494], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2832, 0.2612, 0.2373, 0.2183], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3172, 0.236 , 0.2231, 0.2236], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2196, 0.2809, 0.2862, 0.2133], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3113, 0.2212, 0.2164, 0.2512], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2825, 0.2229, 0.2221, 0.2725], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3097, 0.1978, 0.2026, 0.29  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2906, 0.2067, 0.2273, 0.2754], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2589, 0.2409, 0.2435, 0.2567], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2874, 0.2275, 0.2364, 0.2487], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2659, 0.2414, 0.2359, 0.2568], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2733, 0.2392, 0.2481, 0.2395], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.264 , 0.2498, 0.2483, 0.238 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2397, 0.2376, 0.2704, 0.2524], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2878, 0.2394, 0.2324, 0.2405], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4074, 0.1954, 0.1771, 0.2201], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2469, 0.2542, 0.2635, 0.2355], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2647, 0.238 , 0.2392, 0.2581], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.263 , 0.2605, 0.2439, 0.2326], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.289 , 0.1948, 0.2235, 0.2927], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3581, 0.191 , 0.2119, 0.239 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2806, 0.2488, 0.2333, 0.2373], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2879, 0.2327, 0.2308, 0.2486], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2338, 0.2521, 0.2675, 0.2466], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2596, 0.2518, 0.2525, 0.2361], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3131, 0.2074, 0.1916, 0.2878], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3302, 0.2088, 0.1912, 0.2698], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2758, 0.2313, 0.2264, 0.2666], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3231, 0.2135, 0.2121, 0.2513], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3047, 0.2064, 0.2131, 0.2757], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2757, 0.2407, 0.2198, 0.2638], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3173, 0.2059, 0.2131, 0.2637], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3254, 0.2195, 0.2264, 0.2287], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3015, 0.2137, 0.2216, 0.2631], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2608, 0.2338, 0.2516, 0.2537], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2366, 0.2608, 0.2548, 0.2478], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2743, 0.2273, 0.2442, 0.2542], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2913, 0.2166, 0.2321, 0.26  ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3011, 0.2124, 0.229 , 0.2575], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2406, 0.2398, 0.265 , 0.2546], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2193, 0.2544, 0.2737, 0.2527], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2174, 0.2682, 0.2783, 0.2362], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2296, 0.2614, 0.2666, 0.2423], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2439, 0.2382, 0.2315, 0.2864], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2557, 0.2459, 0.2312, 0.2672], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2339, 0.2462, 0.2577, 0.2622], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2477, 0.242 , 0.254 , 0.2562], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2641, 0.2392, 0.2523, 0.2443], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2754, 0.2252, 0.2204, 0.279 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2395, 0.2551, 0.2618, 0.2436], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.257 , 0.251 , 0.2381, 0.254 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.279 , 0.2226, 0.2342, 0.2642], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2748, 0.232 , 0.2359, 0.2573], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2896, 0.2319, 0.2176, 0.261 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2643, 0.2459, 0.2449, 0.2448], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2934, 0.2161, 0.2289, 0.2616], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2639, 0.2115, 0.2326, 0.292 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3232, 0.1909, 0.2084, 0.2775], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2775, 0.1985, 0.2337, 0.2903], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3398, 0.1842, 0.1832, 0.2928], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2767, 0.2345, 0.2329, 0.2559], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2599, 0.246 , 0.2595, 0.2346], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3153, 0.2057, 0.204 , 0.275 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2493, 0.261 , 0.2505, 0.2393], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2287, 0.2637, 0.2552, 0.2525], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2278, 0.2779, 0.2561, 0.2382], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2407, 0.2532, 0.2444, 0.2616], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2434, 0.2433, 0.2366, 0.2766], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3393, 0.2068, 0.185 , 0.2689], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2048, 0.2257, 0.251 , 0.3185], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.363 , 0.1766, 0.1703, 0.2901], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2379, 0.2613, 0.2565, 0.2443], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3242, 0.2299, 0.2232, 0.2227], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.301 , 0.2484, 0.2281, 0.2225], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2921, 0.2309, 0.2234, 0.2536], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2875, 0.2294, 0.2272, 0.256 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2525, 0.2458, 0.2396, 0.2621], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2894, 0.2302, 0.2293, 0.251 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2929, 0.2384, 0.2307, 0.2379], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3298, 0.1916, 0.1928, 0.2859], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2986, 0.2045, 0.2171, 0.2798], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2875, 0.2313, 0.2352, 0.246 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2788, 0.2285, 0.229 , 0.2637], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2575, 0.239 , 0.2417, 0.2618], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2847, 0.2427, 0.2282, 0.2443], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2451, 0.2447, 0.2498, 0.2604], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2648, 0.2632, 0.2515, 0.2205], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3033, 0.2294, 0.2167, 0.2507], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.266 , 0.2374, 0.2319, 0.2647], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2478, 0.2219, 0.2192, 0.3111], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2818, 0.2185, 0.2106, 0.2891], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2615, 0.2383, 0.2415, 0.2587], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2701, 0.2385, 0.2412, 0.2502], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2915, 0.2215, 0.2174, 0.2696], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.284 , 0.239 , 0.2334, 0.2436], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.327 , 0.2178, 0.2013, 0.2539], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2536, 0.2507, 0.2498, 0.246 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2785, 0.2227, 0.2285, 0.2702], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3508, 0.1732, 0.1889, 0.2872], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3378, 0.1791, 0.1962, 0.2869], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.299 , 0.2356, 0.2419, 0.2236], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2726, 0.244 , 0.2469, 0.2364], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2901, 0.2456, 0.2384, 0.2259], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2887, 0.2296, 0.245 , 0.2368], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3149, 0.2143, 0.2127, 0.2581], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2986, 0.2104, 0.2155, 0.2755], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2222, 0.2535, 0.2626, 0.2618], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.23  , 0.2598, 0.2713, 0.2389], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1844, 0.2494, 0.2764, 0.2898], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2982, 0.2116, 0.2247, 0.2655], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2628, 0.248 , 0.2462, 0.243 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2968, 0.2402, 0.2296, 0.2335], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2932, 0.2183, 0.2265, 0.262 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2847, 0.2133, 0.2332, 0.2688], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2651, 0.2318, 0.2472, 0.2559], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3512, 0.193 , 0.1785, 0.2774], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3346, 0.1861, 0.2057, 0.2735], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.386 , 0.1803, 0.1856, 0.2481], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2778, 0.2476, 0.2454, 0.2292], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2696, 0.262 , 0.2437, 0.2248], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2732, 0.2463, 0.2422, 0.2382], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3217, 0.2119, 0.2018, 0.2646], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2578, 0.2231, 0.2418, 0.2773], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3003, 0.2222, 0.2103, 0.2672], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2291, 0.2771, 0.2654, 0.2284], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2358, 0.2444, 0.2511, 0.2688], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2439, 0.247 , 0.2559, 0.2532], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2281, 0.256 , 0.2608, 0.2551], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2948, 0.2377, 0.2311, 0.2363], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2603, 0.2597, 0.2492, 0.2308], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2637, 0.2575, 0.2519, 0.227 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2716, 0.2414, 0.236 , 0.251 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3952, 0.1558, 0.154 , 0.295 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2405, 0.247 , 0.2478, 0.2647], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2383, 0.2431, 0.2272, 0.2914], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2286, 0.2444, 0.2358, 0.2911], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2243, 0.2625, 0.2502, 0.263 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2714, 0.2512, 0.2134, 0.264 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2294, 0.2182, 0.2146, 0.3378], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2654, 0.2638, 0.2464, 0.2243], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2745, 0.2265, 0.1934, 0.3056], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2268, 0.2293, 0.2503, 0.2935], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2193, 0.2478, 0.277 , 0.2559], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2948, 0.2368, 0.2417, 0.2267], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2876, 0.2247, 0.2494, 0.2382], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2465, 0.2358, 0.2523, 0.2654], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2285, 0.2315, 0.2504, 0.2896], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2604, 0.2579, 0.2613, 0.2203], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2764, 0.2529, 0.244 , 0.2268], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2767, 0.2357, 0.2432, 0.2444], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2621, 0.2372, 0.2426, 0.2581], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2999, 0.2407, 0.2371, 0.2223], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3476, 0.1957, 0.1879, 0.2688], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2691, 0.2472, 0.265 , 0.2188], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2481, 0.2419, 0.2531, 0.257 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2509, 0.245 , 0.2564, 0.2477], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2865, 0.2311, 0.229 , 0.2534], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2334, 0.259 , 0.2732, 0.2345], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2289, 0.2438, 0.2535, 0.2738], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2413, 0.245 , 0.2415, 0.2722], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2332, 0.2471, 0.2709, 0.2488], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3542, 0.2016, 0.1913, 0.2529], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3478, 0.2158, 0.1981, 0.2384], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3743, 0.1718, 0.1579, 0.296 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3704, 0.1805, 0.1997, 0.2494], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2677, 0.246 , 0.243 , 0.2433], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2688, 0.2426, 0.2328, 0.2558], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2482, 0.24  , 0.2343, 0.2775], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2432, 0.2537, 0.2578, 0.2454], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.241 , 0.2727, 0.2549, 0.2314], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2431, 0.2581, 0.2469, 0.2519], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2789, 0.2317, 0.2292, 0.2603], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2839, 0.2222, 0.2287, 0.2652], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2613, 0.2029, 0.2258, 0.31  ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2861, 0.2485, 0.2349, 0.2304], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.207 , 0.2949, 0.2821, 0.216 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2412, 0.2771, 0.2528, 0.2289], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3475, 0.1907, 0.1809, 0.2808], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2649, 0.2445, 0.2322, 0.2584], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2537, 0.2421, 0.2598, 0.2444], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2844, 0.2475, 0.2385, 0.2296], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3491, 0.1936, 0.1861, 0.2712], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2505, 0.2464, 0.2662, 0.2369], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2233, 0.2648, 0.2806, 0.2313], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2332, 0.2662, 0.2554, 0.2452], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2476, 0.2465, 0.2485, 0.2573], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2821, 0.2339, 0.2376, 0.2464], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.268 , 0.2514, 0.2373, 0.2434], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2637, 0.227 , 0.2256, 0.2837], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2812, 0.2419, 0.239 , 0.238 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.273 , 0.2481, 0.2193, 0.2596], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2302, 0.2697, 0.2556, 0.2444], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2316, 0.2434, 0.2237, 0.3013], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2229, 0.2656, 0.2795, 0.2321], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2037, 0.2785, 0.2824, 0.2354], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.237 , 0.2424, 0.2592, 0.2614], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2731, 0.2429, 0.2283, 0.2557], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2346, 0.2369, 0.2326, 0.2959], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2365, 0.2534, 0.2544, 0.2557], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3234, 0.2062, 0.214 , 0.2564], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3361, 0.2004, 0.2109, 0.2526], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3029, 0.2206, 0.2211, 0.2554], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.299 , 0.2066, 0.2133, 0.2812], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2669, 0.2213, 0.2259, 0.2859], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2664, 0.2219, 0.2407, 0.271 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3324, 0.2057, 0.1961, 0.2657], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2855, 0.2054, 0.2103, 0.2989], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2871, 0.2462, 0.2254, 0.2413], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2576, 0.2523, 0.2553, 0.2349], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.265 , 0.231 , 0.2551, 0.2488], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3456, 0.1798, 0.1809, 0.2938], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2496, 0.2505, 0.2444, 0.2555], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2834, 0.2498, 0.2557, 0.2111], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3402, 0.2059, 0.2147, 0.2392], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2348, 0.2553, 0.2473, 0.2627], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2951, 0.2323, 0.2062, 0.2664], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3071, 0.2065, 0.2015, 0.2849], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2549, 0.2571, 0.2559, 0.2322], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2447, 0.2413, 0.2545, 0.2595], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2697, 0.2326, 0.2286, 0.269 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2546, 0.2303, 0.2262, 0.2889], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3354, 0.2239, 0.2106, 0.2301], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3433, 0.2088, 0.1975, 0.2504], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2553, 0.2614, 0.2497, 0.2336], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2476, 0.2784, 0.2543, 0.2197], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2777, 0.2401, 0.2255, 0.2567], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2154, 0.2539, 0.2502, 0.2805], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3268, 0.2252, 0.2206, 0.2274], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2954, 0.2235, 0.22  , 0.2611], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2555, 0.2334, 0.2434, 0.2677], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2574, 0.2395, 0.2494, 0.2537], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2515, 0.252 , 0.253 , 0.2435], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2481, 0.2534, 0.2484, 0.2501], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2672, 0.2399, 0.2382, 0.2547], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2653, 0.2497, 0.2484, 0.2366], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2631, 0.2566, 0.2491, 0.2312], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.28  , 0.2267, 0.2503, 0.243 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3198, 0.2169, 0.2114, 0.2519], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2954, 0.2112, 0.2198, 0.2737], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2983, 0.2123, 0.2193, 0.27  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2177, 0.2666, 0.2755, 0.2402], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2322, 0.2527, 0.2578, 0.2573], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2504, 0.2409, 0.2437, 0.265 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2475, 0.2304, 0.2437, 0.2784], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2493, 0.2299, 0.2117, 0.3091], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2308, 0.2612, 0.2327, 0.2753], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2646, 0.2403, 0.2532, 0.2419], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2387, 0.2381, 0.2681, 0.255 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.228 , 0.253 , 0.2656, 0.2533], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.379 , 0.1668, 0.1763, 0.2779], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3697, 0.1699, 0.1974, 0.263 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3515, 0.2082, 0.2035, 0.2368], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2598, 0.2169, 0.236 , 0.2873], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.275 , 0.2126, 0.2237, 0.2888], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.237 , 0.2552, 0.2668, 0.241 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1712, 0.2307, 0.2882, 0.3099], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2641, 0.2264, 0.2467, 0.2628], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2759, 0.2479, 0.2369, 0.2393], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2323, 0.2517, 0.2609, 0.2552], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2414, 0.2449, 0.2479, 0.2658], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.269 , 0.2292, 0.233 , 0.2688], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4043, 0.1718, 0.1815, 0.2424], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3163, 0.2052, 0.1995, 0.2789], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2794, 0.2183, 0.232 , 0.2703], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2543, 0.2411, 0.2548, 0.2498], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2519, 0.2506, 0.2563, 0.2412], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3244, 0.233 , 0.1955, 0.2471], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2902, 0.2228, 0.2282, 0.2587], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2865, 0.2193, 0.2439, 0.2503], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3359, 0.2451, 0.2062, 0.2128], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2505, 0.2699, 0.2668, 0.2128], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3433, 0.1989, 0.1896, 0.2682], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3292, 0.2297, 0.2322, 0.2089], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.264 , 0.2372, 0.2512, 0.2476], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2487, 0.2377, 0.2505, 0.263 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3101, 0.1961, 0.2147, 0.279 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.306 , 0.2001, 0.2308, 0.2632], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2654, 0.2416, 0.2536, 0.2395], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2546, 0.2499, 0.2555, 0.24  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2203, 0.2535, 0.2718, 0.2544], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2216, 0.267 , 0.2763, 0.2352], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.225 , 0.2767, 0.2795, 0.2188], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2954, 0.2289, 0.2216, 0.2541], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2991, 0.2232, 0.232 , 0.2457], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3737, 0.1899, 0.1946, 0.2418], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.221 , 0.2535, 0.2682, 0.2573], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2377, 0.2492, 0.2559, 0.2572], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3194, 0.1907, 0.1952, 0.2948], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2179, 0.2695, 0.2635, 0.2491], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2375, 0.2784, 0.2574, 0.2268], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2423, 0.2456, 0.2595, 0.2526], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2334, 0.2618, 0.261 , 0.2438], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2559, 0.2526, 0.2587, 0.2327], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2829, 0.2221, 0.2253, 0.2696], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3437, 0.1673, 0.1772, 0.3118], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2906, 0.2337, 0.2328, 0.2429], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.252 , 0.2635, 0.2405, 0.244 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2314, 0.2841, 0.2507, 0.2338], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2848, 0.2481, 0.2431, 0.224 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2742, 0.2216, 0.226 , 0.2782], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2917, 0.221 , 0.2344, 0.2529], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.45  , 0.1332, 0.1472, 0.2697], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2875, 0.2147, 0.2221, 0.2757], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2715, 0.2191, 0.2429, 0.2665], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2737, 0.2298, 0.2321, 0.2643], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2502, 0.2508, 0.2585, 0.2404], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2608, 0.2543, 0.2471, 0.2379], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2762, 0.2574, 0.2428, 0.2237], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2563, 0.2523, 0.2474, 0.2441], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2367, 0.2631, 0.2664, 0.2337], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2369, 0.2565, 0.2552, 0.2514], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2861, 0.2396, 0.2374, 0.2369], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3348, 0.1799, 0.2034, 0.2819], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2958, 0.1878, 0.212 , 0.3044], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2984, 0.2056, 0.2504, 0.2457], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3183, 0.1855, 0.2334, 0.2628], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2766, 0.2517, 0.2469, 0.2248], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2626, 0.2443, 0.2567, 0.2364], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2749, 0.2463, 0.2397, 0.2391], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3379, 0.1889, 0.21  , 0.2631], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.296 , 0.2294, 0.2229, 0.2517], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2626, 0.2547, 0.2455, 0.2372], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3143, 0.1932, 0.217 , 0.2756], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3108, 0.2053, 0.2186, 0.2653], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3524, 0.1723, 0.1836, 0.2917], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2317, 0.2553, 0.2607, 0.2523], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2763, 0.2226, 0.2424, 0.2587], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2531, 0.2497, 0.2474, 0.2498], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2945, 0.2397, 0.222 , 0.2439], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2557, 0.2312, 0.2344, 0.2787], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2512, 0.2478, 0.2548, 0.2463], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1963, 0.2813, 0.2894, 0.2331], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1999, 0.2521, 0.2648, 0.2833], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2858, 0.2306, 0.2189, 0.2647], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3223, 0.2202, 0.2092, 0.2483], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3706, 0.207 , 0.186 , 0.2363], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2787, 0.2294, 0.2292, 0.2627], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2889, 0.2502, 0.2393, 0.2217], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3137, 0.2512, 0.2273, 0.2077], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2868, 0.2425, 0.2246, 0.2461], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3693, 0.1766, 0.1955, 0.2587], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.268 , 0.2346, 0.2452, 0.2522], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2494, 0.249 , 0.2502, 0.2514], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2355, 0.2695, 0.2643, 0.2307], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2487, 0.2596, 0.2589, 0.2328], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2717, 0.2541, 0.2361, 0.2381], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4161, 0.1767, 0.1581, 0.2491], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.307 , 0.2152, 0.2411, 0.2367], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2091, 0.247 , 0.2724, 0.2715], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2507, 0.2623, 0.2629, 0.2241], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2177, 0.2022, 0.228 , 0.3521], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2123, 0.2681, 0.2591, 0.2606], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2359, 0.2726, 0.2486, 0.2429], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1444, 0.2399, 0.2794, 0.3363], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2941, 0.2114, 0.1976, 0.2969], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2192, 0.2738, 0.2865, 0.2204], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3087, 0.1797, 0.1871, 0.3244], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2921, 0.2272, 0.215 , 0.2657], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2919, 0.2157, 0.2306, 0.2618], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2135, 0.2768, 0.2803, 0.2294], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2567, 0.2592, 0.2595, 0.2247], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2462, 0.2509, 0.2581, 0.2448], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2507, 0.2519, 0.259 , 0.2384], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2469, 0.2685, 0.2572, 0.2274], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2504, 0.2454, 0.246 , 0.2582], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2377, 0.2662, 0.2708, 0.2253], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2709, 0.2424, 0.233 , 0.2537], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2672, 0.2277, 0.2262, 0.2789], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2258, 0.2401, 0.2434, 0.2907], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2539, 0.2621, 0.2417, 0.2423], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3189, 0.2019, 0.1978, 0.2814], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2522, 0.243 , 0.2318, 0.273 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2693, 0.2354, 0.2215, 0.2738], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2782, 0.2152, 0.2142, 0.2924], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2897, 0.2333, 0.2243, 0.2527], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2687, 0.21  , 0.2328, 0.2884], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2491, 0.2323, 0.2377, 0.2809], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2713, 0.2427, 0.2397, 0.2463], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2834, 0.2401, 0.2312, 0.2453], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2267, 0.2339, 0.2532, 0.2862], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2709, 0.2431, 0.2322, 0.2538], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2667, 0.2488, 0.2477, 0.2367], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3284, 0.2307, 0.2052, 0.2357], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2851, 0.2434, 0.2333, 0.2382], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.273 , 0.2047, 0.1985, 0.3237], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2469, 0.1811, 0.2092, 0.3629], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2144, 0.216 , 0.222 , 0.3476], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2289, 0.2475, 0.2421, 0.2815], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.337 , 0.2376, 0.2115, 0.2139], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2413, 0.2379, 0.2681, 0.2527], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2305, 0.2683, 0.2919, 0.2093], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.235 , 0.2546, 0.2813, 0.2292], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2149, 0.2108, 0.2461, 0.3283], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.289 , 0.2328, 0.2162, 0.2619], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2775, 0.2336, 0.2314, 0.2575], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3257, 0.2304, 0.2156, 0.2284], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2569, 0.2394, 0.238 , 0.2658], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2284, 0.2682, 0.2582, 0.2451], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2297, 0.2574, 0.2592, 0.2537], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2228, 0.2573, 0.2766, 0.2433], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2276, 0.266 , 0.2711, 0.2353], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2212, 0.2776, 0.2693, 0.232 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.246 , 0.2461, 0.2524, 0.2555], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.282 , 0.2115, 0.2324, 0.2741], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.273 , 0.2327, 0.2365, 0.2578], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2666, 0.2411, 0.2605, 0.2318], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2416, 0.2541, 0.258 , 0.2463], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2318, 0.2659, 0.265 , 0.2373], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3055, 0.2484, 0.2206, 0.2255], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2111, 0.2756, 0.2848, 0.2285], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3053, 0.2227, 0.2075, 0.2646], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3252, 0.2145, 0.2002, 0.2602], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.287 , 0.2379, 0.2212, 0.2539], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2495, 0.2556, 0.2292, 0.2657], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2447, 0.2469, 0.252 , 0.2565], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2759, 0.2438, 0.2311, 0.2491], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2745, 0.2456, 0.23  , 0.25  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3353, 0.2095, 0.1915, 0.2636], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2846, 0.24  , 0.2246, 0.2508], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2642, 0.2213, 0.2179, 0.2966], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2748, 0.2441, 0.2371, 0.244 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1932, 0.2724, 0.2521, 0.2823], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1929, 0.268 , 0.2586, 0.2806], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2773, 0.228 , 0.235 , 0.2597], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2768, 0.2282, 0.2348, 0.2602], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2984, 0.212 , 0.211 , 0.2786], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3023, 0.205 , 0.2085, 0.2842], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2228, 0.25  , 0.2674, 0.2599], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2691, 0.238 , 0.243 , 0.2499], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.228 , 0.2275, 0.2695, 0.275 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2502, 0.2437, 0.2461, 0.2599], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3117, 0.2069, 0.213 , 0.2683], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2642, 0.2318, 0.2228, 0.2812], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2768, 0.2553, 0.2372, 0.2307], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2805, 0.2211, 0.2192, 0.2792], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2577, 0.2433, 0.2464, 0.2527], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2346, 0.2326, 0.2358, 0.2969], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3165, 0.2257, 0.2092, 0.2486], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2971, 0.2039, 0.222 , 0.277 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.294 , 0.2207, 0.2376, 0.2477], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2685, 0.2479, 0.2465, 0.2371], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.261 , 0.2552, 0.2561, 0.2277], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2354, 0.2427, 0.2547, 0.2673], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2689, 0.2303, 0.2312, 0.2696], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2653, 0.2389, 0.2163, 0.2795], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2107, 0.26  , 0.2293, 0.2999], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1889, 0.3073, 0.2295, 0.2742], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2527, 0.253 , 0.2535, 0.2408], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2439, 0.2491, 0.239 , 0.268 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3023, 0.2242, 0.2127, 0.2608], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3268, 0.2105, 0.2075, 0.2552], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3245, 0.1975, 0.1993, 0.2787], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.21  , 0.2854, 0.282 , 0.2226], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2016, 0.2825, 0.2808, 0.235 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2011, 0.2722, 0.28  , 0.2467], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2245, 0.2761, 0.2704, 0.2289], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.215 , 0.2842, 0.2707, 0.2301], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2109, 0.2746, 0.2794, 0.235 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2023, 0.2825, 0.2851, 0.2301], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1949, 0.2804, 0.2856, 0.2392], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2057, 0.2767, 0.2843, 0.2332], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2609, 0.2616, 0.2365, 0.241 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2843, 0.2017, 0.2151, 0.2989], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2943, 0.2239, 0.2145, 0.2673], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2436, 0.2231, 0.2396, 0.2937], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2152, 0.2465, 0.2621, 0.2762], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2276, 0.2404, 0.2533, 0.2787], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.207 , 0.245 , 0.2663, 0.2818], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2162, 0.2639, 0.265 , 0.2549], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2478, 0.2402, 0.2689, 0.243 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2707, 0.2234, 0.2394, 0.2665], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3428, 0.2053, 0.1986, 0.2533], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2292, 0.2512, 0.2572, 0.2624], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2373, 0.2612, 0.2623, 0.2391], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.253 , 0.2519, 0.2448, 0.2503], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2718, 0.247 , 0.2355, 0.2457], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2636, 0.2565, 0.2549, 0.225 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2562, 0.2495, 0.2462, 0.2481], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2152, 0.2734, 0.2702, 0.2412], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2811, 0.2367, 0.234 , 0.2482], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3085, 0.2052, 0.2179, 0.2684], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2126, 0.2743, 0.2796, 0.2335], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2241, 0.2483, 0.2483, 0.2793], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2142, 0.2481, 0.2609, 0.2768], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2091, 0.2594, 0.2686, 0.2629], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2913, 0.2396, 0.2307, 0.2384], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2866, 0.2273, 0.2113, 0.2748], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3283, 0.2349, 0.2024, 0.2344], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.288 , 0.2004, 0.2181, 0.2936], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3185, 0.1875, 0.1985, 0.2955], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.322 , 0.1959, 0.1988, 0.2833], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2753, 0.2471, 0.2357, 0.2419], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3371, 0.2008, 0.191 , 0.2712], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2408, 0.2656, 0.245 , 0.2486], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.243 , 0.2534, 0.2514, 0.2522], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2528, 0.2451, 0.2531, 0.2491], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2469, 0.2605, 0.2501, 0.2425], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2353, 0.2449, 0.2457, 0.2741], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2885, 0.2097, 0.2125, 0.2893], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1957, 0.2809, 0.3002, 0.2232], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2611, 0.2408, 0.2311, 0.2669], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2563, 0.2341, 0.2353, 0.2743], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3029, 0.2153, 0.2136, 0.2683], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2091, 0.2368, 0.246 , 0.3081], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2206, 0.2481, 0.2483, 0.283 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2048, 0.2456, 0.2467, 0.3029], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2235, 0.2398, 0.2528, 0.2839], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2338, 0.2304, 0.2504, 0.2854], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2556, 0.2529, 0.2395, 0.2519], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2277, 0.2402, 0.2577, 0.2744], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3841, 0.2135, 0.1755, 0.2269], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3081, 0.2344, 0.2134, 0.2442], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2679, 0.2431, 0.2437, 0.2453], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2903, 0.2222, 0.2357, 0.2518], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2123, 0.2424, 0.2781, 0.2672], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1978, 0.2948, 0.2829, 0.2245], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2078, 0.2876, 0.2929, 0.2117], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3098, 0.2569, 0.2294, 0.204 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3082, 0.2324, 0.2256, 0.2337], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.301 , 0.2371, 0.2226, 0.2393], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.235 , 0.252 , 0.2524, 0.2605], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2427, 0.2457, 0.2476, 0.264 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2476, 0.2443, 0.2637, 0.2444], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2395, 0.257 , 0.2584, 0.2451], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2146, 0.2656, 0.2662, 0.2536], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2227, 0.2645, 0.2495, 0.2633], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2381, 0.2617, 0.2735, 0.2267], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2354, 0.2612, 0.2504, 0.253 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2282, 0.2453, 0.2477, 0.2788], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2437, 0.2464, 0.2474, 0.2625], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2424, 0.2503, 0.2656, 0.2417], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2737, 0.2328, 0.2264, 0.2671], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2881, 0.2455, 0.2285, 0.2378], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3516, 0.2044, 0.2118, 0.2322], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3772, 0.1632, 0.1595, 0.3001], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2676, 0.2462, 0.258 , 0.2282], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2498, 0.2343, 0.2491, 0.2669], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.307 , 0.238 , 0.2251, 0.2299], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2085, 0.2749, 0.268 , 0.2486], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2016, 0.267 , 0.2652, 0.2662], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2183, 0.2596, 0.2522, 0.2699], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2084, 0.2696, 0.2676, 0.2544], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2516, 0.2498, 0.2601, 0.2384], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2745, 0.2137, 0.2616, 0.2502], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.327 , 0.1956, 0.1916, 0.2857], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.309 , 0.2096, 0.2186, 0.2627], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1911, 0.2611, 0.2977, 0.2501], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1778, 0.2756, 0.3037, 0.2429], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2063, 0.2792, 0.284 , 0.2306], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.222 , 0.2562, 0.2703, 0.2516], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4313, 0.2185, 0.1497, 0.2005], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.243 , 0.2555, 0.2566, 0.2449], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.335 , 0.257 , 0.1992, 0.2087], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1686, 0.2565, 0.2892, 0.2857], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2301, 0.2515, 0.2723, 0.2461], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.201 , 0.2791, 0.2878, 0.2321], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2787, 0.2397, 0.2367, 0.2449], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2826, 0.2617, 0.2458, 0.2099], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.397 , 0.1845, 0.1743, 0.2442], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2622, 0.2399, 0.2195, 0.2783], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.239 , 0.225 , 0.2286, 0.3074], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3781, 0.1767, 0.1766, 0.2685], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2314, 0.2753, 0.2702, 0.2231], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2118, 0.2798, 0.2809, 0.2275], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.279 , 0.253 , 0.2431, 0.2249], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2424, 0.2219, 0.2365, 0.2992], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2216, 0.2691, 0.2832, 0.2262], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.19  , 0.2841, 0.3067, 0.2192], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1975, 0.2902, 0.2862, 0.2261], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1921, 0.2862, 0.2969, 0.2247], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2364, 0.2626, 0.2596, 0.2414], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1973, 0.2796, 0.2847, 0.2385], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2025, 0.2733, 0.2728, 0.2514], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.215 , 0.2825, 0.2683, 0.2343], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1998, 0.2837, 0.2743, 0.2421], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3717, 0.2225, 0.1797, 0.226 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3349, 0.2354, 0.2113, 0.2185], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2231, 0.2457, 0.2556, 0.2757], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2438, 0.2543, 0.2493, 0.2526], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2823, 0.2228, 0.2312, 0.2637], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2064, 0.2762, 0.2916, 0.2258], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2102, 0.2629, 0.2806, 0.2463], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2102, 0.2667, 0.2872, 0.2359], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1942, 0.2826, 0.293 , 0.2302], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1748, 0.2917, 0.3061, 0.2274], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1775, 0.2934, 0.3024, 0.2266], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1951, 0.2848, 0.2901, 0.2301], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3252, 0.192 , 0.2456, 0.2373], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2227, 0.2445, 0.2452, 0.2875], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2164, 0.2322, 0.2374, 0.3139], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2128, 0.2263, 0.2532, 0.3077], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2103, 0.294 , 0.2752, 0.2204], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1953, 0.2934, 0.298 , 0.2133], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2403, 0.229 , 0.2073, 0.3234], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2274, 0.2558, 0.2076, 0.3092], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2497, 0.2053, 0.2285, 0.3165], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2128, 0.2802, 0.2811, 0.226 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2134, 0.2798, 0.2878, 0.219 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2427, 0.2561, 0.2562, 0.245 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.221 , 0.268 , 0.2731, 0.2378], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2813, 0.189 , 0.1756, 0.3542], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2115, 0.2226, 0.2053, 0.3606], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2464, 0.2367, 0.2112, 0.3058], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2718, 0.251 , 0.2164, 0.2608], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2673, 0.2254, 0.2295, 0.2778], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2358, 0.2132, 0.2399, 0.3111], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2013, 0.2873, 0.2876, 0.2238], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1853, 0.2883, 0.2873, 0.2391], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.198 , 0.2826, 0.2903, 0.2291], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2694, 0.2412, 0.2209, 0.2685], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2538, 0.2272, 0.2305, 0.2885], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2721, 0.2292, 0.227 , 0.2717], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3101, 0.2306, 0.2172, 0.2421], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3583, 0.2009, 0.196 , 0.2448], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2689, 0.2114, 0.2331, 0.2866], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2684, 0.2169, 0.2378, 0.2769], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3088, 0.2043, 0.2147, 0.2722], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3204, 0.212 , 0.2207, 0.2469], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2639, 0.2114, 0.2233, 0.3014], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2253, 0.2495, 0.2686, 0.2566], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2275, 0.2731, 0.269 , 0.2304], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2352, 0.2641, 0.2475, 0.2532], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3359, 0.2336, 0.2015, 0.229 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2447, 0.2577, 0.2556, 0.242 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2519, 0.2634, 0.255 , 0.2297], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1894, 0.2832, 0.2912, 0.2363], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.236 , 0.2648, 0.259 , 0.2402], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2421, 0.2671, 0.2565, 0.2343], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2345, 0.2702, 0.2703, 0.225 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.249 , 0.2465, 0.2353, 0.2692], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2757, 0.2699, 0.2445, 0.2099], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.308 , 0.208 , 0.2204, 0.2636], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3144, 0.2068, 0.2041, 0.2747], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2045, 0.2856, 0.2917, 0.2182], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2265, 0.2467, 0.2536, 0.2731], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3146, 0.2158, 0.2249, 0.2446], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2414, 0.2385, 0.2607, 0.2594], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2367, 0.2565, 0.2617, 0.2452], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2277, 0.2368, 0.2635, 0.2721], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2234, 0.2618, 0.2641, 0.2508], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1927, 0.2646, 0.289 , 0.2537], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.245 , 0.2364, 0.2427, 0.2759], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1908, 0.285 , 0.297 , 0.2272], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1913, 0.2879, 0.2925, 0.2284], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2099, 0.2824, 0.289 , 0.2187], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2745, 0.2154, 0.2248, 0.2853], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2184, 0.206 , 0.2299, 0.3458], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.258 , 0.2139, 0.2247, 0.3034], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.204 , 0.2095, 0.1789, 0.4076], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1903, 0.2308, 0.2338, 0.345 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2367, 0.2458, 0.2204, 0.2971], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3655, 0.1926, 0.2081, 0.2338], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3524, 0.1846, 0.1992, 0.2638], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2918, 0.2122, 0.2199, 0.2761], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2946, 0.2011, 0.2358, 0.2685], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3187, 0.2047, 0.2075, 0.2692], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2584, 0.2289, 0.2417, 0.271 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3024, 0.2164, 0.2217, 0.2594], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.394 , 0.1609, 0.1491, 0.296 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2993, 0.2277, 0.222 , 0.251 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3667, 0.2258, 0.2099, 0.1977], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2178, 0.2702, 0.2732, 0.2387], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1777, 0.2688, 0.296 , 0.2575], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1854, 0.2909, 0.2985, 0.2252], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2102, 0.2625, 0.2585, 0.2688], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2472, 0.2333, 0.2325, 0.287 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2607, 0.2236, 0.2277, 0.288 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2427, 0.2289, 0.2471, 0.2814], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2519, 0.2437, 0.248 , 0.2564], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2567, 0.2493, 0.2402, 0.2537], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2365, 0.257 , 0.2593, 0.2472], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2408, 0.2594, 0.2524, 0.2473], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2426, 0.2703, 0.2646, 0.2225], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2039, 0.2652, 0.2736, 0.2573], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2217, 0.2755, 0.2681, 0.2347], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2325, 0.261 , 0.2458, 0.2607], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2563, 0.2453, 0.2468, 0.2516], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2735, 0.2276, 0.2266, 0.2722], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2551, 0.2439, 0.2397, 0.2612], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2565, 0.2316, 0.2339, 0.278 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2728, 0.2374, 0.244 , 0.2457], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2456, 0.2402, 0.2516, 0.2626], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2311, 0.2432, 0.2504, 0.2753], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.229 , 0.2383, 0.2636, 0.2691], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2717, 0.2201, 0.2247, 0.2836], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2402, 0.2274, 0.2277, 0.3048], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.271 , 0.2227, 0.2413, 0.265 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3396, 0.2284, 0.2083, 0.2237], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2928, 0.2073, 0.2192, 0.2807], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2893, 0.2137, 0.2162, 0.2807], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3098, 0.2133, 0.2295, 0.2474], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2426, 0.2415, 0.2346, 0.2812], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.252 , 0.2272, 0.2366, 0.2842], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2187, 0.2471, 0.2503, 0.2839], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2397, 0.2591, 0.2666, 0.2346], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2345, 0.2721, 0.267 , 0.2263], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3015, 0.2529, 0.2348, 0.2108], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2973, 0.2363, 0.221 , 0.2454], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3895, 0.1721, 0.1732, 0.2653], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2426, 0.24  , 0.2516, 0.2658], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1775, 0.2069, 0.2531, 0.3625], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2341, 0.2423, 0.2549, 0.2687], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2595, 0.2692, 0.2065, 0.2648], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2304, 0.2445, 0.2132, 0.3119], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2204, 0.2484, 0.2228, 0.3084], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3342, 0.2252, 0.1746, 0.2661], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2613, 0.2455, 0.2072, 0.2859], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1956, 0.295 , 0.2926, 0.2168], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1899, 0.2956, 0.2951, 0.2194], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.262 , 0.2419, 0.2366, 0.2595], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4232, 0.1659, 0.1718, 0.2391], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3242, 0.2078, 0.208 , 0.26  ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.305 , 0.2244, 0.2165, 0.2542], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3362, 0.239 , 0.2116, 0.2132], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2662, 0.22  , 0.2325, 0.2813], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3014, 0.2046, 0.1981, 0.2959], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3039, 0.1879, 0.2039, 0.3043], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2589, 0.231 , 0.2369, 0.2732], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2702, 0.222 , 0.2237, 0.2841], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.322 , 0.1878, 0.1917, 0.2984], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.336 , 0.1852, 0.178 , 0.3008], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.298 , 0.2316, 0.2218, 0.2485], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2855, 0.2013, 0.2011, 0.312 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2711, 0.2218, 0.2323, 0.2748], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2776, 0.1969, 0.2115, 0.314 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2721, 0.1848, 0.2103, 0.3327], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2248, 0.2791, 0.2713, 0.2249], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2988, 0.2631, 0.2331, 0.205 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2732, 0.2332, 0.2337, 0.2599], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2051, 0.2814, 0.2787, 0.2348], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2311, 0.2885, 0.2611, 0.2193], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3196, 0.2343, 0.2238, 0.2223], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2355, 0.2743, 0.2708, 0.2194], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1928, 0.2757, 0.2819, 0.2497], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1757, 0.2972, 0.2859, 0.2412], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1783, 0.3047, 0.3015, 0.2155], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2387, 0.2643, 0.2834, 0.2136], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2819, 0.2508, 0.24  , 0.2273], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.272 , 0.2458, 0.2331, 0.2492], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2827, 0.2434, 0.2032, 0.2707], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.212 , 0.2786, 0.2863, 0.2231], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2018, 0.269 , 0.273 , 0.2563], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2124, 0.2936, 0.2489, 0.2451], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3646, 0.1911, 0.1749, 0.2694], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2432, 0.2502, 0.2241, 0.2825], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2755, 0.2358, 0.207 , 0.2817], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2372, 0.2656, 0.2641, 0.2331], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2027, 0.2626, 0.264 , 0.2707], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2411, 0.2531, 0.2517, 0.2541], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2277, 0.2588, 0.2706, 0.2429], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2907, 0.2186, 0.222 , 0.2687], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2941, 0.2305, 0.2162, 0.2592], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2421, 0.259 , 0.2634, 0.2354], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2192, 0.2829, 0.2772, 0.2207], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3593, 0.1813, 0.1864, 0.273 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3407, 0.1789, 0.2013, 0.279 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2397, 0.2418, 0.2508, 0.2677], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2534, 0.2481, 0.2562, 0.2423], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2588, 0.2667, 0.2599, 0.2146], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2505, 0.2511, 0.2575, 0.2409], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2067, 0.2488, 0.2745, 0.27  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2217, 0.2575, 0.2681, 0.2527], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2399, 0.2693, 0.2698, 0.221 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2155, 0.262 , 0.2809, 0.2416], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.244 , 0.2582, 0.2576, 0.2402], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2026, 0.2591, 0.2738, 0.2645], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2111, 0.2656, 0.2706, 0.2528], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2996, 0.2081, 0.2269, 0.2654], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3035, 0.2219, 0.2219, 0.2527], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2684, 0.2278, 0.251 , 0.2528], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2547, 0.2489, 0.2597, 0.2368], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3704, 0.1759, 0.1835, 0.2702], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3346, 0.2275, 0.1984, 0.2395], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3396, 0.2255, 0.2076, 0.2272], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2022, 0.2693, 0.2426, 0.2859], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1889, 0.2655, 0.2395, 0.3061], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2235, 0.2426, 0.2569, 0.277 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2329, 0.2506, 0.2463, 0.2702], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.243 , 0.226 , 0.2506, 0.2805], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.256 , 0.2407, 0.2383, 0.2649], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2337, 0.2285, 0.2418, 0.296 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2452, 0.2438, 0.2528, 0.2582], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2772, 0.241 , 0.2358, 0.246 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2481, 0.2579, 0.2487, 0.2453], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2801, 0.1865, 0.2224, 0.311 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2314, 0.224 , 0.2565, 0.288 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2269, 0.2478, 0.2614, 0.2639], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2685, 0.2571, 0.2373, 0.2371], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3397, 0.1877, 0.1992, 0.2735], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3475, 0.1815, 0.1908, 0.2802], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3107, 0.2102, 0.218 , 0.261 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2305, 0.2567, 0.2467, 0.2662], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2317, 0.2502, 0.2468, 0.2713], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2579, 0.2541, 0.2527, 0.2353], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2702, 0.241 , 0.2291, 0.2596], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.194 , 0.2787, 0.2871, 0.2402], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1872, 0.2851, 0.2914, 0.2363], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.208 , 0.2758, 0.2832, 0.2331], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2008, 0.2905, 0.2754, 0.2333], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1995, 0.2903, 0.2713, 0.2389], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2213, 0.2911, 0.2617, 0.2259], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2747, 0.2512, 0.2287, 0.2455], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2913, 0.2655, 0.2307, 0.2125], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.248 , 0.2378, 0.2647, 0.2495], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2623, 0.2421, 0.2551, 0.2404], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2227, 0.2522, 0.2823, 0.2428], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2187, 0.2655, 0.2649, 0.251 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2753, 0.232 , 0.2261, 0.2666], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2012, 0.2587, 0.2815, 0.2586], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2306, 0.2546, 0.2558, 0.259 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2236, 0.1369, 0.1841, 0.4553], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.212 , 0.2674, 0.2679, 0.2526], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2617, 0.2482, 0.2313, 0.2588], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2547, 0.2504, 0.2469, 0.248 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2706, 0.2467, 0.2421, 0.2406], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2902, 0.2422, 0.2305, 0.2371], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2571, 0.2381, 0.2376, 0.2671], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3594, 0.18  , 0.1815, 0.279 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2886, 0.2459, 0.2335, 0.232 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2543, 0.2534, 0.2579, 0.2343], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2819, 0.2467, 0.2322, 0.2393], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2804, 0.2201, 0.2267, 0.2728], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2167, 0.2673, 0.2598, 0.2562], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.23  , 0.2665, 0.2544, 0.249 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2788, 0.242 , 0.2468, 0.2324], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3298, 0.184 , 0.1933, 0.2929], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.266 , 0.2502, 0.2589, 0.2249], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3062, 0.2106, 0.2348, 0.2485], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3099, 0.215 , 0.2282, 0.2469], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2954, 0.2131, 0.2455, 0.246 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2171, 0.2402, 0.2457, 0.2971], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2042, 0.2603, 0.2628, 0.2727], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2602, 0.2391, 0.2458, 0.2548], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2419, 0.2255, 0.2398, 0.2928], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2762, 0.2508, 0.2189, 0.2541], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2801, 0.2193, 0.2214, 0.2792], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3114, 0.2115, 0.2207, 0.2564], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2426, 0.2474, 0.2342, 0.2758], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2971, 0.2153, 0.2166, 0.271 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2805, 0.2477, 0.2326, 0.2393], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2691, 0.2424, 0.2383, 0.2502], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3581, 0.2783, 0.1748, 0.1888], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2619, 0.2457, 0.2579, 0.2345], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2277, 0.2504, 0.2661, 0.2558], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.237 , 0.252 , 0.2521, 0.2589], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2385, 0.2403, 0.2614, 0.2599], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2621, 0.2444, 0.2527, 0.2407], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2087, 0.2529, 0.2791, 0.2593], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2992, 0.1888, 0.205 , 0.307 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3457, 0.1895, 0.1844, 0.2804], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2636, 0.2265, 0.2319, 0.278 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.23  , 0.2545, 0.2626, 0.2529], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3531, 0.18  , 0.1774, 0.2894], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3008, 0.1938, 0.1929, 0.3124], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2443, 0.243 , 0.2561, 0.2566], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2411, 0.2706, 0.262 , 0.2263], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2359, 0.2472, 0.2543, 0.2625], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2394, 0.2589, 0.2599, 0.2418], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2458, 0.2617, 0.2591, 0.2335], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2511, 0.2182, 0.2199, 0.3108], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2614, 0.2486, 0.2411, 0.2489], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2326, 0.2728, 0.2818, 0.2128], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.198 , 0.2824, 0.2881, 0.2315], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1822, 0.2768, 0.2802, 0.2608], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2516, 0.2488, 0.2535, 0.2461], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2557, 0.2697, 0.2663, 0.2083], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3246, 0.2313, 0.2178, 0.2264], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2723, 0.2599, 0.248 , 0.2199], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2445, 0.2578, 0.24  , 0.2577], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2046, 0.282 , 0.2847, 0.2288], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.199 , 0.2799, 0.2825, 0.2387], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2199, 0.2725, 0.2702, 0.2375], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2018, 0.2808, 0.2826, 0.2349], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2122, 0.2803, 0.2742, 0.2333], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3335, 0.2356, 0.2136, 0.2173], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3267, 0.253 , 0.2173, 0.203 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2763, 0.2308, 0.2378, 0.2551], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3156, 0.2155, 0.2187, 0.2502], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2794, 0.2321, 0.2333, 0.2551], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2996, 0.2217, 0.2218, 0.2569], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3393, 0.1895, 0.1914, 0.2798], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2083, 0.2375, 0.2559, 0.2982], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2945, 0.1734, 0.1946, 0.3375], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3077, 0.1615, 0.1996, 0.3312], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2892, 0.2243, 0.2295, 0.257 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2432, 0.257 , 0.2571, 0.2427], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2201, 0.2486, 0.2602, 0.2711], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2322, 0.252 , 0.2532, 0.2626], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2402, 0.2483, 0.2669, 0.2446], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2668, 0.2372, 0.2363, 0.2596], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2838, 0.2407, 0.2357, 0.2397], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1221, 0.2334, 0.2713, 0.3732], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2235, 0.215 , 0.1847, 0.3767], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1601, 0.1847, 0.2014, 0.4539], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2535, 0.1651, 0.1624, 0.4191], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2031, 0.163 , 0.1777, 0.4561], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2455, 0.2384, 0.2585, 0.2577], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2661, 0.2328, 0.2447, 0.2564], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2683, 0.2467, 0.2429, 0.2421], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.263 , 0.2299, 0.2443, 0.2628], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2882, 0.2238, 0.238 , 0.25  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2622, 0.2446, 0.2393, 0.2539], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2525, 0.2516, 0.2614, 0.2344], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.298 , 0.2084, 0.2135, 0.2801], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2852, 0.2256, 0.2325, 0.2567], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2468, 0.232 , 0.2482, 0.273 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2707, 0.2368, 0.239 , 0.2536], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2885, 0.1997, 0.1964, 0.3155], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2197, 0.1941, 0.2394, 0.3468], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2083, 0.2296, 0.2411, 0.3209], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2588, 0.2483, 0.242 , 0.2509], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2631, 0.2443, 0.235 , 0.2576], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2298, 0.259 , 0.2725, 0.2387], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2236, 0.2683, 0.2702, 0.2379], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2039, 0.2712, 0.2902, 0.2347], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2721, 0.2608, 0.2493, 0.2178], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2393, 0.2392, 0.2306, 0.291 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2396, 0.2221, 0.2394, 0.2989], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2587, 0.2462, 0.2497, 0.2453], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3249, 0.1826, 0.1987, 0.2938], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2684, 0.2206, 0.2001, 0.3109], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2517, 0.1964, 0.1894, 0.3625], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2497, 0.2135, 0.1809, 0.356 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2466, 0.1933, 0.1703, 0.3898], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2925, 0.1962, 0.1371, 0.3742], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2323, 0.267 , 0.2789, 0.2219], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2311, 0.2566, 0.2674, 0.2449], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2802, 0.2299, 0.2333, 0.2566], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3723, 0.1579, 0.1647, 0.3051], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2657, 0.2469, 0.2484, 0.239 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2711, 0.248 , 0.244 , 0.237 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3116, 0.2162, 0.2098, 0.2624], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.323 , 0.2141, 0.2161, 0.2467], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2309, 0.2522, 0.251 , 0.2659], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2446, 0.2394, 0.2402, 0.2758], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2221, 0.2533, 0.272 , 0.2526], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2908, 0.228 , 0.2271, 0.254 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2807, 0.1846, 0.2173, 0.3173], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2516, 0.2116, 0.2177, 0.3191], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1935, 0.2036, 0.2254, 0.3775], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.178 , 0.2058, 0.2537, 0.3625], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2556, 0.2157, 0.2248, 0.304 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2038, 0.2151, 0.2459, 0.3352], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3514, 0.2124, 0.1824, 0.2538], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.239 , 0.2331, 0.2761, 0.2518], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2476, 0.2352, 0.273 , 0.2442], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3178, 0.1872, 0.2026, 0.2924], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3055, 0.2226, 0.2109, 0.261 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2546, 0.2466, 0.2406, 0.2582], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2678, 0.2292, 0.2406, 0.2624], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4053, 0.1345, 0.143 , 0.3172], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3148, 0.2127, 0.209 , 0.2635], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.296 , 0.2186, 0.2282, 0.2573], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3301, 0.2024, 0.2086, 0.2588], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2042, 0.2559, 0.2399, 0.3   ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2226, 0.2443, 0.2403, 0.2928], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2569, 0.2468, 0.2525, 0.2438], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2417, 0.2634, 0.2549, 0.2399], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.232 , 0.2504, 0.2654, 0.2521], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2323, 0.2551, 0.2646, 0.248 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2113, 0.2676, 0.2784, 0.2427], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2024, 0.269 , 0.2772, 0.2514], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2155, 0.2646, 0.2683, 0.2515], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2074, 0.2726, 0.2779, 0.2421], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2086, 0.2774, 0.2717, 0.2423], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1883, 0.2772, 0.2797, 0.2548], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1925, 0.2787, 0.282 , 0.2468], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.196 , 0.2762, 0.2775, 0.2503], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.263 , 0.2687, 0.2455, 0.2228], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2468, 0.2485, 0.2532, 0.2515], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.225 , 0.255 , 0.2643, 0.2557], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3239, 0.2158, 0.2078, 0.2525], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.212 , 0.2656, 0.2675, 0.255 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2509, 0.2326, 0.2376, 0.2789], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2497, 0.2486, 0.282 , 0.2198], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3029, 0.1992, 0.2148, 0.283 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2983, 0.198 , 0.2162, 0.2875], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2765, 0.1913, 0.2249, 0.3073], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2471, 0.253 , 0.2666, 0.2333], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2508, 0.2579, 0.2564, 0.2349], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2381, 0.2489, 0.2678, 0.2452], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2589, 0.2527, 0.2539, 0.2345], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.262 , 0.2437, 0.2379, 0.2564], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3279, 0.1984, 0.2055, 0.2682], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2518, 0.2605, 0.2478, 0.2399], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2375, 0.2683, 0.2686, 0.2256], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2491, 0.2582, 0.2509, 0.2417], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.268 , 0.2523, 0.2445, 0.2352], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2235, 0.2675, 0.2662, 0.2429], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3178, 0.2078, 0.2143, 0.2601], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2714, 0.2481, 0.2384, 0.242 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2711, 0.2337, 0.2377, 0.2576], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2176, 0.2364, 0.2414, 0.3047], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2115, 0.2815, 0.2808, 0.2263], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.211 , 0.2743, 0.2633, 0.2514], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3852, 0.2649, 0.1627, 0.1872], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2696, 0.2509, 0.2051, 0.2744], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2314, 0.2327, 0.201 , 0.3349], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2242, 0.2375, 0.2066, 0.3317], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3434, 0.1995, 0.1835, 0.2736], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2179, 0.2711, 0.2802, 0.2308], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2401, 0.2647, 0.2528, 0.2423], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2748, 0.2362, 0.2322, 0.2568], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2751, 0.2511, 0.2436, 0.2303], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2757, 0.2432, 0.2474, 0.2337], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3212, 0.2446, 0.2092, 0.2251], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2044, 0.2833, 0.2848, 0.2275], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2085, 0.2868, 0.2859, 0.2188], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2204, 0.2785, 0.2713, 0.2298], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2243, 0.2798, 0.2709, 0.2249], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.216 , 0.2666, 0.2813, 0.2361], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2315, 0.262 , 0.2578, 0.2486], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2505, 0.2662, 0.2527, 0.2306], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.249 , 0.2457, 0.2486, 0.2567], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2723, 0.2306, 0.2197, 0.2774], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2452, 0.2431, 0.2516, 0.2601], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2589, 0.217 , 0.2279, 0.2962], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3353, 0.2048, 0.19  , 0.2698], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3791, 0.1633, 0.1627, 0.295 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2436, 0.2543, 0.2509, 0.2512], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.248 , 0.2477, 0.245 , 0.2593], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3199, 0.2274, 0.2081, 0.2447], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3071, 0.2306, 0.2217, 0.2406], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3031, 0.2255, 0.214 , 0.2574], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3974, 0.1687, 0.1718, 0.2621], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2398, 0.2037, 0.2377, 0.3187], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2357, 0.2227, 0.2401, 0.3016], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.226 , 0.2043, 0.2303, 0.3394], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2181, 0.2284, 0.2473, 0.3062], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2335, 0.2203, 0.2646, 0.2816], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.323 , 0.1804, 0.1903, 0.3064], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2557, 0.2506, 0.245 , 0.2488], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2637, 0.2484, 0.2345, 0.2534], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2601, 0.2404, 0.2434, 0.2561], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2587, 0.2379, 0.25  , 0.2534], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2303, 0.2591, 0.2568, 0.2538], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.181 , 0.2957, 0.3044, 0.2189], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1785, 0.3031, 0.3114, 0.207 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1816, 0.3086, 0.2988, 0.211 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3256, 0.2242, 0.2055, 0.2447], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2302, 0.2107, 0.2159, 0.3433], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2066, 0.1655, 0.186 , 0.4418], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3682, 0.1989, 0.1821, 0.2509], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2262, 0.2519, 0.271 , 0.2509], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.26  , 0.2456, 0.2439, 0.2505], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2635, 0.2475, 0.2395, 0.2494], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2357, 0.2673, 0.2612, 0.2358], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3011, 0.2078, 0.2215, 0.2696], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2684, 0.2439, 0.248 , 0.2396], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2629, 0.2512, 0.2415, 0.2443], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.267 , 0.2133, 0.2359, 0.2838], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2827, 0.232 , 0.2231, 0.2622], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2567, 0.2401, 0.2604, 0.2428], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2163, 0.2628, 0.2772, 0.2436], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2235, 0.2713, 0.276 , 0.2291], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2986, 0.214 , 0.217 , 0.2704], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2664, 0.243 , 0.2333, 0.2572], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2181, 0.2437, 0.2509, 0.2873], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2907, 0.2283, 0.2211, 0.2598], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3031, 0.2334, 0.2103, 0.2533], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2623, 0.2623, 0.2451, 0.2303], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2677, 0.255 , 0.229 , 0.2482], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2261, 0.2604, 0.2641, 0.2494], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3431, 0.2034, 0.1885, 0.265 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2105, 0.2762, 0.2735, 0.2398], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2294, 0.2747, 0.2766, 0.2194], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2094, 0.2749, 0.2836, 0.2321], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2903, 0.2621, 0.2318, 0.2158], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2464, 0.2689, 0.2574, 0.2273], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2259, 0.269 , 0.2659, 0.2392], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2671, 0.24  , 0.2651, 0.2278], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2754, 0.2448, 0.2366, 0.2432], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2621, 0.2469, 0.2493, 0.2417], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.246 , 0.2629, 0.2573, 0.2338], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.308 , 0.2154, 0.2298, 0.2468], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2553, 0.2324, 0.2523, 0.2601], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2532, 0.2439, 0.2452, 0.2577], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2536, 0.2464, 0.2604, 0.2396], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2626, 0.2424, 0.2427, 0.2523], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2479, 0.2523, 0.2681, 0.2318], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2452, 0.254 , 0.2466, 0.2543], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2652, 0.2309, 0.2411, 0.2628], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2894, 0.2312, 0.2263, 0.2531], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2871, 0.2413, 0.2325, 0.2391], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2609, 0.2423, 0.2438, 0.253 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2459, 0.2621, 0.2541, 0.2379], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2395, 0.2512, 0.2573, 0.2521], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2356, 0.2574, 0.2504, 0.2567], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2604, 0.2593, 0.2452, 0.2351], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2843, 0.2214, 0.2266, 0.2677], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2826, 0.2232, 0.2293, 0.2648], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.291 , 0.2174, 0.2168, 0.2748], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2727, 0.2443, 0.2354, 0.2477], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2784, 0.2391, 0.2385, 0.244 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2659, 0.2469, 0.2336, 0.2536], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3058, 0.2011, 0.2096, 0.2835], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2754, 0.2195, 0.237 , 0.2681], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2399, 0.2503, 0.2528, 0.2569], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2935, 0.2475, 0.2301, 0.2289], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2812, 0.2439, 0.2203, 0.2547], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2731, 0.2527, 0.2264, 0.2478], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3568, 0.2017, 0.188 , 0.2536], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2971, 0.2299, 0.2259, 0.247 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2825, 0.2385, 0.2432, 0.2358], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2892, 0.2273, 0.2335, 0.2501], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2842, 0.2345, 0.2366, 0.2447], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.274 , 0.2327, 0.2402, 0.2531], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2866, 0.2149, 0.2268, 0.2717], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2801, 0.2286, 0.2464, 0.2449], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.252 , 0.2372, 0.2568, 0.254 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2393, 0.2539, 0.2576, 0.2492], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2281, 0.2471, 0.2512, 0.2736], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1908, 0.2832, 0.2746, 0.2514], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2071, 0.2786, 0.2572, 0.2571], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1852, 0.274 , 0.2721, 0.2687], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2292, 0.2647, 0.2459, 0.2602], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1676, 0.2954, 0.3029, 0.2342], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2583, 0.2479, 0.2647, 0.2291], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2239, 0.2795, 0.2746, 0.222 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2564, 0.2497, 0.2378, 0.2561], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2826, 0.245 , 0.2372, 0.2351], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2887, 0.2231, 0.226 , 0.2621], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2674, 0.2397, 0.2423, 0.2507], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2599, 0.2517, 0.2683, 0.2201], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2434, 0.2612, 0.2665, 0.2288], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3007, 0.2015, 0.1991, 0.2987], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.316 , 0.1959, 0.2031, 0.285 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2818, 0.2225, 0.2267, 0.2689], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2597, 0.2187, 0.2316, 0.29  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3207, 0.2056, 0.2033, 0.2704], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3113, 0.2285, 0.2222, 0.2381], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2964, 0.2264, 0.2177, 0.2595], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3568, 0.2155, 0.1935, 0.2342], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.324 , 0.2433, 0.2292, 0.2034], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3304, 0.258 , 0.2176, 0.194 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2309, 0.2764, 0.2661, 0.2266], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2677, 0.2565, 0.2355, 0.2404], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2578, 0.2428, 0.2366, 0.2628], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2463, 0.2422, 0.2397, 0.2719], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2352, 0.2523, 0.249 , 0.2634], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2578, 0.2448, 0.2347, 0.2627], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2311, 0.2442, 0.2537, 0.271 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2372, 0.2471, 0.2503, 0.2654], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2317, 0.2627, 0.2493, 0.2563], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3232, 0.1956, 0.1895, 0.2917], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3838, 0.1983, 0.1928, 0.2251], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2687, 0.2374, 0.2325, 0.2614], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2799, 0.2323, 0.2337, 0.2541], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3869, 0.1886, 0.1914, 0.233 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4043, 0.1802, 0.1844, 0.2312], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1833, 0.297 , 0.3048, 0.2148], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1819, 0.2988, 0.3066, 0.2127], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1774, 0.2967, 0.3202, 0.2056], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.174 , 0.3044, 0.3141, 0.2076], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1756, 0.2981, 0.3161, 0.2102], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2764, 0.245 , 0.2585, 0.2201], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2203, 0.2612, 0.2794, 0.2391], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1904, 0.2483, 0.3043, 0.2569], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3774, 0.1834, 0.1205, 0.3187], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2812, 0.1525, 0.1469, 0.4194], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2785, 0.1721, 0.1639, 0.3854], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1903, 0.1735, 0.1892, 0.447 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3086, 0.2316, 0.2288, 0.231 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2867, 0.2115, 0.2111, 0.2907], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2966, 0.2275, 0.2118, 0.2642], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2447, 0.2591, 0.2521, 0.2441], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2305, 0.266 , 0.2604, 0.2431], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2214, 0.2517, 0.276 , 0.2509], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2245, 0.2632, 0.2675, 0.2447], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2185, 0.2566, 0.2633, 0.2616], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2485, 0.2267, 0.2567, 0.2681], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.248 , 0.206 , 0.2304, 0.3155], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2447, 0.229 , 0.2517, 0.2746], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2778, 0.1912, 0.2186, 0.3124], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3067, 0.2065, 0.2078, 0.279 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2612, 0.2685, 0.2635, 0.2068], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2664, 0.2631, 0.2336, 0.2369], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2711, 0.2397, 0.2229, 0.2662], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2846, 0.2497, 0.2295, 0.2363], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3062, 0.241 , 0.2194, 0.2333], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2106, 0.2685, 0.2872, 0.2337], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3135, 0.2074, 0.2164, 0.2627], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3183, 0.2121, 0.2062, 0.2634], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2617, 0.2125, 0.2175, 0.3082], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2625, 0.221 , 0.2145, 0.302 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2519, 0.2422, 0.226 , 0.2799], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2595, 0.2333, 0.2245, 0.2827], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2868, 0.228 , 0.1956, 0.2895], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2373, 0.2509, 0.2604, 0.2513], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2549, 0.2608, 0.2473, 0.2369], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3042, 0.224 , 0.2154, 0.2564], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2835, 0.2527, 0.2335, 0.2303], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2002, 0.2896, 0.2947, 0.2155], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1849, 0.2877, 0.3098, 0.2177], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1653, 0.2969, 0.3187, 0.2191], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3334, 0.1722, 0.2009, 0.2935], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3337, 0.215 , 0.2364, 0.2148], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2825, 0.1973, 0.2263, 0.2939], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1989, 0.2258, 0.2686, 0.3066], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.183 , 0.2579, 0.2797, 0.2794], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1703, 0.2857, 0.2535, 0.2906], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3511, 0.1959, 0.2118, 0.2412], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2053, 0.2804, 0.2824, 0.2318], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2   , 0.2826, 0.2859, 0.2315], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2075, 0.2553, 0.2784, 0.2588], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2297, 0.2496, 0.2559, 0.2649], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2282, 0.2578, 0.2724, 0.2416], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2085, 0.2654, 0.2807, 0.2453], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.234 , 0.2565, 0.2515, 0.2579], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2905, 0.2318, 0.2168, 0.2609], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.21  , 0.2471, 0.2899, 0.253 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1972, 0.2617, 0.2863, 0.2548], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2196, 0.2392, 0.2463, 0.2949], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.252 , 0.2503, 0.2254, 0.2723], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2384, 0.2563, 0.2623, 0.243 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2838, 0.2357, 0.2235, 0.257 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2116, 0.2436, 0.2509, 0.294 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2607, 0.2135, 0.2275, 0.2983], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2567, 0.214 , 0.2361, 0.2932], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.227 , 0.2434, 0.2479, 0.2817], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2279, 0.2599, 0.2598, 0.2524], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.248 , 0.2512, 0.2482, 0.2526], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2064, 0.2538, 0.2757, 0.2641], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2618, 0.2253, 0.2237, 0.2892], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2261, 0.2485, 0.2814, 0.244 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2038, 0.2707, 0.2925, 0.233 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1953, 0.2766, 0.303 , 0.2251], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2358, 0.2601, 0.2733, 0.2308], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2197, 0.2789, 0.2826, 0.2188], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4134, 0.1117, 0.1333, 0.3416], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2642, 0.2384, 0.2441, 0.2533], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2644, 0.2364, 0.2357, 0.2636], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2712, 0.2589, 0.2375, 0.2324], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2964, 0.1895, 0.2083, 0.3059], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3182, 0.2184, 0.2217, 0.2416], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2789, 0.2336, 0.2341, 0.2534], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3128, 0.2226, 0.226 , 0.2386], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2677, 0.2345, 0.2461, 0.2516], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2415, 0.217 , 0.224 , 0.3175], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1995, 0.2423, 0.2747, 0.2835], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1865, 0.246 , 0.3138, 0.2538], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.216 , 0.2516, 0.2568, 0.2756], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3121, 0.2376, 0.2241, 0.2263], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2257, 0.2568, 0.2646, 0.2529], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2216, 0.2473, 0.265 , 0.2662], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2365, 0.251 , 0.2552, 0.2573], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2486, 0.2425, 0.2546, 0.2544], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2275, 0.244 , 0.2537, 0.2748], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.24  , 0.2452, 0.2584, 0.2564], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2382, 0.2563, 0.2309, 0.2747], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2112, 0.2449, 0.2322, 0.3117], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2498, 0.236 , 0.2485, 0.2658], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1843, 0.2509, 0.2852, 0.2796], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2092, 0.2741, 0.2614, 0.2553], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1731, 0.259 , 0.3001, 0.2678], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1882, 0.2538, 0.2873, 0.2707], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1944, 0.2558, 0.2898, 0.26  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3127, 0.2229, 0.2174, 0.247 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2859, 0.2301, 0.2269, 0.2571], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.343 , 0.1827, 0.1931, 0.2812], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4007, 0.154 , 0.1525, 0.2928], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2168, 0.2551, 0.279 , 0.249 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2652, 0.2117, 0.2369, 0.2862], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2404, 0.2304, 0.2484, 0.2808], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2693, 0.2237, 0.2412, 0.2658], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.254 , 0.2554, 0.2422, 0.2485], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2375, 0.2696, 0.2418, 0.2512], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2194, 0.2795, 0.2648, 0.2364], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3063, 0.2209, 0.2067, 0.2661], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.202 , 0.2605, 0.2755, 0.262 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2241, 0.2396, 0.2628, 0.2736], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2453, 0.2202, 0.2375, 0.297 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2366, 0.2214, 0.2546, 0.2875], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2211, 0.23  , 0.2546, 0.2943], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2178, 0.2452, 0.2627, 0.2744], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1951, 0.2511, 0.2662, 0.2876], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1991, 0.2503, 0.2644, 0.2862], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2927, 0.204 , 0.2151, 0.2882], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3532, 0.2178, 0.2131, 0.2159], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1687, 0.3008, 0.3229, 0.2076], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.195 , 0.2874, 0.2918, 0.2258], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1988, 0.2823, 0.2931, 0.2258], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1921, 0.2943, 0.2996, 0.214 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1757, 0.2928, 0.3013, 0.2302], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1686, 0.2922, 0.3099, 0.2293], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2901, 0.2372, 0.23  , 0.2428], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2958, 0.2403, 0.2176, 0.2463], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2883, 0.2312, 0.2284, 0.252 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2968, 0.227 , 0.224 , 0.2522], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2093, 0.2901, 0.2727, 0.2279], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2846, 0.238 , 0.2042, 0.2732], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4138, 0.1546, 0.1483, 0.2833], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2742, 0.2319, 0.2282, 0.2657], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3092, 0.2226, 0.2264, 0.2419], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2847, 0.232 , 0.2409, 0.2424], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2124, 0.2789, 0.2597, 0.249 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2499, 0.2499, 0.2257, 0.2745], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2467, 0.2745, 0.2558, 0.223 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.234 , 0.266 , 0.2549, 0.2452], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3074, 0.2519, 0.2233, 0.2175], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2893, 0.2573, 0.2376, 0.2158], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2847, 0.2478, 0.2164, 0.251 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3037, 0.2608, 0.2097, 0.2258], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3098, 0.2575, 0.2135, 0.2192], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2747, 0.2255, 0.2493, 0.2506], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3079, 0.243 , 0.2134, 0.2358], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4032, 0.2405, 0.1599, 0.1964], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3811, 0.217 , 0.1739, 0.228 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3798, 0.2357, 0.1837, 0.2008], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2657, 0.2607, 0.2538, 0.2198], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2312, 0.2442, 0.2467, 0.2778], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2207, 0.2626, 0.2694, 0.2473], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2416, 0.2564, 0.2563, 0.2458], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4101, 0.1689, 0.1485, 0.2724], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2551, 0.2533, 0.2377, 0.2539], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2858, 0.2271, 0.2219, 0.2652], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2967, 0.2402, 0.2184, 0.2447], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2503, 0.1861, 0.1851, 0.3784], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2882, 0.2148, 0.2014, 0.2956], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2374, 0.2397, 0.2625, 0.2603], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2447, 0.2543, 0.2615, 0.2395], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3923, 0.2012, 0.1967, 0.2098], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.303 , 0.2006, 0.2228, 0.2735], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3085, 0.2571, 0.2279, 0.2065], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2936, 0.2374, 0.2254, 0.2436], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2485, 0.2259, 0.2425, 0.2831], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2912, 0.2298, 0.2226, 0.2565], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.264 , 0.2446, 0.2394, 0.2521], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2723, 0.222 , 0.2343, 0.2713], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3286, 0.2188, 0.2161, 0.2364], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2702, 0.2811, 0.22  , 0.2286], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2149, 0.2709, 0.2744, 0.2398], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2525, 0.2426, 0.239 , 0.2659], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2717, 0.2043, 0.2255, 0.2984], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2205, 0.209 , 0.2351, 0.3353], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2524, 0.2141, 0.235 , 0.2985], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.5185, 0.1456, 0.0962, 0.2397], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4896, 0.1934, 0.1117, 0.2053], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4279, 0.1756, 0.1495, 0.2469], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2475, 0.2573, 0.2375, 0.2577], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2653, 0.2415, 0.2556, 0.2375], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.251 , 0.2511, 0.2422, 0.2557], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2213, 0.2618, 0.2717, 0.2452], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2418, 0.2514, 0.2535, 0.2533], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2589, 0.2498, 0.2399, 0.2514], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2208, 0.2538, 0.2699, 0.2556], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2971, 0.2501, 0.239 , 0.2137], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2499, 0.2445, 0.2669, 0.2387], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2715, 0.2432, 0.2461, 0.2392], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2366, 0.2293, 0.2595, 0.2745], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2282, 0.2266, 0.2358, 0.3094], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2887, 0.2235, 0.2208, 0.2671], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.228 , 0.2502, 0.2665, 0.2552], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2014, 0.2473, 0.2639, 0.2874], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2489, 0.2519, 0.2703, 0.2288], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2403, 0.2078, 0.2285, 0.3235], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2637, 0.2507, 0.2579, 0.2277], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2532, 0.2537, 0.2475, 0.2456], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.268 , 0.2364, 0.2483, 0.2474], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3227, 0.2166, 0.1981, 0.2626], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2278, 0.2485, 0.2489, 0.2747], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2433, 0.2557, 0.2517, 0.2493], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2467, 0.2516, 0.2419, 0.2598], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3534, 0.1995, 0.197 , 0.2501], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.285 , 0.2111, 0.2157, 0.2882], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2133, 0.272 , 0.2806, 0.2342], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1974, 0.2938, 0.2805, 0.2283], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2022, 0.2908, 0.2729, 0.2341], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2283, 0.2708, 0.2409, 0.26  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2459, 0.2803, 0.2497, 0.2241], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2473, 0.2618, 0.2691, 0.2218], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.325 , 0.1859, 0.1999, 0.2893], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2838, 0.2381, 0.233 , 0.2451], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4195, 0.1521, 0.1477, 0.2807], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2952, 0.2146, 0.2362, 0.254 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2351, 0.2669, 0.2513, 0.2467], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2684, 0.2636, 0.2349, 0.2331], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3776, 0.1671, 0.1649, 0.2904], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2684, 0.2379, 0.2344, 0.2593], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2745, 0.2287, 0.2358, 0.261 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2593, 0.2188, 0.2426, 0.2793], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2704, 0.2182, 0.2263, 0.2851], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2537, 0.2319, 0.2542, 0.2603], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2445, 0.248 , 0.238 , 0.2696], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.289 , 0.2108, 0.205 , 0.2953], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3018, 0.2145, 0.209 , 0.2747], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2756, 0.2317, 0.2079, 0.2847], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2356, 0.2579, 0.2607, 0.2458], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2499, 0.2391, 0.245 , 0.266 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.264 , 0.2283, 0.2375, 0.2702], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2673, 0.2303, 0.2328, 0.2697], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2698, 0.2242, 0.2131, 0.2929], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2819, 0.2458, 0.2276, 0.2448], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2716, 0.2176, 0.2282, 0.2826], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2348, 0.2201, 0.2378, 0.3072], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2784, 0.2436, 0.2183, 0.2597], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2721, 0.2473, 0.2292, 0.2513], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2801, 0.2132, 0.2297, 0.277 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3694, 0.2034, 0.2022, 0.225 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2519, 0.2584, 0.2584, 0.2313], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2853, 0.2479, 0.234 , 0.2328], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2707, 0.256 , 0.2448, 0.2284], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3105, 0.2   , 0.2072, 0.2822], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2909, 0.2405, 0.2276, 0.2411], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2316, 0.2842, 0.2677, 0.2164], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2555, 0.2504, 0.2503, 0.2438], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.249 , 0.235 , 0.243 , 0.2729], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2731, 0.2272, 0.2518, 0.2479], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2017, 0.2868, 0.286 , 0.2255], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2447, 0.2547, 0.2515, 0.249 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2803, 0.2128, 0.2342, 0.2727], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3108, 0.1688, 0.19  , 0.3304], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1964, 0.2048, 0.2351, 0.3636], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1807, 0.2041, 0.2379, 0.3773], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3448, 0.1528, 0.175 , 0.3274], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3027, 0.1968, 0.1913, 0.3092], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4867, 0.1573, 0.129 , 0.227 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4891, 0.167 , 0.1417, 0.2022], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4346, 0.2026, 0.1466, 0.2163], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4341, 0.1928, 0.1314, 0.2417], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4597, 0.2019, 0.141 , 0.1974], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.5074, 0.1779, 0.1224, 0.1923], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.531 , 0.136 , 0.1005, 0.2325], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.5759, 0.1353, 0.1035, 0.1853], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2257, 0.2672, 0.2711, 0.2361], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2369, 0.2688, 0.2509, 0.2434], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2174, 0.2749, 0.2532, 0.2546], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2109, 0.2699, 0.2628, 0.2564], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2668, 0.2343, 0.2344, 0.2644], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2379, 0.2385, 0.2424, 0.2812], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2307, 0.2465, 0.2516, 0.2712], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2988, 0.224 , 0.2123, 0.265 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.328 , 0.2182, 0.2311, 0.2226], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3137, 0.1969, 0.2139, 0.2754], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2582, 0.2519, 0.2358, 0.254 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2887, 0.2361, 0.2342, 0.241 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2499, 0.2484, 0.2312, 0.2705], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2315, 0.2496, 0.2711, 0.2478], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2839, 0.2314, 0.2184, 0.2663], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2311, 0.25  , 0.2617, 0.2572], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2289, 0.2461, 0.2572, 0.2678], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2734, 0.2466, 0.2372, 0.2428], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2303, 0.2286, 0.2347, 0.3063], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2615, 0.2052, 0.2259, 0.3074], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.294 , 0.2143, 0.2106, 0.2811], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3237, 0.2093, 0.2224, 0.2445], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3419, 0.1973, 0.1961, 0.2647], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2525, 0.2627, 0.2679, 0.2169], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2255, 0.2717, 0.2751, 0.2277], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2453, 0.2543, 0.2678, 0.2325], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2284, 0.2907, 0.2743, 0.2067], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2408, 0.2626, 0.2593, 0.2373], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3195, 0.171 , 0.2103, 0.2991], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3413, 0.1804, 0.1965, 0.2818], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2294, 0.2573, 0.2622, 0.2512], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2076, 0.2757, 0.2755, 0.2412], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2382, 0.2788, 0.2656, 0.2175], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2683, 0.2548, 0.234 , 0.2428], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.193 , 0.2418, 0.2558, 0.3093], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2167, 0.2429, 0.2595, 0.2808], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2174, 0.239 , 0.2551, 0.2884], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2068, 0.2468, 0.2669, 0.2796], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2218, 0.2321, 0.2458, 0.3003], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3001, 0.1978, 0.2386, 0.2635], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2529, 0.2058, 0.2439, 0.2974], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2405, 0.2143, 0.2616, 0.2836], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2182, 0.2456, 0.2914, 0.2448], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2156, 0.2596, 0.2985, 0.2263], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2069, 0.2396, 0.2957, 0.2578], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2611, 0.241 , 0.233 , 0.2649], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2581, 0.2422, 0.2513, 0.2484], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2401, 0.2385, 0.2467, 0.2747], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2756, 0.2487, 0.2276, 0.2481], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2852, 0.2241, 0.222 , 0.2686], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3061, 0.2068, 0.2141, 0.2729], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2586, 0.2105, 0.2386, 0.2923], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4229, 0.1595, 0.154 , 0.2635], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2207, 0.2347, 0.2505, 0.294 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2752, 0.2044, 0.2047, 0.3157], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2261, 0.1663, 0.191 , 0.4167], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2095, 0.2673, 0.2892, 0.234 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2031, 0.2716, 0.2909, 0.2343], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2335, 0.2611, 0.2723, 0.2332], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2412, 0.2464, 0.2661, 0.2463], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2582, 0.2258, 0.2406, 0.2754], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.279 , 0.2333, 0.2297, 0.258 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2618, 0.2098, 0.2356, 0.2928], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2724, 0.255 , 0.2481, 0.2245], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2317, 0.2513, 0.2562, 0.2608], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2383, 0.2633, 0.268 , 0.2305], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2436, 0.2582, 0.2503, 0.2478], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2175, 0.2606, 0.254 , 0.2679], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2007, 0.263 , 0.2709, 0.2654], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3286, 0.2318, 0.2263, 0.2133], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2456, 0.258 , 0.2523, 0.2442], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2982, 0.2368, 0.2178, 0.2472], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1783, 0.2482, 0.3012, 0.2723], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2112, 0.2468, 0.2651, 0.2769], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3053, 0.2504, 0.2263, 0.2181], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2251, 0.2376, 0.2723, 0.265 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2011, 0.2598, 0.2994, 0.2397], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.304 , 0.2144, 0.2285, 0.2532], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3376, 0.1538, 0.1777, 0.331 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2837, 0.1813, 0.2241, 0.311 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1815, 0.2889, 0.2493, 0.2804], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2346, 0.2668, 0.2699, 0.2288], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3214, 0.2403, 0.1933, 0.245 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3099, 0.212 , 0.2101, 0.268 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2072, 0.2414, 0.271 , 0.2804], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1909, 0.2152, 0.2448, 0.349 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2032, 0.2002, 0.2604, 0.3362], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1832, 0.2219, 0.2533, 0.3416], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2205, 0.1766, 0.2188, 0.384 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1726, 0.2484, 0.2429, 0.3361], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4246, 0.208 , 0.1535, 0.2139], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3862, 0.1943, 0.1778, 0.2417], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1754, 0.3044, 0.314 , 0.2062], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1728, 0.3074, 0.3152, 0.2046], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2853, 0.2621, 0.2491, 0.2034], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1776, 0.2681, 0.2808, 0.2734], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1818, 0.2907, 0.3029, 0.2245], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1816, 0.2978, 0.3066, 0.2139], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2993, 0.2224, 0.2156, 0.2627], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2379, 0.2555, 0.2604, 0.2461], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1778, 0.2496, 0.2826, 0.2901], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2086, 0.2827, 0.2797, 0.229 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2117, 0.2817, 0.2806, 0.2261], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2004, 0.2954, 0.2854, 0.2188], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1884, 0.2818, 0.2895, 0.2403], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1915, 0.2912, 0.297 , 0.2203], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3566, 0.1488, 0.1606, 0.3339], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2424, 0.2638, 0.2414, 0.2524], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3439, 0.2065, 0.1843, 0.2653], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3477, 0.2077, 0.173 , 0.2716], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3625, 0.2276, 0.1694, 0.2405], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2264, 0.2696, 0.2581, 0.246 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.29  , 0.2234, 0.2168, 0.2699], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2977, 0.2413, 0.2166, 0.2444], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4413, 0.1469, 0.1292, 0.2826], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4492, 0.1555, 0.1286, 0.2667], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4338, 0.1503, 0.1333, 0.2826], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2018, 0.1726, 0.2262, 0.3995], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1783, 0.1925, 0.2411, 0.3882], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1884, 0.189 , 0.2484, 0.3742], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1669, 0.2076, 0.2452, 0.3804], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1754, 0.1798, 0.2359, 0.4089], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.18  , 0.2007, 0.2454, 0.3739], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2122, 0.2123, 0.2387, 0.3368], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1939, 0.1717, 0.2011, 0.4333], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2756, 0.1923, 0.2167, 0.3154], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2637, 0.1933, 0.2088, 0.3342], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2488, 0.2376, 0.2543, 0.2593], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2663, 0.2298, 0.2089, 0.295 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2412, 0.2536, 0.2542, 0.251 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2219, 0.2715, 0.2725, 0.2341], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2452, 0.2255, 0.2392, 0.29  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.333 , 0.2148, 0.1967, 0.2556], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.301 , 0.2139, 0.2076, 0.2776], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2872, 0.1928, 0.2027, 0.3173], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2857, 0.1985, 0.2181, 0.2978], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2824, 0.2046, 0.2212, 0.2918], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3423, 0.1828, 0.1838, 0.2911], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3125, 0.2284, 0.2219, 0.2372], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2711, 0.2309, 0.2333, 0.2648], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2088, 0.2842, 0.2761, 0.2309], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2387, 0.2578, 0.2613, 0.2422], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2749, 0.2324, 0.2492, 0.2435], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2641, 0.1696, 0.1972, 0.3692], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2758, 0.172 , 0.1857, 0.3665], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1811, 0.2654, 0.2815, 0.272 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2944, 0.2229, 0.2366, 0.2461], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3065, 0.2127, 0.2184, 0.2625], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2981, 0.2093, 0.226 , 0.2666], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2365, 0.2214, 0.2462, 0.296 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1898, 0.2445, 0.2672, 0.2985], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.216 , 0.2589, 0.2529, 0.2723], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1554, 0.2721, 0.2784, 0.2941], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3584, 0.1933, 0.1936, 0.2547], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3431, 0.1905, 0.2092, 0.2572], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3402, 0.197 , 0.2088, 0.254 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3408, 0.2089, 0.1992, 0.2511], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2496, 0.2474, 0.2469, 0.2561], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2621, 0.2447, 0.2325, 0.2607], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.261 , 0.2456, 0.2475, 0.2459], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2512, 0.2319, 0.2535, 0.2634], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2386, 0.2495, 0.2769, 0.2351], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2674, 0.2489, 0.2493, 0.2344], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2998, 0.2404, 0.2264, 0.2334], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2795, 0.2377, 0.2336, 0.2492], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3055, 0.2207, 0.2191, 0.2547], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.378 , 0.1819, 0.1694, 0.2707], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3045, 0.196 , 0.2094, 0.2902], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3182, 0.2097, 0.2092, 0.2629], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.241 , 0.2617, 0.2618, 0.2356], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2359, 0.2719, 0.2615, 0.2308], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2493, 0.2531, 0.2607, 0.2369], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2546, 0.2499, 0.2593, 0.2363], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4038, 0.1661, 0.1623, 0.2679], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2904, 0.2392, 0.2503, 0.2201], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2862, 0.2466, 0.2455, 0.2217], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2899, 0.2265, 0.2318, 0.2517], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2923, 0.2215, 0.2312, 0.255 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2822, 0.2261, 0.2379, 0.2538], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2423, 0.2558, 0.2512, 0.2507], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2378, 0.2619, 0.2627, 0.2376], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2408, 0.273 , 0.2565, 0.2297], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2166, 0.2675, 0.277 , 0.2389], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3339, 0.224 , 0.2084, 0.2338], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2602, 0.2343, 0.2471, 0.2584], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2493, 0.249 , 0.2571, 0.2446], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2882, 0.2227, 0.2165, 0.2726], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3064, 0.2336, 0.2131, 0.2468], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2783, 0.2338, 0.2285, 0.2594], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2668, 0.2281, 0.2459, 0.2592], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2894, 0.2258, 0.2355, 0.2493], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2736, 0.2382, 0.2203, 0.2679], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2799, 0.2407, 0.2314, 0.248 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2395, 0.2418, 0.2367, 0.2819], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2716, 0.2434, 0.2242, 0.2608], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3315, 0.1831, 0.2059, 0.2795], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3046, 0.2256, 0.2346, 0.2351], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.308, 0.226, 0.232, 0.234], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2982, 0.2194, 0.2241, 0.2583], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2689, 0.2385, 0.2454, 0.2472], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2886, 0.2253, 0.234 , 0.2521], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2775, 0.2337, 0.244 , 0.2447], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2976, 0.229 , 0.2285, 0.2449], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3205, 0.2185, 0.2132, 0.2478], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3639, 0.1805, 0.1727, 0.2828], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2477, 0.2471, 0.2644, 0.2408], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2754, 0.234 , 0.2291, 0.2615], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2922, 0.2232, 0.2296, 0.2551], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2697, 0.2506, 0.2418, 0.2379], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.327 , 0.1893, 0.1963, 0.2874], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3268, 0.195 , 0.2174, 0.2608], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3025, 0.2092, 0.2254, 0.263 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.294 , 0.2143, 0.2344, 0.2573], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3135, 0.2056, 0.2025, 0.2785], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2384, 0.2498, 0.2542, 0.2577], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3062, 0.2207, 0.2371, 0.2359], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3021, 0.2125, 0.2225, 0.263 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2608, 0.2405, 0.2486, 0.2502], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2497, 0.2379, 0.257 , 0.2554], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2461, 0.24  , 0.2323, 0.2816], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3143, 0.1972, 0.2157, 0.2728], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3424, 0.2114, 0.1996, 0.2466], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2162, 0.2832, 0.2711, 0.2294], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.273 , 0.2364, 0.229 , 0.2616], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2338, 0.2461, 0.2382, 0.2819], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2669, 0.2449, 0.229 , 0.2593], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3   , 0.2206, 0.2134, 0.2661], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3014, 0.2094, 0.228 , 0.2612], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2893, 0.225 , 0.2313, 0.2544], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2688, 0.2385, 0.2387, 0.254 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2734, 0.2377, 0.2328, 0.2561], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2788, 0.2429, 0.2355, 0.2427], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2557, 0.2419, 0.2576, 0.2448], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.26  , 0.2348, 0.2481, 0.257 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.277 , 0.2299, 0.2286, 0.2645], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2257, 0.2412, 0.2627, 0.2704], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2399, 0.2383, 0.261 , 0.2607], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2339, 0.2419, 0.26  , 0.2642], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2266, 0.2622, 0.2718, 0.2394], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.6757, 0.108 , 0.0753, 0.1411], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2592, 0.2437, 0.23  , 0.2671], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2223, 0.2427, 0.2436, 0.2914], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2703, 0.2504, 0.2372, 0.2422], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.299 , 0.2006, 0.196 , 0.3043], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2799, 0.2343, 0.2354, 0.2504], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3501, 0.1802, 0.2154, 0.2544], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2544, 0.2438, 0.2669, 0.235 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2667, 0.2504, 0.2419, 0.241 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2558, 0.2604, 0.2511, 0.2327], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.207 , 0.2727, 0.2791, 0.2412], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.205 , 0.2756, 0.2831, 0.2362], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3076, 0.2188, 0.2198, 0.2538], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2794, 0.2535, 0.2285, 0.2386], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2883, 0.2417, 0.2295, 0.2405], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2635, 0.2527, 0.2549, 0.2289], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2663, 0.2469, 0.2564, 0.2303], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3897, 0.1787, 0.1696, 0.262 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1996, 0.2719, 0.2714, 0.2571], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.21  , 0.2743, 0.2621, 0.2536], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.201 , 0.2787, 0.2757, 0.2446], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2034, 0.2793, 0.2691, 0.2482], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2014, 0.2778, 0.2668, 0.254 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2797, 0.2604, 0.2458, 0.2141], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2763, 0.2648, 0.2436, 0.2153], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.338 , 0.2048, 0.1985, 0.2587], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3457, 0.198 , 0.1986, 0.2577], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.326 , 0.2073, 0.2161, 0.2507], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2957, 0.2175, 0.2298, 0.2569], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3054, 0.2123, 0.2177, 0.2646], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.287 , 0.225 , 0.2408, 0.2472], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2253, 0.2829, 0.2689, 0.2229], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1972, 0.2866, 0.2846, 0.2316], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2369, 0.2352, 0.2311, 0.2968], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2294, 0.2611, 0.2542, 0.2552], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2356, 0.2536, 0.245 , 0.2658], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2395, 0.2486, 0.239 , 0.2728], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.285 , 0.2221, 0.2341, 0.2588], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2852, 0.2134, 0.2176, 0.2838], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2623, 0.2492, 0.253 , 0.2356], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2743, 0.2521, 0.235 , 0.2387], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2413, 0.2752, 0.2631, 0.2203], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2445, 0.2612, 0.2505, 0.2437], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2546, 0.2523, 0.2495, 0.2436], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2915, 0.202 , 0.2062, 0.3002], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2375, 0.2709, 0.2559, 0.2356], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2347, 0.2629, 0.2508, 0.2517], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3186, 0.2012, 0.2119, 0.2682], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2558, 0.2522, 0.2518, 0.2402], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2277, 0.2516, 0.268 , 0.2527], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2334, 0.2551, 0.2629, 0.2486], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2275, 0.246 , 0.2595, 0.267 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2255, 0.2614, 0.2584, 0.2547], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2743, 0.2418, 0.2561, 0.2278], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.266 , 0.2622, 0.248 , 0.2238], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3562, 0.1956, 0.1863, 0.2619], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2629, 0.2425, 0.239 , 0.2555], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.312 , 0.2286, 0.2047, 0.2547], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3475, 0.2405, 0.1915, 0.2205], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2451, 0.2749, 0.2505, 0.2295], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2253, 0.2645, 0.2686, 0.2417], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2607, 0.239 , 0.2461, 0.2542], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2621, 0.2439, 0.2531, 0.2409], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2664, 0.2339, 0.2518, 0.2479], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2366, 0.2558, 0.2558, 0.2518], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.282 , 0.2327, 0.2355, 0.2498], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2726, 0.2357, 0.2418, 0.25  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3245, 0.2116, 0.2076, 0.2563], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2965, 0.2436, 0.228 , 0.232 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2842, 0.2383, 0.2285, 0.2489], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2753, 0.2246, 0.2317, 0.2684], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3802, 0.1719, 0.1805, 0.2674], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2444, 0.2451, 0.2581, 0.2524], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.258 , 0.2444, 0.2586, 0.2391], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.311 , 0.2067, 0.2106, 0.2718], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2717, 0.2341, 0.2366, 0.2576], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2632, 0.2356, 0.2428, 0.2584], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3233, 0.2058, 0.2161, 0.2548], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2747, 0.2296, 0.2353, 0.2604], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2597, 0.2329, 0.2605, 0.2469], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.33  , 0.2112, 0.1982, 0.2605], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2758, 0.2393, 0.2283, 0.2566], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2136, 0.2784, 0.2758, 0.2323], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2308, 0.265 , 0.2605, 0.2436], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2258, 0.2603, 0.2544, 0.2595], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2477, 0.2581, 0.2504, 0.2438], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2877, 0.2289, 0.2321, 0.2512], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2941, 0.2142, 0.2364, 0.2553], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2798, 0.2227, 0.2479, 0.2496], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2972, 0.2283, 0.2157, 0.2588], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.273 , 0.2378, 0.2292, 0.26  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2867, 0.2242, 0.2312, 0.2578], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.311 , 0.2126, 0.2142, 0.2623], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2853, 0.2318, 0.2311, 0.2519], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3101, 0.2135, 0.2105, 0.266 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2527, 0.2448, 0.2481, 0.2544], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2089, 0.2736, 0.2637, 0.2537], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3046, 0.2012, 0.2106, 0.2836], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2848, 0.2411, 0.239 , 0.2351], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2747, 0.2376, 0.2407, 0.247 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2644, 0.2374, 0.2591, 0.2391], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3029, 0.2124, 0.2091, 0.2756], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.305 , 0.2194, 0.2121, 0.2635], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2868, 0.2258, 0.2266, 0.2608], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2876, 0.227 , 0.2304, 0.255 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3029, 0.224 , 0.2218, 0.2514], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2722, 0.2499, 0.2404, 0.2375], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3208, 0.2048, 0.2248, 0.2495], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2483, 0.2217, 0.2661, 0.2639], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3036, 0.2193, 0.2417, 0.2354], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.254, 0.232, 0.261, 0.253], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2462, 0.2459, 0.2639, 0.244 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2967, 0.234 , 0.2268, 0.2426], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3095, 0.2156, 0.218 , 0.2569], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2854, 0.2028, 0.2313, 0.2805], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3232, 0.1646, 0.2237, 0.2885], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3446, 0.1784, 0.1962, 0.2809], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2481, 0.251 , 0.2526, 0.2483], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2621, 0.2461, 0.2424, 0.2494], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2578, 0.2377, 0.2376, 0.2669], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2637, 0.2433, 0.248 , 0.245 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2773, 0.2318, 0.2378, 0.2531], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2499, 0.2344, 0.2515, 0.2641], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3306, 0.2143, 0.2047, 0.2503], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2918, 0.2323, 0.2317, 0.2442], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2723, 0.232 , 0.2447, 0.2511], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3073, 0.2119, 0.2142, 0.2666], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2457, 0.2591, 0.2567, 0.2385], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2527, 0.2612, 0.244 , 0.2421], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2888, 0.2442, 0.2285, 0.2385], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2847, 0.2363, 0.2249, 0.2542], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2514, 0.2554, 0.2582, 0.235 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2252, 0.2726, 0.2809, 0.2213], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.323 , 0.2101, 0.2008, 0.2661], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2158, 0.2644, 0.2808, 0.2391], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2248, 0.2629, 0.2655, 0.2468], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3044, 0.2186, 0.2142, 0.2627], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3261, 0.2101, 0.2039, 0.2599], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3051, 0.2174, 0.2361, 0.2414], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2671, 0.2352, 0.2382, 0.2596], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3068, 0.2339, 0.22  , 0.2393], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3014, 0.2244, 0.2064, 0.2679], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2724, 0.2801, 0.2473, 0.2002], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3075, 0.1578, 0.142 , 0.3927], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2335, 0.2444, 0.2404, 0.2817], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2061, 0.2454, 0.2523, 0.2962], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2814, 0.2155, 0.2175, 0.2855], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2593, 0.256 , 0.2516, 0.2331], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.269 , 0.2582, 0.2502, 0.2226], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2354, 0.2688, 0.2651, 0.2307], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2424, 0.2531, 0.2553, 0.2493], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2765, 0.2699, 0.2351, 0.2185], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2809, 0.242 , 0.2425, 0.2346], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2563, 0.241 , 0.2543, 0.2484], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2302, 0.2487, 0.2702, 0.2509], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2813, 0.2461, 0.2304, 0.2422], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2634, 0.2466, 0.2429, 0.2471], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.38  , 0.1456, 0.1595, 0.315 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2636, 0.1988, 0.2449, 0.2928], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3387, 0.2069, 0.2214, 0.2329], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3314, 0.2181, 0.2257, 0.2248], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3601, 0.2112, 0.1939, 0.2347], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3937, 0.1844, 0.1801, 0.2419], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2971, 0.2271, 0.217 , 0.2588], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2822, 0.235 , 0.2304, 0.2524], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.319 , 0.2076, 0.2211, 0.2523], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3766, 0.1854, 0.1892, 0.2488], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2268, 0.2583, 0.268 , 0.2469], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2134, 0.2646, 0.2765, 0.2456], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.274 , 0.2479, 0.2328, 0.2453], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2555, 0.2556, 0.2447, 0.2442], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2743, 0.2228, 0.2338, 0.2692], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4348, 0.1488, 0.1358, 0.2806], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5672, 0.1456, 0.1249, 0.1623], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4754, 0.136 , 0.1324, 0.2562], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3241, 0.212 , 0.2102, 0.2537], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3219, 0.2092, 0.2083, 0.2606], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3161, 0.2136, 0.2143, 0.2559], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2851, 0.2219, 0.2324, 0.2606], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.284 , 0.2276, 0.2336, 0.2548], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2821, 0.2283, 0.2375, 0.252 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2792, 0.234 , 0.2376, 0.2492], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3388, 0.1959, 0.192 , 0.2732], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2733, 0.2031, 0.2373, 0.2863], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2859, 0.2083, 0.2381, 0.2676], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2867, 0.2215, 0.2548, 0.2371], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2503, 0.255 , 0.2484, 0.2464], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2298, 0.2514, 0.2609, 0.2579], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2508, 0.2459, 0.2506, 0.2526], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2603, 0.2313, 0.2397, 0.2687], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2054, 0.2616, 0.2637, 0.2693], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2101, 0.2682, 0.2702, 0.2516], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2289, 0.2568, 0.2637, 0.2506], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2215, 0.2735, 0.2619, 0.2431], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2649, 0.2443, 0.2306, 0.2602], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.307 , 0.2455, 0.2262, 0.2213], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3459, 0.2177, 0.1955, 0.241 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3382, 0.2144, 0.2005, 0.2469], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2507, 0.2461, 0.2515, 0.2516], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2537, 0.2495, 0.2468, 0.2501], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2293, 0.2433, 0.2592, 0.2683], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2493, 0.2505, 0.2418, 0.2584], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2798, 0.224 , 0.2357, 0.2605], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2496, 0.2351, 0.2433, 0.272 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.231, 0.251, 0.266, 0.252], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2479, 0.2497, 0.2528, 0.2496], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2538, 0.2677, 0.2507, 0.2278], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2437, 0.2481, 0.2643, 0.2438], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3211, 0.2147, 0.2134, 0.2509], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3275, 0.1967, 0.2073, 0.2686], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3013, 0.2163, 0.2125, 0.2698], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2923, 0.2154, 0.2244, 0.2679], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2941, 0.2281, 0.2392, 0.2386], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3622, 0.1903, 0.1839, 0.2635], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3784, 0.1633, 0.1761, 0.2822], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3902, 0.1894, 0.1795, 0.2408], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2999, 0.1901, 0.1984, 0.3117], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3141, 0.1999, 0.2088, 0.2773], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3579, 0.1567, 0.1715, 0.3139], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2769, 0.2396, 0.2432, 0.2404], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3535, 0.2183, 0.2089, 0.2193], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3307, 0.2191, 0.1997, 0.2505], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2717, 0.2199, 0.2176, 0.2908], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2418, 0.2491, 0.2607, 0.2484], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2526, 0.2431, 0.2459, 0.2584], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2693, 0.2392, 0.241 , 0.2505], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2803, 0.2355, 0.2365, 0.2477], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2404, 0.2564, 0.2478, 0.2554], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2361, 0.2666, 0.2773, 0.22  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2245, 0.2341, 0.2556, 0.2858], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2431, 0.2544, 0.2422, 0.2603], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3002, 0.2099, 0.2036, 0.2863], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.283 , 0.2201, 0.2366, 0.2603], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2787, 0.242 , 0.2319, 0.2474], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2327, 0.2498, 0.2596, 0.258 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2296, 0.2682, 0.2645, 0.2377], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3232, 0.1954, 0.1928, 0.2887], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2158, 0.2794, 0.2671, 0.2377], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2971, 0.2246, 0.2251, 0.2532], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2459, 0.2671, 0.2583, 0.2287], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2821, 0.237 , 0.2481, 0.2328], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2761, 0.2292, 0.2281, 0.2666], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3065, 0.223 , 0.2131, 0.2574], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3255, 0.1935, 0.2044, 0.2766], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3468, 0.1939, 0.1971, 0.2622], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3433, 0.2095, 0.2076, 0.2396], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3002, 0.2039, 0.2192, 0.2767], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2747, 0.2303, 0.2482, 0.2467], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3043, 0.2107, 0.2295, 0.2556], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3091, 0.2053, 0.2162, 0.2694], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3265, 0.2123, 0.2093, 0.2519], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2966, 0.2082, 0.2241, 0.271 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3153, 0.2145, 0.2368, 0.2334], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2802, 0.2385, 0.2436, 0.2376], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3245, 0.2254, 0.2241, 0.226 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2988, 0.2281, 0.2253, 0.2478], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2108, 0.2706, 0.2756, 0.2431], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2106, 0.2794, 0.2853, 0.2246], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2009, 0.2791, 0.2946, 0.2253], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.26  , 0.2497, 0.2521, 0.2382], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2497, 0.2575, 0.2422, 0.2506], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3142, 0.2162, 0.2087, 0.2609], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3215, 0.2265, 0.2166, 0.2354], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2751, 0.2389, 0.2315, 0.2545], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2645, 0.2335, 0.2384, 0.2636], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2958, 0.2141, 0.2195, 0.2706], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2399, 0.2501, 0.2512, 0.2589], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3777, 0.1773, 0.1807, 0.2643], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2595, 0.2349, 0.234 , 0.2715], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2689, 0.2336, 0.2348, 0.2627], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2766, 0.2418, 0.2349, 0.2466], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2949, 0.2407, 0.2295, 0.2349], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2433, 0.2461, 0.2397, 0.271 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3131, 0.193 , 0.1879, 0.306 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3184, 0.1875, 0.201 , 0.2932], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3245, 0.2011, 0.197 , 0.2774], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2367, 0.2656, 0.2648, 0.233 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3688, 0.1792, 0.1716, 0.2804], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3559, 0.163 , 0.1641, 0.3171], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2733, 0.2435, 0.2316, 0.2516], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2352, 0.2396, 0.2163, 0.3089], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2549, 0.2533, 0.2304, 0.2613], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2639, 0.2289, 0.2111, 0.2961], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.275 , 0.2494, 0.2242, 0.2515], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2591, 0.2462, 0.2495, 0.2453], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2563, 0.2413, 0.2491, 0.2532], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2113, 0.2699, 0.2695, 0.2492], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2848, 0.2408, 0.2292, 0.2451], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2572, 0.25  , 0.2357, 0.257 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2119, 0.2814, 0.2708, 0.2359], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2133, 0.2708, 0.2799, 0.2359], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.254 , 0.2384, 0.2473, 0.2604], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2533, 0.2524, 0.2547, 0.2396], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.277 , 0.2485, 0.2315, 0.2431], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3112, 0.2216, 0.213 , 0.2543], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3172, 0.2276, 0.2093, 0.2458], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3464, 0.2218, 0.203 , 0.2288], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3927, 0.1726, 0.1824, 0.2523], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2165, 0.2698, 0.2701, 0.2436], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4033, 0.1873, 0.1671, 0.2422], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3383, 0.2   , 0.185 , 0.2768], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3096, 0.1962, 0.1959, 0.2984], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3152, 0.2045, 0.1931, 0.2871], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4007, 0.1854, 0.1588, 0.255 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3589, 0.1805, 0.1854, 0.2751], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3062, 0.1704, 0.1831, 0.3403], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3151, 0.1703, 0.1953, 0.3193], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3015, 0.1867, 0.2149, 0.2969], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3675, 0.1565, 0.1571, 0.3188], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3994, 0.1614, 0.1617, 0.2775], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2672, 0.2292, 0.2585, 0.2451], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2662, 0.2446, 0.2463, 0.2429], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3492, 0.213 , 0.1921, 0.2458], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2149, 0.2785, 0.2828, 0.2239], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2656, 0.2456, 0.2387, 0.2501], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2452, 0.2558, 0.2509, 0.2481], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.267 , 0.2328, 0.2374, 0.2628], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2865, 0.216 , 0.2239, 0.2736], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2484, 0.2633, 0.2598, 0.2285], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2286, 0.257 , 0.269 , 0.2454], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2267, 0.2652, 0.2657, 0.2424], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3102, 0.2091, 0.2013, 0.2794], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3087, 0.2141, 0.2233, 0.2539], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2911, 0.2421, 0.2452, 0.2215], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2628, 0.2567, 0.2428, 0.2377], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3094, 0.2235, 0.2264, 0.2408], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3145, 0.2233, 0.2219, 0.2404], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3681, 0.1818, 0.2005, 0.2496], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3336, 0.1692, 0.1961, 0.3011], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3228, 0.2211, 0.2137, 0.2425], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2218, 0.2503, 0.2561, 0.2718], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2358, 0.2599, 0.251 , 0.2533], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2806, 0.2423, 0.234 , 0.2432], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2805, 0.2355, 0.2323, 0.2517], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3262, 0.2405, 0.2117, 0.2216], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3078, 0.2346, 0.2313, 0.2263], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2767, 0.2238, 0.2246, 0.2749], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4417, 0.2486, 0.1533, 0.1563], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2583, 0.2512, 0.25  , 0.2405], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3145, 0.2548, 0.2418, 0.1889], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2949, 0.202 , 0.2101, 0.2929], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2562, 0.2313, 0.2492, 0.2632], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2307, 0.225 , 0.2809, 0.2634], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.213 , 0.2617, 0.2781, 0.2471], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2678, 0.2375, 0.2442, 0.2506], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2592, 0.2291, 0.2476, 0.2641], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2425, 0.2374, 0.24  , 0.2801], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2729, 0.2172, 0.2236, 0.2862], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2744, 0.2255, 0.2311, 0.2689], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2653, 0.2346, 0.2604, 0.2396], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3651, 0.185 , 0.1768, 0.2731], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3466, 0.1902, 0.2082, 0.2551], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.388 , 0.1715, 0.1804, 0.26  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2826, 0.2357, 0.2457, 0.236 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.334 , 0.1976, 0.2004, 0.268 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.242 , 0.2512, 0.2479, 0.2589], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2585, 0.2471, 0.2414, 0.253 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3149, 0.1951, 0.1888, 0.3012], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3182, 0.1923, 0.2009, 0.2886], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2302, 0.2481, 0.2486, 0.2731], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2081, 0.2405, 0.2491, 0.3023], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2093, 0.2247, 0.2785, 0.2875], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2129, 0.2127, 0.219 , 0.3555], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2321, 0.2531, 0.2504, 0.2644], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2512, 0.2681, 0.25  , 0.2306], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2736, 0.2398, 0.2339, 0.2527], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.247 , 0.2564, 0.2571, 0.2395], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2092, 0.2888, 0.2806, 0.2214], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2523, 0.2359, 0.2439, 0.2678], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2639, 0.23  , 0.2341, 0.272 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2355, 0.2412, 0.2436, 0.2797], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2518, 0.2403, 0.2434, 0.2644], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2528, 0.248 , 0.2521, 0.2472], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2742, 0.2441, 0.2236, 0.2582], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.242 , 0.2369, 0.2509, 0.2702], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4003, 0.191 , 0.1808, 0.2279], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.363 , 0.1806, 0.176 , 0.2805], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2982, 0.189 , 0.2038, 0.3091], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2565, 0.2536, 0.2711, 0.2188], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2474, 0.2549, 0.2507, 0.247 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2762, 0.2495, 0.2416, 0.2328], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2727, 0.2442, 0.2407, 0.2424], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2919, 0.229 , 0.2309, 0.2482], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3115, 0.2244, 0.2306, 0.2335], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3234, 0.2101, 0.2115, 0.255 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2468, 0.2522, 0.2519, 0.2492], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2759, 0.2236, 0.227 , 0.2735], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.289 , 0.2203, 0.2287, 0.262 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2638, 0.2594, 0.2459, 0.231 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2809, 0.2155, 0.2299, 0.2737], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2847, 0.2265, 0.2313, 0.2575], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.274 , 0.2287, 0.2318, 0.2655], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2607, 0.2407, 0.2343, 0.2643], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2261, 0.2747, 0.2665, 0.2326], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2417, 0.2626, 0.2639, 0.2318], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2439, 0.2538, 0.2437, 0.2585], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2421, 0.2746, 0.2521, 0.2312], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2685, 0.2322, 0.2478, 0.2515], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2852, 0.2398, 0.2364, 0.2387], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2385, 0.2463, 0.2572, 0.258 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2516, 0.259 , 0.2548, 0.2347], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3073, 0.202 , 0.2089, 0.2819], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2335, 0.2684, 0.2734, 0.2247], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2218, 0.2882, 0.275 , 0.215 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3128, 0.1907, 0.19  , 0.3065], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2285, 0.2521, 0.2578, 0.2617], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2442, 0.2655, 0.2585, 0.2318], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3018, 0.1931, 0.2187, 0.2865], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3115, 0.2069, 0.2206, 0.261 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2276, 0.2595, 0.2778, 0.2352], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2635, 0.2473, 0.2544, 0.2349], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3219, 0.219 , 0.2144, 0.2447], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3005, 0.1928, 0.1926, 0.3142], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2972, 0.2206, 0.2286, 0.2536], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2994, 0.2302, 0.2258, 0.2446], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2496, 0.2315, 0.2617, 0.2573], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2335, 0.2252, 0.2313, 0.31  ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1882, 0.2331, 0.2189, 0.3598], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2856, 0.2072, 0.2012, 0.3059], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2032, 0.2623, 0.262 , 0.2726], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1563, 0.2573, 0.2849, 0.3014], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1839, 0.2591, 0.2661, 0.291 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2109, 0.285 , 0.2785, 0.2257], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2106, 0.2923, 0.2785, 0.2186], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2153, 0.2694, 0.2849, 0.2304], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.196 , 0.2753, 0.293 , 0.2357], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2977, 0.1835, 0.2157, 0.3031], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2624, 0.2082, 0.2096, 0.3198], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3009, 0.1876, 0.1899, 0.3216], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4101, 0.147 , 0.1258, 0.3171], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3431, 0.1617, 0.1964, 0.2988], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2312, 0.1956, 0.2344, 0.3388], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3287, 0.1536, 0.1656, 0.3521], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2665, 0.2203, 0.2402, 0.2729], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3136, 0.2084, 0.2122, 0.2658], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2745, 0.2297, 0.2256, 0.2701], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2559, 0.2571, 0.2456, 0.2414], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2449, 0.2376, 0.2504, 0.2671], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3082, 0.2174, 0.2117, 0.2627], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2489, 0.2618, 0.2542, 0.235 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.325 , 0.204 , 0.2004, 0.2706], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2619, 0.2508, 0.2466, 0.2408], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2779, 0.2476, 0.2239, 0.2506], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3051, 0.231 , 0.2012, 0.2626], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.289 , 0.2409, 0.2337, 0.2364], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3111, 0.2091, 0.2053, 0.2746], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2411, 0.2482, 0.2608, 0.2499], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2517, 0.2513, 0.2593, 0.2377], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2925, 0.2186, 0.2236, 0.2653], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2715, 0.2422, 0.232 , 0.2543], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3013, 0.2151, 0.2115, 0.2721], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2569, 0.2549, 0.246 , 0.2422], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2418, 0.2617, 0.2583, 0.2382], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2474, 0.2591, 0.2566, 0.2369], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2573, 0.2589, 0.2447, 0.2391], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2744, 0.2518, 0.2174, 0.2565], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3659, 0.2291, 0.1957, 0.2093], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2314, 0.2533, 0.2729, 0.2423], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1962, 0.2511, 0.2628, 0.2898], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2231, 0.2382, 0.2424, 0.2962], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.264 , 0.2527, 0.2495, 0.2339], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2743, 0.253 , 0.2397, 0.233 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2365, 0.2426, 0.2504, 0.2705], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.232 , 0.2466, 0.2611, 0.2603], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2793, 0.2116, 0.2247, 0.2843], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2706, 0.2399, 0.2422, 0.2473], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2858, 0.2581, 0.2525, 0.2036], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3068, 0.2292, 0.2151, 0.2489], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2945, 0.206 , 0.2155, 0.284 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3171, 0.1933, 0.1964, 0.2932], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3039, 0.1918, 0.183 , 0.3213], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3072, 0.2053, 0.2147, 0.2728], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3871, 0.1647, 0.1892, 0.2589], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2375, 0.2608, 0.2668, 0.2349], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2502, 0.2345, 0.2489, 0.2664], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2516, 0.2555, 0.2498, 0.243 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2443, 0.2158, 0.2197, 0.3202], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2782, 0.2342, 0.247 , 0.2405], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3171, 0.2139, 0.2188, 0.2501], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3045, 0.215 , 0.2282, 0.2524], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.276 , 0.2371, 0.2424, 0.2445], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2899, 0.2329, 0.2312, 0.246 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3183, 0.2163, 0.2073, 0.2581], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2709, 0.2633, 0.2537, 0.2121], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3052, 0.2108, 0.2061, 0.278 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2453, 0.2567, 0.2504, 0.2475], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2823, 0.2389, 0.2473, 0.2316], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2778, 0.2388, 0.2412, 0.2422], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.228 , 0.2683, 0.2716, 0.2321], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3042, 0.2261, 0.2115, 0.2583], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3288, 0.2213, 0.2158, 0.2341], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3041, 0.233 , 0.217 , 0.246 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3463, 0.1864, 0.1967, 0.2706], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3036, 0.2291, 0.2215, 0.2458], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2401, 0.2551, 0.2814, 0.2235], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2429, 0.2679, 0.2644, 0.2248], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.278 , 0.2369, 0.2474, 0.2376], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2578, 0.2435, 0.2508, 0.2479], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2662, 0.2424, 0.2405, 0.2509], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2704, 0.2249, 0.2126, 0.2921], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2754, 0.235 , 0.2203, 0.2693], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2832, 0.2313, 0.245 , 0.2406], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4102, 0.1825, 0.1749, 0.2324], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2276, 0.2749, 0.2641, 0.2334], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.279 , 0.24  , 0.2359, 0.245 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2612, 0.2478, 0.2338, 0.2573], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3094, 0.2404, 0.2157, 0.2344], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3197, 0.1992, 0.1993, 0.2817], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3256, 0.1977, 0.1933, 0.2834], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.5412, 0.1737, 0.1145, 0.1705], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3202, 0.19  , 0.1913, 0.2985], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2954, 0.216 , 0.2233, 0.2653], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2395, 0.2518, 0.2602, 0.2485], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2449, 0.2596, 0.2608, 0.2347], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3402, 0.1967, 0.1952, 0.2679], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2177, 0.2558, 0.2727, 0.2538], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2314, 0.2611, 0.269 , 0.2385], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2192, 0.2614, 0.2749, 0.2445], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2439, 0.257 , 0.2647, 0.2344], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2568, 0.2507, 0.2616, 0.2308], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2455, 0.2574, 0.2547, 0.2424], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1972, 0.2446, 0.2749, 0.2834], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2732, 0.2653, 0.2345, 0.227 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.325 , 0.1947, 0.1896, 0.2907], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3296, 0.2255, 0.1918, 0.253 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2839, 0.2231, 0.2142, 0.2789], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.247 , 0.24  , 0.2566, 0.2564], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2505, 0.2549, 0.2529, 0.2417], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2542, 0.2728, 0.2512, 0.2218], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2637, 0.2409, 0.2458, 0.2497], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2425, 0.2638, 0.2461, 0.2476], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3085, 0.198 , 0.1948, 0.2988], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2928, 0.209 , 0.2126, 0.2857], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2634, 0.2364, 0.2389, 0.2613], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1985, 0.2784, 0.2755, 0.2475], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2496, 0.2492, 0.2417, 0.2595], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2775, 0.2345, 0.2442, 0.2438], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2914, 0.2395, 0.2291, 0.24  ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3558, 0.1958, 0.1832, 0.2651], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.413 , 0.136 , 0.1438, 0.3073], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4326, 0.1493, 0.1671, 0.251 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3403, 0.1715, 0.1803, 0.3079], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3635, 0.1873, 0.1786, 0.2706], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2395, 0.238 , 0.2631, 0.2594], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2518, 0.236 , 0.2408, 0.2715], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2902, 0.224 , 0.2356, 0.2502], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3286, 0.1963, 0.1969, 0.2782], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3592, 0.1713, 0.1765, 0.293 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2359, 0.2606, 0.2595, 0.244 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2736, 0.2321, 0.2316, 0.2627], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2757, 0.2241, 0.247 , 0.2532], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2753, 0.2323, 0.2447, 0.2478], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2617, 0.2227, 0.259 , 0.2566], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2194, 0.2447, 0.2694, 0.2665], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2738, 0.2238, 0.2449, 0.2576], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2742, 0.229 , 0.259 , 0.2378], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2542, 0.2554, 0.2498, 0.2405], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2455, 0.2735, 0.2544, 0.2266], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2565, 0.2537, 0.2392, 0.2506], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2958, 0.2304, 0.2188, 0.255 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.291 , 0.2243, 0.2221, 0.2625], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2956, 0.2129, 0.2231, 0.2684], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2503, 0.2551, 0.2392, 0.2554], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2278, 0.2445, 0.2666, 0.2611], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2313, 0.2615, 0.2434, 0.2637], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2505, 0.2435, 0.2496, 0.2564], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2505, 0.2611, 0.2508, 0.2377], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2448, 0.2653, 0.2606, 0.2293], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2473, 0.2667, 0.2591, 0.2269], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3012, 0.2442, 0.22  , 0.2346], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2757, 0.2368, 0.2271, 0.2604], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2769, 0.2434, 0.2392, 0.2405], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2942, 0.218 , 0.2073, 0.2805], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.391 , 0.1802, 0.1824, 0.2465], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2778, 0.2473, 0.2341, 0.2408], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2079, 0.2809, 0.2813, 0.2299], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2021, 0.2883, 0.2795, 0.2301], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1929, 0.279 , 0.2961, 0.2321], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3259, 0.2047, 0.2121, 0.2573], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2605, 0.2501, 0.2482, 0.2411], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2819, 0.2567, 0.2389, 0.2224], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3207, 0.2029, 0.2249, 0.2516], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3339, 0.2016, 0.203 , 0.2614], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3306, 0.1881, 0.2086, 0.2727], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2407, 0.2674, 0.256 , 0.236 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2784, 0.2329, 0.2204, 0.2683], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2544, 0.2428, 0.2528, 0.25  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2539, 0.2724, 0.2453, 0.2284], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2295, 0.2538, 0.2662, 0.2505], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3784, 0.2047, 0.197 , 0.2199], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2374, 0.2469, 0.2646, 0.2511], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2325, 0.2589, 0.2707, 0.2378], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2208, 0.2754, 0.2838, 0.22  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.231 , 0.2628, 0.2793, 0.2269], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2412, 0.2623, 0.2628, 0.2337], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2384, 0.2755, 0.2648, 0.2213], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2282, 0.2606, 0.2778, 0.2335], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.252 , 0.242 , 0.2392, 0.2667], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2808, 0.2274, 0.2297, 0.2621], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2977, 0.2303, 0.2297, 0.2423], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2368, 0.2432, 0.2552, 0.2647], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2687, 0.2383, 0.2373, 0.2557], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2439, 0.2541, 0.2465, 0.2554], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2537, 0.2377, 0.2412, 0.2674], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2472, 0.2524, 0.2679, 0.2324], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2474, 0.2387, 0.2454, 0.2685], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3483, 0.2132, 0.1902, 0.2483], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3105, 0.2345, 0.2169, 0.2382], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2918, 0.2362, 0.2343, 0.2377], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2978, 0.2335, 0.2287, 0.24  ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2666, 0.2465, 0.2412, 0.2457], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3809, 0.1903, 0.1807, 0.2481], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3086, 0.2157, 0.1945, 0.2812], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2807, 0.2216, 0.2264, 0.2713], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2806, 0.2359, 0.2392, 0.2443], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3391, 0.193 , 0.2114, 0.2566], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2015, 0.2754, 0.2821, 0.241 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2195, 0.2807, 0.2746, 0.2252], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2952, 0.2168, 0.2276, 0.2604], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2838, 0.21  , 0.226 , 0.2802], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.243 , 0.2582, 0.247 , 0.2519], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3086, 0.2255, 0.2181, 0.2478], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3579, 0.2086, 0.1818, 0.2517], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.264 , 0.2253, 0.2273, 0.2834], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3107, 0.2234, 0.2086, 0.2573], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3032, 0.2214, 0.2176, 0.2577], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3188, 0.2142, 0.2041, 0.2628], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2896, 0.224 , 0.2149, 0.2715], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3655, 0.1777, 0.1937, 0.2631], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3416, 0.2111, 0.21  , 0.2373], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2933, 0.2095, 0.2251, 0.2721], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3321, 0.2082, 0.2091, 0.2505], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3201, 0.2155, 0.2198, 0.2447], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2263, 0.2672, 0.2552, 0.2513], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2284, 0.2687, 0.2611, 0.2419], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2146, 0.281 , 0.2574, 0.247 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1986, 0.225 , 0.2765, 0.2999], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3086, 0.1885, 0.1674, 0.3355], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.248 , 0.2413, 0.2366, 0.2742], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3672, 0.2097, 0.2219, 0.2012], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3263, 0.1424, 0.1639, 0.3675], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2222, 0.2653, 0.2896, 0.223 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.252 , 0.2615, 0.2512, 0.2352], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2924, 0.2185, 0.2197, 0.2693], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2921, 0.1813, 0.2109, 0.3157], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3321, 0.1835, 0.2076, 0.2768], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.29  , 0.1885, 0.2239, 0.2975], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3205, 0.1759, 0.212 , 0.2916], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3414, 0.2068, 0.2383, 0.2135], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.285 , 0.2338, 0.2302, 0.2509], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2828, 0.238 , 0.2373, 0.2419], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2771, 0.2421, 0.2397, 0.2411], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3103, 0.2199, 0.2256, 0.2443], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3056, 0.2249, 0.2234, 0.2461], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2958, 0.2389, 0.227 , 0.2383], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2878, 0.2254, 0.23  , 0.2567], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2615, 0.2548, 0.2484, 0.2353], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2554, 0.256 , 0.2453, 0.2433], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.532 , 0.1657, 0.097 , 0.2053], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5345, 0.1607, 0.0971, 0.2077], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2985, 0.1944, 0.1925, 0.3147], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2207, 0.2114, 0.2087, 0.3592], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3714, 0.1693, 0.1588, 0.3006], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3123, 0.2413, 0.223 , 0.2234], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3362, 0.1874, 0.2118, 0.2646], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3574, 0.1769, 0.2   , 0.2657], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3229, 0.1853, 0.2077, 0.2841], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2916, 0.2227, 0.2224, 0.2632], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.236 , 0.2468, 0.2606, 0.2566], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2758, 0.2361, 0.2264, 0.2617], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2752, 0.2218, 0.2228, 0.2802], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3186, 0.2051, 0.1961, 0.2802], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2732, 0.2405, 0.2406, 0.2457], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.338 , 0.1945, 0.1982, 0.2693], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2338, 0.264 , 0.2483, 0.2539], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3025, 0.209 , 0.2312, 0.2573], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.248 , 0.2428, 0.2338, 0.2753], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.25  , 0.2709, 0.2674, 0.2117], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2642, 0.2418, 0.2553, 0.2387], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2641, 0.2523, 0.2534, 0.2301], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2363, 0.2568, 0.2635, 0.2435], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2315, 0.2696, 0.2732, 0.2257], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3311, 0.2096, 0.2007, 0.2586], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.268 , 0.2468, 0.2503, 0.2348], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3807, 0.1874, 0.1757, 0.2561], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2542, 0.253 , 0.2476, 0.2452], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2804, 0.2372, 0.2397, 0.2427], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.275 , 0.2459, 0.2325, 0.2467], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2709, 0.2311, 0.2383, 0.2597], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3077, 0.2298, 0.227 , 0.2355], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3354, 0.2037, 0.1989, 0.262 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3188, 0.2162, 0.2236, 0.2414], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2287, 0.269 , 0.269 , 0.2333], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2492, 0.2505, 0.2594, 0.241 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2497, 0.2535, 0.2451, 0.2516], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2455, 0.2703, 0.2515, 0.2327], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.321 , 0.1906, 0.2068, 0.2815], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3159, 0.2071, 0.2285, 0.2485], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2636, 0.2438, 0.2666, 0.226 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2615, 0.2482, 0.2603, 0.23  ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3234, 0.2156, 0.2125, 0.2484], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3211, 0.2117, 0.2077, 0.2595], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3532, 0.1942, 0.1916, 0.2609], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3203, 0.2078, 0.2141, 0.2578], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3186, 0.2126, 0.2142, 0.2545], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3033, 0.2107, 0.225 , 0.261 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2848, 0.2263, 0.2413, 0.2477], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2546, 0.2594, 0.2572, 0.2288], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.27  , 0.2264, 0.2315, 0.2721], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2808, 0.2255, 0.2245, 0.2693], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3301, 0.1898, 0.1928, 0.2873], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2172, 0.2684, 0.2663, 0.2481], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2323, 0.281 , 0.2698, 0.2168], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2625, 0.2426, 0.2451, 0.2499], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2504, 0.2248, 0.25  , 0.2748], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2415, 0.2444, 0.2521, 0.2619], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2553, 0.2295, 0.2407, 0.2746], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2484, 0.2425, 0.2446, 0.2646], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2194, 0.2448, 0.2707, 0.2651], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2252, 0.259 , 0.2652, 0.2507], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2515, 0.2547, 0.2433, 0.2504], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.23  , 0.2696, 0.2626, 0.2379], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2704, 0.2446, 0.2444, 0.2407], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2818, 0.2314, 0.2313, 0.2555], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2463, 0.2515, 0.2459, 0.2563], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2487, 0.2697, 0.2527, 0.2288], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2748, 0.2632, 0.2425, 0.2194], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2256, 0.2368, 0.2431, 0.2944], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2456, 0.2365, 0.2453, 0.2726], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2421, 0.2673, 0.2589, 0.2317], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2459, 0.2402, 0.2342, 0.2797], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3034, 0.2166, 0.2199, 0.2601], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3269, 0.2184, 0.2164, 0.2383], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3214, 0.2161, 0.1959, 0.2667], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2416, 0.2651, 0.259 , 0.2343], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.265 , 0.2457, 0.2505, 0.2388], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2712, 0.2335, 0.2311, 0.2642], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2894, 0.2328, 0.2344, 0.2434], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3203, 0.2234, 0.2116, 0.2447], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.291 , 0.2246, 0.2306, 0.2538], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3112, 0.2172, 0.2088, 0.2628], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2534, 0.2436, 0.2319, 0.2711], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2726, 0.2293, 0.2491, 0.249 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2551, 0.2388, 0.2595, 0.2466], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2392, 0.2532, 0.2817, 0.2259], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1947, 0.2944, 0.2822, 0.2287], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.199 , 0.2956, 0.2931, 0.2122], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2001, 0.2706, 0.276 , 0.2532], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2764, 0.2027, 0.2086, 0.3123], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2972, 0.2045, 0.2025, 0.2958], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2566, 0.1885, 0.1996, 0.3553], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2712, 0.2069, 0.2291, 0.2928], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2704, 0.224 , 0.2221, 0.2836], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2587, 0.2481, 0.2652, 0.228 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2769, 0.2455, 0.2438, 0.2338], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3188, 0.225 , 0.2039, 0.2523], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3038, 0.2067, 0.1941, 0.2953], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2722, 0.2377, 0.2192, 0.2708], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.185 , 0.2161, 0.2185, 0.3803], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2261, 0.2459, 0.2269, 0.3011], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2355, 0.238 , 0.2282, 0.2984], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3553, 0.1752, 0.1894, 0.2801], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2203, 0.2572, 0.2616, 0.261 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3625, 0.2   , 0.2001, 0.2374], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2564, 0.2497, 0.2496, 0.2443], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3088, 0.2   , 0.2076, 0.2836], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3098, 0.2216, 0.2141, 0.2545], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2601, 0.2458, 0.2379, 0.2562], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3657, 0.1916, 0.1743, 0.2684], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2822, 0.235 , 0.2297, 0.2531], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2822, 0.2283, 0.2366, 0.2529], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2745, 0.2381, 0.2484, 0.2391], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2451, 0.2678, 0.252 , 0.2351], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2562, 0.255 , 0.2395, 0.2493], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2541, 0.2546, 0.2459, 0.2453], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2433, 0.2479, 0.2487, 0.2601], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2245, 0.2764, 0.2606, 0.2385], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3057, 0.2261, 0.2148, 0.2533], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2519, 0.2508, 0.2444, 0.2529], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2606, 0.227 , 0.2374, 0.275 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2672, 0.2399, 0.2376, 0.2553], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.326 , 0.2265, 0.2137, 0.2339], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3184, 0.2025, 0.2042, 0.275 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3141, 0.2206, 0.2265, 0.2387], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3426, 0.1828, 0.1986, 0.2759], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3273, 0.1961, 0.2101, 0.2665], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2264, 0.27  , 0.2696, 0.234 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2374, 0.2598, 0.2488, 0.2539], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.208 , 0.2318, 0.2453, 0.3149], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2043, 0.2805, 0.273 , 0.2422], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2763, 0.2613, 0.2522, 0.2101], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2776, 0.2379, 0.2234, 0.2611], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3324, 0.1886, 0.1897, 0.2893], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2578, 0.255 , 0.2528, 0.2345], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3813, 0.1675, 0.1662, 0.285 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3577, 0.1778, 0.1969, 0.2676], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2383, 0.2574, 0.2621, 0.2422], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2728, 0.2496, 0.247 , 0.2306], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2597, 0.246 , 0.2549, 0.2394], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2899, 0.2384, 0.2354, 0.2364], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2788, 0.2053, 0.229 , 0.2869], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2877, 0.2164, 0.2394, 0.2566], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2862, 0.2281, 0.2574, 0.2283], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2953, 0.2222, 0.2101, 0.2723], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3019, 0.2323, 0.2172, 0.2486], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2616, 0.2344, 0.2497, 0.2542], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2625, 0.2442, 0.2436, 0.2497], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2924, 0.2119, 0.225 , 0.2707], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.276 , 0.2425, 0.2285, 0.253 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.257 , 0.2505, 0.2489, 0.2436], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2771, 0.2222, 0.2431, 0.2575], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2724, 0.2378, 0.2428, 0.2471], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2707, 0.249 , 0.2376, 0.2427], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2966, 0.229 , 0.2179, 0.2564], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2902, 0.2147, 0.2112, 0.284 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.324 , 0.2037, 0.1935, 0.2788], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3403, 0.1957, 0.1825, 0.2816], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.223 , 0.2604, 0.2556, 0.261 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2881, 0.2097, 0.2202, 0.282 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2678, 0.2241, 0.2262, 0.2819], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2863, 0.2381, 0.2465, 0.2291], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3002, 0.2388, 0.2265, 0.2344], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2765, 0.2251, 0.2426, 0.2559], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3956, 0.1526, 0.1673, 0.2845], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1878, 0.2862, 0.3022, 0.2238], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2408, 0.2143, 0.2577, 0.2872], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2816, 0.2551, 0.2615, 0.2018], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2423, 0.2573, 0.241 , 0.2595], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3167, 0.2183, 0.207 , 0.2581], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3219, 0.2254, 0.2079, 0.2449], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2681, 0.2229, 0.2481, 0.2609], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.268 , 0.2407, 0.2418, 0.2495], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2618, 0.236 , 0.2396, 0.2626], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2763, 0.2503, 0.2365, 0.237 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.236 , 0.2398, 0.2683, 0.2558], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2934, 0.2272, 0.222 , 0.2574], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.246 , 0.241 , 0.2443, 0.2686], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2564, 0.2386, 0.2426, 0.2625], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2563, 0.2272, 0.2266, 0.2899], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2453, 0.2369, 0.2447, 0.2731], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3385, 0.2545, 0.2065, 0.2005], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2908, 0.2206, 0.2455, 0.2432], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2573, 0.236 , 0.2426, 0.2642], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2952, 0.2189, 0.2194, 0.2666], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2382, 0.2682, 0.2598, 0.2337], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2169, 0.2798, 0.2688, 0.2345], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.23  , 0.2775, 0.2657, 0.2267], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4059, 0.1417, 0.1606, 0.2918], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3679, 0.1756, 0.1707, 0.2858], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2556, 0.2517, 0.2426, 0.2501], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2551, 0.2613, 0.2363, 0.2473], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4205, 0.1405, 0.1474, 0.2917], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4027, 0.1491, 0.1692, 0.279 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3324, 0.1709, 0.1725, 0.3242], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3343, 0.1738, 0.1696, 0.3223], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.365 , 0.1706, 0.1791, 0.2852], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2359, 0.259 , 0.2649, 0.2403], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2166, 0.2526, 0.2645, 0.2663], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.227 , 0.2624, 0.2643, 0.2463], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.257 , 0.2425, 0.2338, 0.2668], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2631, 0.242 , 0.2387, 0.2562], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3043, 0.2337, 0.221 , 0.2411], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2642, 0.2461, 0.2499, 0.2398], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.277 , 0.2497, 0.2383, 0.2351], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.259 , 0.2585, 0.2659, 0.2167], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3171, 0.217 , 0.2178, 0.2481], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.27  , 0.2437, 0.2332, 0.2531], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2854, 0.2389, 0.2392, 0.2365], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2656, 0.2269, 0.2324, 0.275 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2521, 0.2396, 0.2503, 0.258 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2745, 0.2308, 0.238 , 0.2567], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2662, 0.2375, 0.25  , 0.2463], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2765, 0.2433, 0.2507, 0.2295], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3134, 0.213 , 0.2147, 0.2588], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.278 , 0.2322, 0.2348, 0.255 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2063, 0.2662, 0.2774, 0.2501], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2537, 0.2547, 0.2525, 0.239 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2561, 0.2568, 0.2547, 0.2324], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2573, 0.2508, 0.2574, 0.2345], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2522, 0.2455, 0.2458, 0.2566], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2489, 0.2554, 0.2493, 0.2464], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2821, 0.2169, 0.2207, 0.2803], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2933, 0.2241, 0.2242, 0.2584], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3527, 0.176 , 0.1836, 0.2876], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2006, 0.279 , 0.2847, 0.2357], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.28  , 0.2206, 0.2176, 0.2818], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1992, 0.2572, 0.264 , 0.2795], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2026, 0.2767, 0.2976, 0.2231], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2352, 0.2684, 0.2667, 0.2297], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2226, 0.2749, 0.2686, 0.234 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2865, 0.2136, 0.1973, 0.3026], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3407, 0.2125, 0.2053, 0.2415], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3369, 0.1859, 0.179 , 0.2983], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4029, 0.1876, 0.1693, 0.2402], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2066, 0.2594, 0.2855, 0.2485], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1856, 0.266 , 0.3057, 0.2426], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2176, 0.2703, 0.2781, 0.234 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2648, 0.2487, 0.2391, 0.2474], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2426, 0.2769, 0.2419, 0.2386], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2822, 0.2503, 0.202 , 0.2655], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2146, 0.2358, 0.2334, 0.3161], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.625 , 0.1376, 0.0842, 0.1532], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2995, 0.2049, 0.2423, 0.2534], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2986, 0.2142, 0.2228, 0.2644], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2667, 0.2092, 0.2196, 0.3045], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2674, 0.2216, 0.2294, 0.2816], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2614, 0.2341, 0.2422, 0.2623], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2706, 0.2413, 0.2461, 0.2421], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2493, 0.2329, 0.2436, 0.2742], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3044, 0.2425, 0.2233, 0.2298], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3544, 0.2119, 0.193 , 0.2406], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3805, 0.22  , 0.1894, 0.2102], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2724, 0.2337, 0.2273, 0.2667], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2837, 0.1968, 0.2101, 0.3094], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2759, 0.2031, 0.2198, 0.3012], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2998, 0.2082, 0.2068, 0.2852], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2568, 0.2202, 0.2164, 0.3066], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2566, 0.2378, 0.2407, 0.2649], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2433, 0.2525, 0.2574, 0.2468], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2303, 0.2545, 0.2588, 0.2564], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2919, 0.2288, 0.2242, 0.2551], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2844, 0.2267, 0.2323, 0.2566], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2793, 0.2269, 0.2343, 0.2595], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2985, 0.236 , 0.2327, 0.2327], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2642, 0.2298, 0.2356, 0.2705], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2966, 0.2377, 0.2216, 0.2441], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2982, 0.2222, 0.2105, 0.2691], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.246 , 0.2525, 0.2571, 0.2444], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2558, 0.26  , 0.2538, 0.2304], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2718, 0.2147, 0.2225, 0.291 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2961, 0.2184, 0.2062, 0.2792], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3018, 0.2055, 0.2346, 0.258 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2991, 0.2082, 0.2271, 0.2656], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3405, 0.1989, 0.2017, 0.2588], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3157, 0.2216, 0.2092, 0.2536], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2872, 0.2136, 0.2121, 0.2872], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2114, 0.2456, 0.2563, 0.2867], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1986, 0.2532, 0.2736, 0.2747], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2099, 0.2653, 0.2812, 0.2435], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2746, 0.238 , 0.243 , 0.2444], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1958, 0.2168, 0.2551, 0.3323], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2703, 0.1978, 0.2079, 0.3239], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2553, 0.2194, 0.2333, 0.292 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2377, 0.233 , 0.2494, 0.2799], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2793, 0.2271, 0.2362, 0.2573], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2591, 0.2483, 0.2615, 0.2311], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2329, 0.2602, 0.2607, 0.2462], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2278, 0.2671, 0.2704, 0.2347], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3305, 0.2298, 0.1918, 0.2479], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3762, 0.1968, 0.189 , 0.2381], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2523, 0.2365, 0.2381, 0.273 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2598, 0.2593, 0.245 , 0.2359], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2749, 0.232 , 0.2336, 0.2595], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2504, 0.2446, 0.2508, 0.2542], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2663, 0.2477, 0.2446, 0.2414], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2883, 0.2389, 0.2249, 0.2478], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2411, 0.227 , 0.2443, 0.2876], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2896, 0.2284, 0.2259, 0.2561], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.278 , 0.2228, 0.2314, 0.2677], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3018, 0.2095, 0.2085, 0.2802], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2836, 0.2388, 0.2374, 0.2402], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3128, 0.2205, 0.2095, 0.2572], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3398, 0.2247, 0.2076, 0.2279], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3034, 0.1997, 0.2083, 0.2886], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2868, 0.2201, 0.2301, 0.263 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2597, 0.2458, 0.2412, 0.2533], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2699, 0.2457, 0.248 , 0.2364], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2809, 0.2313, 0.2361, 0.2517], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2853, 0.2346, 0.2344, 0.2456], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3192, 0.2236, 0.2167, 0.2405], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2968, 0.2027, 0.2104, 0.2901], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3103, 0.2059, 0.218 , 0.2658], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3473, 0.1881, 0.1921, 0.2724], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2421, 0.2536, 0.2676, 0.2367], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2293, 0.2511, 0.2664, 0.2533], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2537, 0.2429, 0.2624, 0.241 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2711, 0.2394, 0.2372, 0.2523], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2756, 0.2288, 0.2363, 0.2593], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2677, 0.2409, 0.2468, 0.2446], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2766, 0.2519, 0.2418, 0.2297], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3268, 0.204 , 0.2152, 0.254 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3629, 0.2042, 0.198 , 0.235 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3482, 0.1939, 0.1892, 0.2687], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3225, 0.2192, 0.2154, 0.2429], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2922, 0.2229, 0.2324, 0.2525], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2738, 0.2395, 0.2432, 0.2435], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3081, 0.235 , 0.2164, 0.2405], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1757, 0.2209, 0.2503, 0.3531], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1995, 0.2212, 0.2203, 0.3591], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1753, 0.2153, 0.2462, 0.3633], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2002, 0.2795, 0.2751, 0.2453], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2169, 0.2759, 0.2596, 0.2476], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2178, 0.2788, 0.262 , 0.2413], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2577, 0.2395, 0.2235, 0.2793], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2245, 0.2585, 0.2638, 0.2533], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2491, 0.2827, 0.2534, 0.2149], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3057, 0.2698, 0.2262, 0.1983], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1451, 0.2512, 0.3039, 0.2998], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1985, 0.2619, 0.2765, 0.2631], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2378, 0.2476, 0.2518, 0.2628], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2845, 0.2557, 0.24  , 0.2199], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2493, 0.2351, 0.2311, 0.2846], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3021, 0.2283, 0.2178, 0.2519], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3999, 0.1413, 0.1474, 0.3114], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4674, 0.113 , 0.1316, 0.288 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2968, 0.239 , 0.2177, 0.2464], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.147 , 0.1703, 0.2126, 0.4702], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2627, 0.2314, 0.224 , 0.2819], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2418, 0.2341, 0.2379, 0.2862], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2466, 0.2402, 0.2309, 0.2823], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2339, 0.2476, 0.2539, 0.2646], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2281, 0.2338, 0.2444, 0.2937], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2651, 0.2465, 0.2474, 0.241 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2474, 0.2426, 0.2643, 0.2457], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2377, 0.2529, 0.2602, 0.2491], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2402, 0.2568, 0.2642, 0.2388], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2744, 0.2412, 0.2435, 0.2409], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2266, 0.2732, 0.2803, 0.2199], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2799, 0.2733, 0.2413, 0.2055], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2602, 0.25  , 0.2334, 0.2564], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2432, 0.261 , 0.2628, 0.233 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.266 , 0.2573, 0.2361, 0.2406], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2155, 0.262 , 0.2695, 0.253 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2041, 0.2666, 0.2682, 0.2611], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2174, 0.28  , 0.2696, 0.233 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2266, 0.2341, 0.2449, 0.2943], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2336, 0.2506, 0.2408, 0.275 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3277, 0.213 , 0.2229, 0.2365], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3804, 0.1685, 0.1966, 0.2545], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.326 , 0.2058, 0.2029, 0.2654], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2468, 0.2526, 0.2551, 0.2455], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2793, 0.2151, 0.2158, 0.2898], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3561, 0.2054, 0.2068, 0.2318], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3004, 0.1999, 0.2042, 0.2955], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2489, 0.2589, 0.2428, 0.2493], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2611, 0.255 , 0.254 , 0.2299], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3862, 0.1572, 0.1501, 0.3064], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3393, 0.1771, 0.1757, 0.3079], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4108, 0.1557, 0.1549, 0.2786], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2686, 0.2388, 0.2401, 0.2524], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2519, 0.2454, 0.2544, 0.2483], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2939, 0.2359, 0.2314, 0.2388], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2734, 0.2436, 0.2411, 0.2419], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2623, 0.2555, 0.2571, 0.2251], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.242 , 0.2642, 0.2607, 0.233 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2377, 0.2457, 0.2647, 0.2519], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3639, 0.1654, 0.1713, 0.2994], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2192, 0.2452, 0.2604, 0.2752], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2398, 0.2589, 0.2542, 0.2471], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2412, 0.2521, 0.2616, 0.2451], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2401, 0.2415, 0.2533, 0.2651], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2335, 0.2585, 0.2634, 0.2447], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2276, 0.2521, 0.2644, 0.2559], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2708, 0.2346, 0.2436, 0.251 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2883, 0.2312, 0.2374, 0.2431], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2722, 0.2151, 0.2292, 0.2835], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.247 , 0.2578, 0.2664, 0.2288], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2548, 0.2591, 0.267 , 0.2191], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2444, 0.2461, 0.2618, 0.2477], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2458, 0.2494, 0.2599, 0.2449], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3264, 0.2028, 0.1965, 0.2743], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2038, 0.2663, 0.2787, 0.2512], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2355, 0.2684, 0.2723, 0.2239], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2709, 0.2361, 0.2319, 0.261 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3225, 0.2082, 0.1924, 0.277 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3627, 0.1734, 0.1685, 0.2954], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.317 , 0.157 , 0.2002, 0.3257], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2855, 0.1684, 0.2138, 0.3323], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3242, 0.1516, 0.1856, 0.3386], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.262 , 0.1374, 0.1732, 0.4273], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4229, 0.1433, 0.159 , 0.2748], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4999, 0.1102, 0.1033, 0.2866], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.619 , 0.1045, 0.0778, 0.1986], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2495, 0.2605, 0.2534, 0.2366], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3126, 0.2283, 0.2209, 0.2382], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3136, 0.1925, 0.1873, 0.3066], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3449, 0.1824, 0.195 , 0.2776], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3332, 0.1896, 0.1966, 0.2807], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3335, 0.2096, 0.2177, 0.2392], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3067, 0.1845, 0.2023, 0.3064], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2377, 0.252 , 0.2702, 0.2402], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2142, 0.2686, 0.2819, 0.2353], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.201 , 0.2623, 0.29  , 0.2467], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2113, 0.2608, 0.2814, 0.2465], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2091, 0.2685, 0.283 , 0.2394], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2014, 0.2654, 0.2841, 0.2491], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.199 , 0.2761, 0.3004, 0.2245], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2091, 0.2852, 0.2802, 0.2255], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4834, 0.1484, 0.1211, 0.2471], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2546, 0.2524, 0.2519, 0.2411], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.236 , 0.2551, 0.2678, 0.2411], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3587, 0.1563, 0.1805, 0.3045], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3373, 0.1603, 0.1987, 0.3037], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2828, 0.2249, 0.2451, 0.2472], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3186, 0.2114, 0.2184, 0.2516], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5358, 0.1492, 0.0953, 0.2197], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2613, 0.2444, 0.2439, 0.2503], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2569, 0.2413, 0.2592, 0.2426], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2437, 0.2383, 0.2492, 0.2688], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2468, 0.2545, 0.2561, 0.2427], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2508, 0.2452, 0.2458, 0.2583], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2879, 0.2259, 0.2217, 0.2646], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3697, 0.1675, 0.1709, 0.2918], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3053, 0.2095, 0.2229, 0.2623], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3009, 0.2089, 0.2292, 0.261 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2691, 0.2209, 0.2462, 0.2637], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2578, 0.2591, 0.2634, 0.2196], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3709, 0.1944, 0.1714, 0.2633], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2752, 0.233 , 0.2221, 0.2697], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.312 , 0.2268, 0.2168, 0.2444], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2973, 0.2215, 0.2274, 0.2538], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2774, 0.2387, 0.248 , 0.2359], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3012, 0.2433, 0.2397, 0.2159], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3878, 0.168 , 0.1846, 0.2595], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.354 , 0.1681, 0.2121, 0.2659], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2967, 0.2138, 0.2105, 0.279 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3102, 0.2107, 0.2195, 0.2595], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3251, 0.215 , 0.2129, 0.2471], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2863, 0.2232, 0.235 , 0.2555], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2595, 0.2422, 0.2508, 0.2475], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2537, 0.2425, 0.254 , 0.2497], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2749, 0.222 , 0.2288, 0.2744], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2252, 0.2591, 0.264 , 0.2518], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2686, 0.2486, 0.2496, 0.2332], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3294, 0.1868, 0.206 , 0.2777], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2319, 0.2248, 0.2534, 0.2899], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2176, 0.2224, 0.2463, 0.3137], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2467, 0.2388, 0.2457, 0.2688], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.325 , 0.1923, 0.2018, 0.2809], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2482, 0.2559, 0.2662, 0.2297], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3074, 0.2191, 0.2082, 0.2653], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2672, 0.2394, 0.2407, 0.2527], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2451, 0.24  , 0.2418, 0.2731], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2651, 0.2345, 0.2285, 0.2719], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4863, 0.0905, 0.1001, 0.3231], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2131, 0.2733, 0.2644, 0.2492], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2191, 0.2555, 0.2692, 0.2562], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.239 , 0.2446, 0.2544, 0.262 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2392, 0.2564, 0.2544, 0.25  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2041, 0.2765, 0.2697, 0.2497], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3066, 0.2268, 0.225 , 0.2416], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3192, 0.2074, 0.2061, 0.2673], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3033, 0.2002, 0.2249, 0.2717], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2903, 0.2279, 0.2294, 0.2524], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2773, 0.2318, 0.2195, 0.2715], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3162, 0.2286, 0.2245, 0.2307], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3105, 0.2069, 0.2007, 0.282 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2837, 0.2299, 0.231 , 0.2554], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.268 , 0.2252, 0.2505, 0.2563], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3024, 0.2202, 0.2304, 0.247 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3042, 0.214 , 0.2225, 0.2594], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2717, 0.2335, 0.245 , 0.2498], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3233, 0.2133, 0.2148, 0.2485], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2762, 0.2416, 0.2438, 0.2384], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2944, 0.2351, 0.2349, 0.2356], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2616, 0.2395, 0.237 , 0.2619], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2671, 0.2491, 0.246 , 0.2378], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3046, 0.2095, 0.2077, 0.2782], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2705, 0.2268, 0.2317, 0.2711], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3014, 0.2235, 0.2192, 0.2559], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2429, 0.2515, 0.2476, 0.258 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2292, 0.266 , 0.2667, 0.238 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2814, 0.2496, 0.2338, 0.2352], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2656, 0.2478, 0.2371, 0.2495], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3596, 0.2038, 0.1916, 0.2451], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2347, 0.2395, 0.2614, 0.2644], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1774, 0.2742, 0.3035, 0.245 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1995, 0.2718, 0.2892, 0.2395], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1989, 0.278 , 0.2898, 0.2333], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1955, 0.2859, 0.2894, 0.2291], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1723, 0.2871, 0.2978, 0.2428], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2472, 0.2442, 0.2672, 0.2414], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2535, 0.256 , 0.2593, 0.2313], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2638, 0.2439, 0.2446, 0.2477], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2548, 0.2291, 0.2425, 0.2736], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2582, 0.2434, 0.2446, 0.2538], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2673, 0.2535, 0.2463, 0.2329], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2612, 0.237 , 0.2443, 0.2575], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2686, 0.2461, 0.2369, 0.2484], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3059, 0.2043, 0.2122, 0.2776], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2719, 0.2352, 0.2295, 0.2634], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3429, 0.1934, 0.1827, 0.2809], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3202, 0.2139, 0.2196, 0.2463], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2504, 0.2351, 0.251 , 0.2634], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2872, 0.2256, 0.2214, 0.2657], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3083, 0.2132, 0.2013, 0.2773], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2448, 0.2516, 0.2598, 0.2437], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2676, 0.2577, 0.2445, 0.2301], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3011, 0.1961, 0.2148, 0.2881], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2768, 0.2508, 0.2298, 0.2426], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2017, 0.2756, 0.2729, 0.2498], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2199, 0.2778, 0.2647, 0.2376], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2072, 0.2716, 0.2696, 0.2516], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2851, 0.2302, 0.2354, 0.2492], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3551, 0.179 , 0.1792, 0.2867], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3359, 0.1897, 0.2194, 0.2551], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3168, 0.2235, 0.2215, 0.2382], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3417, 0.1933, 0.204 , 0.261 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3417, 0.1982, 0.216 , 0.2442], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2324, 0.2468, 0.2588, 0.262 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2578, 0.2594, 0.2413, 0.2415], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2108, 0.2771, 0.2786, 0.2335], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2056, 0.2787, 0.2741, 0.2416], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2129, 0.2826, 0.2723, 0.2322], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2136, 0.2746, 0.2747, 0.237 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2075, 0.2858, 0.276 , 0.2307], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2451, 0.2581, 0.2506, 0.2462], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2503, 0.2699, 0.2516, 0.2282], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3363, 0.1913, 0.1872, 0.2852], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4061, 0.178 , 0.1475, 0.2684], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3947, 0.1607, 0.157 , 0.2876], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3817, 0.1735, 0.1502, 0.2947], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3882, 0.1379, 0.1473, 0.3266], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.449 , 0.1363, 0.1571, 0.2576], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3219, 0.2258, 0.2159, 0.2365], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2793, 0.2364, 0.2413, 0.243 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2827, 0.2391, 0.2385, 0.2396], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3233, 0.2047, 0.2098, 0.2623], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3497, 0.1876, 0.2069, 0.2558], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2787, 0.222 , 0.2175, 0.2817], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2856, 0.2563, 0.2387, 0.2195], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3133, 0.203 , 0.2215, 0.2622], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3013, 0.2024, 0.2221, 0.2742], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3227, 0.2036, 0.2254, 0.2482], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2749, 0.2198, 0.2396, 0.2657], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2557, 0.2365, 0.2452, 0.2627], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2677, 0.2429, 0.2406, 0.2488], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2825, 0.2385, 0.2396, 0.2394], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2364, 0.2414, 0.2476, 0.2746], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2721, 0.232 , 0.2351, 0.2608], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4068, 0.1487, 0.1572, 0.2873], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2126, 0.2674, 0.2888, 0.2312], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.234 , 0.233 , 0.2789, 0.2541], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2221, 0.2135, 0.2466, 0.3179], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.257 , 0.2559, 0.2606, 0.2265], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2594, 0.2566, 0.2448, 0.2392], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.258 , 0.2467, 0.2692, 0.2261], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.211 , 0.2843, 0.2859, 0.2188], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2282, 0.2617, 0.2574, 0.2527], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2174, 0.2586, 0.2722, 0.2518], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2285, 0.2719, 0.2919, 0.2076], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2144, 0.2699, 0.2781, 0.2375], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2202, 0.2618, 0.277 , 0.241 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5014, 0.1774, 0.1465, 0.1747], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.296 , 0.2208, 0.2348, 0.2485], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2748, 0.2259, 0.2362, 0.2631], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2648, 0.2509, 0.248 , 0.2362], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3294, 0.2032, 0.2123, 0.255 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3291, 0.1942, 0.1989, 0.2777], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3361, 0.1858, 0.1976, 0.2805], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.268 , 0.2278, 0.2356, 0.2686], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2421, 0.2563, 0.2589, 0.2427], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2427, 0.2646, 0.243 , 0.2498], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2509, 0.2612, 0.2611, 0.2268], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.456 , 0.1199, 0.1355, 0.2886], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2813, 0.263 , 0.2427, 0.213 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2248, 0.2751, 0.2658, 0.2342], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2454, 0.2583, 0.2491, 0.2472], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2755, 0.2344, 0.237 , 0.2531], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3344, 0.1897, 0.217 , 0.259 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3235, 0.1968, 0.2321, 0.2476], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2675, 0.2397, 0.2467, 0.246 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3123, 0.2238, 0.2097, 0.2542], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2994, 0.2209, 0.2086, 0.2711], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2953, 0.2171, 0.2136, 0.2741], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3691, 0.1682, 0.1916, 0.271 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2427, 0.2625, 0.2456, 0.2492], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2576, 0.2567, 0.2412, 0.2445], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2753, 0.2313, 0.2252, 0.2682], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2123, 0.2741, 0.2738, 0.2398], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3022, 0.236 , 0.2342, 0.2276], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2782, 0.2376, 0.2419, 0.2424], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2896, 0.234 , 0.2233, 0.2532], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2462, 0.2591, 0.2497, 0.245 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3548, 0.188 , 0.1945, 0.2627], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.321 , 0.1988, 0.2162, 0.264 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.284 , 0.2372, 0.2305, 0.2483], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2254, 0.2732, 0.2695, 0.2319], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3346, 0.2065, 0.1884, 0.2705], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2956, 0.2054, 0.2257, 0.2732], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2771, 0.2443, 0.2414, 0.2373], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.253 , 0.2515, 0.2492, 0.2464], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2453, 0.2516, 0.2676, 0.2355], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2746, 0.2374, 0.243 , 0.2449], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2987, 0.2087, 0.2272, 0.2654], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2651, 0.2213, 0.2297, 0.2839], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3127, 0.2315, 0.2279, 0.2279], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2345, 0.2699, 0.2645, 0.2311], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2723, 0.2548, 0.2409, 0.232 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2447, 0.2713, 0.2566, 0.2274], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3607, 0.1728, 0.1894, 0.2771], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3767, 0.194 , 0.2029, 0.2263], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4118, 0.1952, 0.1696, 0.2235], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3827, 0.1934, 0.194 , 0.2298], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.332 , 0.2156, 0.2071, 0.2454], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2126, 0.2628, 0.2706, 0.2539], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2369, 0.2488, 0.2362, 0.2781], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3434, 0.1451, 0.1323, 0.3792], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3959, 0.1685, 0.1503, 0.2854], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2167, 0.2627, 0.2736, 0.247 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2177, 0.2607, 0.272 , 0.2496], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2407, 0.2436, 0.2545, 0.2612], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3247, 0.1573, 0.1955, 0.3226], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3335, 0.1772, 0.2087, 0.2807], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4272, 0.1693, 0.1723, 0.2312], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3418, 0.2287, 0.2076, 0.222 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4874, 0.1394, 0.1397, 0.2336], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1598, 0.294 , 0.3019, 0.2443], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2656, 0.2413, 0.2431, 0.25  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2884, 0.2239, 0.2325, 0.2552], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2991, 0.2   , 0.1951, 0.3059], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2987, 0.2261, 0.2237, 0.2514], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.199 , 0.2573, 0.2991, 0.2446], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2242, 0.2607, 0.2735, 0.2416], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2104, 0.2701, 0.2811, 0.2384], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2556, 0.2483, 0.2343, 0.2618], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2657, 0.2523, 0.2324, 0.2496], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.281 , 0.2387, 0.2455, 0.2347], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2769, 0.2327, 0.2195, 0.2708], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2794, 0.2225, 0.2216, 0.2765], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2588, 0.2466, 0.2543, 0.2403], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2628, 0.2576, 0.2433, 0.2362], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2838, 0.2431, 0.2304, 0.2427], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2812, 0.2449, 0.2418, 0.232 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3005, 0.2169, 0.2217, 0.2608], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.272 , 0.2401, 0.2338, 0.2541], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2398, 0.2644, 0.2567, 0.2392], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2601, 0.2263, 0.2535, 0.2601], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2426, 0.2372, 0.2512, 0.269 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2315, 0.2514, 0.2457, 0.2713], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2289, 0.262 , 0.2478, 0.2613], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.219 , 0.2566, 0.2538, 0.2705], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2516, 0.2548, 0.2154, 0.2782], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.245 , 0.2002, 0.1657, 0.3891], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3445, 0.1889, 0.2129, 0.2538], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2986, 0.1889, 0.2037, 0.3088], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4263, 0.15  , 0.1615, 0.2621], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3298, 0.1956, 0.2046, 0.27  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2645, 0.2633, 0.2355, 0.2366], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2366, 0.2631, 0.2622, 0.238 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2411, 0.2494, 0.2463, 0.2632], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3011, 0.2287, 0.2188, 0.2514], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2567, 0.239 , 0.2437, 0.2606], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2928, 0.2318, 0.2319, 0.2435], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.295 , 0.233 , 0.2279, 0.244 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2645, 0.2382, 0.2329, 0.2645], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3159, 0.1951, 0.2043, 0.2847], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2883, 0.2292, 0.2271, 0.2554], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2314, 0.2693, 0.286 , 0.2132], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2874, 0.2461, 0.2262, 0.2404], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2744, 0.2288, 0.2207, 0.2761], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.272 , 0.2294, 0.2303, 0.2683], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3043, 0.207 , 0.2099, 0.2788], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2783, 0.2382, 0.2324, 0.2511], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2403, 0.2782, 0.2725, 0.209 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2566, 0.2448, 0.2464, 0.2523], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.244 , 0.2632, 0.2657, 0.2271], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2216, 0.2112, 0.2002, 0.367 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2203, 0.205 , 0.1866, 0.3882], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2717, 0.2182, 0.2228, 0.2873], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1978, 0.2657, 0.2906, 0.2458], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.182 , 0.2774, 0.3052, 0.2353], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3393, 0.1772, 0.197 , 0.2865], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3732, 0.1775, 0.1644, 0.2849], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4267, 0.1421, 0.1615, 0.2696], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3337, 0.2062, 0.2224, 0.2377], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2632, 0.2338, 0.2525, 0.2506], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2462, 0.2459, 0.2656, 0.2424], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2439, 0.2672, 0.2619, 0.2269], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2875, 0.2246, 0.2107, 0.2772], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2513, 0.2621, 0.2428, 0.2439], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2966, 0.2212, 0.2176, 0.2647], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2624, 0.2356, 0.2495, 0.2524], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2842, 0.2506, 0.2382, 0.2271], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2933, 0.2248, 0.2266, 0.2553], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2719, 0.2487, 0.2332, 0.2462], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.232 , 0.2778, 0.2619, 0.2283], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3789, 0.1926, 0.1851, 0.2434], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3021, 0.2229, 0.2198, 0.2552], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3162, 0.2195, 0.2227, 0.2417], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2941, 0.2276, 0.2219, 0.2564], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4704, 0.1232, 0.1264, 0.2799], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3178, 0.2187, 0.2111, 0.2524], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2532, 0.2409, 0.2445, 0.2613], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2782, 0.2232, 0.2191, 0.2796], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2775, 0.2207, 0.2351, 0.2668], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.321 , 0.2247, 0.2158, 0.2385], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2979, 0.2045, 0.2376, 0.26  ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3002, 0.2348, 0.2193, 0.2457], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2611, 0.2352, 0.232 , 0.2717], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3378, 0.1943, 0.2109, 0.257 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4157, 0.1609, 0.1564, 0.267 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2421, 0.2558, 0.2669, 0.2352], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3295, 0.1981, 0.2168, 0.2556], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3383, 0.1907, 0.2074, 0.2636], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3133, 0.2126, 0.2181, 0.2561], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3284, 0.1863, 0.2039, 0.2814], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3014, 0.2299, 0.2333, 0.2354], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.285 , 0.2168, 0.2333, 0.265 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2761, 0.2015, 0.2137, 0.3087], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2095, 0.2694, 0.2772, 0.2439], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1901, 0.2739, 0.2821, 0.2538], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.242 , 0.2638, 0.2502, 0.244 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2536, 0.2522, 0.2581, 0.2361], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3288, 0.1949, 0.1906, 0.2857], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3148, 0.1951, 0.1985, 0.2916], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3842, 0.1697, 0.1771, 0.2689], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3416, 0.1729, 0.1872, 0.2983], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3156, 0.1882, 0.2059, 0.2903], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3046, 0.1934, 0.2079, 0.2941], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3966, 0.207 , 0.1849, 0.2115], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2357, 0.2812, 0.2654, 0.2177], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2136, 0.271 , 0.2745, 0.2409], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2213, 0.2777, 0.2706, 0.2304], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3041, 0.209 , 0.2175, 0.2694], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2964, 0.2409, 0.2313, 0.2313], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2894, 0.1967, 0.2181, 0.2958], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2867, 0.1903, 0.2258, 0.2971], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3069, 0.1939, 0.2065, 0.2927], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.244 , 0.2482, 0.248 , 0.2599], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1985, 0.2725, 0.2781, 0.2508], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1944, 0.2722, 0.2881, 0.2453], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1971, 0.2588, 0.2902, 0.2539], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1522, 0.2614, 0.3127, 0.2737], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1962, 0.2479, 0.2833, 0.2726], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2028, 0.2481, 0.2655, 0.2836], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1714, 0.2454, 0.31  , 0.2732], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2459, 0.2435, 0.2582, 0.2524], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2791, 0.2705, 0.2366, 0.2138], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1934, 0.2612, 0.2527, 0.2927], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1996, 0.2593, 0.2649, 0.2763], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2069, 0.2511, 0.2579, 0.2841], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2335, 0.2594, 0.2524, 0.2548], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2192, 0.2737, 0.2599, 0.2473], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2026, 0.2751, 0.2689, 0.2534], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2212, 0.2689, 0.2789, 0.231 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.281 , 0.251 , 0.2469, 0.2211], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2732, 0.2426, 0.2305, 0.2536], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2642, 0.2328, 0.236 , 0.267 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2947, 0.2205, 0.2113, 0.2735], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3306, 0.1974, 0.2052, 0.2669], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2886, 0.2074, 0.227 , 0.2771], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3071, 0.2066, 0.2134, 0.273 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3134, 0.2182, 0.2074, 0.261 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2739, 0.2222, 0.226 , 0.2779], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3242, 0.1971, 0.2081, 0.2706], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.32  , 0.2055, 0.2115, 0.263 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3125, 0.2165, 0.2199, 0.251 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3147, 0.2211, 0.2227, 0.2414], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3011, 0.2016, 0.2225, 0.2748], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.33  , 0.1983, 0.1992, 0.2725], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2708, 0.2424, 0.2418, 0.245 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2633, 0.2384, 0.2399, 0.2584], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2636, 0.2559, 0.2438, 0.2368], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2519, 0.2418, 0.2368, 0.2696], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2751, 0.2528, 0.2307, 0.2414], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3011, 0.2179, 0.2176, 0.2634], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2829, 0.2209, 0.219 , 0.2772], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3029, 0.2036, 0.2131, 0.2805], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3101, 0.1774, 0.2013, 0.3113], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3525, 0.1747, 0.2013, 0.2715], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3905, 0.1625, 0.1721, 0.2748], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3802, 0.1685, 0.1637, 0.2876], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2872, 0.1648, 0.2141, 0.3339], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2752, 0.1744, 0.2066, 0.3438], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3229, 0.1441, 0.1706, 0.3624], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3514, 0.1492, 0.1716, 0.3278], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3949, 0.1145, 0.1423, 0.3483], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.342 , 0.214 , 0.1845, 0.2595], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4465, 0.0962, 0.0692, 0.388 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4332, 0.0816, 0.0616, 0.4236], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2552, 0.2471, 0.2338, 0.2639], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3201, 0.1891, 0.1779, 0.3129], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3313, 0.1797, 0.1739, 0.3151], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3628, 0.1746, 0.1788, 0.2838], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4536, 0.1726, 0.156 , 0.2178], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3802, 0.1818, 0.1742, 0.2638], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2706, 0.2337, 0.2298, 0.2659], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3104, 0.2008, 0.1997, 0.2891], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2732, 0.2329, 0.2299, 0.264 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4614, 0.1014, 0.112 , 0.3252], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4661, 0.087 , 0.1012, 0.3456], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2336, 0.2449, 0.272 , 0.2495], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2275, 0.2583, 0.2697, 0.2445], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4224, 0.2024, 0.1722, 0.203 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4073, 0.1782, 0.1574, 0.2571], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3841, 0.1664, 0.1719, 0.2775], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2344, 0.2167, 0.2506, 0.2983], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2359, 0.2494, 0.2543, 0.2604], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2642, 0.2167, 0.2227, 0.2965], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3058, 0.2185, 0.2307, 0.245 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2968, 0.2324, 0.2149, 0.2559], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.334 , 0.2189, 0.2079, 0.2392], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2772, 0.2332, 0.2401, 0.2495], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2843, 0.1987, 0.2044, 0.3126], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2811, 0.2342, 0.2326, 0.2522], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2886, 0.2104, 0.2255, 0.2754], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2392, 0.2616, 0.2647, 0.2345], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2339, 0.2692, 0.2572, 0.2396], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3374, 0.2076, 0.2244, 0.2307], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3425, 0.1989, 0.1958, 0.2628], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3047, 0.2106, 0.222 , 0.2627], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2978, 0.2114, 0.2256, 0.2652], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3056, 0.2295, 0.2127, 0.2522], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.28  , 0.24  , 0.2316, 0.2484], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2947, 0.2164, 0.201 , 0.2879], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3586, 0.1573, 0.1628, 0.3213], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2958, 0.2366, 0.2291, 0.2385], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2999, 0.2191, 0.2208, 0.2602], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2862, 0.2123, 0.2207, 0.2809], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3753, 0.2118, 0.1828, 0.2301], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3442, 0.2229, 0.2132, 0.2197], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3746, 0.1699, 0.181 , 0.2745], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2299, 0.2764, 0.2671, 0.2266], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2227, 0.2742, 0.2733, 0.2298], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3157, 0.2095, 0.2323, 0.2426], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3182, 0.2023, 0.2299, 0.2496], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.349 , 0.2029, 0.1991, 0.2491], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3648, 0.1674, 0.1755, 0.2923], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2464, 0.2548, 0.2613, 0.2375], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2722, 0.2319, 0.2398, 0.2562], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2795, 0.2362, 0.238 , 0.2463], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.242 , 0.2438, 0.2582, 0.256 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2801, 0.2249, 0.2276, 0.2674], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3024, 0.2032, 0.1927, 0.3016], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2239, 0.2086, 0.2467, 0.3209], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1668, 0.2212, 0.2593, 0.3527], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2882, 0.2157, 0.2308, 0.2654], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2408, 0.2606, 0.2465, 0.2521], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2272, 0.2609, 0.2681, 0.2439], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2652, 0.2616, 0.2479, 0.2252], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2684, 0.245 , 0.2322, 0.2545], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.244 , 0.2449, 0.2605, 0.2506], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2635, 0.2502, 0.2392, 0.247 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2647, 0.2422, 0.2377, 0.2553], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2625, 0.2318, 0.237 , 0.2687], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2495, 0.2446, 0.2536, 0.2522], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2367, 0.2632, 0.265 , 0.2351], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2886, 0.2301, 0.237 , 0.2443], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3836, 0.1855, 0.1938, 0.2371], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3077, 0.2105, 0.2095, 0.2723], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3671, 0.1642, 0.1926, 0.2761], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2772, 0.2357, 0.237 , 0.2502], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2767, 0.2381, 0.2445, 0.2406], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2753, 0.2455, 0.2417, 0.2375], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2505, 0.2371, 0.2643, 0.2481], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2793, 0.2258, 0.2408, 0.2541], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2508, 0.2314, 0.2286, 0.2892], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2592, 0.2273, 0.2317, 0.2819], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2671, 0.2444, 0.2383, 0.2501], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2863, 0.2331, 0.2173, 0.2634], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2618, 0.2191, 0.2427, 0.2764], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3001, 0.2145, 0.223 , 0.2624], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.316 , 0.2278, 0.2105, 0.2456], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.293 , 0.2185, 0.2126, 0.2759], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2864, 0.2143, 0.2213, 0.2781], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4922, 0.1986, 0.1404, 0.1688], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3992, 0.1752, 0.1629, 0.2627], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4843, 0.1143, 0.1388, 0.2627], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3904, 0.1653, 0.1748, 0.2695], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4104, 0.1555, 0.1601, 0.274 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3485, 0.1638, 0.1789, 0.3089], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3385, 0.1865, 0.2086, 0.2665], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3434, 0.182 , 0.2144, 0.2602], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2519, 0.2261, 0.1983, 0.3237], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4404, 0.1231, 0.1241, 0.3124], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4607, 0.1111, 0.1067, 0.3215], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5853, 0.127 , 0.0997, 0.188 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.6306, 0.1162, 0.0791, 0.174 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5767, 0.12  , 0.1018, 0.2015], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2511, 0.2561, 0.2491, 0.2437], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3518, 0.2123, 0.2022, 0.2338], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3054, 0.2026, 0.2096, 0.2824], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3638, 0.1572, 0.1662, 0.3128], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2445, 0.2492, 0.2638, 0.2425], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2302, 0.2508, 0.2635, 0.2555], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2198, 0.2738, 0.2709, 0.2355], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2471, 0.2544, 0.2665, 0.232 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2325, 0.2705, 0.2769, 0.2201], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2201, 0.281 , 0.2663, 0.2326], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2368, 0.252 , 0.2545, 0.2566], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.233 , 0.2475, 0.2596, 0.2599], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2282, 0.2683, 0.2697, 0.2338], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2498, 0.2461, 0.24  , 0.2641], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2408, 0.2781, 0.2501, 0.231 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2348, 0.2573, 0.2667, 0.2411], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2047, 0.2815, 0.2782, 0.2356], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2148, 0.2802, 0.2762, 0.2288], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2432, 0.2685, 0.2556, 0.2328], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2517, 0.2687, 0.2656, 0.214 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2949, 0.2263, 0.2361, 0.2427], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.32  , 0.1841, 0.2003, 0.2955], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.281 , 0.2185, 0.2238, 0.2767], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3131, 0.2192, 0.2205, 0.2473], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3579, 0.1795, 0.1872, 0.2754], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3692, 0.1681, 0.1815, 0.2813], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2703, 0.2205, 0.2494, 0.2599], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2826, 0.2191, 0.2332, 0.2651], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2396, 0.2615, 0.2566, 0.2423], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2994, 0.2194, 0.2268, 0.2543], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2886, 0.2426, 0.2348, 0.234 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.306 , 0.25  , 0.2256, 0.2184], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4491, 0.1256, 0.1375, 0.2879], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2614, 0.2293, 0.234 , 0.2754], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3224, 0.2335, 0.2068, 0.2373], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2988, 0.2237, 0.2322, 0.2453], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2224, 0.2494, 0.2713, 0.2569], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2208, 0.2696, 0.2733, 0.2363], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2185, 0.2521, 0.2689, 0.2605], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2353, 0.2655, 0.2638, 0.2353], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2497, 0.2579, 0.2685, 0.2238], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2505, 0.2366, 0.2543, 0.2586], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2367, 0.2492, 0.253 , 0.2611], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.311 , 0.2074, 0.2064, 0.2752], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2548, 0.2164, 0.2395, 0.2893], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.275 , 0.2129, 0.2326, 0.2795], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3102, 0.2145, 0.2269, 0.2484], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4132, 0.1728, 0.1651, 0.2489], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3503, 0.1975, 0.2006, 0.2516], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2352, 0.2711, 0.2663, 0.2273], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2401, 0.2635, 0.263 , 0.2334], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2295, 0.2554, 0.2693, 0.2458], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2289, 0.2587, 0.2686, 0.2439], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2221, 0.2616, 0.2751, 0.2412], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2302, 0.2607, 0.2757, 0.2335], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2432, 0.2668, 0.2628, 0.2272], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3577, 0.1822, 0.1896, 0.2705], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3505, 0.1799, 0.1919, 0.2777], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.263 , 0.2371, 0.2473, 0.2526], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3498, 0.2   , 0.2086, 0.2416], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3478, 0.1786, 0.2024, 0.2711], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2019, 0.2764, 0.2861, 0.2357], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2279, 0.2582, 0.2825, 0.2314], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2166, 0.2825, 0.2784, 0.2224], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2204, 0.279 , 0.2704, 0.2302], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2278, 0.207 , 0.2442, 0.321 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1832, 0.201 , 0.2624, 0.3534], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3363, 0.1916, 0.2024, 0.2698], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3018, 0.189 , 0.1913, 0.3179], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2675, 0.1894, 0.2098, 0.3333], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3554, 0.1797, 0.1948, 0.2701], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.5273, 0.173 , 0.1253, 0.1743], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2548, 0.184 , 0.166 , 0.3951], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2092, 0.2579, 0.2695, 0.2634], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2274, 0.2509, 0.2494, 0.2723], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3605, 0.1654, 0.0906, 0.3835], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2656, 0.2407, 0.2429, 0.2508], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2823, 0.2401, 0.2387, 0.2389], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3607, 0.1672, 0.1908, 0.2812], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3296, 0.2156, 0.2004, 0.2543], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2059, 0.2769, 0.2899, 0.2273], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2385, 0.2218, 0.2451, 0.2947], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3583, 0.1871, 0.1905, 0.2641], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4768, 0.1921, 0.1449, 0.1862], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2382, 0.2458, 0.2623, 0.2537], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2617, 0.243 , 0.2472, 0.2481], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.204 , 0.2345, 0.2652, 0.2963], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2266, 0.2111, 0.2287, 0.3337], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2467, 0.245 , 0.2687, 0.2395], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2237, 0.2807, 0.2698, 0.2259], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2166, 0.2717, 0.2765, 0.2352], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2786, 0.2279, 0.204 , 0.2894], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2101, 0.277 , 0.2801, 0.2328], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2087, 0.2733, 0.2818, 0.2362], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1816, 0.2514, 0.3054, 0.2616], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2237, 0.2564, 0.2571, 0.2628], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2388, 0.2614, 0.2575, 0.2423], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.246 , 0.2263, 0.2265, 0.3012], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3139, 0.2196, 0.2251, 0.2415], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3256, 0.1963, 0.2052, 0.2729], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2857, 0.2247, 0.2307, 0.2588], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3844, 0.1591, 0.1692, 0.2873], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1729, 0.2056, 0.2526, 0.3688], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2653, 0.1922, 0.1918, 0.3507], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2839, 0.184 , 0.2004, 0.3317], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4473, 0.1823, 0.1587, 0.2118], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.474 , 0.2062, 0.1345, 0.1853], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3222, 0.219 , 0.2088, 0.25  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1965, 0.1779, 0.2127, 0.4129], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4105, 0.1507, 0.1411, 0.2978], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.5139, 0.1715, 0.133 , 0.1816], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.241 , 0.2521, 0.2716, 0.2353], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2881, 0.2287, 0.2377, 0.2455], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.301 , 0.2052, 0.2064, 0.2874], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3376, 0.1774, 0.1738, 0.3112], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4714, 0.1706, 0.1094, 0.2486], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4608, 0.1871, 0.1283, 0.2238], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2351, 0.1872, 0.2159, 0.3618], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3823, 0.158 , 0.1381, 0.3216], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1879, 0.2873, 0.2958, 0.2291], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2081, 0.249 , 0.2485, 0.2943], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2492, 0.2343, 0.2253, 0.2912], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.259 , 0.2344, 0.2112, 0.2954], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2499, 0.2128, 0.1997, 0.3377], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.271 , 0.2045, 0.1958, 0.3287], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2466, 0.2209, 0.1967, 0.3358], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3202, 0.225 , 0.1746, 0.2802], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1984, 0.2653, 0.2658, 0.2705], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2079, 0.2725, 0.2608, 0.2588], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2147, 0.2586, 0.2545, 0.2723], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2402, 0.2557, 0.2572, 0.2469], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2998, 0.2335, 0.2118, 0.2549], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2928, 0.2429, 0.236 , 0.2283], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3361, 0.197 , 0.186 , 0.2809], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.265 , 0.2674, 0.2497, 0.2179], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2534, 0.2589, 0.2593, 0.2283], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2334, 0.2552, 0.2597, 0.2518], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.329 , 0.2242, 0.1956, 0.2512], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3183, 0.1747, 0.1826, 0.3244], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3297, 0.1683, 0.1837, 0.3183], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3273, 0.1978, 0.2092, 0.2658], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3335, 0.2058, 0.2069, 0.2538], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4158, 0.1578, 0.1573, 0.2691], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4806, 0.1407, 0.1309, 0.2479], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3185, 0.2346, 0.2101, 0.2368], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3438, 0.2445, 0.1955, 0.2162], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3008, 0.2387, 0.1883, 0.2722], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.306 , 0.1585, 0.2114, 0.3241], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4245, 0.1402, 0.1405, 0.2948], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4471, 0.1484, 0.1366, 0.268 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4071, 0.1429, 0.1327, 0.3173], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3958, 0.1389, 0.1439, 0.3214], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4079, 0.1516, 0.1404, 0.3001], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3238, 0.1704, 0.1737, 0.3321], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3287, 0.1538, 0.1584, 0.3592], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3008, 0.1696, 0.1708, 0.3588], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3567, 0.1498, 0.1395, 0.3541], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2615, 0.2433, 0.2488, 0.2463], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2251, 0.2439, 0.259 , 0.2719], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2788, 0.2479, 0.2306, 0.2428], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2947, 0.245 , 0.2302, 0.2302], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2876, 0.2405, 0.2319, 0.2401], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2187, 0.237 , 0.2648, 0.2795], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4507, 0.1661, 0.1582, 0.225 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2188, 0.273 , 0.2856, 0.2226], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2554, 0.2483, 0.2513, 0.2449], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2558, 0.2624, 0.2537, 0.2281], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2572, 0.2477, 0.2476, 0.2474], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3457, 0.1857, 0.2033, 0.2652], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3001, 0.1995, 0.2058, 0.2945], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2836, 0.2081, 0.2034, 0.3049], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.298 , 0.226 , 0.2049, 0.2711], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2406, 0.2407, 0.2604, 0.2584], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2264, 0.2458, 0.2666, 0.2612], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3205, 0.2309, 0.2175, 0.2311], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3097, 0.192 , 0.2051, 0.2932], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3055, 0.2248, 0.2126, 0.2572], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2468, 0.2239, 0.2795, 0.2498], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2444, 0.2515, 0.2472, 0.2569], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2421, 0.2508, 0.253 , 0.2541], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2408, 0.2554, 0.2509, 0.2529], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.244 , 0.2555, 0.2632, 0.2373], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2913, 0.2341, 0.2255, 0.2491], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.308 , 0.2319, 0.2225, 0.2376], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2956, 0.2363, 0.2098, 0.2584], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2728, 0.2133, 0.2315, 0.2823], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2338, 0.2685, 0.2694, 0.2282], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2385, 0.2633, 0.2513, 0.2469], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2691, 0.2403, 0.2361, 0.2545], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2611, 0.2499, 0.2584, 0.2306], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3003, 0.1941, 0.2122, 0.2934], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2053, 0.2656, 0.2993, 0.2298], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2318, 0.2835, 0.2728, 0.212 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.22  , 0.2893, 0.2742, 0.2166], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2334, 0.2653, 0.2817, 0.2196], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1875, 0.2755, 0.2864, 0.2506], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.325 , 0.185 , 0.1878, 0.3022], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3256, 0.1851, 0.1924, 0.297 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2508, 0.2472, 0.2421, 0.26  ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3361, 0.2091, 0.2116, 0.2432], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2939, 0.1974, 0.2217, 0.287 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2761, 0.228 , 0.2358, 0.26  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3405, 0.199 , 0.2099, 0.2506], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2659, 0.2454, 0.2437, 0.245 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2784, 0.2372, 0.2373, 0.2471], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2495, 0.2382, 0.2521, 0.2602], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2817, 0.2345, 0.2361, 0.2477], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3428, 0.1904, 0.1961, 0.2707], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2789, 0.2429, 0.2387, 0.2395], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2769, 0.2347, 0.2408, 0.2476], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3394, 0.2119, 0.2055, 0.2432], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.317 , 0.2105, 0.1986, 0.2739], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2619, 0.2527, 0.2618, 0.2237], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.591 , 0.1276, 0.0877, 0.1937], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.5043, 0.142 , 0.1133, 0.2403], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2968, 0.2197, 0.23  , 0.2534], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2737, 0.2318, 0.2312, 0.2634], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3003, 0.2076, 0.2269, 0.2652], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3309, 0.1832, 0.1776, 0.3083], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2881, 0.2252, 0.2359, 0.2508], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3348, 0.2056, 0.2128, 0.2467], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3479, 0.1798, 0.181 , 0.2912], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3753, 0.1889, 0.1782, 0.2577], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.27  , 0.2449, 0.2326, 0.2525], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2928, 0.2106, 0.226 , 0.2706], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2169, 0.2776, 0.2837, 0.2217], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.327 , 0.1771, 0.1896, 0.3063], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2695, 0.2304, 0.2372, 0.263 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2459, 0.2449, 0.2564, 0.2528], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2418, 0.2524, 0.2555, 0.2503], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2732, 0.2342, 0.244 , 0.2487], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3025, 0.2461, 0.2252, 0.2261], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2592, 0.2376, 0.2411, 0.2621], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3183, 0.248 , 0.2161, 0.2175], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2625, 0.2321, 0.2458, 0.2596], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2697, 0.2467, 0.2396, 0.244 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.336 , 0.1635, 0.1842, 0.3162], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3072, 0.2079, 0.2252, 0.2597], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3348, 0.1956, 0.2103, 0.2592], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2598, 0.2341, 0.2509, 0.2553], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.259 , 0.2451, 0.2546, 0.2413], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.321 , 0.2135, 0.2041, 0.2614], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2406, 0.2245, 0.2515, 0.2833], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2074, 0.2431, 0.2721, 0.2774], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3645, 0.1927, 0.1915, 0.2512], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3386, 0.2089, 0.211 , 0.2415], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3266, 0.2185, 0.2146, 0.2402], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1833, 0.2685, 0.3003, 0.2479], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2325, 0.2386, 0.2572, 0.2717], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2187, 0.2605, 0.2779, 0.2429], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3275, 0.2065, 0.2091, 0.2569], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2985, 0.2252, 0.2248, 0.2515], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.522 , 0.152 , 0.1299, 0.1961], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2158, 0.2767, 0.2625, 0.245 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.216 , 0.2763, 0.2731, 0.2346], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2057, 0.2849, 0.2783, 0.2311], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2614, 0.2344, 0.2383, 0.266 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2162, 0.3095, 0.2797, 0.1947], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.328 , 0.1988, 0.1838, 0.2894], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3263, 0.215 , 0.2159, 0.2428], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.4952, 0.0852, 0.1019, 0.3177], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2435, 0.2517, 0.2601, 0.2447], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2323, 0.279 , 0.2761, 0.2125], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2506, 0.2481, 0.2696, 0.2317], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2227, 0.2601, 0.274 , 0.2432], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2886, 0.2164, 0.239 , 0.256 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2441, 0.2634, 0.2759, 0.2167], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2794, 0.2427, 0.2167, 0.2612], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2147, 0.2494, 0.2449, 0.291 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2222, 0.2407, 0.2471, 0.29  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2572, 0.2572, 0.2365, 0.2491], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2888, 0.2238, 0.2371, 0.2503], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2369, 0.2181, 0.2306, 0.3144], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2548, 0.2356, 0.2262, 0.2834], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3471, 0.2194, 0.1895, 0.2439], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3219, 0.1942, 0.1885, 0.2953], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.1946, 0.2967, 0.294 , 0.2148], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2725, 0.2294, 0.2348, 0.2633], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2431, 0.2468, 0.2613, 0.2488], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2214, 0.2673, 0.269 , 0.2422], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2118, 0.2691, 0.2753, 0.2438], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3188, 0.1994, 0.194 , 0.2877], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2714, 0.2381, 0.2522, 0.2382], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2057, 0.2755, 0.24  , 0.2788], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3263, 0.2802, 0.2024, 0.1911], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2427, 0.2609, 0.265 , 0.2315], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2288, 0.2559, 0.2701, 0.2452], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2187, 0.2671, 0.2823, 0.232 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2733, 0.2359, 0.2353, 0.2555], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2854, 0.2455, 0.2464, 0.2227], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2508, 0.2631, 0.2532, 0.2328], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2648, 0.2289, 0.2283, 0.2779], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2315, 0.279 , 0.2601, 0.2293], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2025, 0.2938, 0.2952, 0.2085], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2577, 0.2505, 0.2548, 0.237 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2051, 0.2725, 0.278 , 0.2444], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3783, 0.1625, 0.1827, 0.2764], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3241, 0.1942, 0.2121, 0.2696], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2874, 0.2348, 0.2217, 0.2561], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2863, 0.2262, 0.2326, 0.2548], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2794, 0.233 , 0.232 , 0.2557], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2667, 0.2335, 0.2516, 0.2482], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3273, 0.2049, 0.2001, 0.2677], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2857, 0.2293, 0.2281, 0.257 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3296, 0.1944, 0.1873, 0.2887], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.27  , 0.2312, 0.246 , 0.2529], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3138, 0.2192, 0.2133, 0.2537], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2428, 0.2399, 0.2439, 0.2734], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3261, 0.2319, 0.215 , 0.227 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2359, 0.2913, 0.2612, 0.2116], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2297, 0.2823, 0.2598, 0.2282], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2568, 0.2792, 0.2732, 0.1908], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2878, 0.2579, 0.2189, 0.2354], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4785, 0.2335, 0.1403, 0.1477], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2529, 0.2552, 0.2664, 0.2254], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2685, 0.2133, 0.2226, 0.2956], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2716, 0.2035, 0.2231, 0.3018], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2574, 0.2579, 0.2513, 0.2334], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3415, 0.18  , 0.1995, 0.279 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3049, 0.2006, 0.2448, 0.2497], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2961, 0.2325, 0.2232, 0.2482], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2645, 0.2507, 0.248 , 0.2368], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2854, 0.2388, 0.2409, 0.2349], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2447, 0.253 , 0.2637, 0.2385], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4497, 0.2178, 0.1357, 0.1968], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3695, 0.2273, 0.2122, 0.1909], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2551, 0.2387, 0.2369, 0.2692], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2815, 0.2154, 0.2338, 0.2693], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2655, 0.2171, 0.1857, 0.3316], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4339, 0.1191, 0.1189, 0.328 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.304 , 0.2163, 0.2297, 0.25  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2941, 0.2266, 0.236 , 0.2432], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2424, 0.2515, 0.2519, 0.2542], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2938, 0.2221, 0.2195, 0.2646], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2744, 0.2355, 0.2359, 0.2542], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2223, 0.2275, 0.2478, 0.3024], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2111, 0.2285, 0.259 , 0.3014], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2213, 0.2602, 0.2627, 0.2559], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2209, 0.2627, 0.2717, 0.2447], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2752, 0.2129, 0.249 , 0.2629], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2419, 0.2413, 0.2675, 0.2493], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.251 , 0.2681, 0.2546, 0.2263], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2692, 0.2402, 0.2375, 0.2531], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2672, 0.2426, 0.2378, 0.2524], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3014, 0.2209, 0.2348, 0.2429], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2398, 0.2668, 0.2546, 0.2388], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3805, 0.1694, 0.1899, 0.2602], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2161, 0.2696, 0.2707, 0.2436], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.202 , 0.2495, 0.2974, 0.2512], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.4914, 0.2116, 0.1306, 0.1664], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2151, 0.2797, 0.2796, 0.2256], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2174, 0.2869, 0.2766, 0.2191], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2186, 0.2657, 0.3015, 0.2142], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.202 , 0.2786, 0.2881, 0.2313], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2284, 0.2586, 0.2809, 0.2321], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2759, 0.231 , 0.2431, 0.25  ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2768, 0.2166, 0.2455, 0.2611], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3014, 0.2443, 0.2217, 0.2325], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.287 , 0.2075, 0.2458, 0.2597], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2856, 0.2041, 0.2446, 0.2657], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2355, 0.2426, 0.2642, 0.2577], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2761, 0.2187, 0.2378, 0.2674], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2902, 0.2131, 0.2456, 0.2511], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2485, 0.2251, 0.2304, 0.296 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2828, 0.237 , 0.2282, 0.2521], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3301, 0.1847, 0.2134, 0.2717], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3271, 0.1917, 0.2227, 0.2586], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3149, 0.2002, 0.1946, 0.2903], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3268, 0.1999, 0.205 , 0.2684], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3721, 0.17  , 0.1946, 0.2633], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.1969, 0.2908, 0.2908, 0.2215], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.1899, 0.2708, 0.3037, 0.2356], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2837, 0.2713, 0.248 , 0.1969], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2681, 0.2633, 0.2412, 0.2274], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2195, 0.2707, 0.2748, 0.235 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2855, 0.2263, 0.259 , 0.2292], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2758, 0.2081, 0.2616, 0.2545], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.254 , 0.2444, 0.2411, 0.2606], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2658, 0.2098, 0.2579, 0.2664], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2507, 0.2456, 0.2407, 0.263 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3125, 0.2375, 0.2129, 0.2371], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.269 , 0.2092, 0.2568, 0.265 ], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2646, 0.2345, 0.2513, 0.2497], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3098, 0.2129, 0.2109, 0.2664], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3331, 0.2012, 0.1873, 0.2785], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2923, 0.2184, 0.2567, 0.2327], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2999, 0.2372, 0.2234, 0.2395], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2714, 0.2258, 0.252 , 0.2508], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.336 , 0.2377, 0.2042, 0.2221], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.378 , 0.1766, 0.1785, 0.267 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.4828, 0.139 , 0.1401, 0.2381], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2397, 0.2579, 0.2691, 0.2333], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3197, 0.2094, 0.2145, 0.2564], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2721, 0.2171, 0.2506, 0.2602], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3051, 0.2234, 0.2121, 0.2593], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2659, 0.2364, 0.2432, 0.2545], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2626, 0.2337, 0.2284, 0.2752], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.6308, 0.1441, 0.0897, 0.1353], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2705, 0.2475, 0.2662, 0.2158], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2339, 0.263 , 0.2577, 0.2454], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2487, 0.2515, 0.2684, 0.2314], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3159, 0.2161, 0.2168, 0.2511], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2628, 0.2581, 0.2649, 0.2142], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2661, 0.2548, 0.2425, 0.2366], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2632, 0.254 , 0.2462, 0.2366], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2679, 0.2439, 0.2415, 0.2467], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2864, 0.2519, 0.2317, 0.23  ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2574, 0.2386, 0.2432, 0.2608], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2709, 0.2349, 0.2287, 0.2655], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2544, 0.247 , 0.2648, 0.2338], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3216, 0.2035, 0.2192, 0.2557], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3   , 0.1941, 0.21  , 0.2959], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.355 , 0.1815, 0.2151, 0.2483], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3587, 0.1753, 0.1977, 0.2683], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3582, 0.201 , 0.1866, 0.2542], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.3161, 0.1933, 0.2061, 0.2844], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2067, 0.2825, 0.2885, 0.2223], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2318, 0.2705, 0.2714, 0.2263], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2851, 0.239 , 0.2368, 0.2391], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2157, 0.2702, 0.2801, 0.234 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2795, 0.2471, 0.2499, 0.2235], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2412, 0.2593, 0.2664, 0.2331], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2751, 0.2324, 0.2327, 0.2597], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2683, 0.2314, 0.2396, 0.2608], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.347 , 0.1742, 0.1932, 0.2857], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2483, 0.2828, 0.2601, 0.2088], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2711, 0.2467, 0.2477, 0.2345], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3026, 0.2153, 0.213 , 0.2691], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3151, 0.2177, 0.2195, 0.2477], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.284 , 0.2216, 0.2297, 0.2648], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2971, 0.2128, 0.2202, 0.2699], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3029, 0.2069, 0.2273, 0.2629], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3294, 0.1978, 0.1921, 0.2808], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.329 , 0.2056, 0.2023, 0.2631], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2989, 0.2211, 0.2322, 0.2478], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3834, 0.1814, 0.1809, 0.2543], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2792, 0.2439, 0.2409, 0.236 ], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2465, 0.2414, 0.2423, 0.2698], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2572, 0.2428, 0.2467, 0.2533], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2582, 0.2472, 0.2458, 0.2488], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3533, 0.187 , 0.201 , 0.2588], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4277, 0.1375, 0.1428, 0.292 ], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.4918, 0.0984, 0.1087, 0.3011], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2105, 0.2684, 0.2746, 0.2466], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2953, 0.206 , 0.2168, 0.2818], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2128, 0.2621, 0.2603, 0.2647], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.316 , 0.2029, 0.2207, 0.2604], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3173, 0.2275, 0.2145, 0.2407], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2531, 0.2787, 0.2535, 0.2147], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2553, 0.2323, 0.2754, 0.237 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2568, 0.2458, 0.2421, 0.2554], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2979, 0.2044, 0.2015, 0.2963], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3118, 0.2213, 0.2119, 0.255 ], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.258 , 0.2228, 0.2487, 0.2705], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2202, 0.2577, 0.2512, 0.2708], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2326, 0.2738, 0.2536, 0.2401], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3289, 0.2256, 0.2116, 0.2339], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3346, 0.2191, 0.211 , 0.2353], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2886, 0.2389, 0.2207, 0.2517], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2284, 0.2482, 0.223 , 0.3003], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.2783, 0.2779, 0.2464, 0.1974], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.3053, 0.2054, 0.2266, 0.2627], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3224, 0.1934, 0.2064, 0.2778], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.307 , 0.2292, 0.2019, 0.2619], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2651, 0.2423, 0.2353, 0.2573], dtype=float32), array([0, 1, 0, 0], dtype=uint8))\n",
      "(array([0.3341, 0.1863, 0.2108, 0.2688], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2488, 0.2638, 0.2658, 0.2216], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.3321, 0.1889, 0.2196, 0.2593], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.2928, 0.2289, 0.2485, 0.2298], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.1556, 0.2465, 0.3125, 0.2854], dtype=float32), array([0, 0, 1, 0], dtype=uint8))\n",
      "(array([0.464 , 0.1875, 0.1491, 0.1994], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n",
      "(array([0.2178, 0.2187, 0.2444, 0.3191], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.2886, 0.2374, 0.2373, 0.2367], dtype=float32), array([0, 0, 0, 1], dtype=uint8))\n",
      "(array([0.314 , 0.2172, 0.2154, 0.2534], dtype=float32), array([1, 0, 0, 0], dtype=uint8))\n"
     ]
    }
   ],
   "source": [
    "bestModel = kerasModelResults.iloc[index][\"ModelPointer\"]\n",
    "#bestPredictions = bestModel.predict(x=xWinTest)\n",
    "bestPredictions = bestModel.predict(x=xVal)\n",
    "for n in zip(bestPredictions.round(4), yVal.values):\n",
    "    print(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.015439402671390037, pvalue=2.2053801427697247e-05)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.scatter(bestPredictions, yTest)\n",
    "plt.scatter(bestPredictions[:,0], yVal.iloc[:,0])\n",
    "#plt.scatter(bestPredictions[:,1], yVal[:,1])\n",
    "#plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "#pearsonr(bestPredictions[:,0], yTest[:,0])\n",
    "pearsonr(bestPredictions[:,0], yVal.iloc[:,0])\n",
    "#pearsonr(bestPredictions[:, 1], yVal[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resPath = \"C:\\\\Users\\RobinForMLThesis\\\\OneDrive - Hanken Svenska handelshogskolan\\\\Master's_Thesis\\\\DataAnalysis\\\\resultsPython.csv\"\n",
    "\n",
    "# finalResults = pd.DataFrame(MSEMatrix)\n",
    "# pd.DataFrame.to_csv(finalResults, resPath)\n",
    "# #pd.DataFrame.to_csv(finalResults, \"resultsPython.csv\")\n",
    "\n",
    "# MSEMatrix.tofile(resPath, sep = ',')\n",
    "\n",
    "# finalResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEMatrix.info(verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "if classification:\n",
    "    formattedMSEMatrix = MSEMatrix.drop([\"ModelPointer\"], axis=1)\n",
    "    formattedMSEMatrix = formattedMSEMatrix.sort_values(by=[\"accuracy\"], axis = 0, ascending=False)\n",
    "else: \n",
    "    formattedMSEMatrix = formattedMSEMatrix.sort_values(by=[\"mean_absolute_error\", \"mean_squared_error\"], axis = 0)\n",
    "\n",
    "\n",
    "def RobRound(x): \n",
    "    if(x==None or math.isnan(x)): return None\n",
    "    else: return np.format_float_positional(x, precision=2, unique=False, fractional=False, trim='-', min_digits=None)\n",
    "\n",
    "def RobRoundFrac(x): \n",
    "    if(x==None or math.isnan(x)): return None\n",
    "    else: return np.format_float_positional(x, precision=3, unique=False, fractional=True, min_digits=None)\n",
    "\n",
    "def RobRoundTime(x): \n",
    "    if(x==None or math.isnan(x)): return None\n",
    "    else: return np.format_float_positional(x, precision=2, unique=False, fractional=True, min_digits=None)\n",
    "\n",
    "\n",
    "formattedMSEMatrix[\"L1\"] = formattedMSEMatrix[\"L1\"].apply(lambda x: RobRound(x))\n",
    "formattedMSEMatrix[\"L2\"] = formattedMSEMatrix[\"L2\"].apply(lambda x: RobRound(x))\n",
    "formattedMSEMatrix[\"loss\"] = formattedMSEMatrix[\"loss\"].apply(lambda x: RobRoundFrac(x))\n",
    "formattedMSEMatrix[\"Learning rate\"] = formattedMSEMatrix[\"Learning rate\"].apply(lambda x: RobRound(x))\n",
    "formattedMSEMatrix[\"Training Time (minutes)\"] = formattedMSEMatrix[\"Training Time (minutes)\"].apply(lambda x: RobRoundTime(float(x)))\n",
    "\n",
    "\n",
    "if classification:\n",
    "    formattedMSEMatrix[\"accuracy\"] = formattedMSEMatrix[\"accuracy\"].apply(lambda x: RobRoundFrac(x))\n",
    "    formattedMSEMatrix[\"categorical_accuracy\"] = formattedMSEMatrix[\"categorical_accuracy\"].apply(lambda x: RobRoundFrac(x))\n",
    "    formattedMSEMatrix[\"mean_directional_accuracy\"] = formattedMSEMatrix[\"mean_directional_accuracy\"].apply(lambda x: RobRoundFrac(x))\n",
    "else:\n",
    "    formattedMSEMatrix[\"mean_squared_error\"] = formattedMSEMatrix[\"mean_squared_error\"].apply(lambda x: RobRoundFrac(x))\n",
    "    formattedMSEMatrix[\"mean_absolute_error\"] = formattedMSEMatrix[\"mean_absolute_error\"].apply(lambda x: RobRoundFrac(x))\n",
    "\n",
    "\n",
    "#formattedMSEMatrix = formattedMSEMatrix.fillna(value=np.nan)\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime(\"%d-%m-%Y_%H%M\")\n",
    "# modelComparisonPathHTML = f\"Results//{target}//ModelComparison_\" + str(now) + \".html\"\n",
    "# modelComparisonPathCSV = f\"Results//{target}//ModelComparison_\" + str(now) + \".csv\"\n",
    "modelComparisonPathHTML = f\"Results//{target}/{dataSample}/ModelComparison\" + \".html\"\n",
    "modelComparisonPathCSV = f\"Results//{target}/{dataSample}/ModelComparison\" + \".csv\"\n",
    "\n",
    "#pd.DataFrame.to_csv(MSEMatrix, \"Results/ModelComparison.csv\", index=False)\n",
    "#pd.DataFrame.to_excel(MSEMatrix, \"Results/ModelComparison.xlsx\")\n",
    "pd.DataFrame.to_html(formattedMSEMatrix, modelComparisonPathHTML, index=False)\n",
    "pd.DataFrame.to_csv(formattedMSEMatrix, modelComparisonPathCSV, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSEMatrixContinuation = pd.read_csv(modelComparisonPathCSV)\n",
    "\n",
    "\n",
    "# import os\n",
    "# rootdir = 'C:/Users/sid/Desktop/test'\n",
    "\n",
    "# for subdir, dirs, files in os.walk(rootdir):\n",
    "#     for file in files:\n",
    "#         print(os.path.join(subdir, file))\n",
    "\n",
    "models = []\n",
    "for i in range(5):\n",
    "    modelPath = f\"Results/{target}/{dataSample}/BestKerasModel\" + str(i + 1)\n",
    "    if classification:\n",
    "        bModel = tf.keras.models.load_model(modelPath, custom_objects={\"MDA\": MDA})\n",
    "    else:\n",
    "        bModel = tf.keras.models.load_model(modelPath)\n",
    "    models = models + [bModel]\n",
    "\n",
    "# modelPath = f\"Results/{target}/KerasLinearModel\"\n",
    "# OLSModel = tf.keras.models.load_model(modelPath)\n",
    "# models = models + [OLSModel]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.382115364074707, 'accuracy': 0.3215031325817108, 'categorical_accuracy': 0.3215031325817108, 'mean_directional_accuracy': 0.557411253452301} on model 1 variable 1\n",
      "{'loss': 1.3818104267120361, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 2\n",
      "{'loss': 1.3815374374389648, 'accuracy': 0.3252870440483093, 'categorical_accuracy': 0.3252870440483093, 'mean_directional_accuracy': 0.5587160587310791} on model 1 variable 3\n",
      "{'loss': 1.382304072380066, 'accuracy': 0.32163360714912415, 'categorical_accuracy': 0.32163360714912415, 'mean_directional_accuracy': 0.5576722621917725} on model 1 variable 4\n",
      "{'loss': 1.3818154335021973, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5583246350288391} on model 1 variable 5\n",
      "{'loss': 1.3817178010940552, 'accuracy': 0.3233298659324646, 'categorical_accuracy': 0.3233298659324646, 'mean_directional_accuracy': 0.5602818131446838} on model 1 variable 6\n",
      "{'loss': 1.3818825483322144, 'accuracy': 0.3228079378604889, 'categorical_accuracy': 0.3228079378604889, 'mean_directional_accuracy': 0.5588465332984924} on model 1 variable 7\n",
      "{'loss': 1.3818117380142212, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5585855841636658} on model 1 variable 8\n",
      "{'loss': 1.381812572479248, 'accuracy': 0.3229384124279022, 'categorical_accuracy': 0.3229384124279022, 'mean_directional_accuracy': 0.5579332113265991} on model 1 variable 9\n",
      "{'loss': 1.3818559646606445, 'accuracy': 0.32254695892333984, 'categorical_accuracy': 0.32254695892333984, 'mean_directional_accuracy': 0.557411253452301} on model 1 variable 10\n",
      "{'loss': 1.381791353225708, 'accuracy': 0.32450416684150696, 'categorical_accuracy': 0.32450416684150696, 'mean_directional_accuracy': 0.5588465332984924} on model 1 variable 11\n",
      "{'loss': 1.3818069696426392, 'accuracy': 0.3233298659324646, 'categorical_accuracy': 0.3233298659324646, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 12\n",
      "{'loss': 1.3819491863250732, 'accuracy': 0.3229384124279022, 'categorical_accuracy': 0.3229384124279022, 'mean_directional_accuracy': 0.5585855841636658} on model 1 variable 13\n",
      "{'loss': 1.3817940950393677, 'accuracy': 0.32346034049987793, 'categorical_accuracy': 0.32346034049987793, 'mean_directional_accuracy': 0.5583246350288391} on model 1 variable 14\n",
      "{'loss': 1.3824381828308105, 'accuracy': 0.32163360714912415, 'categorical_accuracy': 0.32163360714912415, 'mean_directional_accuracy': 0.5542797446250916} on model 1 variable 15\n",
      "{'loss': 1.3818066120147705, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 16\n",
      "{'loss': 1.3818999528884888, 'accuracy': 0.3195459246635437, 'categorical_accuracy': 0.3195459246635437, 'mean_directional_accuracy': 0.557411253452301} on model 1 variable 17\n",
      "{'loss': 1.3818254470825195, 'accuracy': 0.3237212896347046, 'categorical_accuracy': 0.3237212896347046, 'mean_directional_accuracy': 0.5593684911727905} on model 1 variable 18\n",
      "{'loss': 1.3818120956420898, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5579332113265991} on model 1 variable 19\n",
      "{'loss': 1.3817987442016602, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5575417280197144} on model 1 variable 20\n",
      "{'loss': 1.3818159103393555, 'accuracy': 0.3233298659324646, 'categorical_accuracy': 0.3233298659324646, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 21\n",
      "{'loss': 1.3821779489517212, 'accuracy': 0.32124218344688416, 'categorical_accuracy': 0.32124218344688416, 'mean_directional_accuracy': 0.5534968972206116} on model 1 variable 22\n",
      "{'loss': 1.382215142250061, 'accuracy': 0.3208507299423218, 'categorical_accuracy': 0.3208507299423218, 'mean_directional_accuracy': 0.5571503043174744} on model 1 variable 23\n",
      "{'loss': 1.3818612098693848, 'accuracy': 0.3238517642021179, 'categorical_accuracy': 0.3238517642021179, 'mean_directional_accuracy': 0.5593684911727905} on model 1 variable 24\n",
      "{'loss': 1.3817976713180542, 'accuracy': 0.32267746329307556, 'categorical_accuracy': 0.32267746329307556, 'mean_directional_accuracy': 0.5575417280197144} on model 1 variable 25\n",
      "{'loss': 1.3818150758743286, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 26\n",
      "{'loss': 1.3818045854568481, 'accuracy': 0.32346034049987793, 'categorical_accuracy': 0.32346034049987793, 'mean_directional_accuracy': 0.5587160587310791} on model 1 variable 27\n",
      "{'loss': 1.3818126916885376, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5585855841636658} on model 1 variable 28\n",
      "{'loss': 1.3818155527114868, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5584551095962524} on model 1 variable 29\n",
      "{'loss': 1.3818143606185913, 'accuracy': 0.3233298659324646, 'categorical_accuracy': 0.3233298659324646, 'mean_directional_accuracy': 0.5580636858940125} on model 1 variable 30\n",
      "{'loss': 1.381845235824585, 'accuracy': 0.32346034049987793, 'categorical_accuracy': 0.32346034049987793, 'mean_directional_accuracy': 0.5576722621917725} on model 1 variable 31\n",
      "{'loss': 1.3818142414093018, 'accuracy': 0.3222860097885132, 'categorical_accuracy': 0.3222860097885132, 'mean_directional_accuracy': 0.5578027367591858} on model 1 variable 32\n",
      "{'loss': 1.3818129301071167, 'accuracy': 0.32306888699531555, 'categorical_accuracy': 0.32306888699531555, 'mean_directional_accuracy': 0.5580636858940125} on model 1 variable 33\n",
      "{'loss': 1.3818916082382202, 'accuracy': 0.32267746329307556, 'categorical_accuracy': 0.32267746329307556, 'mean_directional_accuracy': 0.5585855841636658} on model 1 variable 34\n",
      "{'loss': 1.3818126916885376, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5583246350288391} on model 1 variable 35\n",
      "{'loss': 1.3818087577819824, 'accuracy': 0.32359081506729126, 'categorical_accuracy': 0.32359081506729126, 'mean_directional_accuracy': 0.5580636858940125} on model 1 variable 36\n",
      "{'loss': 1.3818129301071167, 'accuracy': 0.32346034049987793, 'categorical_accuracy': 0.32346034049987793, 'mean_directional_accuracy': 0.5583246350288391} on model 1 variable 37\n",
      "{'loss': 1.3827762603759766, 'accuracy': 0.3237212896347046, 'categorical_accuracy': 0.3237212896347046, 'mean_directional_accuracy': 0.5584551095962524} on model 1 variable 38\n",
      "{'loss': 1.382627010345459, 'accuracy': 0.3228079378604889, 'categorical_accuracy': 0.3228079378604889, 'mean_directional_accuracy': 0.5544102191925049} on model 1 variable 39\n",
      "{'loss': 1.3818213939666748, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5580636858940125} on model 1 variable 40\n",
      "{'loss': 1.381954550743103, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5604122877120972} on model 1 variable 41\n",
      "{'loss': 1.3822407722473145, 'accuracy': 0.3204592764377594, 'categorical_accuracy': 0.3204592764377594, 'mean_directional_accuracy': 0.554932177066803} on model 1 variable 42\n",
      "{'loss': 1.3836365938186646, 'accuracy': 0.31928497552871704, 'categorical_accuracy': 0.31928497552871704, 'mean_directional_accuracy': 0.5571503043174744} on model 1 variable 43\n",
      "{'loss': 1.3818137645721436, 'accuracy': 0.32346034049987793, 'categorical_accuracy': 0.32346034049987793, 'mean_directional_accuracy': 0.5587160587310791} on model 1 variable 44\n",
      "{'loss': 1.382723331451416, 'accuracy': 0.3237212896347046, 'categorical_accuracy': 0.3237212896347046, 'mean_directional_accuracy': 0.5576722621917725} on model 1 variable 45\n",
      "{'loss': 1.3819003105163574, 'accuracy': 0.32411274313926697, 'categorical_accuracy': 0.32411274313926697, 'mean_directional_accuracy': 0.5602818131446838} on model 1 variable 46\n",
      "{'loss': 1.3817951679229736, 'accuracy': 0.3238517642021179, 'categorical_accuracy': 0.3238517642021179, 'mean_directional_accuracy': 0.5641962289810181} on model 1 variable 47\n",
      "{'loss': 1.3818281888961792, 'accuracy': 0.3233298659324646, 'categorical_accuracy': 0.3233298659324646, 'mean_directional_accuracy': 0.5578027367591858} on model 1 variable 48\n",
      "{'loss': 1.381839632987976, 'accuracy': 0.3238517642021179, 'categorical_accuracy': 0.3238517642021179, 'mean_directional_accuracy': 0.5596294403076172} on model 1 variable 49\n",
      "{'loss': 1.3818509578704834, 'accuracy': 0.32359081506729126, 'categorical_accuracy': 0.32359081506729126, 'mean_directional_accuracy': 0.5589770078659058} on model 1 variable 50\n",
      "{'loss': 1.3820947408676147, 'accuracy': 0.3233298659324646, 'categorical_accuracy': 0.3233298659324646, 'mean_directional_accuracy': 0.556106448173523} on model 1 variable 51\n",
      "{'loss': 1.3822059631347656, 'accuracy': 0.3205897808074951, 'categorical_accuracy': 0.3205897808074951, 'mean_directional_accuracy': 0.5534968972206116} on model 1 variable 52\n",
      "{'loss': 1.381801962852478, 'accuracy': 0.3233298659324646, 'categorical_accuracy': 0.3233298659324646, 'mean_directional_accuracy': 0.5583246350288391} on model 1 variable 53\n",
      "{'loss': 1.3858134746551514, 'accuracy': 0.3228079378604889, 'categorical_accuracy': 0.3228079378604889, 'mean_directional_accuracy': 0.56289142370224} on model 1 variable 54\n",
      "{'loss': 1.384811520576477, 'accuracy': 0.3130219280719757, 'categorical_accuracy': 0.3130219280719757, 'mean_directional_accuracy': 0.5450156331062317} on model 1 variable 55\n",
      "{'loss': 1.381903052330017, 'accuracy': 0.32306888699531555, 'categorical_accuracy': 0.32306888699531555, 'mean_directional_accuracy': 0.5596294403076172} on model 1 variable 56\n",
      "{'loss': 1.3829160928726196, 'accuracy': 0.32032880187034607, 'categorical_accuracy': 0.32032880187034607, 'mean_directional_accuracy': 0.5519310832023621} on model 1 variable 57\n",
      "{'loss': 1.3817716836929321, 'accuracy': 0.3242432177066803, 'categorical_accuracy': 0.3242432177066803, 'mean_directional_accuracy': 0.5601513385772705} on model 1 variable 58\n",
      "{'loss': 1.3818151950836182, 'accuracy': 0.3233298659324646, 'categorical_accuracy': 0.3233298659324646, 'mean_directional_accuracy': 0.5579332113265991} on model 1 variable 59\n",
      "{'loss': 1.381811499595642, 'accuracy': 0.32359081506729126, 'categorical_accuracy': 0.32359081506729126, 'mean_directional_accuracy': 0.5583246350288391} on model 1 variable 60\n",
      "{'loss': 1.3818107843399048, 'accuracy': 0.32346034049987793, 'categorical_accuracy': 0.32346034049987793, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 61\n",
      "{'loss': 1.3818145990371704, 'accuracy': 0.32346034049987793, 'categorical_accuracy': 0.32346034049987793, 'mean_directional_accuracy': 0.5585855841636658} on model 1 variable 62\n",
      "{'loss': 1.3816947937011719, 'accuracy': 0.3264613747596741, 'categorical_accuracy': 0.3264613747596741, 'mean_directional_accuracy': 0.5606732964515686} on model 1 variable 63\n",
      "{'loss': 1.3818132877349854, 'accuracy': 0.3237212896347046, 'categorical_accuracy': 0.3237212896347046, 'mean_directional_accuracy': 0.5583246350288391} on model 1 variable 64\n",
      "{'loss': 1.3817938566207886, 'accuracy': 0.32346034049987793, 'categorical_accuracy': 0.32346034049987793, 'mean_directional_accuracy': 0.5583246350288391} on model 1 variable 65\n",
      "{'loss': 1.3816850185394287, 'accuracy': 0.32346034049987793, 'categorical_accuracy': 0.32346034049987793, 'mean_directional_accuracy': 0.5587160587310791} on model 1 variable 66\n",
      "{'loss': 1.3817675113677979, 'accuracy': 0.3237212896347046, 'categorical_accuracy': 0.3237212896347046, 'mean_directional_accuracy': 0.5591075420379639} on model 1 variable 67\n",
      "{'loss': 1.381831407546997, 'accuracy': 0.3228079378604889, 'categorical_accuracy': 0.3228079378604889, 'mean_directional_accuracy': 0.5580636858940125} on model 1 variable 68\n",
      "{'loss': 1.3821628093719482, 'accuracy': 0.32489562034606934, 'categorical_accuracy': 0.32489562034606934, 'mean_directional_accuracy': 0.5580636858940125} on model 1 variable 69\n",
      "{'loss': 1.3818107843399048, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5583246350288391} on model 1 variable 70\n",
      "{'loss': 1.3818148374557495, 'accuracy': 0.3237212896347046, 'categorical_accuracy': 0.3237212896347046, 'mean_directional_accuracy': 0.5584551095962524} on model 1 variable 71\n",
      "{'loss': 1.3819198608398438, 'accuracy': 0.32267746329307556, 'categorical_accuracy': 0.32267746329307556, 'mean_directional_accuracy': 0.5575417280197144} on model 1 variable 72\n",
      "{'loss': 1.3820672035217285, 'accuracy': 0.3215031325817108, 'categorical_accuracy': 0.3215031325817108, 'mean_directional_accuracy': 0.5521920919418335} on model 1 variable 73\n",
      "{'loss': 1.3817996978759766, 'accuracy': 0.3237212896347046, 'categorical_accuracy': 0.3237212896347046, 'mean_directional_accuracy': 0.5589770078659058} on model 1 variable 74\n",
      "{'loss': 1.3818135261535645, 'accuracy': 0.3233298659324646, 'categorical_accuracy': 0.3233298659324646, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 75\n",
      "{'loss': 1.3818830251693726, 'accuracy': 0.32489562034606934, 'categorical_accuracy': 0.32489562034606934, 'mean_directional_accuracy': 0.5598903894424438} on model 1 variable 76\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 77\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 78\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 79\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 80\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 81\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 82\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 83\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 84\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 85\n",
      "{'loss': 1.3818107843399048, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 86\n",
      "{'loss': 1.3812144994735718, 'accuracy': 0.32437369227409363, 'categorical_accuracy': 0.32437369227409363, 'mean_directional_accuracy': 0.5600208640098572} on model 1 variable 87\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 88\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 89\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 90\n",
      "{'loss': 1.3818104267120361, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 91\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 92\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 93\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 94\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 95\n",
      "{'loss': 1.3818105459213257, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 96\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 97\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 98\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 99\n",
      "{'loss': 1.3818105459213257, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 100\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 101\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 102\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 103\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 104\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 105\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 106\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 107\n",
      "{'loss': 1.3818107843399048, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 108\n",
      "{'loss': 1.3818105459213257, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 109\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 110\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 111\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 112\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 113\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 114\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 115\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 116\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 117\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 118\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 119\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 120\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 121\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 122\n",
      "{'loss': 1.3818105459213257, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 123\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 124\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 125\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 126\n",
      "{'loss': 1.3818105459213257, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 127\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 128\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 129\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5580636858940125} on model 1 variable 130\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 131\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 132\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 133\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 134\n",
      "{'loss': 1.3818107843399048, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 135\n",
      "{'loss': 1.3818118572235107, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 136\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 137\n",
      "{'loss': 1.381809949874878, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 138\n",
      "{'loss': 1.381810188293457, 'accuracy': 0.3233298659324646, 'categorical_accuracy': 0.3233298659324646, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 139\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 140\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 141\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 142\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 143\n",
      "{'loss': 1.3818109035491943, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 144\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 145\n",
      "{'loss': 1.3818103075027466, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 146\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 147\n",
      "{'loss': 1.3818095922470093, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5580636858940125} on model 1 variable 148\n",
      "{'loss': 1.3818086385726929, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 149\n",
      "{'loss': 1.3818104267120361, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 150\n",
      "{'loss': 1.3818104267120361, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 151\n",
      "{'loss': 1.3818105459213257, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 152\n",
      "{'loss': 1.3818107843399048, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 153\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 154\n",
      "{'loss': 1.3818100690841675, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 155\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 156\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 157\n",
      "{'loss': 1.3818105459213257, 'accuracy': 0.3233298659324646, 'categorical_accuracy': 0.3233298659324646, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 158\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 159\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 160\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 161\n",
      "{'loss': 1.3818107843399048, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 162\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 163\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 164\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 165\n",
      "{'loss': 1.381846308708191, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5584551095962524} on model 1 variable 166\n",
      "{'loss': 1.3817945718765259, 'accuracy': 0.32346034049987793, 'categorical_accuracy': 0.32346034049987793, 'mean_directional_accuracy': 0.5584551095962524} on model 1 variable 167\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 168\n",
      "{'loss': 1.3818106651306152, 'accuracy': 0.3231993615627289, 'categorical_accuracy': 0.3231993615627289, 'mean_directional_accuracy': 0.5581941604614258} on model 1 variable 169\n",
      "{'loss': 1.3857812881469727, 'accuracy': 0.31550103425979614, 'categorical_accuracy': 0.31550103425979614, 'mean_directional_accuracy': 0.5326200127601624} on model 2 variable 1\n",
      "{'loss': 1.3853181600570679, 'accuracy': 0.31706681847572327, 'categorical_accuracy': 0.31706681847572327, 'mean_directional_accuracy': 0.5367953777313232} on model 2 variable 2\n",
      "{'loss': 1.3854304552078247, 'accuracy': 0.3171972930431366, 'categorical_accuracy': 0.3171972930431366, 'mean_directional_accuracy': 0.5360125303268433} on model 2 variable 3\n",
      "{'loss': 1.385453701019287, 'accuracy': 0.31745824217796326, 'categorical_accuracy': 0.31745824217796326, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 4\n",
      "{'loss': 1.385453462600708, 'accuracy': 0.3162839114665985, 'categorical_accuracy': 0.3162839114665985, 'mean_directional_accuracy': 0.5348381996154785} on model 2 variable 5\n",
      "{'loss': 1.3851292133331299, 'accuracy': 0.3181106448173523, 'categorical_accuracy': 0.3181106448173523, 'mean_directional_accuracy': 0.5370563864707947} on model 2 variable 6\n",
      "{'loss': 1.3852107524871826, 'accuracy': 0.31745824217796326, 'categorical_accuracy': 0.31745824217796326, 'mean_directional_accuracy': 0.5381001830101013} on model 2 variable 7\n",
      "{'loss': 1.385420322418213, 'accuracy': 0.31563153862953186, 'categorical_accuracy': 0.31563153862953186, 'mean_directional_accuracy': 0.5350991487503052} on model 2 variable 8\n",
      "{'loss': 1.3853918313980103, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5366649031639099} on model 2 variable 9\n",
      "{'loss': 1.385480284690857, 'accuracy': 0.3181106448173523, 'categorical_accuracy': 0.3181106448173523, 'mean_directional_accuracy': 0.537969708442688} on model 2 variable 10\n",
      "{'loss': 1.3854061365127563, 'accuracy': 0.31693631410598755, 'categorical_accuracy': 0.31693631410598755, 'mean_directional_accuracy': 0.5366649031639099} on model 2 variable 11\n",
      "{'loss': 1.3853107690811157, 'accuracy': 0.3173277676105499, 'categorical_accuracy': 0.3173277676105499, 'mean_directional_accuracy': 0.5374478101730347} on model 2 variable 12\n",
      "{'loss': 1.385413408279419, 'accuracy': 0.3166753649711609, 'categorical_accuracy': 0.3166753649711609, 'mean_directional_accuracy': 0.5362734794616699} on model 2 variable 13\n",
      "{'loss': 1.3853204250335693, 'accuracy': 0.3171972930431366, 'categorical_accuracy': 0.3171972930431366, 'mean_directional_accuracy': 0.5366649031639099} on model 2 variable 14\n",
      "{'loss': 1.385377049446106, 'accuracy': 0.3157620131969452, 'categorical_accuracy': 0.3157620131969452, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 15\n",
      "{'loss': 1.3853532075881958, 'accuracy': 0.31706681847572327, 'categorical_accuracy': 0.31706681847572327, 'mean_directional_accuracy': 0.5366649031639099} on model 2 variable 16\n",
      "{'loss': 1.3851031064987183, 'accuracy': 0.3177192211151123, 'categorical_accuracy': 0.3177192211151123, 'mean_directional_accuracy': 0.5381001830101013} on model 2 variable 17\n",
      "{'loss': 1.3853700160980225, 'accuracy': 0.31602296233177185, 'categorical_accuracy': 0.31602296233177185, 'mean_directional_accuracy': 0.5360125303268433} on model 2 variable 18\n",
      "{'loss': 1.3854396343231201, 'accuracy': 0.3173277676105499, 'categorical_accuracy': 0.3173277676105499, 'mean_directional_accuracy': 0.5366649031639099} on model 2 variable 19\n",
      "{'loss': 1.385450005531311, 'accuracy': 0.31693631410598755, 'categorical_accuracy': 0.31693631410598755, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 20\n",
      "{'loss': 1.3854129314422607, 'accuracy': 0.31641441583633423, 'categorical_accuracy': 0.31641441583633423, 'mean_directional_accuracy': 0.5360125303268433} on model 2 variable 21\n",
      "{'loss': 1.3858531713485718, 'accuracy': 0.3158924877643585, 'categorical_accuracy': 0.3158924877643585, 'mean_directional_accuracy': 0.5358820557594299} on model 2 variable 22\n",
      "{'loss': 1.3860291242599487, 'accuracy': 0.3162839114665985, 'categorical_accuracy': 0.3162839114665985, 'mean_directional_accuracy': 0.5360125303268433} on model 2 variable 23\n",
      "{'loss': 1.385582447052002, 'accuracy': 0.31602296233177185, 'categorical_accuracy': 0.31602296233177185, 'mean_directional_accuracy': 0.5366649031639099} on model 2 variable 24\n",
      "{'loss': 1.385332703590393, 'accuracy': 0.3171972930431366, 'categorical_accuracy': 0.3171972930431366, 'mean_directional_accuracy': 0.5366649031639099} on model 2 variable 25\n",
      "{'loss': 1.3854334354400635, 'accuracy': 0.31602296233177185, 'categorical_accuracy': 0.31602296233177185, 'mean_directional_accuracy': 0.5352296233177185} on model 2 variable 26\n",
      "{'loss': 1.3853600025177002, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5366649031639099} on model 2 variable 27\n",
      "{'loss': 1.3853659629821777, 'accuracy': 0.31745824217796326, 'categorical_accuracy': 0.31745824217796326, 'mean_directional_accuracy': 0.5373173356056213} on model 2 variable 28\n",
      "{'loss': 1.385401964187622, 'accuracy': 0.3166753649711609, 'categorical_accuracy': 0.3166753649711609, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 29\n",
      "{'loss': 1.3853942155838013, 'accuracy': 0.31706681847572327, 'categorical_accuracy': 0.31706681847572327, 'mean_directional_accuracy': 0.5367953777313232} on model 2 variable 30\n",
      "{'loss': 1.3852416276931763, 'accuracy': 0.3175887167453766, 'categorical_accuracy': 0.3175887167453766, 'mean_directional_accuracy': 0.537186861038208} on model 2 variable 31\n",
      "{'loss': 1.3852951526641846, 'accuracy': 0.31550103425979614, 'categorical_accuracy': 0.31550103425979614, 'mean_directional_accuracy': 0.5361430048942566} on model 2 variable 32\n",
      "{'loss': 1.385392189025879, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5366649031639099} on model 2 variable 33\n",
      "{'loss': 1.3853529691696167, 'accuracy': 0.3173277676105499, 'categorical_accuracy': 0.3173277676105499, 'mean_directional_accuracy': 0.5369259119033813} on model 2 variable 34\n",
      "{'loss': 1.385352611541748, 'accuracy': 0.3171972930431366, 'categorical_accuracy': 0.3171972930431366, 'mean_directional_accuracy': 0.5367953777313232} on model 2 variable 35\n",
      "{'loss': 1.3854097127914429, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5365344285964966} on model 2 variable 36\n",
      "{'loss': 1.385422945022583, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5360125303268433} on model 2 variable 37\n",
      "{'loss': 1.3864775896072388, 'accuracy': 0.31419622898101807, 'categorical_accuracy': 0.31419622898101807, 'mean_directional_accuracy': 0.5343163013458252} on model 2 variable 38\n",
      "{'loss': 1.3872084617614746, 'accuracy': 0.30897703766822815, 'categorical_accuracy': 0.30897703766822815, 'mean_directional_accuracy': 0.5267484188079834} on model 2 variable 39\n",
      "{'loss': 1.3856098651885986, 'accuracy': 0.31550103425979614, 'categorical_accuracy': 0.31550103425979614, 'mean_directional_accuracy': 0.5358820557594299} on model 2 variable 40\n",
      "{'loss': 1.3862453699111938, 'accuracy': 0.3152400851249695, 'categorical_accuracy': 0.3152400851249695, 'mean_directional_accuracy': 0.5377087593078613} on model 2 variable 41\n",
      "{'loss': 1.385837197303772, 'accuracy': 0.3162839114665985, 'categorical_accuracy': 0.3162839114665985, 'mean_directional_accuracy': 0.5319676399230957} on model 2 variable 42\n",
      "{'loss': 1.3864818811416626, 'accuracy': 0.31406575441360474, 'categorical_accuracy': 0.31406575441360474, 'mean_directional_accuracy': 0.5358820557594299} on model 2 variable 43\n",
      "{'loss': 1.3856033086776733, 'accuracy': 0.31654489040374756, 'categorical_accuracy': 0.31654489040374756, 'mean_directional_accuracy': 0.5365344285964966} on model 2 variable 44\n",
      "{'loss': 1.3859431743621826, 'accuracy': 0.31510961055755615, 'categorical_accuracy': 0.31510961055755615, 'mean_directional_accuracy': 0.5352296233177185} on model 2 variable 45\n",
      "{'loss': 1.3854424953460693, 'accuracy': 0.3162839114665985, 'categorical_accuracy': 0.3162839114665985, 'mean_directional_accuracy': 0.5373173356056213} on model 2 variable 46\n",
      "{'loss': 1.385799527168274, 'accuracy': 0.3153705596923828, 'categorical_accuracy': 0.3153705596923828, 'mean_directional_accuracy': 0.5361430048942566} on model 2 variable 47\n",
      "{'loss': 1.3859540224075317, 'accuracy': 0.3162839114665985, 'categorical_accuracy': 0.3162839114665985, 'mean_directional_accuracy': 0.5353600978851318} on model 2 variable 48\n",
      "{'loss': 1.3851909637451172, 'accuracy': 0.3152400851249695, 'categorical_accuracy': 0.3152400851249695, 'mean_directional_accuracy': 0.5362734794616699} on model 2 variable 49\n",
      "{'loss': 1.3857392072677612, 'accuracy': 0.3153705596923828, 'categorical_accuracy': 0.3153705596923828, 'mean_directional_accuracy': 0.5365344285964966} on model 2 variable 50\n",
      "{'loss': 1.385438323020935, 'accuracy': 0.3166753649711609, 'categorical_accuracy': 0.3166753649711609, 'mean_directional_accuracy': 0.5349686741828918} on model 2 variable 51\n",
      "{'loss': 1.385459542274475, 'accuracy': 0.31550103425979614, 'categorical_accuracy': 0.31550103425979614, 'mean_directional_accuracy': 0.5344467759132385} on model 2 variable 52\n",
      "{'loss': 1.3853237628936768, 'accuracy': 0.31602296233177185, 'categorical_accuracy': 0.31602296233177185, 'mean_directional_accuracy': 0.5362734794616699} on model 2 variable 53\n",
      "{'loss': 1.3883832693099976, 'accuracy': 0.31080377101898193, 'categorical_accuracy': 0.31080377101898193, 'mean_directional_accuracy': 0.5378392338752747} on model 2 variable 54\n",
      "{'loss': 1.3876885175704956, 'accuracy': 0.3088465631008148, 'categorical_accuracy': 0.3088465631008148, 'mean_directional_accuracy': 0.530793309211731} on model 2 variable 55\n",
      "{'loss': 1.3858981132507324, 'accuracy': 0.31550103425979614, 'categorical_accuracy': 0.31550103425979614, 'mean_directional_accuracy': 0.5366649031639099} on model 2 variable 56\n",
      "{'loss': 1.3860312700271606, 'accuracy': 0.31563153862953186, 'categorical_accuracy': 0.31563153862953186, 'mean_directional_accuracy': 0.5356211066246033} on model 2 variable 57\n",
      "{'loss': 1.385420560836792, 'accuracy': 0.3175887167453766, 'categorical_accuracy': 0.3175887167453766, 'mean_directional_accuracy': 0.5367953777313232} on model 2 variable 58\n",
      "{'loss': 1.385420560836792, 'accuracy': 0.3175887167453766, 'categorical_accuracy': 0.3175887167453766, 'mean_directional_accuracy': 0.5367953777313232} on model 2 variable 59\n",
      "{'loss': 1.3853795528411865, 'accuracy': 0.3182411193847656, 'categorical_accuracy': 0.3182411193847656, 'mean_directional_accuracy': 0.5367953777313232} on model 2 variable 60\n",
      "{'loss': 1.3853892087936401, 'accuracy': 0.31784969568252563, 'categorical_accuracy': 0.31784969568252563, 'mean_directional_accuracy': 0.5366649031639099} on model 2 variable 61\n",
      "{'loss': 1.3854131698608398, 'accuracy': 0.3162839114665985, 'categorical_accuracy': 0.3162839114665985, 'mean_directional_accuracy': 0.5365344285964966} on model 2 variable 62\n",
      "{'loss': 1.3854033946990967, 'accuracy': 0.31706681847572327, 'categorical_accuracy': 0.31706681847572327, 'mean_directional_accuracy': 0.5365344285964966} on model 2 variable 63\n",
      "{'loss': 1.3853867053985596, 'accuracy': 0.3173277676105499, 'categorical_accuracy': 0.3173277676105499, 'mean_directional_accuracy': 0.5369259119033813} on model 2 variable 64\n",
      "{'loss': 1.3853117227554321, 'accuracy': 0.31706681847572327, 'categorical_accuracy': 0.31706681847572327, 'mean_directional_accuracy': 0.5367953777313232} on model 2 variable 65\n",
      "{'loss': 1.3854037523269653, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5365344285964966} on model 2 variable 66\n",
      "{'loss': 1.3854024410247803, 'accuracy': 0.31693631410598755, 'categorical_accuracy': 0.31693631410598755, 'mean_directional_accuracy': 0.5366649031639099} on model 2 variable 67\n",
      "{'loss': 1.3854413032531738, 'accuracy': 0.31641441583633423, 'categorical_accuracy': 0.31641441583633423, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 68\n",
      "{'loss': 1.3853758573532104, 'accuracy': 0.31706681847572327, 'categorical_accuracy': 0.31706681847572327, 'mean_directional_accuracy': 0.5369259119033813} on model 2 variable 69\n",
      "{'loss': 1.3853691816329956, 'accuracy': 0.31693631410598755, 'categorical_accuracy': 0.31693631410598755, 'mean_directional_accuracy': 0.5366649031639099} on model 2 variable 70\n",
      "{'loss': 1.3853774070739746, 'accuracy': 0.3175887167453766, 'categorical_accuracy': 0.3175887167453766, 'mean_directional_accuracy': 0.5370563864707947} on model 2 variable 71\n",
      "{'loss': 1.3853236436843872, 'accuracy': 0.31745824217796326, 'categorical_accuracy': 0.31745824217796326, 'mean_directional_accuracy': 0.5377087593078613} on model 2 variable 72\n",
      "{'loss': 1.385683536529541, 'accuracy': 0.3162839114665985, 'categorical_accuracy': 0.3162839114665985, 'mean_directional_accuracy': 0.5326200127601624} on model 2 variable 73\n",
      "{'loss': 1.3854037523269653, 'accuracy': 0.3173277676105499, 'categorical_accuracy': 0.3173277676105499, 'mean_directional_accuracy': 0.537186861038208} on model 2 variable 74\n",
      "{'loss': 1.3855644464492798, 'accuracy': 0.31654489040374756, 'categorical_accuracy': 0.31654489040374756, 'mean_directional_accuracy': 0.5350991487503052} on model 2 variable 75\n",
      "{'loss': 1.3855091333389282, 'accuracy': 0.31602296233177185, 'categorical_accuracy': 0.31602296233177185, 'mean_directional_accuracy': 0.5335333943367004} on model 2 variable 76\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 77\n",
      "{'loss': 1.3853987455368042, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 78\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 79\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 80\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 81\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 82\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 83\n",
      "{'loss': 1.385398268699646, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 84\n",
      "{'loss': 1.3853988647460938, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 85\n",
      "{'loss': 1.3853965997695923, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 86\n",
      "{'loss': 1.384652018547058, 'accuracy': 0.3199373781681061, 'categorical_accuracy': 0.3199373781681061, 'mean_directional_accuracy': 0.5442327857017517} on model 2 variable 87\n",
      "{'loss': 1.3853989839553833, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 88\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 89\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 90\n",
      "{'loss': 1.3853979110717773, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 91\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 92\n",
      "{'loss': 1.3853986263275146, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 93\n",
      "{'loss': 1.385398030281067, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 94\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 95\n",
      "{'loss': 1.3853994607925415, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 96\n",
      "{'loss': 1.3853976726531982, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 97\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 98\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 99\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 100\n",
      "{'loss': 1.3853981494903564, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 101\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 102\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 103\n",
      "{'loss': 1.3853999376296997, 'accuracy': 0.31693631410598755, 'categorical_accuracy': 0.31693631410598755, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 104\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 105\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 106\n",
      "{'loss': 1.3853983879089355, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 107\n",
      "{'loss': 1.3853992223739624, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 108\n",
      "{'loss': 1.3853979110717773, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 109\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 110\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 111\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 112\n",
      "{'loss': 1.385399341583252, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 113\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 114\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 115\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 116\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 117\n",
      "{'loss': 1.3853973150253296, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 118\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 119\n",
      "{'loss': 1.3853986263275146, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 120\n",
      "{'loss': 1.3853986263275146, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 121\n",
      "{'loss': 1.3853987455368042, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 122\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 123\n",
      "{'loss': 1.385398268699646, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 124\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 125\n",
      "{'loss': 1.3853973150253296, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 126\n",
      "{'loss': 1.3853905200958252, 'accuracy': 0.31693631410598755, 'categorical_accuracy': 0.31693631410598755, 'mean_directional_accuracy': 0.5365344285964966} on model 2 variable 127\n",
      "{'loss': 1.385398030281067, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 128\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 129\n",
      "{'loss': 1.3853983879089355, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 130\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 131\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 132\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 133\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 134\n",
      "{'loss': 1.385398030281067, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 135\n",
      "{'loss': 1.3854237794876099, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 136\n",
      "{'loss': 1.3853986263275146, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 137\n",
      "{'loss': 1.3854013681411743, 'accuracy': 0.31706681847572327, 'categorical_accuracy': 0.31706681847572327, 'mean_directional_accuracy': 0.5367953777313232} on model 2 variable 138\n",
      "{'loss': 1.3854159116744995, 'accuracy': 0.3173277676105499, 'categorical_accuracy': 0.3173277676105499, 'mean_directional_accuracy': 0.5367953777313232} on model 2 variable 139\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 140\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 141\n",
      "{'loss': 1.3853987455368042, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 142\n",
      "{'loss': 1.3853977918624878, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 143\n",
      "{'loss': 1.385404109954834, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5365344285964966} on model 2 variable 144\n",
      "{'loss': 1.385401964187622, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5366649031639099} on model 2 variable 145\n",
      "{'loss': 1.385398030281067, 'accuracy': 0.31693631410598755, 'categorical_accuracy': 0.31693631410598755, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 146\n",
      "{'loss': 1.3853983879089355, 'accuracy': 0.31693631410598755, 'categorical_accuracy': 0.31693631410598755, 'mean_directional_accuracy': 0.5365344285964966} on model 2 variable 147\n",
      "{'loss': 1.3853527307510376, 'accuracy': 0.31706681847572327, 'categorical_accuracy': 0.31706681847572327, 'mean_directional_accuracy': 0.5365344285964966} on model 2 variable 148\n",
      "{'loss': 1.3854185342788696, 'accuracy': 0.3166753649711609, 'categorical_accuracy': 0.3166753649711609, 'mean_directional_accuracy': 0.5354906320571899} on model 2 variable 149\n",
      "{'loss': 1.3853987455368042, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 150\n",
      "{'loss': 1.3853964805603027, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 151\n",
      "{'loss': 1.385398030281067, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 152\n",
      "{'loss': 1.3854037523269653, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 153\n",
      "{'loss': 1.3853986263275146, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 154\n",
      "{'loss': 1.3854010105133057, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 155\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 156\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 157\n",
      "{'loss': 1.3854000568389893, 'accuracy': 0.31693631410598755, 'categorical_accuracy': 0.31693631410598755, 'mean_directional_accuracy': 0.5365344285964966} on model 2 variable 158\n",
      "{'loss': 1.385398030281067, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 159\n",
      "{'loss': 1.3853986263275146, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 160\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 161\n",
      "{'loss': 1.3853989839553833, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 162\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 163\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 164\n",
      "{'loss': 1.3854011297225952, 'accuracy': 0.3166753649711609, 'categorical_accuracy': 0.3166753649711609, 'mean_directional_accuracy': 0.5362734794616699} on model 2 variable 165\n",
      "{'loss': 1.385477900505066, 'accuracy': 0.3175887167453766, 'categorical_accuracy': 0.3175887167453766, 'mean_directional_accuracy': 0.5369259119033813} on model 2 variable 166\n",
      "{'loss': 1.3849765062332153, 'accuracy': 0.31745824217796326, 'categorical_accuracy': 0.31745824217796326, 'mean_directional_accuracy': 0.537969708442688} on model 2 variable 167\n",
      "{'loss': 1.3853988647460938, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 168\n",
      "{'loss': 1.385398507118225, 'accuracy': 0.3168058395385742, 'categorical_accuracy': 0.3168058395385742, 'mean_directional_accuracy': 0.5364039540290833} on model 2 variable 169\n",
      "{'loss': 1.3750596046447754, 'accuracy': 0.3064979016780853, 'categorical_accuracy': 0.3064979016780853, 'mean_directional_accuracy': 0.524530291557312} on model 3 variable 1\n",
      "{'loss': 1.3743853569030762, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 2\n",
      "{'loss': 1.374432921409607, 'accuracy': 0.306367427110672, 'categorical_accuracy': 0.306367427110672, 'mean_directional_accuracy': 0.5228340029716492} on model 3 variable 3\n",
      "{'loss': 1.3742234706878662, 'accuracy': 0.3064979016780853, 'categorical_accuracy': 0.3064979016780853, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 4\n",
      "{'loss': 1.3743975162506104, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5230950117111206} on model 3 variable 5\n",
      "{'loss': 1.373915672302246, 'accuracy': 0.30519309639930725, 'categorical_accuracy': 0.30519309639930725, 'mean_directional_accuracy': 0.5204854011535645} on model 3 variable 6\n",
      "{'loss': 1.3744641542434692, 'accuracy': 0.3067588806152344, 'categorical_accuracy': 0.3067588806152344, 'mean_directional_accuracy': 0.5240083336830139} on model 3 variable 7\n",
      "{'loss': 1.374419093132019, 'accuracy': 0.30427974462509155, 'categorical_accuracy': 0.30427974462509155, 'mean_directional_accuracy': 0.5206158757209778} on model 3 variable 8\n",
      "{'loss': 1.374040126800537, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5223121047019958} on model 3 variable 9\n",
      "{'loss': 1.3747942447662354, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.5220511555671692} on model 3 variable 10\n",
      "{'loss': 1.3736183643341064, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.5212682485580444} on model 3 variable 11\n",
      "{'loss': 1.3747169971466064, 'accuracy': 0.30558454990386963, 'categorical_accuracy': 0.30558454990386963, 'mean_directional_accuracy': 0.5242692828178406} on model 3 variable 12\n",
      "{'loss': 1.3745466470718384, 'accuracy': 0.30519309639930725, 'categorical_accuracy': 0.30519309639930725, 'mean_directional_accuracy': 0.5220511555671692} on model 3 variable 13\n",
      "{'loss': 1.3736674785614014, 'accuracy': 0.3068893551826477, 'categorical_accuracy': 0.3068893551826477, 'mean_directional_accuracy': 0.5230950117111206} on model 3 variable 14\n",
      "{'loss': 1.3744804859161377, 'accuracy': 0.30662840604782104, 'categorical_accuracy': 0.30662840604782104, 'mean_directional_accuracy': 0.5236169099807739} on model 3 variable 15\n",
      "{'loss': 1.3744981288909912, 'accuracy': 0.30519309639930725, 'categorical_accuracy': 0.30519309639930725, 'mean_directional_accuracy': 0.5220511555671692} on model 3 variable 16\n",
      "{'loss': 1.3741092681884766, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5213987231254578} on model 3 variable 17\n",
      "{'loss': 1.374098539352417, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5212682485580444} on model 3 variable 18\n",
      "{'loss': 1.3744999170303345, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5225730538368225} on model 3 variable 19\n",
      "{'loss': 1.3745369911193848, 'accuracy': 0.30610647797584534, 'categorical_accuracy': 0.30610647797584534, 'mean_directional_accuracy': 0.5250521898269653} on model 3 variable 20\n",
      "{'loss': 1.3754065036773682, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.524530291557312} on model 3 variable 21\n",
      "{'loss': 1.3742852210998535, 'accuracy': 0.3031054139137268, 'categorical_accuracy': 0.3031054139137268, 'mean_directional_accuracy': 0.5200939178466797} on model 3 variable 22\n",
      "{'loss': 1.3742254972457886, 'accuracy': 0.30245304107666016, 'categorical_accuracy': 0.30245304107666016, 'mean_directional_accuracy': 0.5189196467399597} on model 3 variable 23\n",
      "{'loss': 1.3745297193527222, 'accuracy': 0.3050626218318939, 'categorical_accuracy': 0.3050626218318939, 'mean_directional_accuracy': 0.5225730538368225} on model 3 variable 24\n",
      "{'loss': 1.3744744062423706, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5228340029716492} on model 3 variable 25\n",
      "{'loss': 1.3743577003479004, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5227035284042358} on model 3 variable 26\n",
      "{'loss': 1.3745553493499756, 'accuracy': 0.30558454990386963, 'categorical_accuracy': 0.30558454990386963, 'mean_directional_accuracy': 0.5236169099807739} on model 3 variable 27\n",
      "{'loss': 1.3742831945419312, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5211377739906311} on model 3 variable 28\n",
      "{'loss': 1.3745739459991455, 'accuracy': 0.30610647797584534, 'categorical_accuracy': 0.30610647797584534, 'mean_directional_accuracy': 0.5241388082504272} on model 3 variable 29\n",
      "{'loss': 1.3743999004364014, 'accuracy': 0.30519309639930725, 'categorical_accuracy': 0.30519309639930725, 'mean_directional_accuracy': 0.5224425792694092} on model 3 variable 30\n",
      "{'loss': 1.3736076354980469, 'accuracy': 0.3050626218318939, 'categorical_accuracy': 0.3050626218318939, 'mean_directional_accuracy': 0.5206158757209778} on model 3 variable 31\n",
      "{'loss': 1.3739608526229858, 'accuracy': 0.306367427110672, 'categorical_accuracy': 0.306367427110672, 'mean_directional_accuracy': 0.5227035284042358} on model 3 variable 32\n",
      "{'loss': 1.3744091987609863, 'accuracy': 0.30610647797584534, 'categorical_accuracy': 0.30610647797584534, 'mean_directional_accuracy': 0.5228340029716492} on model 3 variable 33\n",
      "{'loss': 1.3744282722473145, 'accuracy': 0.30662840604782104, 'categorical_accuracy': 0.30662840604782104, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 34\n",
      "{'loss': 1.3741968870162964, 'accuracy': 0.30558454990386963, 'categorical_accuracy': 0.30558454990386963, 'mean_directional_accuracy': 0.5227035284042358} on model 3 variable 35\n",
      "{'loss': 1.3746228218078613, 'accuracy': 0.30480167269706726, 'categorical_accuracy': 0.30480167269706726, 'mean_directional_accuracy': 0.5220511555671692} on model 3 variable 36\n",
      "{'loss': 1.3742738962173462, 'accuracy': 0.30519309639930725, 'categorical_accuracy': 0.30519309639930725, 'mean_directional_accuracy': 0.5199634432792664} on model 3 variable 37\n",
      "{'loss': 1.3748173713684082, 'accuracy': 0.30336639285087585, 'categorical_accuracy': 0.30336639285087585, 'mean_directional_accuracy': 0.519441545009613} on model 3 variable 38\n",
      "{'loss': 1.375605583190918, 'accuracy': 0.30623695254325867, 'categorical_accuracy': 0.30623695254325867, 'mean_directional_accuracy': 0.525313138961792} on model 3 variable 39\n",
      "{'loss': 1.3743343353271484, 'accuracy': 0.3064979016780853, 'categorical_accuracy': 0.3064979016780853, 'mean_directional_accuracy': 0.5215292572975159} on model 3 variable 40\n",
      "{'loss': 1.3749715089797974, 'accuracy': 0.3040187954902649, 'categorical_accuracy': 0.3040187954902649, 'mean_directional_accuracy': 0.519050121307373} on model 3 variable 41\n",
      "{'loss': 1.3749500513076782, 'accuracy': 0.30480167269706726, 'categorical_accuracy': 0.30480167269706726, 'mean_directional_accuracy': 0.5236169099807739} on model 3 variable 42\n",
      "{'loss': 1.3754111528396606, 'accuracy': 0.3032359182834625, 'categorical_accuracy': 0.3032359182834625, 'mean_directional_accuracy': 0.518136739730835} on model 3 variable 43\n",
      "{'loss': 1.3740602731704712, 'accuracy': 0.30480167269706726, 'categorical_accuracy': 0.30480167269706726, 'mean_directional_accuracy': 0.519050121307373} on model 3 variable 44\n",
      "{'loss': 1.3744752407073975, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.5213987231254578} on model 3 variable 45\n",
      "{'loss': 1.3742420673370361, 'accuracy': 0.3040187954902649, 'categorical_accuracy': 0.3040187954902649, 'mean_directional_accuracy': 0.5204854011535645} on model 3 variable 46\n",
      "{'loss': 1.37466299533844, 'accuracy': 0.3041492700576782, 'categorical_accuracy': 0.3041492700576782, 'mean_directional_accuracy': 0.5200939178466797} on model 3 variable 47\n",
      "{'loss': 1.3744285106658936, 'accuracy': 0.30245304107666016, 'categorical_accuracy': 0.30245304107666016, 'mean_directional_accuracy': 0.5208768248558044} on model 3 variable 48\n",
      "{'loss': 1.3738627433776855, 'accuracy': 0.3034968674182892, 'categorical_accuracy': 0.3034968674182892, 'mean_directional_accuracy': 0.5200939178466797} on model 3 variable 49\n",
      "{'loss': 1.3758063316345215, 'accuracy': 0.3029749393463135, 'categorical_accuracy': 0.3029749393463135, 'mean_directional_accuracy': 0.5240083336830139} on model 3 variable 50\n",
      "{'loss': 1.3745923042297363, 'accuracy': 0.3041492700576782, 'categorical_accuracy': 0.3041492700576782, 'mean_directional_accuracy': 0.5207463502883911} on model 3 variable 51\n",
      "{'loss': 1.3739339113235474, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5204854011535645} on model 3 variable 52\n",
      "{'loss': 1.3740414381027222, 'accuracy': 0.3049321472644806, 'categorical_accuracy': 0.3049321472644806, 'mean_directional_accuracy': 0.5212682485580444} on model 3 variable 53\n",
      "{'loss': 1.376956582069397, 'accuracy': 0.29827767610549927, 'categorical_accuracy': 0.29827767610549927, 'mean_directional_accuracy': 0.519441545009613} on model 3 variable 54\n",
      "{'loss': 1.3774006366729736, 'accuracy': 0.3036273419857025, 'categorical_accuracy': 0.3036273419857025, 'mean_directional_accuracy': 0.5247912406921387} on model 3 variable 55\n",
      "{'loss': 1.374467134475708, 'accuracy': 0.30519309639930725, 'categorical_accuracy': 0.30519309639930725, 'mean_directional_accuracy': 0.5230950117111206} on model 3 variable 56\n",
      "{'loss': 1.3744555711746216, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5224425792694092} on model 3 variable 57\n",
      "{'loss': 1.374344825744629, 'accuracy': 0.30715030431747437, 'categorical_accuracy': 0.30715030431747437, 'mean_directional_accuracy': 0.5227035284042358} on model 3 variable 58\n",
      "{'loss': 1.374395489692688, 'accuracy': 0.30662840604782104, 'categorical_accuracy': 0.30662840604782104, 'mean_directional_accuracy': 0.5230950117111206} on model 3 variable 59\n",
      "{'loss': 1.3744075298309326, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 60\n",
      "{'loss': 1.3744722604751587, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5237473845481873} on model 3 variable 61\n",
      "{'loss': 1.3745046854019165, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5229645371437073} on model 3 variable 62\n",
      "{'loss': 1.3745579719543457, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5211377739906311} on model 3 variable 63\n",
      "{'loss': 1.3746731281280518, 'accuracy': 0.30388832092285156, 'categorical_accuracy': 0.30388832092285156, 'mean_directional_accuracy': 0.5221816301345825} on model 3 variable 64\n",
      "{'loss': 1.3741674423217773, 'accuracy': 0.306367427110672, 'categorical_accuracy': 0.306367427110672, 'mean_directional_accuracy': 0.5241388082504272} on model 3 variable 65\n",
      "{'loss': 1.3748725652694702, 'accuracy': 0.3049321472644806, 'categorical_accuracy': 0.3049321472644806, 'mean_directional_accuracy': 0.5227035284042358} on model 3 variable 66\n",
      "{'loss': 1.3738645315170288, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5232254862785339} on model 3 variable 67\n",
      "{'loss': 1.3743399381637573, 'accuracy': 0.3049321472644806, 'categorical_accuracy': 0.3049321472644806, 'mean_directional_accuracy': 0.5217902064323425} on model 3 variable 68\n",
      "{'loss': 1.3742767572402954, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5220511555671692} on model 3 variable 69\n",
      "{'loss': 1.3747129440307617, 'accuracy': 0.30558454990386963, 'categorical_accuracy': 0.30558454990386963, 'mean_directional_accuracy': 0.5237473845481873} on model 3 variable 70\n",
      "{'loss': 1.374508261680603, 'accuracy': 0.30480167269706726, 'categorical_accuracy': 0.30480167269706726, 'mean_directional_accuracy': 0.5232254862785339} on model 3 variable 71\n",
      "{'loss': 1.3749958276748657, 'accuracy': 0.30610647797584534, 'categorical_accuracy': 0.30610647797584534, 'mean_directional_accuracy': 0.5243998169898987} on model 3 variable 72\n",
      "{'loss': 1.374868392944336, 'accuracy': 0.3040187954902649, 'categorical_accuracy': 0.3040187954902649, 'mean_directional_accuracy': 0.5223121047019958} on model 3 variable 73\n",
      "{'loss': 1.3742915391921997, 'accuracy': 0.30427974462509155, 'categorical_accuracy': 0.30427974462509155, 'mean_directional_accuracy': 0.5220511555671692} on model 3 variable 74\n",
      "{'loss': 1.374536395072937, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5212682485580444} on model 3 variable 75\n",
      "{'loss': 1.3748779296875, 'accuracy': 0.30662840604782104, 'categorical_accuracy': 0.30662840604782104, 'mean_directional_accuracy': 0.5241388082504272} on model 3 variable 76\n",
      "{'loss': 1.3744248151779175, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 77\n",
      "{'loss': 1.3744454383850098, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 78\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 79\n",
      "{'loss': 1.3744306564331055, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 80\n",
      "{'loss': 1.3744227886199951, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 81\n",
      "{'loss': 1.3744311332702637, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 82\n",
      "{'loss': 1.374432921409607, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 83\n",
      "{'loss': 1.3744087219238281, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 84\n",
      "{'loss': 1.374434232711792, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 85\n",
      "{'loss': 1.3744311332702637, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 86\n",
      "{'loss': 1.37141752243042, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5210072994232178} on model 3 variable 87\n",
      "{'loss': 1.3744468688964844, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 88\n",
      "{'loss': 1.3744314908981323, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 89\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 90\n",
      "{'loss': 1.3743970394134521, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 91\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 92\n",
      "{'loss': 1.3744394779205322, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 93\n",
      "{'loss': 1.374430537223816, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 94\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 95\n",
      "{'loss': 1.3744271993637085, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 96\n",
      "{'loss': 1.3744293451309204, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 97\n",
      "{'loss': 1.3744183778762817, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 98\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 99\n",
      "{'loss': 1.3744027614593506, 'accuracy': 0.30558454990386963, 'categorical_accuracy': 0.30558454990386963, 'mean_directional_accuracy': 0.5230950117111206} on model 3 variable 100\n",
      "{'loss': 1.3744248151779175, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5236169099807739} on model 3 variable 101\n",
      "{'loss': 1.3744311332702637, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 102\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 103\n",
      "{'loss': 1.3744298219680786, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5237473845481873} on model 3 variable 104\n",
      "{'loss': 1.3744310140609741, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 105\n",
      "{'loss': 1.3744317293167114, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 106\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 107\n",
      "{'loss': 1.3744306564331055, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 108\n",
      "{'loss': 1.374429702758789, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5236169099807739} on model 3 variable 109\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 110\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 111\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 112\n",
      "{'loss': 1.3744304180145264, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 113\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 114\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 115\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 116\n",
      "{'loss': 1.3744311332702637, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 117\n",
      "{'loss': 1.3744138479232788, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5236169099807739} on model 3 variable 118\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 119\n",
      "{'loss': 1.3744285106658936, 'accuracy': 0.30610647797584534, 'categorical_accuracy': 0.30610647797584534, 'mean_directional_accuracy': 0.5237473845481873} on model 3 variable 120\n",
      "{'loss': 1.3744308948516846, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 121\n",
      "{'loss': 1.374429702758789, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5236169099807739} on model 3 variable 122\n",
      "{'loss': 1.374423623085022, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 123\n",
      "{'loss': 1.3744359016418457, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5236169099807739} on model 3 variable 124\n",
      "{'loss': 1.374436378479004, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 125\n",
      "{'loss': 1.3744457960128784, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5232254862785339} on model 3 variable 126\n",
      "{'loss': 1.3744871616363525, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5236169099807739} on model 3 variable 127\n",
      "{'loss': 1.3744478225708008, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 128\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 129\n",
      "{'loss': 1.3743969202041626, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 130\n",
      "{'loss': 1.374431848526001, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 131\n",
      "{'loss': 1.374431848526001, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 132\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 133\n",
      "{'loss': 1.3744235038757324, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 134\n",
      "{'loss': 1.3743990659713745, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 135\n",
      "{'loss': 1.3747307062149048, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5241388082504272} on model 3 variable 136\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 137\n",
      "{'loss': 1.3745195865631104, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5229645371437073} on model 3 variable 138\n",
      "{'loss': 1.3746068477630615, 'accuracy': 0.30480167269706726, 'categorical_accuracy': 0.30480167269706726, 'mean_directional_accuracy': 0.5225730538368225} on model 3 variable 139\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 140\n",
      "{'loss': 1.3744316101074219, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 141\n",
      "{'loss': 1.3744322061538696, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 142\n",
      "{'loss': 1.3744510412216187, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5236169099807739} on model 3 variable 143\n",
      "{'loss': 1.3744688034057617, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5230950117111206} on model 3 variable 144\n",
      "{'loss': 1.3743618726730347, 'accuracy': 0.3067588806152344, 'categorical_accuracy': 0.3067588806152344, 'mean_directional_accuracy': 0.5242692828178406} on model 3 variable 145\n",
      "{'loss': 1.374421238899231, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 146\n",
      "{'loss': 1.3744609355926514, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 147\n",
      "{'loss': 1.3737549781799316, 'accuracy': 0.3064979016780853, 'categorical_accuracy': 0.3064979016780853, 'mean_directional_accuracy': 0.5223121047019958} on model 3 variable 148\n",
      "{'loss': 1.3742748498916626, 'accuracy': 0.3031054139137268, 'categorical_accuracy': 0.3031054139137268, 'mean_directional_accuracy': 0.5195720195770264} on model 3 variable 149\n",
      "{'loss': 1.3744099140167236, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 150\n",
      "{'loss': 1.3744443655014038, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 151\n",
      "{'loss': 1.3743677139282227, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5230950117111206} on model 3 variable 152\n",
      "{'loss': 1.3744335174560547, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 153\n",
      "{'loss': 1.3744308948516846, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 154\n",
      "{'loss': 1.374424934387207, 'accuracy': 0.30623695254325867, 'categorical_accuracy': 0.30623695254325867, 'mean_directional_accuracy': 0.5237473845481873} on model 3 variable 155\n",
      "{'loss': 1.3744266033172607, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 156\n",
      "{'loss': 1.374430775642395, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 157\n",
      "{'loss': 1.3744866847991943, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5229645371437073} on model 3 variable 158\n",
      "{'loss': 1.3744080066680908, 'accuracy': 0.30610647797584534, 'categorical_accuracy': 0.30610647797584534, 'mean_directional_accuracy': 0.5236169099807739} on model 3 variable 159\n",
      "{'loss': 1.3744343519210815, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 160\n",
      "{'loss': 1.374431848526001, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 161\n",
      "{'loss': 1.3744255304336548, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5232254862785339} on model 3 variable 162\n",
      "{'loss': 1.374444603919983, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 163\n",
      "{'loss': 1.3744343519210815, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 164\n",
      "{'loss': 1.3744299411773682, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 165\n",
      "{'loss': 1.3745061159133911, 'accuracy': 0.30388832092285156, 'categorical_accuracy': 0.30388832092285156, 'mean_directional_accuracy': 0.5220511555671692} on model 3 variable 166\n",
      "{'loss': 1.3721240758895874, 'accuracy': 0.3067588806152344, 'categorical_accuracy': 0.3067588806152344, 'mean_directional_accuracy': 0.5233559608459473} on model 3 variable 167\n",
      "{'loss': 1.3744323253631592, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 168\n",
      "{'loss': 1.3744313716888428, 'accuracy': 0.3058454990386963, 'categorical_accuracy': 0.3058454990386963, 'mean_directional_accuracy': 0.5234864354133606} on model 3 variable 169\n",
      "{'loss': 1.389161467552185, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.5241388082504272} on model 4 variable 1\n",
      "{'loss': 1.3890986442565918, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5258350968360901} on model 4 variable 2\n",
      "{'loss': 1.3891043663024902, 'accuracy': 0.3064979016780853, 'categorical_accuracy': 0.3064979016780853, 'mean_directional_accuracy': 0.5234864354133606} on model 4 variable 3\n",
      "{'loss': 1.389275312423706, 'accuracy': 0.306367427110672, 'categorical_accuracy': 0.306367427110672, 'mean_directional_accuracy': 0.5251826643943787} on model 4 variable 4\n",
      "{'loss': 1.3893799781799316, 'accuracy': 0.3049321472644806, 'categorical_accuracy': 0.3049321472644806, 'mean_directional_accuracy': 0.5230950117111206} on model 4 variable 5\n",
      "{'loss': 1.3890568017959595, 'accuracy': 0.306367427110672, 'categorical_accuracy': 0.306367427110672, 'mean_directional_accuracy': 0.525313138961792} on model 4 variable 6\n",
      "{'loss': 1.3893871307373047, 'accuracy': 0.3050626218318939, 'categorical_accuracy': 0.3050626218318939, 'mean_directional_accuracy': 0.5247912406921387} on model 4 variable 7\n",
      "{'loss': 1.3892621994018555, 'accuracy': 0.30519309639930725, 'categorical_accuracy': 0.30519309639930725, 'mean_directional_accuracy': 0.5230950117111206} on model 4 variable 8\n",
      "{'loss': 1.3891963958740234, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5243998169898987} on model 4 variable 9\n",
      "{'loss': 1.389326572418213, 'accuracy': 0.3032359182834625, 'categorical_accuracy': 0.3032359182834625, 'mean_directional_accuracy': 0.5229645371437073} on model 4 variable 10\n",
      "{'loss': 1.3890793323516846, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5241388082504272} on model 4 variable 11\n",
      "{'loss': 1.388451099395752, 'accuracy': 0.3074112832546234, 'categorical_accuracy': 0.3074112832546234, 'mean_directional_accuracy': 0.5270093679428101} on model 4 variable 12\n",
      "{'loss': 1.3894357681274414, 'accuracy': 0.3041492700576782, 'categorical_accuracy': 0.3041492700576782, 'mean_directional_accuracy': 0.5232254862785339} on model 4 variable 13\n",
      "{'loss': 1.3888070583343506, 'accuracy': 0.30701982975006104, 'categorical_accuracy': 0.30701982975006104, 'mean_directional_accuracy': 0.5258350968360901} on model 4 variable 14\n",
      "{'loss': 1.3886717557907104, 'accuracy': 0.3064979016780853, 'categorical_accuracy': 0.3064979016780853, 'mean_directional_accuracy': 0.5279227495193481} on model 4 variable 15\n",
      "{'loss': 1.3893013000488281, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5234864354133606} on model 4 variable 16\n",
      "{'loss': 1.3891408443450928, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5257046222686768} on model 4 variable 17\n",
      "{'loss': 1.3893024921417236, 'accuracy': 0.3049321472644806, 'categorical_accuracy': 0.3049321472644806, 'mean_directional_accuracy': 0.5229645371437073} on model 4 variable 18\n",
      "{'loss': 1.3895694017410278, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5263569951057434} on model 4 variable 19\n",
      "{'loss': 1.3893332481384277, 'accuracy': 0.30480167269706726, 'categorical_accuracy': 0.30480167269706726, 'mean_directional_accuracy': 0.5243998169898987} on model 4 variable 20\n",
      "{'loss': 1.3892658948898315, 'accuracy': 0.30519309639930725, 'categorical_accuracy': 0.30519309639930725, 'mean_directional_accuracy': 0.5237473845481873} on model 4 variable 21\n",
      "{'loss': 1.3896600008010864, 'accuracy': 0.30336639285087585, 'categorical_accuracy': 0.30336639285087585, 'mean_directional_accuracy': 0.5232254862785339} on model 4 variable 22\n",
      "{'loss': 1.3894448280334473, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5229645371437073} on model 4 variable 23\n",
      "{'loss': 1.3892035484313965, 'accuracy': 0.30558454990386963, 'categorical_accuracy': 0.30558454990386963, 'mean_directional_accuracy': 0.5250521898269653} on model 4 variable 24\n",
      "{'loss': 1.3892287015914917, 'accuracy': 0.3034968674182892, 'categorical_accuracy': 0.3034968674182892, 'mean_directional_accuracy': 0.5247912406921387} on model 4 variable 25\n",
      "{'loss': 1.389238953590393, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5238778591156006} on model 4 variable 26\n",
      "{'loss': 1.3892699480056763, 'accuracy': 0.30467119812965393, 'categorical_accuracy': 0.30467119812965393, 'mean_directional_accuracy': 0.5238778591156006} on model 4 variable 27\n",
      "{'loss': 1.389268159866333, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 28\n",
      "{'loss': 1.3893688917160034, 'accuracy': 0.3034968674182892, 'categorical_accuracy': 0.3034968674182892, 'mean_directional_accuracy': 0.5219206809997559} on model 4 variable 29\n",
      "{'loss': 1.3890539407730103, 'accuracy': 0.3074112832546234, 'categorical_accuracy': 0.3074112832546234, 'mean_directional_accuracy': 0.5264874696731567} on model 4 variable 30\n",
      "{'loss': 1.3889437913894653, 'accuracy': 0.30701982975006104, 'categorical_accuracy': 0.30701982975006104, 'mean_directional_accuracy': 0.5264874696731567} on model 4 variable 31\n",
      "{'loss': 1.3890713453292847, 'accuracy': 0.306367427110672, 'categorical_accuracy': 0.306367427110672, 'mean_directional_accuracy': 0.5246607661247253} on model 4 variable 32\n",
      "{'loss': 1.3888245820999146, 'accuracy': 0.30754175782203674, 'categorical_accuracy': 0.30754175782203674, 'mean_directional_accuracy': 0.5260960459709167} on model 4 variable 33\n",
      "{'loss': 1.3892487287521362, 'accuracy': 0.3078027069568634, 'categorical_accuracy': 0.3078027069568634, 'mean_directional_accuracy': 0.525313138961792} on model 4 variable 34\n",
      "{'loss': 1.3893296718597412, 'accuracy': 0.3040187954902649, 'categorical_accuracy': 0.3040187954902649, 'mean_directional_accuracy': 0.5223121047019958} on model 4 variable 35\n",
      "{'loss': 1.3890674114227295, 'accuracy': 0.30610647797584534, 'categorical_accuracy': 0.30610647797584534, 'mean_directional_accuracy': 0.5251826643943787} on model 4 variable 36\n",
      "{'loss': 1.3892285823822021, 'accuracy': 0.30480167269706726, 'categorical_accuracy': 0.30480167269706726, 'mean_directional_accuracy': 0.5234864354133606} on model 4 variable 37\n",
      "{'loss': 1.3896039724349976, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.5236169099807739} on model 4 variable 38\n",
      "{'loss': 1.3904160261154175, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5191805958747864} on model 4 variable 39\n",
      "{'loss': 1.3895636796951294, 'accuracy': 0.30388832092285156, 'categorical_accuracy': 0.30388832092285156, 'mean_directional_accuracy': 0.5203549265861511} on model 4 variable 40\n",
      "{'loss': 1.3897700309753418, 'accuracy': 0.3027139902114868, 'categorical_accuracy': 0.3027139902114868, 'mean_directional_accuracy': 0.5236169099807739} on model 4 variable 41\n",
      "{'loss': 1.3897446393966675, 'accuracy': 0.3034968674182892, 'categorical_accuracy': 0.3034968674182892, 'mean_directional_accuracy': 0.5268788933753967} on model 4 variable 42\n",
      "{'loss': 1.3902405500411987, 'accuracy': 0.30610647797584534, 'categorical_accuracy': 0.30610647797584534, 'mean_directional_accuracy': 0.5258350968360901} on model 4 variable 43\n",
      "{'loss': 1.3891104459762573, 'accuracy': 0.3068893551826477, 'categorical_accuracy': 0.3068893551826477, 'mean_directional_accuracy': 0.5260960459709167} on model 4 variable 44\n",
      "{'loss': 1.3900690078735352, 'accuracy': 0.30610647797584534, 'categorical_accuracy': 0.30610647797584534, 'mean_directional_accuracy': 0.5236169099807739} on model 4 variable 45\n",
      "{'loss': 1.389229655265808, 'accuracy': 0.30701982975006104, 'categorical_accuracy': 0.30701982975006104, 'mean_directional_accuracy': 0.5264874696731567} on model 4 variable 46\n",
      "{'loss': 1.389343500137329, 'accuracy': 0.30701982975006104, 'categorical_accuracy': 0.30701982975006104, 'mean_directional_accuracy': 0.524921715259552} on model 4 variable 47\n",
      "{'loss': 1.3901447057724, 'accuracy': 0.3029749393463135, 'categorical_accuracy': 0.3029749393463135, 'mean_directional_accuracy': 0.5223121047019958} on model 4 variable 48\n",
      "{'loss': 1.3887032270431519, 'accuracy': 0.3076722323894501, 'categorical_accuracy': 0.3076722323894501, 'mean_directional_accuracy': 0.5270093679428101} on model 4 variable 49\n",
      "{'loss': 1.3899626731872559, 'accuracy': 0.306367427110672, 'categorical_accuracy': 0.306367427110672, 'mean_directional_accuracy': 0.5280532240867615} on model 4 variable 50\n",
      "{'loss': 1.389147162437439, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5242692828178406} on model 4 variable 51\n",
      "{'loss': 1.3887443542480469, 'accuracy': 0.3081941604614258, 'categorical_accuracy': 0.3081941604614258, 'mean_directional_accuracy': 0.5305323600769043} on model 4 variable 52\n",
      "{'loss': 1.3890146017074585, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.5262265205383301} on model 4 variable 53\n",
      "{'loss': 1.3917733430862427, 'accuracy': 0.30088725686073303, 'categorical_accuracy': 0.30088725686073303, 'mean_directional_accuracy': 0.5255740880966187} on model 4 variable 54\n",
      "{'loss': 1.391198754310608, 'accuracy': 0.30101773142814636, 'categorical_accuracy': 0.30101773142814636, 'mean_directional_accuracy': 0.5213987231254578} on model 4 variable 55\n",
      "{'loss': 1.3897676467895508, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5271399021148682} on model 4 variable 56\n",
      "{'loss': 1.3895436525344849, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.5230950117111206} on model 4 variable 57\n",
      "{'loss': 1.3889762163162231, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5242692828178406} on model 4 variable 58\n",
      "{'loss': 1.3891704082489014, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5254436135292053} on model 4 variable 59\n",
      "{'loss': 1.3892323970794678, 'accuracy': 0.3049321472644806, 'categorical_accuracy': 0.3049321472644806, 'mean_directional_accuracy': 0.5232254862785339} on model 4 variable 60\n",
      "{'loss': 1.3892078399658203, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 61\n",
      "{'loss': 1.3892155885696411, 'accuracy': 0.3049321472644806, 'categorical_accuracy': 0.3049321472644806, 'mean_directional_accuracy': 0.5237473845481873} on model 4 variable 62\n",
      "{'loss': 1.3897299766540527, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5241388082504272} on model 4 variable 63\n",
      "{'loss': 1.3893409967422485, 'accuracy': 0.30467119812965393, 'categorical_accuracy': 0.30467119812965393, 'mean_directional_accuracy': 0.5230950117111206} on model 4 variable 64\n",
      "{'loss': 1.38882577419281, 'accuracy': 0.30623695254325867, 'categorical_accuracy': 0.30623695254325867, 'mean_directional_accuracy': 0.5255740880966187} on model 4 variable 65\n",
      "{'loss': 1.3888423442840576, 'accuracy': 0.3067588806152344, 'categorical_accuracy': 0.3067588806152344, 'mean_directional_accuracy': 0.525313138961792} on model 4 variable 66\n",
      "{'loss': 1.388999342918396, 'accuracy': 0.3068893551826477, 'categorical_accuracy': 0.3068893551826477, 'mean_directional_accuracy': 0.5254436135292053} on model 4 variable 67\n",
      "{'loss': 1.389514446258545, 'accuracy': 0.3041492700576782, 'categorical_accuracy': 0.3041492700576782, 'mean_directional_accuracy': 0.5215292572975159} on model 4 variable 68\n",
      "{'loss': 1.389101505279541, 'accuracy': 0.3068893551826477, 'categorical_accuracy': 0.3068893551826477, 'mean_directional_accuracy': 0.5260960459709167} on model 4 variable 69\n",
      "{'loss': 1.3893784284591675, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5225730538368225} on model 4 variable 70\n",
      "{'loss': 1.3890202045440674, 'accuracy': 0.3068893551826477, 'categorical_accuracy': 0.3068893551826477, 'mean_directional_accuracy': 0.5271399021148682} on model 4 variable 71\n",
      "{'loss': 1.388655185699463, 'accuracy': 0.3096294403076172, 'categorical_accuracy': 0.3096294403076172, 'mean_directional_accuracy': 0.5301409363746643} on model 4 variable 72\n",
      "{'loss': 1.3895708322525024, 'accuracy': 0.30336639285087585, 'categorical_accuracy': 0.30336639285087585, 'mean_directional_accuracy': 0.5199634432792664} on model 4 variable 73\n",
      "{'loss': 1.388845682144165, 'accuracy': 0.30480167269706726, 'categorical_accuracy': 0.30480167269706726, 'mean_directional_accuracy': 0.5229645371437073} on model 4 variable 74\n",
      "{'loss': 1.3892700672149658, 'accuracy': 0.3050626218318939, 'categorical_accuracy': 0.3050626218318939, 'mean_directional_accuracy': 0.5237473845481873} on model 4 variable 75\n",
      "{'loss': 1.3894695043563843, 'accuracy': 0.30662840604782104, 'categorical_accuracy': 0.30662840604782104, 'mean_directional_accuracy': 0.5277922749519348} on model 4 variable 76\n",
      "{'loss': 1.3892152309417725, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 77\n",
      "{'loss': 1.3892158269882202, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 78\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 79\n",
      "{'loss': 1.389214277267456, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 80\n",
      "{'loss': 1.3892146348953247, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 81\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 82\n",
      "{'loss': 1.3892148733139038, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 83\n",
      "{'loss': 1.3892254829406738, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5243998169898987} on model 4 variable 84\n",
      "{'loss': 1.3892139196395874, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 85\n",
      "{'loss': 1.389217734336853, 'accuracy': 0.30519309639930725, 'categorical_accuracy': 0.30519309639930725, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 86\n",
      "{'loss': 1.3879802227020264, 'accuracy': 0.3076722323894501, 'categorical_accuracy': 0.3076722323894501, 'mean_directional_accuracy': 0.5306628346443176} on model 4 variable 87\n",
      "{'loss': 1.389215111732483, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 88\n",
      "{'loss': 1.3892148733139038, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 89\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 90\n",
      "{'loss': 1.3892126083374023, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 91\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 92\n",
      "{'loss': 1.3892152309417725, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 93\n",
      "{'loss': 1.3892148733139038, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 94\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 95\n",
      "{'loss': 1.3892309665679932, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 96\n",
      "{'loss': 1.389207124710083, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5241388082504272} on model 4 variable 97\n",
      "{'loss': 1.389216423034668, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 98\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 99\n",
      "{'loss': 1.3892180919647217, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 100\n",
      "{'loss': 1.3892148733139038, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 101\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 102\n",
      "{'loss': 1.389215111732483, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 103\n",
      "{'loss': 1.38922119140625, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 104\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 105\n",
      "{'loss': 1.389215111732483, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 106\n",
      "{'loss': 1.3892148733139038, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 107\n",
      "{'loss': 1.389214277267456, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 108\n",
      "{'loss': 1.389230489730835, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5238778591156006} on model 4 variable 109\n",
      "{'loss': 1.3892145156860352, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 110\n",
      "{'loss': 1.389215111732483, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 111\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 112\n",
      "{'loss': 1.3892158269882202, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 113\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 114\n",
      "{'loss': 1.3892152309417725, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 115\n",
      "{'loss': 1.3892154693603516, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 116\n",
      "{'loss': 1.389215111732483, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 117\n",
      "{'loss': 1.3892152309417725, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 118\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 119\n",
      "{'loss': 1.3892138004302979, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 120\n",
      "{'loss': 1.3892148733139038, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 121\n",
      "{'loss': 1.3892145156860352, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 122\n",
      "{'loss': 1.3892179727554321, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 123\n",
      "{'loss': 1.3892152309417725, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 124\n",
      "{'loss': 1.389215350151062, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 125\n",
      "{'loss': 1.3892121315002441, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 126\n",
      "{'loss': 1.3892210721969604, 'accuracy': 0.30519309639930725, 'categorical_accuracy': 0.30519309639930725, 'mean_directional_accuracy': 0.5238778591156006} on model 4 variable 127\n",
      "{'loss': 1.3892159461975098, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 128\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 129\n",
      "{'loss': 1.3891957998275757, 'accuracy': 0.3050626218318939, 'categorical_accuracy': 0.3050626218318939, 'mean_directional_accuracy': 0.5238778591156006} on model 4 variable 130\n",
      "{'loss': 1.3892143964767456, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 131\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 132\n",
      "{'loss': 1.389215111732483, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 133\n",
      "{'loss': 1.3892146348953247, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 134\n",
      "{'loss': 1.3892136812210083, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 135\n",
      "{'loss': 1.389424204826355, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5230950117111206} on model 4 variable 136\n",
      "{'loss': 1.3892145156860352, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 137\n",
      "{'loss': 1.3892120122909546, 'accuracy': 0.30480167269706726, 'categorical_accuracy': 0.30480167269706726, 'mean_directional_accuracy': 0.5237473845481873} on model 4 variable 138\n",
      "{'loss': 1.3892532587051392, 'accuracy': 0.3050626218318939, 'categorical_accuracy': 0.3050626218318939, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 139\n",
      "{'loss': 1.3892143964767456, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 140\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5241388082504272} on model 4 variable 141\n",
      "{'loss': 1.3892143964767456, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 142\n",
      "{'loss': 1.389215111732483, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 143\n",
      "{'loss': 1.3892111778259277, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 144\n",
      "{'loss': 1.3892112970352173, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5241388082504272} on model 4 variable 145\n",
      "{'loss': 1.3892210721969604, 'accuracy': 0.30467119812965393, 'categorical_accuracy': 0.30467119812965393, 'mean_directional_accuracy': 0.5238778591156006} on model 4 variable 146\n",
      "{'loss': 1.389216661453247, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5242692828178406} on model 4 variable 147\n",
      "{'loss': 1.3889602422714233, 'accuracy': 0.3067588806152344, 'categorical_accuracy': 0.3067588806152344, 'mean_directional_accuracy': 0.5260960459709167} on model 4 variable 148\n",
      "{'loss': 1.3892288208007812, 'accuracy': 0.30467119812965393, 'categorical_accuracy': 0.30467119812965393, 'mean_directional_accuracy': 0.5224425792694092} on model 4 variable 149\n",
      "{'loss': 1.3892157077789307, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5241388082504272} on model 4 variable 150\n",
      "{'loss': 1.3892171382904053, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 151\n",
      "{'loss': 1.3892244100570679, 'accuracy': 0.30519309639930725, 'categorical_accuracy': 0.30519309639930725, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 152\n",
      "{'loss': 1.3891957998275757, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 153\n",
      "{'loss': 1.38921320438385, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 154\n",
      "{'loss': 1.3892226219177246, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 155\n",
      "{'loss': 1.389214038848877, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 156\n",
      "{'loss': 1.3892155885696411, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 157\n",
      "{'loss': 1.389211893081665, 'accuracy': 0.30519309639930725, 'categorical_accuracy': 0.30519309639930725, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 158\n",
      "{'loss': 1.3892161846160889, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 159\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 160\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 161\n",
      "{'loss': 1.3892182111740112, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 162\n",
      "{'loss': 1.3892146348953247, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 163\n",
      "{'loss': 1.3892155885696411, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 164\n",
      "{'loss': 1.3892171382904053, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 165\n",
      "{'loss': 1.3892158269882202, 'accuracy': 0.30623695254325867, 'categorical_accuracy': 0.30623695254325867, 'mean_directional_accuracy': 0.5257046222686768} on model 4 variable 166\n",
      "{'loss': 1.388753890991211, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.524530291557312} on model 4 variable 167\n",
      "{'loss': 1.3892154693603516, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 168\n",
      "{'loss': 1.3892149925231934, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5240083336830139} on model 4 variable 169\n",
      "{'loss': 1.3801056146621704, 'accuracy': 0.2971033453941345, 'categorical_accuracy': 0.2971033453941345, 'mean_directional_accuracy': 0.5251826643943787} on model 5 variable 1\n",
      "{'loss': 1.3793405294418335, 'accuracy': 0.30245304107666016, 'categorical_accuracy': 0.30245304107666016, 'mean_directional_accuracy': 0.5345772504806519} on model 5 variable 2\n",
      "{'loss': 1.3793425559997559, 'accuracy': 0.30480167269706726, 'categorical_accuracy': 0.30480167269706726, 'mean_directional_accuracy': 0.5344467759132385} on model 5 variable 3\n",
      "{'loss': 1.380269169807434, 'accuracy': 0.300234854221344, 'categorical_accuracy': 0.300234854221344, 'mean_directional_accuracy': 0.5318371653556824} on model 5 variable 4\n",
      "{'loss': 1.379651665687561, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5367953777313232} on model 5 variable 5\n",
      "{'loss': 1.379231333732605, 'accuracy': 0.3040187954902649, 'categorical_accuracy': 0.3040187954902649, 'mean_directional_accuracy': 0.5343163013458252} on model 5 variable 6\n",
      "{'loss': 1.3801565170288086, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5364039540290833} on model 5 variable 7\n",
      "{'loss': 1.3795061111450195, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5384916663169861} on model 5 variable 8\n",
      "{'loss': 1.3794862031936646, 'accuracy': 0.306367427110672, 'categorical_accuracy': 0.306367427110672, 'mean_directional_accuracy': 0.5339248180389404} on model 5 variable 9\n",
      "{'loss': 1.379740834236145, 'accuracy': 0.3034968674182892, 'categorical_accuracy': 0.3034968674182892, 'mean_directional_accuracy': 0.531706690788269} on model 5 variable 10\n",
      "{'loss': 1.3798986673355103, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5360125303268433} on model 5 variable 11\n",
      "{'loss': 1.3790117502212524, 'accuracy': 0.3072807788848877, 'categorical_accuracy': 0.3072807788848877, 'mean_directional_accuracy': 0.5358820557594299} on model 5 variable 12\n",
      "{'loss': 1.3796604871749878, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.5364039540290833} on model 5 variable 13\n",
      "{'loss': 1.3787562847137451, 'accuracy': 0.30519309639930725, 'categorical_accuracy': 0.30519309639930725, 'mean_directional_accuracy': 0.5370563864707947} on model 5 variable 14\n",
      "{'loss': 1.3796042203903198, 'accuracy': 0.30427974462509155, 'categorical_accuracy': 0.30427974462509155, 'mean_directional_accuracy': 0.5339248180389404} on model 5 variable 15\n",
      "{'loss': 1.3795171976089478, 'accuracy': 0.3018006384372711, 'categorical_accuracy': 0.3018006384372711, 'mean_directional_accuracy': 0.5318371653556824} on model 5 variable 16\n",
      "{'loss': 1.3792510032653809, 'accuracy': 0.30571502447128296, 'categorical_accuracy': 0.30571502447128296, 'mean_directional_accuracy': 0.5356211066246033} on model 5 variable 17\n",
      "{'loss': 1.3800230026245117, 'accuracy': 0.3040187954902649, 'categorical_accuracy': 0.3040187954902649, 'mean_directional_accuracy': 0.531184732913971} on model 5 variable 18\n",
      "{'loss': 1.3794612884521484, 'accuracy': 0.30375781655311584, 'categorical_accuracy': 0.30375781655311584, 'mean_directional_accuracy': 0.5337943434715271} on model 5 variable 19\n",
      "{'loss': 1.3793193101882935, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5357515811920166} on model 5 variable 20\n",
      "{'loss': 1.3795071840286255, 'accuracy': 0.30662840604782104, 'categorical_accuracy': 0.30662840604782104, 'mean_directional_accuracy': 0.5335333943367004} on model 5 variable 21\n",
      "{'loss': 1.3798311948776245, 'accuracy': 0.3034968674182892, 'categorical_accuracy': 0.3034968674182892, 'mean_directional_accuracy': 0.5276618003845215} on model 5 variable 22\n",
      "{'loss': 1.3793466091156006, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5340553522109985} on model 5 variable 23\n",
      "{'loss': 1.3797215223312378, 'accuracy': 0.3021920621395111, 'categorical_accuracy': 0.3021920621395111, 'mean_directional_accuracy': 0.5341858267784119} on model 5 variable 24\n",
      "{'loss': 1.3794885873794556, 'accuracy': 0.3032359182834625, 'categorical_accuracy': 0.3032359182834625, 'mean_directional_accuracy': 0.5358820557594299} on model 5 variable 25\n",
      "{'loss': 1.379626750946045, 'accuracy': 0.30467119812965393, 'categorical_accuracy': 0.30467119812965393, 'mean_directional_accuracy': 0.5365344285964966} on model 5 variable 26\n",
      "{'loss': 1.379528522491455, 'accuracy': 0.3031054139137268, 'categorical_accuracy': 0.3031054139137268, 'mean_directional_accuracy': 0.5354906320571899} on model 5 variable 27\n",
      "{'loss': 1.3798331022262573, 'accuracy': 0.3031054139137268, 'categorical_accuracy': 0.3031054139137268, 'mean_directional_accuracy': 0.5334029197692871} on model 5 variable 28\n",
      "{'loss': 1.3794167041778564, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.5361430048942566} on model 5 variable 29\n",
      "{'loss': 1.3797414302825928, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.5339248180389404} on model 5 variable 30\n",
      "{'loss': 1.3791875839233398, 'accuracy': 0.3034968674182892, 'categorical_accuracy': 0.3034968674182892, 'mean_directional_accuracy': 0.5349686741828918} on model 5 variable 31\n",
      "{'loss': 1.379431128501892, 'accuracy': 0.30336639285087585, 'categorical_accuracy': 0.30336639285087585, 'mean_directional_accuracy': 0.5340553522109985} on model 5 variable 32\n",
      "{'loss': 1.379410743713379, 'accuracy': 0.30245304107666016, 'categorical_accuracy': 0.30245304107666016, 'mean_directional_accuracy': 0.5344467759132385} on model 5 variable 33\n",
      "{'loss': 1.3794701099395752, 'accuracy': 0.3031054139137268, 'categorical_accuracy': 0.3031054139137268, 'mean_directional_accuracy': 0.5337943434715271} on model 5 variable 34\n",
      "{'loss': 1.3801313638687134, 'accuracy': 0.30558454990386963, 'categorical_accuracy': 0.30558454990386963, 'mean_directional_accuracy': 0.5365344285964966} on model 5 variable 35\n",
      "{'loss': 1.3794586658477783, 'accuracy': 0.30558454990386963, 'categorical_accuracy': 0.30558454990386963, 'mean_directional_accuracy': 0.5356211066246033} on model 5 variable 36\n",
      "{'loss': 1.379416823387146, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.5354906320571899} on model 5 variable 37\n",
      "{'loss': 1.3799943923950195, 'accuracy': 0.30427974462509155, 'categorical_accuracy': 0.30427974462509155, 'mean_directional_accuracy': 0.5314457416534424} on model 5 variable 38\n",
      "{'loss': 1.3809771537780762, 'accuracy': 0.29997390508651733, 'categorical_accuracy': 0.29997390508651733, 'mean_directional_accuracy': 0.5250521898269653} on model 5 variable 39\n",
      "{'loss': 1.3795925378799438, 'accuracy': 0.3011482357978821, 'categorical_accuracy': 0.3011482357978821, 'mean_directional_accuracy': 0.5323590636253357} on model 5 variable 40\n",
      "{'loss': 1.3800089359283447, 'accuracy': 0.3034968674182892, 'categorical_accuracy': 0.3034968674182892, 'mean_directional_accuracy': 0.5347077250480652} on model 5 variable 41\n",
      "{'loss': 1.37994384765625, 'accuracy': 0.30336639285087585, 'categorical_accuracy': 0.30336639285087585, 'mean_directional_accuracy': 0.5322285890579224} on model 5 variable 42\n",
      "{'loss': 1.3805760145187378, 'accuracy': 0.3034968674182892, 'categorical_accuracy': 0.3034968674182892, 'mean_directional_accuracy': 0.5328810214996338} on model 5 variable 43\n",
      "{'loss': 1.3796786069869995, 'accuracy': 0.305976003408432, 'categorical_accuracy': 0.305976003408432, 'mean_directional_accuracy': 0.5357515811920166} on model 5 variable 44\n",
      "{'loss': 1.3800241947174072, 'accuracy': 0.30936846137046814, 'categorical_accuracy': 0.30936846137046814, 'mean_directional_accuracy': 0.5421450734138489} on model 5 variable 45\n",
      "{'loss': 1.379285216331482, 'accuracy': 0.3076722323894501, 'categorical_accuracy': 0.3076722323894501, 'mean_directional_accuracy': 0.537578284740448} on model 5 variable 46\n",
      "{'loss': 1.3797229528427124, 'accuracy': 0.3088465631008148, 'categorical_accuracy': 0.3088465631008148, 'mean_directional_accuracy': 0.5378392338752747} on model 5 variable 47\n",
      "{'loss': 1.379699468612671, 'accuracy': 0.30467119812965393, 'categorical_accuracy': 0.30467119812965393, 'mean_directional_accuracy': 0.5309237837791443} on model 5 variable 48\n",
      "{'loss': 1.380117654800415, 'accuracy': 0.30388832092285156, 'categorical_accuracy': 0.30388832092285156, 'mean_directional_accuracy': 0.5331419706344604} on model 5 variable 49\n",
      "{'loss': 1.3799899816513062, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5369259119033813} on model 5 variable 50\n",
      "{'loss': 1.379642367362976, 'accuracy': 0.3034968674182892, 'categorical_accuracy': 0.3034968674182892, 'mean_directional_accuracy': 0.5339248180389404} on model 5 variable 51\n",
      "{'loss': 1.3795167207717896, 'accuracy': 0.30153965950012207, 'categorical_accuracy': 0.30153965950012207, 'mean_directional_accuracy': 0.5314457416534424} on model 5 variable 52\n",
      "{'loss': 1.3793023824691772, 'accuracy': 0.3027139902114868, 'categorical_accuracy': 0.3027139902114868, 'mean_directional_accuracy': 0.5343163013458252} on model 5 variable 53\n",
      "{'loss': 1.3821163177490234, 'accuracy': 0.2994519770145416, 'categorical_accuracy': 0.2994519770145416, 'mean_directional_accuracy': 0.5345772504806519} on model 5 variable 54\n",
      "{'loss': 1.3818289041519165, 'accuracy': 0.3003653585910797, 'categorical_accuracy': 0.3003653585910797, 'mean_directional_accuracy': 0.5284446477890015} on model 5 variable 55\n",
      "{'loss': 1.3798243999481201, 'accuracy': 0.3074112832546234, 'categorical_accuracy': 0.3074112832546234, 'mean_directional_accuracy': 0.5378392338752747} on model 5 variable 56\n",
      "{'loss': 1.380394458770752, 'accuracy': 0.30388832092285156, 'categorical_accuracy': 0.30388832092285156, 'mean_directional_accuracy': 0.531184732913971} on model 5 variable 57\n",
      "{'loss': 1.3800368309020996, 'accuracy': 0.3027139902114868, 'categorical_accuracy': 0.3027139902114868, 'mean_directional_accuracy': 0.5348381996154785} on model 5 variable 58\n",
      "{'loss': 1.379217267036438, 'accuracy': 0.30610647797584534, 'categorical_accuracy': 0.30610647797584534, 'mean_directional_accuracy': 0.5349686741828918} on model 5 variable 59\n",
      "{'loss': 1.3795827627182007, 'accuracy': 0.30467119812965393, 'categorical_accuracy': 0.30467119812965393, 'mean_directional_accuracy': 0.5349686741828918} on model 5 variable 60\n",
      "{'loss': 1.3794549703598022, 'accuracy': 0.30623695254325867, 'categorical_accuracy': 0.30623695254325867, 'mean_directional_accuracy': 0.5374478101730347} on model 5 variable 61\n",
      "{'loss': 1.3798059225082397, 'accuracy': 0.3036273419857025, 'categorical_accuracy': 0.3036273419857025, 'mean_directional_accuracy': 0.5345772504806519} on model 5 variable 62\n",
      "{'loss': 1.3797049522399902, 'accuracy': 0.30375781655311584, 'categorical_accuracy': 0.30375781655311584, 'mean_directional_accuracy': 0.5356211066246033} on model 5 variable 63\n",
      "{'loss': 1.3793097734451294, 'accuracy': 0.30375781655311584, 'categorical_accuracy': 0.30375781655311584, 'mean_directional_accuracy': 0.5353600978851318} on model 5 variable 64\n",
      "{'loss': 1.3788766860961914, 'accuracy': 0.3032359182834625, 'categorical_accuracy': 0.3032359182834625, 'mean_directional_accuracy': 0.5339248180389404} on model 5 variable 65\n",
      "{'loss': 1.3786908388137817, 'accuracy': 0.3029749393463135, 'categorical_accuracy': 0.3029749393463135, 'mean_directional_accuracy': 0.5343163013458252} on model 5 variable 66\n",
      "{'loss': 1.3790851831436157, 'accuracy': 0.30480167269706726, 'categorical_accuracy': 0.30480167269706726, 'mean_directional_accuracy': 0.5374478101730347} on model 5 variable 67\n",
      "{'loss': 1.3796569108963013, 'accuracy': 0.3050626218318939, 'categorical_accuracy': 0.3050626218318939, 'mean_directional_accuracy': 0.5353600978851318} on model 5 variable 68\n",
      "{'loss': 1.379287600517273, 'accuracy': 0.30427974462509155, 'categorical_accuracy': 0.30427974462509155, 'mean_directional_accuracy': 0.5349686741828918} on model 5 variable 69\n",
      "{'loss': 1.3796164989471436, 'accuracy': 0.30519309639930725, 'categorical_accuracy': 0.30519309639930725, 'mean_directional_accuracy': 0.5353600978851318} on model 5 variable 70\n",
      "{'loss': 1.379225492477417, 'accuracy': 0.30467119812965393, 'categorical_accuracy': 0.30467119812965393, 'mean_directional_accuracy': 0.5352296233177185} on model 5 variable 71\n",
      "{'loss': 1.3794270753860474, 'accuracy': 0.30193111300468445, 'categorical_accuracy': 0.30193111300468445, 'mean_directional_accuracy': 0.5337943434715271} on model 5 variable 72\n",
      "{'loss': 1.37924325466156, 'accuracy': 0.3049321472644806, 'categorical_accuracy': 0.3049321472644806, 'mean_directional_accuracy': 0.5356211066246033} on model 5 variable 73\n",
      "{'loss': 1.3794829845428467, 'accuracy': 0.3049321472644806, 'categorical_accuracy': 0.3049321472644806, 'mean_directional_accuracy': 0.5353600978851318} on model 5 variable 74\n",
      "{'loss': 1.3797051906585693, 'accuracy': 0.3049321472644806, 'categorical_accuracy': 0.3049321472644806, 'mean_directional_accuracy': 0.5328810214996338} on model 5 variable 75\n",
      "{'loss': 1.3792805671691895, 'accuracy': 0.306367427110672, 'categorical_accuracy': 0.306367427110672, 'mean_directional_accuracy': 0.5383611917495728} on model 5 variable 76\n",
      "{'loss': 1.3794723749160767, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 77\n",
      "{'loss': 1.3794864416122437, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 78\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 79\n",
      "{'loss': 1.3794728517532349, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 80\n",
      "{'loss': 1.379475474357605, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 81\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 82\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 83\n",
      "{'loss': 1.379473328590393, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 84\n",
      "{'loss': 1.3794739246368408, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 85\n",
      "{'loss': 1.3794729709625244, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 86\n",
      "{'loss': 1.3776425123214722, 'accuracy': 0.3076722323894501, 'categorical_accuracy': 0.3076722323894501, 'mean_directional_accuracy': 0.5397964715957642} on model 5 variable 87\n",
      "{'loss': 1.379479169845581, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 88\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 89\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 90\n",
      "{'loss': 1.3794716596603394, 'accuracy': 0.30427974462509155, 'categorical_accuracy': 0.30427974462509155, 'mean_directional_accuracy': 0.5349686741828918} on model 5 variable 91\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 92\n",
      "{'loss': 1.3794728517532349, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 93\n",
      "{'loss': 1.3794732093811035, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 94\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 95\n",
      "{'loss': 1.3794797658920288, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 96\n",
      "{'loss': 1.3794728517532349, 'accuracy': 0.30467119812965393, 'categorical_accuracy': 0.30467119812965393, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 97\n",
      "{'loss': 1.3794746398925781, 'accuracy': 0.30467119812965393, 'categorical_accuracy': 0.30467119812965393, 'mean_directional_accuracy': 0.5353600978851318} on model 5 variable 98\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 99\n",
      "{'loss': 1.3794792890548706, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 100\n",
      "{'loss': 1.3794759511947632, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 101\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 102\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 103\n",
      "{'loss': 1.3794636726379395, 'accuracy': 0.30427974462509155, 'categorical_accuracy': 0.30427974462509155, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 104\n",
      "{'loss': 1.3794734477996826, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 105\n",
      "{'loss': 1.3794723749160767, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 106\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 107\n",
      "{'loss': 1.379476547241211, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 108\n",
      "{'loss': 1.3794951438903809, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5347077250480652} on model 5 variable 109\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 110\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 111\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 112\n",
      "{'loss': 1.3794718980789185, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 113\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 114\n",
      "{'loss': 1.3794716596603394, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 115\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 116\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 117\n",
      "{'loss': 1.3794769048690796, 'accuracy': 0.3040187954902649, 'categorical_accuracy': 0.3040187954902649, 'mean_directional_accuracy': 0.5345772504806519} on model 5 variable 118\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 119\n",
      "{'loss': 1.3794721364974976, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 120\n",
      "{'loss': 1.3794736862182617, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5348381996154785} on model 5 variable 121\n",
      "{'loss': 1.3794734477996826, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 122\n",
      "{'loss': 1.3794748783111572, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5349686741828918} on model 5 variable 123\n",
      "{'loss': 1.3794728517532349, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 124\n",
      "{'loss': 1.379473328590393, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 125\n",
      "{'loss': 1.3794677257537842, 'accuracy': 0.3040187954902649, 'categorical_accuracy': 0.3040187954902649, 'mean_directional_accuracy': 0.5348381996154785} on model 5 variable 126\n",
      "{'loss': 1.3794740438461304, 'accuracy': 0.30388832092285156, 'categorical_accuracy': 0.30388832092285156, 'mean_directional_accuracy': 0.5344467759132385} on model 5 variable 127\n",
      "{'loss': 1.3794746398925781, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 128\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 129\n",
      "{'loss': 1.3794655799865723, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 130\n",
      "{'loss': 1.3794727325439453, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 131\n",
      "{'loss': 1.3794723749160767, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 132\n",
      "{'loss': 1.3794723749160767, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 133\n",
      "{'loss': 1.3794736862182617, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 134\n",
      "{'loss': 1.3794710636138916, 'accuracy': 0.30427974462509155, 'categorical_accuracy': 0.30427974462509155, 'mean_directional_accuracy': 0.5349686741828918} on model 5 variable 135\n",
      "{'loss': 1.379514217376709, 'accuracy': 0.3049321472644806, 'categorical_accuracy': 0.3049321472644806, 'mean_directional_accuracy': 0.5344467759132385} on model 5 variable 136\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 137\n",
      "{'loss': 1.379584789276123, 'accuracy': 0.30427974462509155, 'categorical_accuracy': 0.30427974462509155, 'mean_directional_accuracy': 0.5352296233177185} on model 5 variable 138\n",
      "{'loss': 1.3795145750045776, 'accuracy': 0.30375781655311584, 'categorical_accuracy': 0.30375781655311584, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 139\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 140\n",
      "{'loss': 1.3794735670089722, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 141\n",
      "{'loss': 1.3794723749160767, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 142\n",
      "{'loss': 1.3794751167297363, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.5352296233177185} on model 5 variable 143\n",
      "{'loss': 1.3794784545898438, 'accuracy': 0.30467119812965393, 'categorical_accuracy': 0.30467119812965393, 'mean_directional_accuracy': 0.5354906320571899} on model 5 variable 144\n",
      "{'loss': 1.3794745206832886, 'accuracy': 0.30388832092285156, 'categorical_accuracy': 0.30388832092285156, 'mean_directional_accuracy': 0.5349686741828918} on model 5 variable 145\n",
      "{'loss': 1.3794630765914917, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 146\n",
      "{'loss': 1.379470944404602, 'accuracy': 0.30467119812965393, 'categorical_accuracy': 0.30467119812965393, 'mean_directional_accuracy': 0.5353600978851318} on model 5 variable 147\n",
      "{'loss': 1.3792728185653687, 'accuracy': 0.3040187954902649, 'categorical_accuracy': 0.3040187954902649, 'mean_directional_accuracy': 0.5348381996154785} on model 5 variable 148\n",
      "{'loss': 1.379185676574707, 'accuracy': 0.30532360076904297, 'categorical_accuracy': 0.30532360076904297, 'mean_directional_accuracy': 0.5369259119033813} on model 5 variable 149\n",
      "{'loss': 1.3794978857040405, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.5352296233177185} on model 5 variable 150\n",
      "{'loss': 1.379488468170166, 'accuracy': 0.30427974462509155, 'categorical_accuracy': 0.30427974462509155, 'mean_directional_accuracy': 0.5348381996154785} on model 5 variable 151\n",
      "{'loss': 1.3794680833816528, 'accuracy': 0.30427974462509155, 'categorical_accuracy': 0.30427974462509155, 'mean_directional_accuracy': 0.5348381996154785} on model 5 variable 152\n",
      "{'loss': 1.3795268535614014, 'accuracy': 0.30427974462509155, 'categorical_accuracy': 0.30427974462509155, 'mean_directional_accuracy': 0.5345772504806519} on model 5 variable 153\n",
      "{'loss': 1.3794711828231812, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 154\n",
      "{'loss': 1.379478096961975, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5352296233177185} on model 5 variable 155\n",
      "{'loss': 1.3794718980789185, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 156\n",
      "{'loss': 1.3794723749160767, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 157\n",
      "{'loss': 1.3794653415679932, 'accuracy': 0.30467119812965393, 'categorical_accuracy': 0.30467119812965393, 'mean_directional_accuracy': 0.5354906320571899} on model 5 variable 158\n",
      "{'loss': 1.3794752359390259, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 159\n",
      "{'loss': 1.3794723749160767, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 160\n",
      "{'loss': 1.379472017288208, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 161\n",
      "{'loss': 1.3794716596603394, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 162\n",
      "{'loss': 1.3794708251953125, 'accuracy': 0.30467119812965393, 'categorical_accuracy': 0.30467119812965393, 'mean_directional_accuracy': 0.5352296233177185} on model 5 variable 163\n",
      "{'loss': 1.3794724941253662, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 164\n",
      "{'loss': 1.3794678449630737, 'accuracy': 0.3045407235622406, 'categorical_accuracy': 0.3045407235622406, 'mean_directional_accuracy': 0.5353600978851318} on model 5 variable 165\n",
      "{'loss': 1.3796494007110596, 'accuracy': 0.3054540753364563, 'categorical_accuracy': 0.3054540753364563, 'mean_directional_accuracy': 0.5349686741828918} on model 5 variable 166\n",
      "{'loss': 1.3778204917907715, 'accuracy': 0.30701982975006104, 'categorical_accuracy': 0.30701982975006104, 'mean_directional_accuracy': 0.5382307171821594} on model 5 variable 167\n",
      "{'loss': 1.3794715404510498, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 168\n",
      "{'loss': 1.3794726133346558, 'accuracy': 0.3044102191925049, 'categorical_accuracy': 0.3044102191925049, 'mean_directional_accuracy': 0.5350991487503052} on model 5 variable 169\n"
     ]
    }
   ],
   "source": [
    "#Variable importance\n",
    "\n",
    "\n",
    "#rows = len(finalResults[\"ModelPointer\"])\n",
    "rows = len(models)\n",
    "cols = len(xVal.columns)\n",
    "zeroedXVal = np.array(xVal)\n",
    "\n",
    "#rows = 5\n",
    "\n",
    "if classification:\n",
    "    metric = \"accuracy\"\n",
    "else:\n",
    "    metric = \"mean_absolute_error\"\n",
    "\n",
    "\n",
    "#VarImpResults = pd.DataFrame(columns = X.columns)\n",
    "VarImpArray = np.empty((rows, cols), dtype=float, order='C')\n",
    "normalMSEArray = np.empty((rows, 1), dtype=float, order='C')\n",
    "\n",
    "for row in range(0, rows):\n",
    "    \n",
    "    # model = finalResults[\"ModelPointer\"][row]\n",
    "    model = models[row]\n",
    "\n",
    "    loss = model.evaluate(zeroedXVal, yVal, batch_size=128, verbose = 0, return_dict=True)\n",
    "    normalMSE = loss[metric]\n",
    "    normalMSEArray[row, 0] = normalMSE\n",
    "\n",
    "    for col in range(0, cols):\n",
    "\n",
    "        # print(\"\\n\")\n",
    "        # print(\"Model \", str(row+1), \" out of \", str(rows))\n",
    "        # print(\"Variable \", str(col+1), \" out of \", str(cols))\n",
    "\n",
    "        zeroedXVal[:,col] = 0\n",
    "        #VarImpData[col].values[:] = 0\n",
    "        loss = model.evaluate(zeroedXVal, yVal, batch_size=128, verbose = 0, return_dict=True)\n",
    "        zeroedXVal[:,col] = np.array(xVal[xVal.columns[col]])\n",
    "\n",
    "        try:\n",
    "            VarImpMSE = loss[metric]\n",
    "            VarImpArray[row, col] = VarImpMSE\n",
    "            print(\"%s on model %s variable %s\" % (loss, row+1, col+1))\n",
    "        except:     \n",
    "            print(\"RobError!: Loss is %s on model %s variable %s\" % (loss, row+1, col+1))\n",
    "\n",
    "\n",
    "\n",
    "VarImpResults = pd.DataFrame(VarImpArray, columns = xVal.columns)\n",
    "normalMSEArray = pd.DataFrame(normalMSEArray, columns = [\"AllVariables\"])\n",
    "\n",
    "VarImpResults = pd.concat([VarImpResults, normalMSEArray], axis=1)\n",
    "\n",
    "VarImpMean = VarImpResults.mean(axis=0)\n",
    "\n",
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N\n",
       "1                                    Company Name (CONM)\n",
       "2                       International Security ID (ISIN)\n",
       "3                                          SEDOL (SEDOL)\n",
       "4                            Stock Exchange Code (EXCHG)\n",
       "5                                  Fiscal Year-End (FYR)\n",
       "                             ...                        \n",
       "404    Selling, General and Administrative Expenses (...\n",
       "405                        Staff Expense - Other (XSTOY)\n",
       "406                Staff Expense - Wages/Salaries (XSTY)\n",
       "407                               Expense - Sundry (XSY)\n",
       "408                                Expense - Total (XTY)\n",
       "Name: ShortDescr, Length: 408, dtype: object"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "variableDescriptions = pd.read_excel(\n",
    "    \"Data/CompustatVariableDescriptions/VariableDescriptions.xlsx\",\n",
    "    header=2,\n",
    "    index_col=\"N\")\n",
    "\n",
    "variableDescriptions[\"Variable Name\"] = [n.strip() for n in variableDescriptions[\"Variable Name\"]]\n",
    "\n",
    "import re\n",
    "variableDescriptions[\"ShortDescr\"] = variableDescriptions[\"Description\"].apply(lambda x : re.sub(r\".*-- \", \"\", x))\n",
    "#variableDescriptions[\"ShortDescr\"] = variableDescriptions[\"ShortDescr\"].apply(lambda x : re.sub(r\" >.*\", \"\", x))\n",
    "#variableDescriptions[\"ShortDescr\"] = variableDescriptions[\"ShortDescr\"].apply(lambda x : re.sub(r\" -.*\", \"\", x))\n",
    "\n",
    "variableDescriptions[\"ShortDescr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VarImpMean = VarImpResults.mean(axis=0)\n",
    "\n",
    "if classification:\n",
    "    VarImpMeanSorted = VarImpMean.sort_values(ascending=True)\n",
    "    VarImpMeanSorted = VarImpMean[\"AllVariables\"] - VarImpMeanSorted\n",
    "\n",
    "else:\n",
    "    VarImpMeanSorted = VarImpMean.sort_values(ascending=False)\n",
    "    VarImpMeanSorted = VarImpMeanSorted - VarImpMean[\"AllVariables\"]\n",
    "\n",
    "\n",
    "VarImpMeanSorted = VarImpMeanSorted.rename(\"Error Increase\")\n",
    "VarImpMeanSorted.index = VarImpMeanSorted.index.str.upper()\n",
    "\n",
    "# VarImpMeanSorted = pd.merge(\n",
    "#     VarImpMeanSorted, variableDescriptions, how=\"left\",\n",
    "#     left_index=True, right_on=\"Variable Name\")\n",
    "\n",
    "VarImpMeanSorted = pd.DataFrame(VarImpMeanSorted)\n",
    "\n",
    "pd.DataFrame.to_html(VarImpMeanSorted, f\"Results/{target}/{dataSample}/VariableImportance.html\", index=False)\n",
    "pd.DataFrame.to_csv(VarImpMeanSorted, f\"Results/{target}/{dataSample}/VariableImportance.csv\", index=False)\n",
    "\n",
    "VarImpMeanBest = VarImpMeanSorted[1:25]\n",
    "# VarImpMeanScaled = VarImpMeanBest - VarImpMeanBest.mean()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.barh(\n",
    "    VarImpMeanBest.index, VarImpMeanBest[\"Error Increase\"], log=False, color=\"green\")  # , height = 0.4)#, width, bottom=VarImpMean.min(), align)\n",
    "    #VarImpMeanBest[\"Variable Name\"], VarImpMeanBest[\"Error Increase\"], log=True, color=\"green\")  # , height = 0.4)#, width, bottom=VarImpMean.min(), align)\n",
    "    #VarImpMeanBest[\"ShortDescr\"], VarImpMeanBest[\"Error Increase\"], log=True, color=\"green\")  # , height = 0.4)#, width, bottom=VarImpMean.min(), align)\n",
    "\n",
    "plt.gca().invert_yaxis()  # labels read top-to-bottom\n",
    "\n",
    "\n",
    "plt.title(\"Variable Importance\")\n",
    "plt.ylabel(\"Variable\")\n",
    "\n",
    "if classification:\n",
    "    plt.xlabel(\"Accuracy decrease when variable is removed\")\n",
    "else:\n",
    "    plt.xlabel(\"Error increase when variable is removed\")\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime(\"%d-%m-%Y_%H%M\")\n",
    "#variableImportancePath = f\"Results/{target}/VariableImportance_\" + str(now) + \".png\"\n",
    "variableImportancePath = f\"Results/{target}/{dataSample}/VariableImportance\" + \".png\"\n",
    "\n",
    "plt.savefig(variableImportancePath)\n",
    "# from matplotlib.ticker import ScalarFormatter\n",
    "# plt.gca().xaxis.set_major_formatter(ScalarFormatter())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalData.loc[: ,[\"returns\", \"past5YearReturn\", \"past4YearReturn\", \"past3YearReturn\", \"past2YearReturn\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '169)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [352], line 22\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mann_visualizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvisualize\u001b[39;00m \u001b[39mimport\u001b[39;00m ann_viz\n\u001b[0;32m     18\u001b[0m \u001b[39m# from datetime import datetime\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# now = datetime.now().strftime(\"%d-%m-%Y_%H%M\")\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# learningCurvePath = \"Results/LearningCurve_\" + str(now) + \".png\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m ann_viz(bestModel, view\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, filename\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mResults/\u001b[39;49m\u001b[39m{\u001b[39;49;00mtarget\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mdataSample\u001b[39m}\u001b[39;49;00m\u001b[39m/BestModelVisualized\u001b[39;49m\u001b[39m\"\u001b[39;49m, title\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBest Model Visualized\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\RobinForMLThesis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ann_visualizer\\visualize.py:42\u001b[0m, in \u001b[0;36mann_viz\u001b[1;34m(model, view, filename, title)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m(layer \u001b[39m==\u001b[39m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]):\n\u001b[1;32m---> 42\u001b[0m         input_layer \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(\u001b[39mstr\u001b[39;49m(layer\u001b[39m.\u001b[39;49minput_shape)\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39;49m][\u001b[39m1\u001b[39;49m:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]);\n\u001b[0;32m     43\u001b[0m         hidden_layers_nr \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m;\n\u001b[0;32m     44\u001b[0m         \u001b[39mif\u001b[39;00m (\u001b[39mtype\u001b[39m(layer) \u001b[39m==\u001b[39m keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mcore\u001b[39m.\u001b[39mDense):\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '169)'"
     ]
    }
   ],
   "source": [
    "#Visualize network\n",
    "modelPath = f'Results/{target}/{dataSample}/BestKerasModel1'\n",
    "if classification:\n",
    "    bestModel = tf.keras.models.load_model(modelPath, custom_objects={\"MDA\": MDA})\n",
    "else:\n",
    "    bestModel = tf.keras.models.load_model(modelPath)\n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'\n",
    "\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "from ann_visualizer.visualize import ann_viz\n",
    "\n",
    "\n",
    "# from datetime import datetime\n",
    "# now = datetime.now().strftime(\"%d-%m-%Y_%H%M\")\n",
    "# learningCurvePath = \"Results/LearningCurve_\" + str(now) + \".png\"\n",
    "\n",
    "ann_viz(bestModel, view=True, filename=f\"Results/{target}/{dataSample}/BestModelVisualized\", title=\"Best Model Visualized\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bestModel = keras.models.load_model(f'Results/{target}/BestKerasModel1')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Print summary\n",
    "modelSummary = bestModel.summary()\n",
    "print(modelSummary)\n",
    "\n",
    "\n",
    "# from datetime import datetime\n",
    "# now = datetime.now().strftime(\"%d-%m-%Y_%H%M\")\n",
    "# learningCurvePath = \"Results/LearningCurve_\" + str(now) + \".png\"\n",
    "\n",
    "with open(f'Results/{target}/BestModelSummary.html', 'w') as f:\n",
    "\n",
    "    bestModel.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'\n",
    "\n",
    "# import pydot\n",
    "# import pydotplus\n",
    "# import graphviz\n",
    "\n",
    "# tf.keras.utils.plot_model(\n",
    "#     bestModel,\n",
    "#     to_file=\"Results/BestModel.png\",\n",
    "#     show_shapes=True,\n",
    "#     show_dtype=False,\n",
    "#     show_layer_names=True,\n",
    "#     rankdir=\"LR\",\n",
    "#     expand_nested=False,\n",
    "#     dpi=96,\n",
    "#     layer_range=None,\n",
    "#     show_layer_activations=True,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "#pred = model.predict(xWinVal, batch_size=128)\n",
    "\n",
    "#plt.scatter(pred, yVal)\n",
    "#pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further training on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "bestModel = tf.keras.models.load_model(f'Results/{target}/BestKerasModel1')\n",
    "#plot(model)\n",
    "\n",
    "\n",
    "bestModel = tf.keras.Sequential()\n",
    "bestModel.add(tf.keras.layers.Dense(units=5, activation=\"relu\"))\n",
    "bestModel.add(tf.keras.layers.Dropout(0.3))\n",
    "bestModel.add(tf.keras.layers.Dense(units=5, activation=\"relu\"))\n",
    "bestModel.add(tf.keras.layers.Dropout(0.3))\n",
    "bestModel.add(tf.keras.layers.Dense(units=5, activation=\"relu\"))\n",
    "bestModel.add(tf.keras.layers.Dropout(0.3))\n",
    "bestModel.add(tf.keras.layers.Dense(units=5, activation=\"relu\"))\n",
    "bestModel.add(tf.keras.layers.Dropout(0.3))\n",
    "bestModel.add(tf.keras.layers.Dense(units=1))\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n",
    "\n",
    "\n",
    "bestModel.reset_states()\n",
    "\n",
    "\n",
    "compiledBestModel = bestModel.compile(optimizer=\"RMSprop\",  # Adam(learning_rate=0.0001) #RMSprop #sgd\n",
    "                                      loss=\"MeanSquaredError\",  # 'tf.keras.losses.MeanSquaredError()',\n",
    "                                      metrics=['MeanAbsoluteError'])\n",
    "\n",
    "bestModel.reset_states()\n",
    "\n",
    "\n",
    "\n",
    "#xTrainValiPooled = scaled_X[:validationSize,:]\n",
    "xTrainValiPooled = X.loc[X[\"Split\"]==\"Validation\", :]\n",
    "#yTrainValiPooled = Y[:validationSize]\n",
    "yTrainValiPooled = Y[Y[\"Split\"]==\"Validation\"]\n",
    "history = bestModel.fit(x=xTrainValiPooled, y=yTrainValiPooled, batch_size=32,\n",
    "                        epochs=300, verbose=2, validation_data=(xTest, yTest), callbacks = [callback])\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime(\"%d-%m-%Y_%H%M\")\n",
    "learningCurvePath = \"Results/LearningCurve_\" + str(now) + \".png\"\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "#plt.savefig(learningCurvePath)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 200 Complete [00h 00m 10s]\n",
      "val_mean_absolute_error: 15.432453155517578\n",
      "\n",
      "Best val_mean_absolute_error So Far: 15.431215286254883\n",
      "Total elapsed time: 01h 00m 30s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "NN3Layer\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "\n",
    "        optimizer = hp.Choice(\"optimizer\", [\"Adam\", \"RMSprop\"], default=\"Adam\")\n",
    "        #learning_rate = hp.Float(\"learning_rate\", min_value=0.00001, max_value=0.01, sampling =\"reverse_log\")\n",
    "        learning_rate = hp.Choice(\"learning_rate\", [0.005, 0.001, 0.0005], ordered=True, default=0.001)\n",
    "        # L1 = 0.0\n",
    "        # L2 = 0.0\n",
    "        # modelType = hp.Choice(\"Model_type\", [\"StandardLinear\", \"RegularizedLinear\", \"FeedForward\"])\n",
    "        # with hp.conditional_scope(\"Model_type\", [\"RegularizedLinear\", \"FeedForward\"]):\n",
    "        #     if modelType == \"FeedForward\" or modelType == \"RegularizedLinear\":\n",
    "        #         # L1 = hp.Float(\"L1\", min_value=0.00001, max_value=10, sampling = \"log\")\n",
    "        #         # L2 = hp.Float(\"L2\", min_value=0.00001, max_value=10, sampling = \"log\")\n",
    "        #         L1 = hp.Choice(\"L1\", (10**np.arange(start=2, stop=-6, step=-1.0)).tolist())\n",
    "        #         L2 = hp.Choice(\"L2\", (10**np.arange(start=2, stop=-6, step=-1.0)).tolist())\n",
    "\n",
    "        # with hp.conditional_scope(\"Model_type\", [\"FeedForward\"]):\n",
    "        #     if modelType == \"FeedForward\":\n",
    "        #         #layers = hp.Int(\"Layers\", min_value=1, max_value=5, sampling = \"log\")\n",
    "        #         layers = hp.Int(\"Layers\", min_value=1, max_value=5)\n",
    "        #         activation = hp.Choice(\"Activation\", [\"linear\", \"relu\", \"sigmoid\", \"tanh\"])\n",
    "        #         dropout = hp.Float(\"Dropout\", min_value=0, max_value=0.99)\n",
    "\n",
    "        #         #layers = hp.Choice(\"units\", (2**np.arange(7)).tolist())\n",
    "        #         #units = hp.Choice(\"units\", (2**np.arange(7)).tolist())\n",
    "        #         #units = hp.Int(\"Units\", min_value=1, max_value=2**8, sampling = \"log\")\n",
    "\n",
    "        #         units = []\n",
    "        #         for n in range(0, layers):\n",
    "        #             with hp.conditional_scope(parent_name=\"Layers\", parent_values=list(range(n+1, layers+1))):\n",
    "        #                 #units_layer = hp.Int(\"Units_Layer\" + str(n+1), min_value=1, max_value=2**8, sampling = \"log\")\n",
    "        #                 units_layer = hp.Int(\"Units_Layer\" + str(n+1), min_value=1, max_value=2**7)\n",
    "        #                 #print(\"Hey! \" + str(n))\n",
    "        #                 #units_layer = hp.Choice(\"Units_Layer\" + str(n+1), (2**np.arange(7)).tolist())\n",
    "        #                 units = units + [units_layer]\n",
    "        #                 #print(\"Hey! \" + str(units_layer))\n",
    "\n",
    "\n",
    "        L1 = hp.Choice(\"L1\", (10**np.arange(start=2, stop=-6, step=-1.0)).tolist() + [0.0], default=0.0)\n",
    "        L2 = hp.Choice(\"L2\", (10**np.arange(start=2, stop=-6, step=-1.0)).tolist() + [0.0], default=0.0)\n",
    "        max_layers = 5\n",
    "        layers = hp.Int(\"Layers\", min_value=1, max_value=max_layers, default=1)\n",
    "        \n",
    "        if layers > 0: \n",
    "            modelType = \"FeedForward\"\n",
    "        elif L1 > 0.0 and L2 > 0.0:\n",
    "            modelType = \"RegularizedLinear\"\n",
    "        else:\n",
    "            modelType = \"StandardLinear\"\n",
    "\n",
    "\n",
    "        with hp.conditional_scope(\"Layers\", list(range(1, max_layers+1))):\n",
    "            if layers > 0:\n",
    "                activation = hp.Choice(\"Activation\", [\"linear\", \"relu\", \"sigmoid\", \"tanh\"], default=\"relu\")\n",
    "                dropout = hp.Float(\"Dropout\", min_value=0, max_value=0.99, default=0)\n",
    "\n",
    "        # units = []\n",
    "        # #activation = []\n",
    "        # dropout = []\n",
    "        # for m in range(0, layers):\n",
    "        #     with hp.conditional_scope(parent_name=\"Layers\", parent_values=list(range(m+1, layers+1))):\n",
    "        #         units_layer = hp.Choice(\"Units_Layer\" + str(m+1), (2**np.arange(7)).tolist(), default=1)\n",
    "        #         units = units + [units_layer]\n",
    "\n",
    "        #         #activation_layer = hp.Choice(\"Activation\" + str(m+1), [\"linear\", \"relu\", \"sigmoid\", \"tanh\"])\n",
    "        #         #activation = activation + [activation_layer]\n",
    "\n",
    "        #         dropout_layer = hp.Float(\"Dropout\" + str(m+1), min_value=0, max_value=0.99)\n",
    "        #         dropout = dropout + [dropout_layer]\n",
    "\n",
    "\n",
    "        if layers == 3:\n",
    "            units = [64, 32, 16]\n",
    "            dropout = [dropout, dropout, dropout]\n",
    "        elif layers == 4:\n",
    "            units = [64, 32, 16, 8]\n",
    "            dropout = [dropout, dropout, dropout, dropout]\n",
    "        else:\n",
    "            print(\"What!?\")\n",
    "\n",
    "\n",
    "        finalActivation = \"linear\"\n",
    "        loss = \"MeanAbsoluteError\"   # 'tf.keras.losses.MeanSquaredError()'  #Huber #MeanAbsoluteError #MeanSquaredError \n",
    "        #loss = \"MeanSquaredError\"   # 'tf.keras.losses.MeanSquaredError()'  #Huber #MeanAbsoluteError #MeanSquaredError \n",
    "        metrics = [\"MeanAbsoluteError\", \"MeanSquaredError\"] #, \"RootMeanSquaredError\"]) #accuracy #MeanSquaredLogarithmicError #RootMeanSquaredError #MeanAbsolutePercentageError \n",
    "        EarlyStopping_monitor = \"val_mean_absolute_error\"\n",
    "\n",
    "        if classification:\n",
    "            finalActivation = \"softmax\"\n",
    "            loss = \"categorical_crossentropy\"\n",
    "            metrics = [\"accuracy\"]\n",
    "            EarlyStopping_monitor = \"val_loss\"\n",
    "        \n",
    "        # Adam(learning_rate=0.0001) #RMSprop #sgd\n",
    "        if optimizer == \"RMSprop\":\n",
    "            optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "        elif optimizer == \"Adam\":\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        else:\n",
    "            optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "        #Model creation\n",
    "        input = tf.keras.layers.Input(shape=(xTrain.shape[1],))\n",
    " \n",
    "        if modelType == \"FeedForward\":\n",
    "            try:\n",
    "                hidden = tf.keras.layers.Dense(units=units[0],\n",
    "                                activation=activation,\n",
    "                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=L1, l2=L2))(input)\n",
    "                hidden = tf.keras.layers.Dropout(rate=dropout[0])(hidden)\n",
    "                for n in range(0, layers):\n",
    "                    hidden = tf.keras.layers.Dense(units=units[n],\n",
    "                                activation=activation,\n",
    "                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=L1, l2=L2))(hidden)\n",
    "                    hidden = tf.keras.layers.Dropout(rate=dropout[n])(hidden)\n",
    "                print(f\"NN{layers}Layer\")\n",
    "\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                print(\"NN1Layer_WhyDidWeEndUpHere?\")\n",
    "\n",
    "\n",
    "        outputLayer = tf.keras.layers.Dense(units=yTrain.shape[1], activation=finalActivation,\n",
    "                                            kernel_regularizer=tf.keras.regularizers.L1L2(l1=L1, l2=L2))\n",
    "\n",
    "        if modelType == \"RegularizedLinear\" or modelType == \"StandardLinear\":\n",
    "            output = outputLayer(input)\n",
    "            print(\"Linear\")\n",
    "        else:\n",
    "            output = outputLayer(hidden)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=[input], outputs=output, name=f\"modelType_{runningDate}\")\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                    loss=loss,\n",
    "                    metrics=metrics)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        \n",
    "        performCV = False\n",
    "        #Implement cross-validation here\n",
    "\n",
    "        #tensorboardLogPath = f\"Results//NN_KerasTuner_ResultsDir//TensorBoard_{runningDate}\"\n",
    "        #tensorboardLogPath = f\"Results/logs/{runningDate}/{modelType}_{layers}L_{units}U_{activation}_{dropout}DO_{L1}L1_{L2}L2_{batch_size}Batch_{optimizer._name}_{learning_rate}LR\"\n",
    "        \n",
    "        callbacks = kwargs.pop('callbacks')\n",
    "\n",
    "        return model.fit(\n",
    "            x=xTrain, y=yTrain,\n",
    "            batch_size=hp.Choice(\"batch_size\", (2**np.arange(4, 10)).tolist(), default=32), \n",
    "            epochs = epochs,\n",
    "            validation_data=(xVal, yVal),\n",
    "            steps_per_epoch=500,\n",
    "            #steps_per_epoch=hp.Choice(\"batch_size\", (2**np.arange(4, 10)).tolist())\n",
    "            callbacks = callbacks\n",
    "            # callbacks=[tf.keras.callbacks.EarlyStopping(monitor=EarlyStopping_monitor, patience=patience),\n",
    "            #    tf.keras.callbacks.TensorBoard(log_dir=tensorboardLogPath)]\n",
    "        )\n",
    "\n",
    "\n",
    "objective = \"val_mean_absolute_error\"\n",
    "EarlyStopping_monitor = \"val_mean_absolute_error\"\n",
    "\n",
    "# objective = \"val_mean_squared_error\"\n",
    "# EarlyStopping_monitor = \"val_mean_squared_error\"\n",
    "\n",
    "\n",
    "if classification:\n",
    "    objective = \"val_accuracy\"\n",
    "    EarlyStopping_monitor = \"val_loss\"\n",
    "\n",
    "epochs = 300\n",
    "patience = 1 #5\n",
    "verbose = 1\n",
    "\n",
    "\n",
    "seed=190\n",
    "trials:int = 200\n",
    "\n",
    "\n",
    "hp = kt.HyperParameters()\n",
    "# This will override the parameters with your own selection of choices\n",
    "#hp.Int(\"Layers\", min_value=3, max_value=4)\n",
    "hp.Choice(\"Layers\", [3, 4])\n",
    "#hp.Float(\"learning_rate\", min_value=0.000005, max_value=0.01, sampling=\"log\")\n",
    "#hp.Float(\"learning_rate\", min_value=0.000001, max_value=0.01)\n",
    "hp.Choice(\"learning_rate\", [0.001, 0.01])\n",
    "#hp.Choice(\"L1\", (10**np.arange(start=2, stop=-6, step=-1.0)).tolist() + [0.0], default=0.0)\n",
    "#hp.Float(\"L1\", min_value=0, max_value=100)\n",
    "hp.Float(\"L1\", min_value=0.00001, max_value=0.001)\n",
    "hp.Float(\"Dropout\", min_value=0, max_value=0.99, default=0)\n",
    "#hp.Fixed(\"batch_size\", 10000)\n",
    "hp.Choice(\"batch_size\", [32, 64, 128, 256])\n",
    "\n",
    "from datetime import datetime\n",
    "runningDate = datetime.now().strftime(\"%d-%m-%Y_%H%M\") #comment out to continue from previous\n",
    "\n",
    "# tuner = kt.RandomSearch(\n",
    "#     MyHyperModel(),\n",
    "#     objective=objective,\n",
    "#     max_trials=trials,\n",
    "#     overwrite=True,\n",
    "#     seed=seed,\n",
    "#     hyperparameters=hp,\n",
    "#     tune_new_entries=False,\n",
    "#     # allow_new_entries=True,\n",
    "#     directory=\"Results//NN_KerasTuner_ResultsDir\",\n",
    "#     project_name=f\"tune_hypermodel_{runningDate}\"\n",
    "# )\n",
    "\n",
    "#keras_tuner.SklearnTuner(oracle, hypermodel, scoring=None, metrics=None, cv=None, **kwargs)\n",
    "\n",
    "# tuner = kt.Hyperband(\n",
    "#     hypermodel=MyHyperModel(),\n",
    "#     objective=objective,\n",
    "#     overwrite=True,\n",
    "#     seed=seed,\n",
    "#     hyperparameters=None,\n",
    "#     tune_new_entries=True,\n",
    "#     allow_new_entries=True,\n",
    "#     directory=\"Results//NN_KerasTuner_ResultsDir\",\n",
    "#     project_name=f\"tune_hypermodel_{runningDate}\"\n",
    "# )\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel=MyHyperModel(),\n",
    "    objective=objective,\n",
    "    max_trials=trials,\n",
    "    #overwrite=True,\n",
    "    #num_initial_points=2,\n",
    "    #alpha=0.0001,\n",
    "    #beta=2.6,\n",
    "    seed=seed,\n",
    "    hyperparameters=hp,\n",
    "    tune_new_entries=False,\n",
    "    #allow_new_entries=True,\n",
    "    directory=\"Results//NN_KerasTuner_ResultsDir\",\n",
    "    project_name=f\"tune_hypermodel_{runningDate}_v3\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "tensorboardLogPath = f\"Results//NN_KerasTuner_ResultsDir//tune_hypermodel_{runningDate}//TensorBoard\"\n",
    "tuner.search(\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=EarlyStopping_monitor, patience=patience)]#, tf.keras.callbacks.TensorBoard(log_dir=tensorboardLogPath)]\n",
    ")\n",
    "\n",
    "bestHPs = tuner.get_best_hyperparameters(num_trials=trials)\n",
    "bestHPs[0].values\n",
    "\n",
    "bestModel = tuner.get_best_models(num_models=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00453%'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(round((1 - 15.431215286254883/zeroForecastMAE)*100, 5)) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in Results//NN_KerasTuner_ResultsDir\\tune_hypermodel_23-01-2023_0129_v3\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x0000023C02550070>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Layers: 3\n",
      "learning_rate: 0.01\n",
      "L1: 0.00048676815782853344\n",
      "Dropout: 0.99\n",
      "batch_size: 32\n",
      "optimizer: Adam\n",
      "L2: 0.0\n",
      "Activation: relu\n",
      "Score: 15.431215286254883\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Layers: 3\n",
      "learning_rate: 0.01\n",
      "L1: 0.001\n",
      "Dropout: 0.99\n",
      "batch_size: 32\n",
      "optimizer: Adam\n",
      "L2: 0.0\n",
      "Activation: relu\n",
      "Score: 15.431517601013184\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Layers: 3\n",
      "learning_rate: 0.001\n",
      "L1: 1e-05\n",
      "Dropout: 0.99\n",
      "batch_size: 128\n",
      "optimizer: Adam\n",
      "L2: 0.0\n",
      "Activation: relu\n",
      "Score: 15.43159294128418\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Layers: 3\n",
      "learning_rate: 0.001\n",
      "L1: 0.001\n",
      "Dropout: 0.99\n",
      "batch_size: 128\n",
      "optimizer: Adam\n",
      "L2: 0.0\n",
      "Activation: relu\n",
      "Score: 15.431756019592285\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Layers: 3\n",
      "learning_rate: 0.001\n",
      "L1: 0.001\n",
      "Dropout: 0.99\n",
      "batch_size: 128\n",
      "optimizer: Adam\n",
      "L2: 0.0\n",
      "Activation: relu\n",
      "Score: 15.431771278381348\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Layers: 3\n",
      "learning_rate: 0.001\n",
      "L1: 1e-05\n",
      "Dropout: 0.99\n",
      "batch_size: 128\n",
      "optimizer: Adam\n",
      "L2: 0.0\n",
      "Activation: relu\n",
      "Score: 15.431771278381348\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Layers: 3\n",
      "learning_rate: 0.001\n",
      "L1: 1e-05\n",
      "Dropout: 0.99\n",
      "batch_size: 128\n",
      "optimizer: Adam\n",
      "L2: 0.0\n",
      "Activation: relu\n",
      "Score: 15.431832313537598\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Layers: 3\n",
      "learning_rate: 0.001\n",
      "L1: 0.001\n",
      "Dropout: 0.99\n",
      "batch_size: 128\n",
      "optimizer: Adam\n",
      "L2: 0.0\n",
      "Activation: relu\n",
      "Score: 15.43185043334961\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Layers: 3\n",
      "learning_rate: 0.001\n",
      "L1: 0.001\n",
      "Dropout: 0.99\n",
      "batch_size: 128\n",
      "optimizer: Adam\n",
      "L2: 0.0\n",
      "Activation: relu\n",
      "Score: 15.431854248046875\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Layers: 3\n",
      "learning_rate: 0.001\n",
      "L1: 1e-05\n",
      "Dropout: 0.99\n",
      "batch_size: 128\n",
      "optimizer: Adam\n",
      "L2: 0.0\n",
      "Activation: relu\n",
      "Score: 15.43186092376709\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tensorboardLogPath\n",
    "\n",
    "#http://localhost:6006/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ynew = model.predict_classes(Xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Geron 2019, 76, 320)\n",
    "\n",
    "#from sklearn import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "#(Geron 2019, 76)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "scoring='neg_mean_squared_error',\n",
    "return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "\n",
    "\n",
    "grid_search.best_params_\n",
    "\n",
    "grid_search.best_estimator_\n",
    "\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#(Geron 2019, 320)\n",
    "\n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "\n",
    "\n",
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "validation_data=(X_valid, y_valid),\n",
    "callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "validation_data=(X_valid, y_valid),\n",
    "callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "\n",
    "\n",
    "rnd_search_cv.best_params_\n",
    "rnd_search_cv.best_score_\n",
    "model = rnd_search_cv.best_estimator_.model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Chollet: DL for Python\n",
    "import kerastuner as kt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#If your search process crashes, you can always restart it—just specify overwrite=False in the tuner so that it can resume from the trial logs stored on disk.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6646144ec7618d86cefdfa307b5e5ba5f5893ee78adb19daf28ba13bcfecfb8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
