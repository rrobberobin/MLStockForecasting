{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(\"Num GPUs Available: \", len(physical_devices))\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#data_file_path = os.path.join(os.path.dirname(__file__), \"YX.csv\")\n",
    "data_file_path = os.path.join(os.getcwd(), \"Data\\YXWithSplit.csv\")\n",
    "#data_file_path = os.path.join(os.getcwd(), \"Compustat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>returns</th>\n",
       "      <th>MktCap</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>fdateq</th>\n",
       "      <th>datadate</th>\n",
       "      <th>datacqtr</th>\n",
       "      <th>curcdq</th>\n",
       "      <th>accdq</th>\n",
       "      <th>acoq</th>\n",
       "      <th>acoxq</th>\n",
       "      <th>...</th>\n",
       "      <th>fyrc</th>\n",
       "      <th>ggroup</th>\n",
       "      <th>gind</th>\n",
       "      <th>gsector</th>\n",
       "      <th>gsubind</th>\n",
       "      <th>idbflag</th>\n",
       "      <th>loc</th>\n",
       "      <th>naics</th>\n",
       "      <th>sic</th>\n",
       "      <th>Split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10.098212</td>\n",
       "      <td>1.077633e+09</td>\n",
       "      <td>1166</td>\n",
       "      <td>20100503</td>\n",
       "      <td>20100803</td>\n",
       "      <td>2010.00</td>\n",
       "      <td>EUR</td>\n",
       "      <td>67.0350</td>\n",
       "      <td>47.3020</td>\n",
       "      <td>47.3020</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>4530</td>\n",
       "      <td>453010</td>\n",
       "      <td>45</td>\n",
       "      <td>45301010</td>\n",
       "      <td>B</td>\n",
       "      <td>NLD</td>\n",
       "      <td>333242</td>\n",
       "      <td>3559</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.358543</td>\n",
       "      <td>9.688111e+08</td>\n",
       "      <td>1166</td>\n",
       "      <td>20100907</td>\n",
       "      <td>20101103</td>\n",
       "      <td>2010.25</td>\n",
       "      <td>EUR</td>\n",
       "      <td>113.1330</td>\n",
       "      <td>55.8040</td>\n",
       "      <td>55.8040</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>4530</td>\n",
       "      <td>453010</td>\n",
       "      <td>45</td>\n",
       "      <td>45301010</td>\n",
       "      <td>B</td>\n",
       "      <td>NLD</td>\n",
       "      <td>333242</td>\n",
       "      <td>3559</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.413472</td>\n",
       "      <td>1.514819e+09</td>\n",
       "      <td>1166</td>\n",
       "      <td>20101105</td>\n",
       "      <td>20110203</td>\n",
       "      <td>2010.50</td>\n",
       "      <td>EUR</td>\n",
       "      <td>87.2820</td>\n",
       "      <td>70.5220</td>\n",
       "      <td>70.5220</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>4530</td>\n",
       "      <td>453010</td>\n",
       "      <td>45</td>\n",
       "      <td>45301010</td>\n",
       "      <td>B</td>\n",
       "      <td>NLD</td>\n",
       "      <td>333242</td>\n",
       "      <td>3559</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-28.336876</td>\n",
       "      <td>1.611971e+09</td>\n",
       "      <td>1166</td>\n",
       "      <td>20110330</td>\n",
       "      <td>20110503</td>\n",
       "      <td>2010.75</td>\n",
       "      <td>EUR</td>\n",
       "      <td>63.8670</td>\n",
       "      <td>67.9700</td>\n",
       "      <td>67.9700</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>4530</td>\n",
       "      <td>453010</td>\n",
       "      <td>45</td>\n",
       "      <td>45301010</td>\n",
       "      <td>B</td>\n",
       "      <td>NLD</td>\n",
       "      <td>333242</td>\n",
       "      <td>3559</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.981888</td>\n",
       "      <td>1.138692e+09</td>\n",
       "      <td>1166</td>\n",
       "      <td>20110811</td>\n",
       "      <td>20111103</td>\n",
       "      <td>2011.25</td>\n",
       "      <td>EUR</td>\n",
       "      <td>284.1220</td>\n",
       "      <td>82.1470</td>\n",
       "      <td>82.1470</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>4530</td>\n",
       "      <td>453010</td>\n",
       "      <td>45</td>\n",
       "      <td>45301010</td>\n",
       "      <td>B</td>\n",
       "      <td>NLD</td>\n",
       "      <td>333242</td>\n",
       "      <td>3559</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57111</th>\n",
       "      <td>-15.275744</td>\n",
       "      <td>3.187034e+09</td>\n",
       "      <td>352058</td>\n",
       "      <td>20220317</td>\n",
       "      <td>20220503</td>\n",
       "      <td>2020.50</td>\n",
       "      <td>TRY</td>\n",
       "      <td>80.4916</td>\n",
       "      <td>77.7721</td>\n",
       "      <td>73.4729</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>1510</td>\n",
       "      <td>151040</td>\n",
       "      <td>15</td>\n",
       "      <td>15104050</td>\n",
       "      <td>I</td>\n",
       "      <td>TUR</td>\n",
       "      <td>331210</td>\n",
       "      <td>3317</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57112</th>\n",
       "      <td>17.064187</td>\n",
       "      <td>4.974126e+09</td>\n",
       "      <td>352242</td>\n",
       "      <td>20220218</td>\n",
       "      <td>20220503</td>\n",
       "      <td>2020.75</td>\n",
       "      <td>CNY</td>\n",
       "      <td>24.4102</td>\n",
       "      <td>22.6773</td>\n",
       "      <td>8.7507</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>1510</td>\n",
       "      <td>151010</td>\n",
       "      <td>15</td>\n",
       "      <td>15101050</td>\n",
       "      <td>I</td>\n",
       "      <td>CHN</td>\n",
       "      <td>325180</td>\n",
       "      <td>2810</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57113</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.137380e+10</td>\n",
       "      <td>352246</td>\n",
       "      <td>20220302</td>\n",
       "      <td>20220503</td>\n",
       "      <td>2021.25</td>\n",
       "      <td>CNY</td>\n",
       "      <td>403.2107</td>\n",
       "      <td>40.6519</td>\n",
       "      <td>40.6519</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "      <td>201060</td>\n",
       "      <td>20</td>\n",
       "      <td>20106020</td>\n",
       "      <td>I</td>\n",
       "      <td>CHN</td>\n",
       "      <td>333249</td>\n",
       "      <td>3559</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57114</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.547266e+10</td>\n",
       "      <td>352430</td>\n",
       "      <td>20220302</td>\n",
       "      <td>20220503</td>\n",
       "      <td>2020.50</td>\n",
       "      <td>TWD</td>\n",
       "      <td>122.9320</td>\n",
       "      <td>155.1190</td>\n",
       "      <td>93.0920</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "      <td>201010</td>\n",
       "      <td>20</td>\n",
       "      <td>20101010</td>\n",
       "      <td>I</td>\n",
       "      <td>TWN</td>\n",
       "      <td>488190</td>\n",
       "      <td>4581</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57115</th>\n",
       "      <td>-34.375000</td>\n",
       "      <td>8.002705e+09</td>\n",
       "      <td>352678</td>\n",
       "      <td>20220331</td>\n",
       "      <td>20220503</td>\n",
       "      <td>2021.25</td>\n",
       "      <td>CNY</td>\n",
       "      <td>321.4160</td>\n",
       "      <td>97.8940</td>\n",
       "      <td>32.1710</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3510</td>\n",
       "      <td>351020</td>\n",
       "      <td>35</td>\n",
       "      <td>35102015</td>\n",
       "      <td>I</td>\n",
       "      <td>CHN</td>\n",
       "      <td>621210</td>\n",
       "      <td>8000</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57116 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         returns        MktCap   gvkey    fdateq  datadate  datacqtr curcdq  \\\n",
       "0     -10.098212  1.077633e+09    1166  20100503  20100803   2010.00    EUR   \n",
       "1      56.358543  9.688111e+08    1166  20100907  20101103   2010.25    EUR   \n",
       "2       6.413472  1.514819e+09    1166  20101105  20110203   2010.50    EUR   \n",
       "3     -28.336876  1.611971e+09    1166  20110330  20110503   2010.75    EUR   \n",
       "4      30.981888  1.138692e+09    1166  20110811  20111103   2011.25    EUR   \n",
       "...          ...           ...     ...       ...       ...       ...    ...   \n",
       "57111 -15.275744  3.187034e+09  352058  20220317  20220503   2020.50    TRY   \n",
       "57112  17.064187  4.974126e+09  352242  20220218  20220503   2020.75    CNY   \n",
       "57113   0.000000  2.137380e+10  352246  20220302  20220503   2021.25    CNY   \n",
       "57114   0.000000  2.547266e+10  352430  20220302  20220503   2020.50    TWD   \n",
       "57115 -34.375000  8.002705e+09  352678  20220331  20220503   2021.25    CNY   \n",
       "\n",
       "          accdq      acoq    acoxq  ...  fyrc  ggroup    gind  gsector  \\\n",
       "0       67.0350   47.3020  47.3020  ...    12    4530  453010       45   \n",
       "1      113.1330   55.8040  55.8040  ...    12    4530  453010       45   \n",
       "2       87.2820   70.5220  70.5220  ...    12    4530  453010       45   \n",
       "3       63.8670   67.9700  67.9700  ...    12    4530  453010       45   \n",
       "4      284.1220   82.1470  82.1470  ...    12    4530  453010       45   \n",
       "...         ...       ...      ...  ...   ...     ...     ...      ...   \n",
       "57111   80.4916   77.7721  73.4729  ...    12    1510  151040       15   \n",
       "57112   24.4102   22.6773   8.7507  ...    12    1510  151010       15   \n",
       "57113  403.2107   40.6519  40.6519  ...    12    2010  201060       20   \n",
       "57114  122.9320  155.1190  93.0920  ...    12    2010  201010       20   \n",
       "57115  321.4160   97.8940  32.1710  ...     3    3510  351020       35   \n",
       "\n",
       "        gsubind  idbflag  loc   naics   sic  Split  \n",
       "0      45301010        B  NLD  333242  3559  Train  \n",
       "1      45301010        B  NLD  333242  3559  Train  \n",
       "2      45301010        B  NLD  333242  3559  Train  \n",
       "3      45301010        B  NLD  333242  3559  Train  \n",
       "4      45301010        B  NLD  333242  3559  Train  \n",
       "...         ...      ...  ...     ...   ...    ...  \n",
       "57111  15104050        I  TUR  331210  3317   Test  \n",
       "57112  15101050        I  CHN  325180  2810   Test  \n",
       "57113  20106020        I  CHN  333249  3559   Test  \n",
       "57114  20101010        I  TWN  488190  4581   Test  \n",
       "57115  35102015        I  CHN  621210  8000   Test  \n",
       "\n",
       "[57116 rows x 105 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "RData = pd.read_csv(data_file_path)\n",
    "finalData=RData\n",
    "finalData\n",
    "\n",
    "\n",
    "# finalData = dataset.map(..., num_parallel_calls=10)\n",
    "# finalData = dataset.prefetch(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe\n",
    "# finalData = dask.dataframe.read_csv(data_file_path)\n",
    "# finalData\n",
    "finalData.dtypes\n",
    "\n",
    "\n",
    "testDF = pd.DataFrame({\n",
    "    \"A\": [1, 2],\n",
    "    \"B\": [6, 7]\n",
    "}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainY = finalData[\"returns\"][0:20].compute()\n",
    "# trainY\n",
    "# trainX = finalData.loc[:, finalData.columns != \"returns\"]\n",
    "# trainX\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "factorCols = [\"gvkey\", \"datacqtr\", \"sic\", \"naics\", \"loc\", \"idbflag\",\n",
    "              \"gsubind\", \"ggroup\", \"gind\", \"gsector\",\n",
    "              \"fic\", \"city\", \"fyrc\",\n",
    "              \"curcdq\", \"exchg\", \"Split\"]\n",
    "\n",
    "finalData[factorCols] = finalData[factorCols].astype(\"str\")\n",
    "\n",
    "\n",
    "nonNumCols = [\"returns\", \"MktCap\", \"fdateq\", \"datacqtr\", \"datadate\"] + factorCols\n",
    "numCols     = finalData.columns.difference(nonNumCols)\n",
    "\n",
    "\n",
    "finalData[\"fdateq\"] = pd.to_datetime(finalData[\"fdateq\"], format = \"%Y%m%d\")\n",
    "finalData[\"datadate\"] = pd.to_datetime(finalData[\"datadate\"], format = \"%Y%m%d\")\n",
    "# #finalData[\"fdateq\"] = pd.to_datetime(finalData[\"datacqtr\"], format = \"%Y%q\") #check quarter format\n",
    "\n",
    "\n",
    "#Currently not grouped by quarter\n",
    "finalData = finalData.sort_values(by=['datacqtr', \"fdateq\"])\n",
    "\n",
    "trainSize = round(len(finalData.index)*0.4)\n",
    "validationSize = round(len(finalData.index)*0.7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Y = finalData[\"returns\"]\n",
    "Y = np.asarray(Y).astype('float32')\n",
    "yTrain, yVal, yTest = Y[:trainSize], Y[trainSize:validationSize], Y[validationSize:]\n",
    "\n",
    "X = finalData.loc[:, finalData.columns != \"returns\"]\n",
    "\n",
    "\n",
    "\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "#X = X.dropna()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 57116 entries, 5086 to 40105\n",
      "Data columns (total 85 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   accdq     57116 non-null  float64\n",
      " 1   acoq      57116 non-null  float64\n",
      " 2   acoxq     57116 non-null  float64\n",
      " 3   actq      57116 non-null  float64\n",
      " 4   ancq      57116 non-null  float64\n",
      " 5   aoq       57116 non-null  float64\n",
      " 6   apq       57116 non-null  float64\n",
      " 7   atq       57116 non-null  float64\n",
      " 8   capsq     57116 non-null  float64\n",
      " 9   capxy     57116 non-null  float64\n",
      " 10  ceqq      57116 non-null  float64\n",
      " 11  chechy    57116 non-null  float64\n",
      " 12  cheq      57116 non-null  float64\n",
      " 13  cogsq     57116 non-null  float64\n",
      " 14  cogsy     57116 non-null  float64\n",
      " 15  cstkq     57116 non-null  float64\n",
      " 16  dfxaq     57116 non-null  float64\n",
      " 17  dfxay     57116 non-null  float64\n",
      " 18  dlcq      57116 non-null  float64\n",
      " 19  dlttq     57116 non-null  float64\n",
      " 20  dpcy      57116 non-null  float64\n",
      " 21  dpq       57116 non-null  float64\n",
      " 22  dpy       57116 non-null  float64\n",
      " 23  eqrtq     57116 non-null  float64\n",
      " 24  eroq      57116 non-null  float64\n",
      " 25  fincfy    57116 non-null  float64\n",
      " 26  fopoy     57116 non-null  float64\n",
      " 27  gpq       57116 non-null  float64\n",
      " 28  gpy       57116 non-null  float64\n",
      " 29  ibcy      57116 non-null  float64\n",
      " 30  ibmiiq    57116 non-null  float64\n",
      " 31  ibmiiy    57116 non-null  float64\n",
      " 32  ibq       57116 non-null  float64\n",
      " 33  iby       57116 non-null  float64\n",
      " 34  iditq     57116 non-null  float64\n",
      " 35  idity     57116 non-null  float64\n",
      " 36  intanq    57116 non-null  float64\n",
      " 37  invtq     57116 non-null  float64\n",
      " 38  ivaoq     57116 non-null  float64\n",
      " 39  ivncfy    57116 non-null  float64\n",
      " 40  lcoq      57116 non-null  float64\n",
      " 41  lcoxq     57116 non-null  float64\n",
      " 42  lctq      57116 non-null  float64\n",
      " 43  lltq      57116 non-null  float64\n",
      " 44  loq       57116 non-null  float64\n",
      " 45  lseq      57116 non-null  float64\n",
      " 46  ltdchy    57116 non-null  float64\n",
      " 47  ltmibq    57116 non-null  float64\n",
      " 48  ltq       57116 non-null  float64\n",
      " 49  nopioq    57116 non-null  float64\n",
      " 50  nopioy    57116 non-null  float64\n",
      " 51  nopiq     57116 non-null  float64\n",
      " 52  nopiy     57116 non-null  float64\n",
      " 53  oancfy    57116 non-null  float64\n",
      " 54  oiadpq    57116 non-null  float64\n",
      " 55  oiadpy    57116 non-null  float64\n",
      " 56  oibdpq    57116 non-null  float64\n",
      " 57  oibdpy    57116 non-null  float64\n",
      " 58  piq       57116 non-null  float64\n",
      " 59  piy       57116 non-null  float64\n",
      " 60  ppentq    57116 non-null  float64\n",
      " 61  recchy    57116 non-null  float64\n",
      " 62  reccoq    57116 non-null  float64\n",
      " 63  rectoq    57116 non-null  float64\n",
      " 64  rectq     57116 non-null  float64\n",
      " 65  rectrq    57116 non-null  float64\n",
      " 66  req       57116 non-null  float64\n",
      " 67  revtq     57116 non-null  float64\n",
      " 68  revty     57116 non-null  float64\n",
      " 69  saleq     57116 non-null  float64\n",
      " 70  saley     57116 non-null  float64\n",
      " 71  sctq      57116 non-null  float64\n",
      " 72  seqq      57116 non-null  float64\n",
      " 73  teqq      57116 non-null  float64\n",
      " 74  txtq      57116 non-null  float64\n",
      " 75  txty      57116 non-null  float64\n",
      " 76  wcapopcy  57116 non-null  float64\n",
      " 77  xintq     57116 non-null  float64\n",
      " 78  xinty     57116 non-null  float64\n",
      " 79  xoproq    57116 non-null  float64\n",
      " 80  xoproy    57116 non-null  float64\n",
      " 81  xoprq     57116 non-null  float64\n",
      " 82  xopry     57116 non-null  float64\n",
      " 83  xsgaq     57116 non-null  float64\n",
      " 84  xsgay     57116 non-null  float64\n",
      "dtypes: float64(85)\n",
      "memory usage: 37.5 MB\n"
     ]
    }
   ],
   "source": [
    "#X.dtypes\n",
    "\n",
    "#finalData.info(verbose=True)\n",
    "finalData[numCols].info(verbose=True)\n",
    "#finalData.dtypes\n",
    "#print(finalData.dtypes)\n",
    "#finalData.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>returns</th>\n",
       "      <th>MktCap</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>fdateq</th>\n",
       "      <th>datadate</th>\n",
       "      <th>datacqtr</th>\n",
       "      <th>curcdq</th>\n",
       "      <th>accdq</th>\n",
       "      <th>acoq</th>\n",
       "      <th>acoxq</th>\n",
       "      <th>...</th>\n",
       "      <th>fyrc</th>\n",
       "      <th>ggroup</th>\n",
       "      <th>gind</th>\n",
       "      <th>gsector</th>\n",
       "      <th>gsubind</th>\n",
       "      <th>idbflag</th>\n",
       "      <th>loc</th>\n",
       "      <th>naics</th>\n",
       "      <th>sic</th>\n",
       "      <th>Split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5086</th>\n",
       "      <td>15.062762</td>\n",
       "      <td>2.397360e+09</td>\n",
       "      <td>201336</td>\n",
       "      <td>2009-11-30</td>\n",
       "      <td>2010-02-03</td>\n",
       "      <td>2009.5</td>\n",
       "      <td>ARS</td>\n",
       "      <td>1.495808e-07</td>\n",
       "      <td>4.996121e-08</td>\n",
       "      <td>3.867379e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>3020</td>\n",
       "      <td>302020</td>\n",
       "      <td>30</td>\n",
       "      <td>30202010</td>\n",
       "      <td>B</td>\n",
       "      <td>ARG</td>\n",
       "      <td>999977</td>\n",
       "      <td>9997</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967</th>\n",
       "      <td>15.425532</td>\n",
       "      <td>7.305680e+09</td>\n",
       "      <td>200144</td>\n",
       "      <td>2009-12-19</td>\n",
       "      <td>2010-02-03</td>\n",
       "      <td>2009.5</td>\n",
       "      <td>ARS</td>\n",
       "      <td>1.953666e-08</td>\n",
       "      <td>3.615748e-09</td>\n",
       "      <td>2.146987e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1510</td>\n",
       "      <td>151040</td>\n",
       "      <td>15</td>\n",
       "      <td>15104010</td>\n",
       "      <td>I</td>\n",
       "      <td>ARG</td>\n",
       "      <td>331313</td>\n",
       "      <td>3334</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33538</th>\n",
       "      <td>-7.738095</td>\n",
       "      <td>3.598494e+08</td>\n",
       "      <td>278800</td>\n",
       "      <td>2010-01-23</td>\n",
       "      <td>2010-02-03</td>\n",
       "      <td>2009.5</td>\n",
       "      <td>ARS</td>\n",
       "      <td>7.727150e-08</td>\n",
       "      <td>9.710952e-08</td>\n",
       "      <td>9.609604e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>3020</td>\n",
       "      <td>302020</td>\n",
       "      <td>30</td>\n",
       "      <td>30202030</td>\n",
       "      <td>I</td>\n",
       "      <td>ARG</td>\n",
       "      <td>3114</td>\n",
       "      <td>2030</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11593</th>\n",
       "      <td>18.545366</td>\n",
       "      <td>1.272073e+09</td>\n",
       "      <td>210918</td>\n",
       "      <td>2010-03-19</td>\n",
       "      <td>2010-05-03</td>\n",
       "      <td>2009.5</td>\n",
       "      <td>EUR</td>\n",
       "      <td>2.767136e-08</td>\n",
       "      <td>2.224715e-08</td>\n",
       "      <td>1.226344e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>3020</td>\n",
       "      <td>302020</td>\n",
       "      <td>30</td>\n",
       "      <td>30202010</td>\n",
       "      <td>I</td>\n",
       "      <td>FRA</td>\n",
       "      <td>111422</td>\n",
       "      <td>100</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36061</th>\n",
       "      <td>-22.125000</td>\n",
       "      <td>1.059303e+08</td>\n",
       "      <td>282729</td>\n",
       "      <td>2010-03-21</td>\n",
       "      <td>2010-05-03</td>\n",
       "      <td>2009.5</td>\n",
       "      <td>EUR</td>\n",
       "      <td>3.382979e-07</td>\n",
       "      <td>8.050577e-08</td>\n",
       "      <td>8.050577e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>5020</td>\n",
       "      <td>502020</td>\n",
       "      <td>50</td>\n",
       "      <td>50202010</td>\n",
       "      <td>I</td>\n",
       "      <td>FRA</td>\n",
       "      <td>711211</td>\n",
       "      <td>7941</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39597</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.524995e+08</td>\n",
       "      <td>288572</td>\n",
       "      <td>2022-06-08</td>\n",
       "      <td>2022-08-03</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>PLN</td>\n",
       "      <td>1.075531e-06</td>\n",
       "      <td>1.257119e-07</td>\n",
       "      <td>1.257119e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "      <td>201030</td>\n",
       "      <td>20</td>\n",
       "      <td>20103010</td>\n",
       "      <td>I</td>\n",
       "      <td>POL</td>\n",
       "      <td>237</td>\n",
       "      <td>1600</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8334</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.370305e+12</td>\n",
       "      <td>204867</td>\n",
       "      <td>2022-06-11</td>\n",
       "      <td>2022-08-03</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>INR</td>\n",
       "      <td>5.265273e-08</td>\n",
       "      <td>2.802467e-08</td>\n",
       "      <td>2.134241e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4510</td>\n",
       "      <td>451020</td>\n",
       "      <td>45</td>\n",
       "      <td>45102010</td>\n",
       "      <td>B</td>\n",
       "      <td>IND</td>\n",
       "      <td>541512</td>\n",
       "      <td>7373</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46649</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.393536e+09</td>\n",
       "      <td>297105</td>\n",
       "      <td>2022-06-13</td>\n",
       "      <td>2022-08-03</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>TWD</td>\n",
       "      <td>8.316973e-08</td>\n",
       "      <td>5.913446e-08</td>\n",
       "      <td>9.864834e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>3510</td>\n",
       "      <td>351010</td>\n",
       "      <td>35</td>\n",
       "      <td>35101020</td>\n",
       "      <td>I</td>\n",
       "      <td>TWN</td>\n",
       "      <td>334510</td>\n",
       "      <td>3845</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50126</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.562824e+09</td>\n",
       "      <td>313975</td>\n",
       "      <td>2022-06-13</td>\n",
       "      <td>2022-08-03</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>PLN</td>\n",
       "      <td>5.151891e-08</td>\n",
       "      <td>9.112075e-07</td>\n",
       "      <td>9.112075e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>5510</td>\n",
       "      <td>551010</td>\n",
       "      <td>55</td>\n",
       "      <td>55101010</td>\n",
       "      <td>I</td>\n",
       "      <td>POL</td>\n",
       "      <td>2211</td>\n",
       "      <td>4911</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40105</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.545787e+10</td>\n",
       "      <td>289052</td>\n",
       "      <td>2022-07-20</td>\n",
       "      <td>2022-08-03</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>RUB</td>\n",
       "      <td>1.996798e-07</td>\n",
       "      <td>2.446469e-08</td>\n",
       "      <td>2.318536e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>5510</td>\n",
       "      <td>551010</td>\n",
       "      <td>55</td>\n",
       "      <td>55101010</td>\n",
       "      <td>I</td>\n",
       "      <td>RUS</td>\n",
       "      <td>2211</td>\n",
       "      <td>4911</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57116 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         returns        MktCap   gvkey     fdateq   datadate datacqtr curcdq  \\\n",
       "5086   15.062762  2.397360e+09  201336 2009-11-30 2010-02-03   2009.5    ARS   \n",
       "3967   15.425532  7.305680e+09  200144 2009-12-19 2010-02-03   2009.5    ARS   \n",
       "33538  -7.738095  3.598494e+08  278800 2010-01-23 2010-02-03   2009.5    ARS   \n",
       "11593  18.545366  1.272073e+09  210918 2010-03-19 2010-05-03   2009.5    EUR   \n",
       "36061 -22.125000  1.059303e+08  282729 2010-03-21 2010-05-03   2009.5    EUR   \n",
       "...          ...           ...     ...        ...        ...      ...    ...   \n",
       "39597   0.000000  1.524995e+08  288572 2022-06-08 2022-08-03   2022.0    PLN   \n",
       "8334    0.000000  2.370305e+12  204867 2022-06-11 2022-08-03   2022.0    INR   \n",
       "46649   0.000000  1.393536e+09  297105 2022-06-13 2022-08-03   2022.0    TWD   \n",
       "50126   0.000000  1.562824e+09  313975 2022-06-13 2022-08-03   2022.0    PLN   \n",
       "40105   0.000000  2.545787e+10  289052 2022-07-20 2022-08-03   2022.0    RUB   \n",
       "\n",
       "              accdq          acoq         acoxq  ...  fyrc  ggroup    gind  \\\n",
       "5086   1.495808e-07  4.996121e-08  3.867379e-08  ...     6    3020  302020   \n",
       "3967   1.953666e-08  3.615748e-09  2.146987e-09  ...     6    1510  151040   \n",
       "33538  7.727150e-08  9.710952e-08  9.609604e-08  ...    12    3020  302020   \n",
       "11593  2.767136e-08  2.224715e-08  1.226344e-08  ...     6    3020  302020   \n",
       "36061  3.382979e-07  8.050577e-08  8.050577e-08  ...     6    5020  502020   \n",
       "...             ...           ...           ...  ...   ...     ...     ...   \n",
       "39597  1.075531e-06  1.257119e-07  1.257119e-07  ...    12    2010  201030   \n",
       "8334   5.265273e-08  2.802467e-08  2.134241e-08  ...     3    4510  451020   \n",
       "46649  8.316973e-08  5.913446e-08  9.864834e-09  ...    12    3510  351010   \n",
       "50126  5.151891e-08  9.112075e-07  9.112075e-07  ...    12    5510  551010   \n",
       "40105  1.996798e-07  2.446469e-08  2.318536e-08  ...    12    5510  551010   \n",
       "\n",
       "       gsector   gsubind  idbflag  loc   naics   sic  Split  \n",
       "5086        30  30202010        B  ARG  999977  9997  Train  \n",
       "3967        15  15104010        I  ARG  331313  3334  Train  \n",
       "33538       30  30202030        I  ARG    3114  2030  Train  \n",
       "11593       30  30202010        I  FRA  111422   100  Train  \n",
       "36061       50  50202010        I  FRA  711211  7941  Train  \n",
       "...        ...       ...      ...  ...     ...   ...    ...  \n",
       "39597       20  20103010        I  POL     237  1600   Test  \n",
       "8334        45  45102010        B  IND  541512  7373   Test  \n",
       "46649       35  35101020        I  TWN  334510  3845   Test  \n",
       "50126       55  55101010        I  POL    2211  4911   Test  \n",
       "40105       55  55101010        I  RUS    2211  4911   Test  \n",
       "\n",
       "[57116 rows x 105 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ratio data\n",
    "#xRatioData = finalData.apply(lambda x: x/finalData[\"revtq\"])\n",
    "xRatioData = finalData\n",
    "\n",
    "xRatioData[numCols] = finalData[numCols].div(finalData[\"MktCap\"].values, axis=0)\n",
    "#xRatioData = X.div(finalData[\"MktCap\"].values, axis=0)\n",
    "#xRatioData = xRatioData.loc[:, xRatioData.columns != \"MktCap\"]\n",
    "\n",
    "\n",
    "#xRatioData = xRatioData.dropna()\n",
    "xRatioData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Winsorised data\n",
    "import scipy\n",
    "\n",
    "\n",
    "# XArray = np.asarray(xRatioData).astype('float32')\n",
    "# # numberOfVariables = XArray.shape[1]\n",
    "\n",
    "# xWinData = scipy.stats.mstats.winsorize(XArray, limits = (0.01, 0.01))\n",
    "# #yWinData = scipy.stats.mstats.winsorize(XArray, limits = (0.01, 0.01))\n",
    "\n",
    "\n",
    "xWinData=xRatioData\n",
    "\n",
    "xWinData[numCols] = xRatioData[numCols].apply(lambda x: scipy.stats.mstats.winsorize(x, limits = (0.01, 0.01)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# scaled_X = scaler.fit_transform(XArray)\n",
    "# #scaled_X = np.apply_along_axis(func1d=scaler.fit_transform, axis=1, arr=X) #Why over rows??? prob axis=0 instead\n",
    "\n",
    "# xTrain, xVal, xTest = scaled_X[:trainSize,:], scaled_X[trainSize:validationSize,:], scaled_X[validationSize:, :]\n",
    "\n",
    "# scaled_winX = scaler.fit_transform(xWinData)\n",
    "# xWinTrain, xWinVal, xWinTest = scaled_winX[:trainSize,:], scaled_winX[trainSize:validationSize,:], scaled_winX[validationSize:, :]\n",
    "\n",
    "\n",
    "xScaledData=xWinData\n",
    "#xScaledData[numCols] = xScaledData[numCols].apply(lambda x: scaler.fit_transform(x))\n",
    "xScaledData[numCols] = scaler.fit_transform(xScaledData[numCols])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#scaled_X = scaled_X.apply(lambda x : x.reshape(-1, 1))\n",
    "# droppedCols = [\"ajexdi\", \"prccd\", \"trfd\"]\n",
    "# trainX.drop(droppedCols, axis=1, inplace=True)\n",
    "# trainX['gvkey'] = trainX['gvkey'].astype(object)\n",
    "# trainX['gvkey'] = trainX['gvkey'].astype(object)\n",
    "\n",
    "#trainX.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScaledWinsRatioPath = os.path.join(os.getcwd(), \"Data\\ScaledWinsRatioData.csv\")\n",
    "\n",
    "\n",
    "# XOutputDF = pd.DataFrame(scaled_winX)\n",
    "# YOutputDF = pd.DataFrame(Y)\n",
    "\n",
    "# NNReadyDataWithLabels = XOutputDF.append(finalData[\"fdateq\"], finalData[\"datadate\"], finalData[\"datacqtr\"], YOutputDF)\n",
    "\n",
    "#np.savetxt(ScaledWinsRatioPath, xScaledData, delimiter=\",\")\n",
    "pd.DataFrame(xScaledData).to_csv(ScaledWinsRatioPath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file_path = os.path.join(os.getcwd(), \"Data\\YXWithSplit.csv\")\n",
    "# splittedData = pd.read_csv(data_file_path)\n",
    "\n",
    "# #Currently not grouped by quarter\n",
    "# splittedData = splittedData.sort_values(by=['datacqtr', \"fdateq\"])\n",
    "\n",
    "\n",
    "# # trainSize = round(len(xScaledData.columns)*0.4)\n",
    "# # validationSize = round(len(xScaledData.columns)*0.7)\n",
    "\n",
    "\n",
    "split = xScaledData[\"Split\"]\n",
    "# xWinTrain, xWinVal, xWinTest = splittedData.loc[split==\"Train\", :], splittedData.loc[split==\"Validation\", :], splittedData.loc[split==\"Test\", :]\n",
    "\n",
    "\n",
    "Y = xScaledData[\"returns\"]\n",
    "Y = np.asarray(Y).astype('float32')\n",
    "yTrain, yVal, yTest = Y[split==\"Train\"], Y[split==\"Validation\"], Y[split==\"Test\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.asarray(Y)\n",
    "yTrain, yVal, yTest = Y[split==\"Train\"], Y[split==\"Validation\"], Y[split==\"Test\"]\n",
    "\n",
    "X = xScaledData.drop([\"returns\", \"MktCap\"], axis=1)\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "xWinTrain, xWinVal, xWinTest = X.loc[split==\"Train\", :], X.loc[split==\"Validation\", :], X.loc[split==\"Test\", :]\n",
    "\n",
    "\n",
    "\n",
    "# xWinTrain, xWinVal, xWinTest = X[:trainSize, :], X[trainSize:validationSize, :], X[validationSize:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaledWinsRatioPathWithSplit = os.path.join(os.getcwd(), \"Data\\ScaledWinsRatioDataWithSplit.csv\")\n",
    "# pd.DataFrame(xScaledData).to_csv(scaledWinsRatioPathWithSplit, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# groupedY = Y.groupby([\"state\", \"gender\"])[\"last_name\"].count()\n",
    "# yTrain, yVal, yTest = Y[:trainSize], Y[trainSize:validationSize], Y[validationSize:]\n",
    "\n",
    "\n",
    "\n",
    "# groupedX = scaled_X.groupby([\"state\", \"gender\"])[\"last_name\"].count()\n",
    "# xTrain, xVal, xTest = scaled_X[:trainSize,:], scaled_X[trainSize:validationSize,:], scaled_X[validationSize:, :]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(\"Num GPUs Available: \", len(physical_devices))\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#     Dense(units=16,  activation='relu'), #input_shape=(, 10, numberOfVariables),\n",
    "#     Dense(units=32, activation='relu'),\n",
    "#     Dense(units=1)\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential()\n",
    "\n",
    "# model.add(Dense(units=10,  activation='relu'))\n",
    "# model.add(tf.keras.layers.Dropout(.5))\n",
    "# model.add(Dense(units=10, activation='relu'))\n",
    "# model.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=\"sgd\",  # Adam(learning_rate=0.0001),\n",
    "#               loss=\"MeanSquaredError\", #'tf.keras.losses.MeanSquaredError()',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x=xTrain, y=yTrain,\n",
    "#           batch_size=10, epochs=30, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_and_metrics = model.evaluate(xVal, yVal, batch_size=128)\n",
    "\n",
    "# model.predict(xVal, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NNfunction(modelType, layers, hidUnits, actvFunc, dropout, L1, L2):\n",
    "    \n",
    "    callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, verbose=1, restore_best_weights=True) #Can this be taken out from the loop? => Less computing\n",
    "\n",
    "    #tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "    #callback = tf.keras.callbacks.BackupAndRestore(backup_dir=\"/tmp/backup\")\n",
    "\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    if modelType == \"LSTM\":\n",
    "        model.add(tf.keras.layers.LSTM(units=hidUnits, input_shape=(2,)))\n",
    "    elif modelType == \"GRU\":\n",
    "        model.add(tf.keras.layers.GRU(units=hidUnits))\n",
    "    elif modelType == \"SimpleRNN\":\n",
    "        # keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "        model.add(tf.keras.layers.SimpleRNN(units=hidUnits))\n",
    "    elif modelType == \"Linear\":\n",
    "        model.add(Dense(units=1,\n",
    "                        kernel_regularizer=tf.keras.regularizers.L1L2(l1=L1, l2=L2)))   #insert lasso and ridge here\n",
    "    else:\n",
    "        try:\n",
    "            for n in range(0, layers):\n",
    "                model.add(Dense(units=hidUnits,\n",
    "                                activation=actvFunc,\n",
    "                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=L1, l2=L2)))\n",
    "                model.add(tf.keras.layers.Dropout(dropout))\n",
    "                # scaler.fit_transform\n",
    "                # tf.keras.layers.BatchNormalization(\n",
    "        except:\n",
    "            model.add(Dense(units=hidUnits, activation=actvFunc,\n",
    "                            kernel_regularizer=tf.keras.regularizers.L1L2(l1=L1, l2=L2)))\n",
    "            model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "            \n",
    "        model.add(Dense(units=1))\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(optimizer=\"RMSprop\",  # Adam(learning_rate=0.0001) #RMSprop #sgd\n",
    "                  loss=\"MeanSquaredError\",  # 'tf.keras.losses.MeanSquaredError()'  #Huber   #MeanAbsoluteError\n",
    "                  metrics=[\"MeanAbsoluteError\"]) #accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # model.fit(x=xTrain, y=yTrain,\n",
    "    #           batch_size=16, epochs=epo, verbose=2)\n",
    "\n",
    "    #loss_and_metrics = model.evaluate(xVal, yVal, batch_size=128)\n",
    "    \n",
    "\n",
    "\n",
    "    history = model.fit(x=xWinTrain, y=yTrain,\n",
    "              batch_size=32, epochs=300,\n",
    "              callbacks= [callback], verbose=2, validation_data = (xWinVal, yVal))\n",
    "\n",
    "    loss_and_metrics = model.evaluate(xWinVal, yVal, batch_size=32)\n",
    "    \n",
    "    epochs_EarlyStopping = len(history.history['loss'])\n",
    "    #history.history[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # quarters = len(yVal)\n",
    "    # CV = []\n",
    "    # for q in range(0, quarters):\n",
    "\n",
    "    #     model.fit(x=xTrain, y=yTrain,\n",
    "    #           batch_size=16, epochs=epo, verbose=2)\n",
    "\n",
    "    #     validationError = model.evaluate(xVal, yVal, batch_size=128)\n",
    "    #     CV = [CV, validationError]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        loss_and_metrics = [x for x in loss_and_metrics]\n",
    "    except:     \n",
    "        print(loss_and_metrics)\n",
    "\n",
    "    return loss_and_metrics, model, epochs_EarlyStopping\n",
    "\n",
    "    #model.predict(xVal, batch_size=128)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper-Parameters\n",
    "NNModelType = {\"Forward\"} #, \"LSTM\", \"GRU\"}\n",
    "hiddenLayers = {2, 3, 4, 5}\n",
    "hiddenUnits = {2, 3, 4, 5, 10}\n",
    "activationFunc = {\"relu\", \"sigmoid\", \"tanh\"} #etc. https://keras.io/api/layers/activations/\n",
    "dropOut = {0.5, 0.2, 0}\n",
    "#epochs = [100] #Just choose best model. => Run for 100 epochs, but if 20th epoch is the best, choose it, i.e. early stopping\n",
    "#inputSize = numberOfVariables #ncol(trainXPools[[1]])\n",
    "#other parameters: loss func, node structure, location of the regularization function\n",
    "batchSize = {8, 16, 32, 64}\n",
    "optim = {\"RMSprop\", \"Adam\", \"sgd\"}\n",
    "\n",
    "\n",
    "none = {\"NA\"}\n",
    "\n",
    "L1L2Grid = 10**np.linspace(start=3, stop=-5, num=3) #3 and -10\n",
    "L1L2Grid = set(np.append(L1L2Grid, 0))\n",
    "\n",
    "\n",
    "NNParams = {\"Model type\": NNModelType,  # , \"LSTM\", \"GRU\"},\n",
    "            \"Hidden layers\": hiddenLayers,\n",
    "            \"Hidden units\": hiddenUnits,\n",
    "            \"Activation function\": activationFunc,\n",
    "            \"Dropout\": dropOut,\n",
    "            \"L1\": L1L2Grid,\n",
    "            \"L2\": L1L2Grid\n",
    "            }\n",
    "\n",
    "LinearParams = {\"Model type\": {\"Linear\"},\n",
    "               \"Hidden layers\": none,\n",
    "               \"Hidden units\": none,\n",
    "               \"Activation function\": none,\n",
    "               \"Dropout\": none,\n",
    "               \"L1\": L1L2Grid,\n",
    "               \"L2\": L1L2Grid\n",
    "               }\n",
    "\n",
    "\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "NNGrid = pd.DataFrame(itertools.product(\n",
    "    *NNParams.values()), columns=NNParams.keys())\n",
    "\n",
    "linearGrid = pd.DataFrame(itertools.product(\n",
    "    *LinearParams.values()), columns=LinearParams.keys())\n",
    "\n",
    "\n",
    "#Random grid search\n",
    "#import random\n",
    "#random.seed(2302)\n",
    "randomNNGrid = NNGrid.sample(n=160)\n",
    "\n",
    "HPGrid = pd.concat([linearGrid, randomNNGrid])\n",
    "\n",
    "\n",
    "# hyperParams = np.concatenate((LinearParams, NNParams, RNNParams))\n",
    "#hyperParams = np.concatenate(grid, LinearParams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Epoch 1/100\n",
      "304/304 - 3s - loss: 8321.0166 - mean_absolute_error: 25.8817 - val_loss: 2029.1328 - val_mean_absolute_error: 20.3339 - 3s/epoch - 9ms/step\n",
      "Epoch 2/100\n",
      "304/304 - 2s - loss: 5329.1289 - mean_absolute_error: 25.8741 - val_loss: 2026.5406 - val_mean_absolute_error: 20.3558 - 2s/epoch - 5ms/step\n",
      "Epoch 3/100\n",
      "304/304 - 1s - loss: 5324.5527 - mean_absolute_error: 25.8774 - val_loss: 2029.7979 - val_mean_absolute_error: 20.3784 - 1s/epoch - 5ms/step\n",
      "Epoch 4/100\n",
      "304/304 - 1s - loss: 5320.1758 - mean_absolute_error: 25.8819 - val_loss: 2029.2203 - val_mean_absolute_error: 20.4040 - 1s/epoch - 5ms/step\n",
      "Epoch 5/100\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "304/304 - 1s - loss: 5316.0078 - mean_absolute_error: 25.8896 - val_loss: 2027.4626 - val_mean_absolute_error: 20.4295 - 1s/epoch - 5ms/step\n",
      "Epoch 5: early stopping\n",
      "676/676 [==============================] - 1s 2ms/step - loss: 2026.5406 - mean_absolute_error: 20.3558\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Epoch 1/100\n",
      "304/304 - 2s - loss: 8320.7852 - mean_absolute_error: 25.8718 - val_loss: 2030.3481 - val_mean_absolute_error: 20.3332 - 2s/epoch - 8ms/step\n",
      "Epoch 2/100\n",
      "304/304 - 2s - loss: 5329.3979 - mean_absolute_error: 25.8719 - val_loss: 2030.3989 - val_mean_absolute_error: 20.3554 - 2s/epoch - 5ms/step\n",
      "Epoch 3/100\n",
      "304/304 - 2s - loss: 5324.4946 - mean_absolute_error: 25.8774 - val_loss: 2028.2448 - val_mean_absolute_error: 20.3785 - 2s/epoch - 5ms/step\n",
      "Epoch 4/100\n",
      "304/304 - 2s - loss: 5320.0801 - mean_absolute_error: 25.8823 - val_loss: 2024.4487 - val_mean_absolute_error: 20.4040 - 2s/epoch - 5ms/step\n",
      "Epoch 5/100\n",
      "304/304 - 2s - loss: 5315.8647 - mean_absolute_error: 25.8896 - val_loss: 2027.4486 - val_mean_absolute_error: 20.4315 - 2s/epoch - 5ms/step\n",
      "Epoch 6/100\n",
      "304/304 - 2s - loss: 5311.4517 - mean_absolute_error: 25.8997 - val_loss: 2025.3217 - val_mean_absolute_error: 20.4585 - 2s/epoch - 5ms/step\n",
      "Epoch 7/100\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "304/304 - 2s - loss: 5307.6201 - mean_absolute_error: 25.9093 - val_loss: 2025.8706 - val_mean_absolute_error: 20.4877 - 2s/epoch - 6ms/step\n",
      "Epoch 7: early stopping\n",
      "676/676 [==============================] - 1s 2ms/step - loss: 2024.4487 - mean_absolute_error: 20.4040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model type</th>\n",
       "      <th>Hidden layers</th>\n",
       "      <th>Hidden units</th>\n",
       "      <th>Activation function</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>ModelPointer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2024.448730</td>\n",
       "      <td>20.403952</td>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2026.540649</td>\n",
       "      <td>20.355841</td>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model type Hidden layers Hidden units Activation function Dropout      L1  \\\n",
       "1     Linear            NA           NA                  NA      NA  1000.0   \n",
       "0     Linear            NA           NA                  NA      NA  1000.0   \n",
       "\n",
       "       L2  Epochs          MSE        MAE  \\\n",
       "1     0.0       7  2024.448730  20.403952   \n",
       "0  1000.0       5  2026.540649  20.355841   \n",
       "\n",
       "                                        ModelPointer  \n",
       "1  <keras.engine.sequential.Sequential object at ...  \n",
       "0  <keras.engine.sequential.Sequential object at ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSEMatrix = pd.DataFrame()\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "#validationSize = 2 #length(validXPools)\n",
    "#HPsize = len(hyperParams.index)\n",
    "HPsize = np.shape(HPGrid)[0]\n",
    "#HPsize = 2   #176 models took 40 min\n",
    "\n",
    "for n in range(0, HPsize):\n",
    "  print(\"\\n\")\n",
    "  print(\"Model \", str(n+1), \" out of \", str(HPsize))\n",
    "\n",
    "  HP = HPGrid.iloc[n]\n",
    "  CV = NNfunction(*HP)\n",
    "\n",
    "  #a = pd.DataFrame([CV])\n",
    "  newMSE = HPGrid.iloc[[n]]\n",
    "\n",
    "  MSE = CV[0][0]\n",
    "  MAE = CV[0][1]\n",
    "  Epochs = CV[2]\n",
    "  ModelPointer = CV[1]\n",
    "\n",
    "  # newMSE['MSE'] = MSE\n",
    "  # newMSE['MAE'] = MAE\n",
    "\n",
    "  newMSE.loc[:,\"Epochs\"] = Epochs\n",
    "  newMSE.loc[:,\"MSE\"] = MSE\n",
    "  newMSE.loc[:,\"MAE\"] = MAE\n",
    "  newMSE.loc[:,\"ModelPointer\"] = ModelPointer\n",
    "\n",
    "  MSEMatrix = pd.concat((MSEMatrix, newMSE), axis=0)\n",
    "  \n",
    "  # newMSE = a.join(b)\n",
    "  # newMSE2 = pd.concat([b, a])\n",
    "  #newMSE = pd.concat([b, a], axis=1, ignore_index=True, sort=False)\n",
    "  #newMSE =  pd.concat(HPGrid.iloc[[n]], CV)\n",
    "  #newMSE = np.concatenate((HP, CV))\n",
    "  #MSEMatrix = np.concatenate((MSEMatrix, newMSE))\n",
    "  #MSEMatrix = pd.concat(MSEMatrix, newMSE)\n",
    "  #MSEMatrix = pd.concat([MSEMatrix, [HP, CV]])\n",
    "\n",
    "finalResults = MSEMatrix\n",
    "\n",
    "#finalResults[:,-1] = apply(finalResults[,-1], MARGIN=2, FUN=as.numeric)\n",
    "#finalResults = finalResults[order(finalResults[,\"MSE\"]),]\n",
    "#finalResults <- apply(finalResults, 2, as.character)\n",
    "\n",
    "#write.csv2(finalResults, \"results3.csv\", row.names = FALSE)\n",
    "\n",
    "\n",
    "#pd.DataFrame.to_csv(MSEMatrix, \"resultsPython.csv\")\n",
    "\n",
    "\n",
    "#finalResults = finalResults[order(finalResults[,\"MSE\"]),]\n",
    "\n",
    "formattedMSEMatrix = finalResults.sort_values(by=[\"MSE\"], axis = 0)\n",
    "formattedMSEMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model type</th>\n",
       "      <th>Hidden layers</th>\n",
       "      <th>Hidden units</th>\n",
       "      <th>Activation function</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>L1</th>\n",
       "      <th>L2</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>ModelPointer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2024.448730</td>\n",
       "      <td>20.403952</td>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2026.540649</td>\n",
       "      <td>20.355841</td>\n",
       "      <td>&lt;keras.engine.sequential.Sequential object at ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model type Hidden layers Hidden units Activation function Dropout      L1  \\\n",
       "1     Linear            NA           NA                  NA      NA  1000.0   \n",
       "0     Linear            NA           NA                  NA      NA  1000.0   \n",
       "\n",
       "       L2  Epochs          MSE        MAE  \\\n",
       "1     0.0       7  2024.448730  20.403952   \n",
       "0  1000.0       5  2026.540649  20.355841   \n",
       "\n",
       "                                        ModelPointer  \n",
       "1  <keras.engine.sequential.Sequential object at ...  \n",
       "0  <keras.engine.sequential.Sequential object at ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formattedMSEMatrix = finalResults.sort_values(by=[\"MSE\"], axis = 0)\n",
    "formattedMSEMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "490/490 - 2s - loss: 4539.3115 - mean_absolute_error: 24.3555 - val_loss: 1551.1266 - val_mean_absolute_error: 19.9014 - 2s/epoch - 3ms/step\n",
      "Epoch 2/100\n",
      "490/490 - 2s - loss: 4535.0312 - mean_absolute_error: 24.3762 - val_loss: 1552.9523 - val_mean_absolute_error: 19.9446 - 2s/epoch - 3ms/step\n",
      "Epoch 3/100\n",
      "490/490 - 1s - loss: 4530.7168 - mean_absolute_error: 24.3981 - val_loss: 1554.1863 - val_mean_absolute_error: 19.9907 - 1s/epoch - 3ms/step\n",
      "Epoch 4/100\n",
      "490/490 - 1s - loss: 4527.0093 - mean_absolute_error: 24.4221 - val_loss: 1544.3671 - val_mean_absolute_error: 20.0385 - 1s/epoch - 3ms/step\n",
      "Epoch 5/100\n",
      "490/490 - 1s - loss: 4523.1050 - mean_absolute_error: 24.4481 - val_loss: 1553.9336 - val_mean_absolute_error: 20.0877 - 1s/epoch - 3ms/step\n",
      "Epoch 6/100\n",
      "490/490 - 1s - loss: 4519.6509 - mean_absolute_error: 24.4775 - val_loss: 1550.7220 - val_mean_absolute_error: 20.1377 - 1s/epoch - 3ms/step\n",
      "Epoch 7/100\n",
      "490/490 - 1s - loss: 4516.5430 - mean_absolute_error: 24.5046 - val_loss: 1548.8359 - val_mean_absolute_error: 20.1889 - 1s/epoch - 3ms/step\n",
      "Epoch 8/100\n",
      "490/490 - 1s - loss: 4513.4678 - mean_absolute_error: 24.5341 - val_loss: 1555.3141 - val_mean_absolute_error: 20.2418 - 1s/epoch - 3ms/step\n",
      "Epoch 9/100\n",
      "490/490 - 1s - loss: 4510.5425 - mean_absolute_error: 24.5653 - val_loss: 1555.1814 - val_mean_absolute_error: 20.2960 - 1s/epoch - 3ms/step\n",
      "Epoch 10/100\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "490/490 - 1s - loss: 4507.9307 - mean_absolute_error: 24.5965 - val_loss: 1552.4418 - val_mean_absolute_error: 20.3481 - 1s/epoch - 3ms/step\n",
      "Epoch 10: early stopping\n",
      "INFO:tensorflow:Assets written to: Results/BestKerasModel6\\assets\n"
     ]
    }
   ],
   "source": [
    "#MSEMatrix\n",
    "\n",
    "finalResults = finalResults.reset_index()\n",
    "index = finalResults['MSE'].idxmin()\n",
    "#bestParams = finalResults.iloc[index][:-3]\n",
    "#bestModel = finalResults(*bestParams)[1]\n",
    "#bestParams = finalResults[finalResults['MSE']==finalResults['MSE'].min()]\n",
    "\n",
    "\n",
    "\n",
    "# xTrainValiPooled = scaled_X[:validationSize,:]\n",
    "# yTrainValiPooled = Y[:validationSize]\n",
    "\n",
    "#xTrainValiPooled = scaled_X[:validationSize,:]\n",
    "xTrainValiPooled = X.loc[split!=\"Test\", :]\n",
    "#yTrainValiPooled = Y[:validationSize]\n",
    "yTrainValiPooled = Y[split!=\"Test\"]\n",
    "\n",
    "#bestModel = bestParams[\"ModelPointer\"]\n",
    "bestModel = finalResults.iloc[index][\"ModelPointer\"]\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=6, verbose=1, restore_best_weights=True)\n",
    "\n",
    "history = bestModel.fit(x=xTrainValiPooled, y=yTrainValiPooled,\n",
    "                        batch_size=32, epochs=100,\n",
    "                        callbacks=[callback], verbose=2, validation_split=0.5)\n",
    "\n",
    "bestModel.save('Results/BestKerasModel7')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resPath = \"C:\\\\Users\\RobinForMLThesis\\\\OneDrive - Hanken Svenska handelshogskolan\\\\Master's_Thesis\\\\DataAnalysis\\\\resultsPython.csv\"\n",
    "\n",
    "# finalResults = pd.DataFrame(MSEMatrix)\n",
    "# pd.DataFrame.to_csv(finalResults, resPath)\n",
    "# #pd.DataFrame.to_csv(finalResults, \"resultsPython.csv\")\n",
    "\n",
    "# MSEMatrix.tofile(resPath, sep = ',')\n",
    "\n",
    "# finalResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "formattedMSEMatrix = MSEMatrix.drop([\"ModelPointer\"], axis=1)\n",
    "formattedMSEMatrix = formattedMSEMatrix.sort_values(by=[\"MSE\"], axis = 0)\n",
    "#formattedMSEMatrix[[\"L1\", \"L2\"]] = round(formattedMSEMatrix[[\"L1\", \"L2\"]], 1)\n",
    "#formattedMSEMatrix = round(formattedMSEMatrix, 1)\n",
    "\n",
    "def RobRound(x): \n",
    "    return np.format_float_positional(x, precision=2, unique=False, fractional=False, trim='-')\n",
    "\n",
    "\n",
    "formattedMSEMatrix[\"L1\"] = formattedMSEMatrix[\"L1\"].apply(lambda x: RobRound(x))\n",
    "formattedMSEMatrix[\"L2\"] = formattedMSEMatrix[\"L2\"].apply(lambda x: RobRound(x))\n",
    "formattedMSEMatrix[\"MSE\"] = formattedMSEMatrix[\"MSE\"].apply(lambda x: round(x))\n",
    "formattedMSEMatrix[\"MAE\"] = formattedMSEMatrix[\"MAE\"].apply(lambda x: RobRound(x))\n",
    "\n",
    "\n",
    "#pd.DataFrame.to_csv(MSEMatrix, \"Results/ModelComparison.csv\", index=False)\n",
    "#pd.DataFrame.to_excel(MSEMatrix, \"Results/ModelComparison.xlsx\")\n",
    "pd.DataFrame.to_html(formattedMSEMatrix, \"Results/ModelComparison7.html\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  1  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2026.5413 - mean_absolute_error: 20.3558\n",
      "[2026.541259765625, 20.3558406829834]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  2  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5413 - mean_absolute_error: 20.3558\n",
      "[2026.541259765625, 20.355838775634766]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  3  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5420 - mean_absolute_error: 20.3558\n",
      "[2026.5419921875, 20.3558292388916]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  4  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5417 - mean_absolute_error: 20.3558\n",
      "[2026.541748046875, 20.355831146240234]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  5  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5421 - mean_absolute_error: 20.3558\n",
      "[2026.5421142578125, 20.355825424194336]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  6  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5427 - mean_absolute_error: 20.3558\n",
      "[2026.542724609375, 20.355823516845703]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  7  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2026.5428 - mean_absolute_error: 20.3558\n",
      "[2026.5428466796875, 20.35582160949707]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  8  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5421 - mean_absolute_error: 20.3558\n",
      "[2026.5421142578125, 20.35582733154297]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  9  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5416 - mean_absolute_error: 20.3558\n",
      "[2026.5416259765625, 20.3558292388916]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  10  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5410 - mean_absolute_error: 20.3558\n",
      "[2026.541015625, 20.355838775634766]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  11  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5403 - mean_absolute_error: 20.3558\n",
      "[2026.540283203125, 20.355838775634766]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  12  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5391 - mean_absolute_error: 20.3558\n",
      "[2026.5390625, 20.3558406829834]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  13  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2026.5391 - mean_absolute_error: 20.3558\n",
      "[2026.5390625, 20.355844497680664]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  14  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5383 - mean_absolute_error: 20.3558\n",
      "[2026.538330078125, 20.3558406829834]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  15  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2026.5375 - mean_absolute_error: 20.3558\n",
      "[2026.5374755859375, 20.35584831237793]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  16  out of  85\n",
      "169/169 [==============================] - 0s 3ms/step - loss: 2026.5371 - mean_absolute_error: 20.3559\n",
      "[2026.537109375, 20.355850219726562]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  17  out of  85\n",
      "169/169 [==============================] - 1s 2ms/step - loss: 2026.5383 - mean_absolute_error: 20.3558\n",
      "[2026.538330078125, 20.355844497680664]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  18  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5391 - mean_absolute_error: 20.3558\n",
      "[2026.5390625, 20.355838775634766]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  19  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2026.5399 - mean_absolute_error: 20.3558\n",
      "[2026.5399169921875, 20.3558292388916]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  20  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5394 - mean_absolute_error: 20.3558\n",
      "[2026.5394287109375, 20.355831146240234]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  21  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5396 - mean_absolute_error: 20.3558\n",
      "[2026.53955078125, 20.355791091918945]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  22  out of  85\n",
      "169/169 [==============================] - 0s 3ms/step - loss: 2026.5413 - mean_absolute_error: 20.3558\n",
      "[2026.541259765625, 20.35576057434082]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  23  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5413 - mean_absolute_error: 20.3558\n",
      "[2026.541259765625, 20.35576057434082]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  24  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5413 - mean_absolute_error: 20.3558\n",
      "[2026.541259765625, 20.35576057434082]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  25  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5421 - mean_absolute_error: 20.3558\n",
      "[2026.5421142578125, 20.35575294494629]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  26  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5424 - mean_absolute_error: 20.3557\n",
      "[2026.5423583984375, 20.355749130249023]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  27  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5421 - mean_absolute_error: 20.3558\n",
      "[2026.5421142578125, 20.355751037597656]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  28  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5424 - mean_absolute_error: 20.3558\n",
      "[2026.5423583984375, 20.355751037597656]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  29  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5421 - mean_absolute_error: 20.3557\n",
      "[2026.5421142578125, 20.355749130249023]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  30  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5420 - mean_absolute_error: 20.3558\n",
      "[2026.5419921875, 20.355751037597656]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  31  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5425 - mean_absolute_error: 20.3557\n",
      "[2026.54248046875, 20.35574722290039]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  32  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5425 - mean_absolute_error: 20.3557\n",
      "[2026.54248046875, 20.355749130249023]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  33  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5435 - mean_absolute_error: 20.3557\n",
      "[2026.54345703125, 20.355743408203125]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  34  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5432 - mean_absolute_error: 20.3557\n",
      "[2026.543212890625, 20.355743408203125]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  35  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5460 - mean_absolute_error: 20.3557\n",
      "[2026.5460205078125, 20.355688095092773]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  36  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5466 - mean_absolute_error: 20.3556\n",
      "[2026.546630859375, 20.35563850402832]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  37  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5459 - mean_absolute_error: 20.3557\n",
      "[2026.5458984375, 20.355653762817383]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  38  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5460 - mean_absolute_error: 20.3557\n",
      "[2026.5460205078125, 20.35565757751465]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  39  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5460 - mean_absolute_error: 20.3557\n",
      "[2026.5460205078125, 20.35565757751465]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  40  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5470 - mean_absolute_error: 20.3557\n",
      "[2026.5469970703125, 20.35565948486328]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  41  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5463 - mean_absolute_error: 20.3557\n",
      "[2026.5462646484375, 20.355653762817383]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  42  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5463 - mean_absolute_error: 20.3557\n",
      "[2026.5462646484375, 20.355655670166016]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  43  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5463 - mean_absolute_error: 20.3557\n",
      "[2026.5462646484375, 20.35565185546875]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  44  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5460 - mean_absolute_error: 20.3557\n",
      "[2026.5460205078125, 20.355663299560547]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  45  out of  85\n",
      "169/169 [==============================] - 0s 3ms/step - loss: 2026.5459 - mean_absolute_error: 20.3556\n",
      "[2026.5458984375, 20.355642318725586]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  46  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5459 - mean_absolute_error: 20.3556\n",
      "[2026.5458984375, 20.355642318725586]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  47  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5457 - mean_absolute_error: 20.3556\n",
      "[2026.545654296875, 20.35564422607422]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  48  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5466 - mean_absolute_error: 20.3556\n",
      "[2026.546630859375, 20.355642318725586]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  49  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5453 - mean_absolute_error: 20.3556\n",
      "[2026.5452880859375, 20.355648040771484]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  50  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5453 - mean_absolute_error: 20.3556\n",
      "[2026.5452880859375, 20.355649948120117]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  51  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5449 - mean_absolute_error: 20.3557\n",
      "[2026.544921875, 20.35565185546875]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  52  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5448 - mean_absolute_error: 20.3556\n",
      "[2026.5447998046875, 20.355649948120117]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  53  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5444 - mean_absolute_error: 20.3557\n",
      "[2026.54443359375, 20.35565185546875]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  54  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5441 - mean_absolute_error: 20.3557\n",
      "[2026.5440673828125, 20.355653762817383]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  55  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5444 - mean_absolute_error: 20.3557\n",
      "[2026.54443359375, 20.35565185546875]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  56  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2026.5449 - mean_absolute_error: 20.3557\n",
      "[2026.544921875, 20.35565185546875]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  57  out of  85\n",
      "169/169 [==============================] - 0s 3ms/step - loss: 2026.5455 - mean_absolute_error: 20.3557\n",
      "[2026.5455322265625, 20.35565948486328]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  58  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5460 - mean_absolute_error: 20.3557\n",
      "[2026.5460205078125, 20.35565757751465]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  59  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5470 - mean_absolute_error: 20.3557\n",
      "[2026.5469970703125, 20.35565185546875]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  60  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5471 - mean_absolute_error: 20.3557\n",
      "[2026.547119140625, 20.35565185546875]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  61  out of  85\n",
      "169/169 [==============================] - 0s 3ms/step - loss: 2026.5468 - mean_absolute_error: 20.3557\n",
      "[2026.5467529296875, 20.355655670166016]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  62  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5446 - mean_absolute_error: 20.3557\n",
      "[2026.5445556640625, 20.35569953918457]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  63  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5455 - mean_absolute_error: 20.3557\n",
      "[2026.5455322265625, 20.355693817138672]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  64  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5464 - mean_absolute_error: 20.3557\n",
      "[2026.54638671875, 20.355701446533203]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  65  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5486 - mean_absolute_error: 20.3556\n",
      "[2026.548583984375, 20.355634689331055]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  66  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5493 - mean_absolute_error: 20.3556\n",
      "[2026.54931640625, 20.355623245239258]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  67  out of  85\n",
      "169/169 [==============================] - 0s 3ms/step - loss: 2026.5514 - mean_absolute_error: 20.3556\n",
      "[2026.5513916015625, 20.35556411743164]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  68  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2026.5510 - mean_absolute_error: 20.3556\n",
      "[2026.551025390625, 20.35556983947754]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  69  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5514 - mean_absolute_error: 20.3556\n",
      "[2026.5513916015625, 20.355562210083008]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  70  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2026.5529 - mean_absolute_error: 20.3555\n",
      "[2026.5528564453125, 20.355533599853516]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  71  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5547 - mean_absolute_error: 20.3555\n",
      "[2026.5546875, 20.355472564697266]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  72  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2026.5557 - mean_absolute_error: 20.3554\n",
      "[2026.5556640625, 20.35544776916504]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  73  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2026.5557 - mean_absolute_error: 20.3554\n",
      "[2026.5556640625, 20.355438232421875]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  74  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5575 - mean_absolute_error: 20.3554\n",
      "[2026.5574951171875, 20.35541343688965]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  75  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5575 - mean_absolute_error: 20.3554\n",
      "[2026.5574951171875, 20.35541343688965]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  76  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5588 - mean_absolute_error: 20.3554\n",
      "[2026.558837890625, 20.355377197265625]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  77  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5575 - mean_absolute_error: 20.3554\n",
      "[2026.5574951171875, 20.35540771484375]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  78  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5570 - mean_absolute_error: 20.3554\n",
      "[2026.5570068359375, 20.35540771484375]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  79  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5579 - mean_absolute_error: 20.3554\n",
      "[2026.557861328125, 20.355405807495117]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  80  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5588 - mean_absolute_error: 20.3554\n",
      "[2026.558837890625, 20.355379104614258]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  81  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5607 - mean_absolute_error: 20.3553\n",
      "[2026.5606689453125, 20.3553466796875]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  82  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5604 - mean_absolute_error: 20.3554\n",
      "[2026.5604248046875, 20.3553524017334]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  83  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5608 - mean_absolute_error: 20.3554\n",
      "[2026.560791015625, 20.35535430908203]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  84  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2026.5607 - mean_absolute_error: 20.3554\n",
      "[2026.5606689453125, 20.35535430908203]\n",
      "\n",
      "\n",
      "Model  1  out of  2\n",
      "Variable  85  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2026.5610 - mean_absolute_error: 20.3553\n",
      "[2026.56103515625, 20.3553466796875]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  1  out of  85\n",
      "169/169 [==============================] - 1s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  2  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  3  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  4  out of  85\n",
      "169/169 [==============================] - 0s 3ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  5  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  6  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  7  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  8  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  9  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  10  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  11  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  12  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  13  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  14  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  15  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  16  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  17  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  18  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  19  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  20  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  21  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  22  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  23  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  24  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  25  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  26  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  27  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  28  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  29  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  30  out of  85\n",
      "169/169 [==============================] - 1s 4ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  31  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  32  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  33  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  34  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  35  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  36  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  37  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  38  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  39  out of  85\n",
      "169/169 [==============================] - 1s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  40  out of  85\n",
      "169/169 [==============================] - 1s 3ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  41  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  42  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  43  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  44  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  45  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  46  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  47  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  48  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  49  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  50  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  51  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  52  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  53  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  54  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  55  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  56  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  57  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  58  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  59  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  60  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  61  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  62  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  63  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  64  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  65  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  66  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  67  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  68  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  69  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  70  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  71  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  72  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  73  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  74  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  75  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  76  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  77  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  78  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  79  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  80  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  81  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  82  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  83  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  84  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n",
      "\n",
      "\n",
      "Model  2  out of  2\n",
      "Variable  85  out of  85\n",
      "169/169 [==============================] - 0s 2ms/step - loss: 2016.9769 - mean_absolute_error: 20.5709\n",
      "[2016.9769287109375, 20.57090950012207]\n"
     ]
    }
   ],
   "source": [
    "#Variable importance\n",
    "\n",
    "\n",
    "\n",
    "# VarImpData = np.asarray(xRatioData).astype('float32')\n",
    "# VarImpData = scipy.stats.mstats.winsorize(VarImpData, limits = (0.05, 0.95))\n",
    "# VarImpData = scaler.fit_transform(VarImpData)\n",
    "\n",
    "rows = len(finalResults[\"ModelPointer\"])\n",
    "cols = len(xWinVal.columns)\n",
    "xWinValArray = np.array(xWinVal)\n",
    "\n",
    "#VarImpResults = pd.DataFrame(columns = X.columns)\n",
    "VarImpArray = np.empty((rows,cols), dtype=float, order='C')\n",
    "for row in range(0, rows):\n",
    "    \n",
    "    model = finalResults[\"ModelPointer\"][row]\n",
    "    VarImpData = xWinValArray\n",
    "    for col in range(0, cols):\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"Model \", str(row+1), \" out of \", str(rows))\n",
    "        print(\"Variable \", str(col+1), \" out of \", str(cols))\n",
    "\n",
    "\n",
    "        VarImpData[:,col] = 0\n",
    "        #VarImpData[col].values[:] = 0\n",
    "        loss = model.evaluate(VarImpData, yVal, batch_size=128)\n",
    "\n",
    "        try:\n",
    "            loss = [x for x in loss]\n",
    "            VarImpMSE = loss[0][0]\n",
    "            VarImpArray[row, col] = VarImpMSE\n",
    "        except:     \n",
    "            print(loss)\n",
    "\n",
    "\n",
    "VarImpResults = pd.DataFrame(VarImpArray, columns = xWinVal.columns)\n",
    "\n",
    "VarImpMean = VarImpResults.mean(axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VarImpMean = VarImpResults.mean(axis=0)\n",
    "VarImpMeanSorted = VarImpMean.sort_values(ascending=False)\n",
    "VarImpMeanBest = VarImpMeanSorted[1:25]\n",
    "#VarImpMeanScaled = VarImpMeanBest - VarImpMeanBest.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.bar(VarImpMeanBest.index, VarImpMeanBest, log=True, color ='maroon', width = 0.4)#, width, bottom=VarImpMean.min(), align)\n",
    "plt.barh(VarImpMeanBest.index, VarImpMeanBest, log=True, color ='green')#, height = 0.4)#, width, bottom=VarImpMean.min(), align)\n",
    "plt.gca().invert_yaxis()  # labels read top-to-bottom\n",
    "\n",
    "\n",
    "plt.title('Variable Importance')\n",
    "plt.ylabel('Variable')\n",
    "plt.xlabel('MSE reduction')\n",
    "#plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "\n",
    "plt.savefig('Results/VariableImportance7.png')\n",
    "#from matplotlib.ticker import ScalarFormatter\n",
    "#plt.gca().xaxis.set_major_formatter(ScalarFormatter())\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "676/676 - 5s - loss: 1986.6534 - mean_absolute_error: 20.3532 - val_loss: 2671.4270 - val_mean_absolute_error: 21.5914 - 5s/epoch - 7ms/step\n",
      "Epoch 2/300\n",
      "676/676 - 3s - loss: 1984.5618 - mean_absolute_error: 20.4130 - val_loss: 2665.3560 - val_mean_absolute_error: 21.6110 - 3s/epoch - 4ms/step\n",
      "Epoch 3/300\n",
      "676/676 - 3s - loss: 1982.2057 - mean_absolute_error: 20.4761 - val_loss: 2662.5491 - val_mean_absolute_error: 21.6174 - 3s/epoch - 4ms/step\n",
      "Epoch 4/300\n",
      "676/676 - 3s - loss: 1982.1211 - mean_absolute_error: 20.4793 - val_loss: 2659.1809 - val_mean_absolute_error: 21.6375 - 3s/epoch - 4ms/step\n",
      "Epoch 5/300\n",
      "676/676 - 2s - loss: 1980.4836 - mean_absolute_error: 20.5323 - val_loss: 2657.9526 - val_mean_absolute_error: 21.6343 - 2s/epoch - 4ms/step\n",
      "Epoch 6/300\n",
      "676/676 - 2s - loss: 1979.2751 - mean_absolute_error: 20.5259 - val_loss: 2655.4902 - val_mean_absolute_error: 21.6411 - 2s/epoch - 4ms/step\n",
      "Epoch 7/300\n",
      "676/676 - 3s - loss: 1978.4633 - mean_absolute_error: 20.5290 - val_loss: 2653.8403 - val_mean_absolute_error: 21.6456 - 3s/epoch - 4ms/step\n",
      "Epoch 8/300\n",
      "676/676 - 3s - loss: 1979.9008 - mean_absolute_error: 20.5514 - val_loss: 2654.1409 - val_mean_absolute_error: 21.6388 - 3s/epoch - 4ms/step\n",
      "Epoch 9/300\n",
      "676/676 - 3s - loss: 1977.8114 - mean_absolute_error: 20.5300 - val_loss: 2653.3992 - val_mean_absolute_error: 21.6244 - 3s/epoch - 4ms/step\n",
      "Epoch 10/300\n",
      "676/676 - 4s - loss: 1975.5931 - mean_absolute_error: 20.5430 - val_loss: 2658.2485 - val_mean_absolute_error: 21.5980 - 4s/epoch - 6ms/step\n",
      "Epoch 11/300\n",
      "676/676 - 3s - loss: 1977.5414 - mean_absolute_error: 20.5122 - val_loss: 2652.0652 - val_mean_absolute_error: 21.6214 - 3s/epoch - 4ms/step\n",
      "Epoch 12/300\n",
      "676/676 - 3s - loss: 1976.5725 - mean_absolute_error: 20.5170 - val_loss: 2651.8958 - val_mean_absolute_error: 21.6121 - 3s/epoch - 4ms/step\n",
      "Epoch 13/300\n",
      "676/676 - 2s - loss: 1973.3776 - mean_absolute_error: 20.5543 - val_loss: 2646.8408 - val_mean_absolute_error: 21.6593 - 2s/epoch - 4ms/step\n",
      "Epoch 14/300\n",
      "676/676 - 3s - loss: 1976.0900 - mean_absolute_error: 20.5568 - val_loss: 2653.0981 - val_mean_absolute_error: 21.5983 - 3s/epoch - 4ms/step\n",
      "Epoch 15/300\n",
      "676/676 - 2s - loss: 1974.5933 - mean_absolute_error: 20.5336 - val_loss: 2648.4719 - val_mean_absolute_error: 21.6324 - 2s/epoch - 4ms/step\n",
      "Epoch 16/300\n",
      "676/676 - 3s - loss: 1975.5057 - mean_absolute_error: 20.5670 - val_loss: 2651.8328 - val_mean_absolute_error: 21.6060 - 3s/epoch - 4ms/step\n",
      "Epoch 17/300\n",
      "676/676 - 2s - loss: 1974.1116 - mean_absolute_error: 20.5324 - val_loss: 2653.5830 - val_mean_absolute_error: 21.5860 - 2s/epoch - 4ms/step\n",
      "Epoch 18/300\n",
      "676/676 - 3s - loss: 1973.5619 - mean_absolute_error: 20.5297 - val_loss: 2647.5608 - val_mean_absolute_error: 21.6279 - 3s/epoch - 4ms/step\n",
      "Epoch 19/300\n",
      "676/676 - 2s - loss: 1971.6813 - mean_absolute_error: 20.5210 - val_loss: 2652.6711 - val_mean_absolute_error: 21.5871 - 2s/epoch - 4ms/step\n",
      "Epoch 20/300\n",
      "676/676 - 2s - loss: 1975.1544 - mean_absolute_error: 20.5467 - val_loss: 2655.3069 - val_mean_absolute_error: 21.5760 - 2s/epoch - 4ms/step\n",
      "Epoch 21/300\n",
      "676/676 - 2s - loss: 1973.8750 - mean_absolute_error: 20.5379 - val_loss: 2651.2886 - val_mean_absolute_error: 21.6021 - 2s/epoch - 4ms/step\n",
      "Epoch 22/300\n",
      "676/676 - 2s - loss: 1973.0601 - mean_absolute_error: 20.5246 - val_loss: 2641.1099 - val_mean_absolute_error: 21.7108 - 2s/epoch - 4ms/step\n",
      "Epoch 23/300\n",
      "676/676 - 2s - loss: 1973.3519 - mean_absolute_error: 20.5967 - val_loss: 2646.1887 - val_mean_absolute_error: 21.6424 - 2s/epoch - 4ms/step\n",
      "Epoch 24/300\n",
      "676/676 - 2s - loss: 1972.0803 - mean_absolute_error: 20.6024 - val_loss: 2649.7622 - val_mean_absolute_error: 21.6121 - 2s/epoch - 4ms/step\n",
      "Epoch 25/300\n",
      "676/676 - 2s - loss: 1974.1868 - mean_absolute_error: 20.5460 - val_loss: 2647.2944 - val_mean_absolute_error: 21.6326 - 2s/epoch - 4ms/step\n",
      "Epoch 26/300\n",
      "676/676 - 2s - loss: 1974.4198 - mean_absolute_error: 20.5770 - val_loss: 2648.0193 - val_mean_absolute_error: 21.6310 - 2s/epoch - 4ms/step\n",
      "Epoch 27/300\n",
      "676/676 - 2s - loss: 1971.5873 - mean_absolute_error: 20.5352 - val_loss: 2648.2617 - val_mean_absolute_error: 21.6314 - 2s/epoch - 4ms/step\n",
      "Epoch 28/300\n",
      "676/676 - 2s - loss: 1969.3903 - mean_absolute_error: 20.6095 - val_loss: 2652.5310 - val_mean_absolute_error: 21.5962 - 2s/epoch - 4ms/step\n",
      "Epoch 29/300\n",
      "676/676 - 2s - loss: 1974.4589 - mean_absolute_error: 20.5893 - val_loss: 2644.4451 - val_mean_absolute_error: 21.6674 - 2s/epoch - 4ms/step\n",
      "Epoch 30/300\n",
      "676/676 - 2s - loss: 1974.4194 - mean_absolute_error: 20.5860 - val_loss: 2648.2847 - val_mean_absolute_error: 21.6231 - 2s/epoch - 4ms/step\n",
      "Epoch 31/300\n",
      "676/676 - 2s - loss: 1972.3882 - mean_absolute_error: 20.5727 - val_loss: 2653.8413 - val_mean_absolute_error: 21.5875 - 2s/epoch - 4ms/step\n",
      "Epoch 32/300\n",
      "676/676 - 3s - loss: 1974.5579 - mean_absolute_error: 20.5428 - val_loss: 2652.0129 - val_mean_absolute_error: 21.5990 - 3s/epoch - 4ms/step\n",
      "Epoch 33/300\n",
      "676/676 - 3s - loss: 1971.4680 - mean_absolute_error: 20.5709 - val_loss: 2650.6069 - val_mean_absolute_error: 21.6067 - 3s/epoch - 4ms/step\n",
      "Epoch 34/300\n",
      "676/676 - 2s - loss: 1972.1842 - mean_absolute_error: 20.5689 - val_loss: 2648.1997 - val_mean_absolute_error: 21.6234 - 2s/epoch - 3ms/step\n",
      "Epoch 35/300\n",
      "676/676 - 2s - loss: 1974.9034 - mean_absolute_error: 20.5942 - val_loss: 2652.1226 - val_mean_absolute_error: 21.6035 - 2s/epoch - 3ms/step\n",
      "Epoch 36/300\n",
      "676/676 - 2s - loss: 1974.8881 - mean_absolute_error: 20.5732 - val_loss: 2641.8040 - val_mean_absolute_error: 21.7220 - 2s/epoch - 3ms/step\n",
      "Epoch 37/300\n",
      "676/676 - 2s - loss: 1973.2106 - mean_absolute_error: 20.5968 - val_loss: 2643.1196 - val_mean_absolute_error: 21.7000 - 2s/epoch - 3ms/step\n",
      "Epoch 38/300\n",
      "676/676 - 2s - loss: 1972.8728 - mean_absolute_error: 20.5684 - val_loss: 2649.0132 - val_mean_absolute_error: 21.6186 - 2s/epoch - 3ms/step\n",
      "Epoch 39/300\n",
      "676/676 - 2s - loss: 1973.0125 - mean_absolute_error: 20.5859 - val_loss: 2646.3503 - val_mean_absolute_error: 21.6592 - 2s/epoch - 3ms/step\n",
      "Epoch 40/300\n",
      "676/676 - 2s - loss: 1972.6447 - mean_absolute_error: 20.5755 - val_loss: 2655.7214 - val_mean_absolute_error: 21.5795 - 2s/epoch - 3ms/step\n",
      "Epoch 41/300\n",
      "676/676 - 2s - loss: 1970.7449 - mean_absolute_error: 20.5804 - val_loss: 2649.2400 - val_mean_absolute_error: 21.6119 - 2s/epoch - 3ms/step\n",
      "Epoch 42/300\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "676/676 - 2s - loss: 1972.3958 - mean_absolute_error: 20.5639 - val_loss: 2645.8789 - val_mean_absolute_error: 21.6478 - 2s/epoch - 3ms/step\n",
      "Epoch 42: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "bestModel = keras.models.load_model('Results/BestKerasModel6')\n",
    "#plot(model)\n",
    "\n",
    "\n",
    "bestModel = tf.keras.Sequential()\n",
    "bestModel.add(Dense(units=5, activation=\"relu\"))\n",
    "bestModel.add(tf.keras.layers.Dropout(0.3))\n",
    "bestModel.add(Dense(units=5, activation=\"relu\"))\n",
    "bestModel.add(tf.keras.layers.Dropout(0.3))\n",
    "bestModel.add(Dense(units=5, activation=\"relu\"))\n",
    "bestModel.add(tf.keras.layers.Dropout(0.3))\n",
    "bestModel.add(Dense(units=5, activation=\"relu\"))\n",
    "bestModel.add(tf.keras.layers.Dropout(0.3))\n",
    "bestModel.add(Dense(units=1))\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)\n",
    "\n",
    "\n",
    "bestModel.reset_states()\n",
    "\n",
    "\n",
    "compiledBestModel = bestModel.compile(optimizer=\"RMSprop\",  # Adam(learning_rate=0.0001) #RMSprop #sgd\n",
    "                                      loss=\"MeanSquaredError\",  # 'tf.keras.losses.MeanSquaredError()',\n",
    "                                      metrics=['MeanAbsoluteError'])\n",
    "\n",
    "bestModel.reset_states()\n",
    "\n",
    "\n",
    "\n",
    "#xTrainValiPooled = scaled_X[:validationSize,:]\n",
    "xTrainValiPooled = X.loc[split==\"Validation\", :]\n",
    "#yTrainValiPooled = Y[:validationSize]\n",
    "yTrainValiPooled = Y[split==\"Validation\"]\n",
    "history = bestModel.fit(x=xTrainValiPooled, y=yTrainValiPooled, batch_size=32,\n",
    "                        epochs=300, verbose=2, validation_data=(xWinTest, yTest), callbacks = [callback])\n",
    "\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.savefig('Results/LearningCurve9.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize network\n",
    "\n",
    "bestModel = keras.models.load_model('Results/BestKerasModel7')\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'\n",
    "\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "from ann_visualizer.visualize import ann_viz\n",
    "\n",
    "ann_viz(bestModel, view=True, filename=\"Results/BestModel7\", title=\"Best Model Visualized\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_69 (Dense)            (None, 5)                 430       \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 5)                 30        \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 5)                 30        \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 5)                 30        \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 5)                 30        \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 556\n",
      "Trainable params: 556\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bestModel = keras.models.load_model('Results/BestKerasModel7')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Print summary\n",
    "modelSummary = bestModel.summary()\n",
    "print(modelSummary)\n",
    "\n",
    "\n",
    "with open('Results/BestModelSummary7.html', 'w') as f:\n",
    "\n",
    "    bestModel.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'\n",
    "\n",
    "# import pydot\n",
    "# import pydotplus\n",
    "# import graphviz\n",
    "\n",
    "# tf.keras.utils.plot_model(\n",
    "#     bestModel,\n",
    "#     to_file=\"Results/BestModel.png\",\n",
    "#     show_shapes=True,\n",
    "#     show_dtype=False,\n",
    "#     show_layer_names=True,\n",
    "#     rankdir=\"LR\",\n",
    "#     expand_nested=False,\n",
    "#     dpi=96,\n",
    "#     layer_range=None,\n",
    "#     show_layer_activations=True,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "#pred = model.predict(xWinVal, batch_size=128)\n",
    "\n",
    "#plt.scatter(pred, yVal)\n",
    "#pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ynew = model.predict_classes(Xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Geron 2019, 76, 320)\n",
    "\n",
    "#from sklearn import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "#(Geron 2019, 76)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "scoring='neg_mean_squared_error',\n",
    "return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "\n",
    "\n",
    "grid_search.best_params_\n",
    "\n",
    "grid_search.best_estimator_\n",
    "\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#(Geron 2019, 320)\n",
    "\n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "\n",
    "\n",
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "validation_data=(X_valid, y_valid),\n",
    "callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "validation_data=(X_valid, y_valid),\n",
    "callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "\n",
    "\n",
    "rnd_search_cv.best_params_\n",
    "rnd_search_cv.best_score_\n",
    "model = rnd_search_cv.best_estimator_.model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Chollet: DL for Python\n",
    "import kerastuner as kt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#If your search process crashes, you can always restart it—just specify overwrite=False in the tuner so that it can resume from the trial logs stored on disk.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6646144ec7618d86cefdfa307b5e5ba5f5893ee78adb19daf28ba13bcfecfb8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
