---
title: "Master's Thesis"
author: "Robin Perala"
date: "2022-09-10"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown


```{r libs}


#Check files: RnD, Lab8-Garch, OptimalHedgeRatio, Ex4Part2, nnboston, HW7, Gridsearch, LSTM



# library(torch)
# library(luz) # high-level interface for torch
# library(torchvision) # for datasets and image transformation
# library(torchdatasets) # for datasets we are going to use
# library(zeallot)


# library(dplyr) #Contains "mutate_if" function
# library(magrittr) #Contains pipes, e.g. %<>%
# # library(neuralnet) #For fitting neural networks
# # library(keras) #Contains "keras_model_sequential" function
# library(glmnet) #For regularized linear models (e.g. lasso and ridge)



```




```{r CompustatFundamentals}

#I should clean data
#Check all variables what they are. Remove unnecessary
#2 414 247 obs. of  408 =>

#Import data. The data includes both industrials and banks
require(data.table)
data <- fread("Data/OriginalData/FundamentalsQuarterly.csv")
# #orderedData = data[order(names(data))]
# #setcolorder(data, order(names(data)))
# #orderedData = setorderv(data, order(names(data)))[]
# 
# #Look at data
View(data[1:1000,])
# names(data)
# 
# #Compare quarterly and YearToDate data. Remove yearly data?
# data[gvkey %in% c(1166, 1491), .(gvkey, pdateq, fdateq, datacqtr, cogsq, cogsy, revtq, revty)]
# View(data[gvkey %in% c(1166, 1491), .(gvkey, pdateq, fdateq, datacqtr, cogsq, cogsy, revtq, revty)])

#Which firms do not have quarters?
quarterIndex = !data$datacqtr==""
sum(!quarterIndex)
#View(data[!quarterIndex, ])
relevantData = data[quarterIndex, ]
#data = Filter(!data$datacqtr=="", data)


variableNames = c("gvkey", "fdateq", "datadate", "datacqtr", "accdq", "acoq", "acoxq", "actq", "ancq", "aoq", "apq", "atq", "capsq", "ceqq", "cheq", "cogsq", "cstkq", "dfxaq", "dlcq", "dlttq", "dpq", "eqrtq", "eroq", "gpq", "ibmiiq", "ibq", "iditq", "intanq", "invtq", "ivaoq", "lcoq", "lcoxq", "lctq", "lltq", "loq", "lseq", "ltmibq", "ltq", "nopioq", "nopiq", "oiadpq", "oibdpq", "piq", "ppentq", "reccoq", "rectoq", "rectq", "rectrq", "req", "revtq", "saleq", "sctq", "seqq", "teqq", "txtq", "xintq", "xoproq", "xoprq", "xsgaq", "loc", "city", "naics", "acctstdq",  "compstq", "curcdq")

relevantData = relevantData[, ..variableNames]

#Just removed: 
# "idbflag", "fyrc", "sic",
# "gsector", "ggroup", "gind", "gsubind",
# "fic",  "exchg",
# "bsprq", "scfq", "staltq"

# #Inspecting data
# View(data[is.na(data$naics),])



#require(data.table)
#data <- fread("Data/CleanDataQ.csv")

# gvkey = factor(na.omit(data$gvkey))
# #Industry
# sic = factor(na.omit(data$sic))
# naics = factor(na.omit(data$naics))
# gsector = factor(na.omit(data$gsector))
# ggroup = factor(na.omit(data$ggroup))
# gind = factor(na.omit(data$gind))
# gsubind = factor(na.omit(data$gsubind))
# 
# #Location
# exchg = factor(na.omit(data$exchg))
# fic = factor(na.omit(data$fic))
# loc = factor(na.omit(data$loc))
# city = factor(na.omit(data$city))
# 
# #Other
# acctstdq = factor(na.omit(data$acctstdq))
# bsprq = factor(na.omit(data$bsprq))
# compstq = factor(na.omit(data$compstq))
# scfq = factor(na.omit(data$scfq))
# srcq = factor(na.omit(data$srcq))
# staltq = factor(na.omit(data$staltq))
# stko = factor(na.omit(data$stko))
# updq = factor(na.omit(data$updq))
# 
# View(data[data$staltq != ""])
# View(data[data$gvkey == 212145])
# 
# table(acctstdq)
# table(bsprq)
# table(compstq)
# table(scfq)
# table(srcq)
# table(staltq)
# table(stko)
# table(updq)
# 
# #Deletion
# dlrsn = factor(na.omit(data$dlrsn))
# dldte = factor(na.omit(data$dldte))
# ipodate = factor(na.omit(data$ipodate))
# 
# table(dlrsn)
# sum(!is.na(dldte))
# sum(!is.na(ipodate))
# 
# View(data[!is.na(data$dlrsn)])
# View(data[data$gvkey == 13470])
# # Do not include dlrsn or dldte as variables. They include look-ahead bias


missing = sum(is.na(data))
notMissing = sum(!is.na(data))

missing / (missing + notMissing)

#Clean data
sumOfNotNA = colSums(!is.na(relevantData))
View(sumOfNotNA)

#First: Remove columns with over half NA
minimumNumberOfRows = nrow(relevantData)*0.5
lessThanHalfNA = relevantData[, sumOfNotNA > minimumNumberOfRows, with=FALSE]
View(lessThanHalfNA[1:1000,])


# sumOfNotNA2 = colSums(!is.na(data))
# View(sumOfNotNA2)
# 
# minimumNumberOfRows = nrow(data)*0.5
# lessThanHalfNA = data[, sumOfNotNA2 > minimumNumberOfRows, with=FALSE]
# View(lessThanHalfNA[1:1000,])
#names(lessThanHalfNA)

#Second: Remove all rows with NA
dataNoNA = na.omit(lessThanHalfNA)
# View(dataNoNA[1:1000,])
# #78 166 x 137
# 
# nrow(data[!staltq==""])
# View(data[!staltq==""])

fwrite(dataNoNA,"Data/CleanDataQ.csv", row.names = FALSE)



```



```{r GGPlot}


startQuarter = min(data[!data$datacqtr == "", datacqtr])
endQuarter = max(data[!data$datacqtr == "", datacqtr])
dataRange = paste(as.character(startQuarter), "-", as.character(endQuarter))
quarters = length(unique(data$datacqtr))
years = quarters/4

#sortedQuarters = sort(unique(YXfactor$datacqtr))


data$datacqtr = zoo::as.yearqtr(data$datacqtr, format = "%YQ%q")


#Plot for document
require(ggplot2)
ggplot(data = data[datacqtr>"2009 Q3",], aes(x = datacqtr)) +
  geom_bar(stat = "count") +
  labs(title = "Compustat Full Data",
       subtitle = dataRange,
       x = "Quarter", y = "Observations")

yearlyFrequencyOriginalData = table(data$datacqtr)
View(yearlyFrequencyOriginalData)






#Same for cleaned data
cleanData = fread("Data/CleanDataQ.csv")

cleanData$datacqtr = zoo::as.yearqtr(cleanData$datacqtr, format = "%YQ%q")
startQuarter = min(cleanData[!cleanData$datacqtr=="", datacqtr])
endQuarter = max(cleanData[!cleanData$datacqtr=="",datacqtr])
dataRange = paste(as.character(startQuarter), "-", as.character(endQuarter))
quarters = length(unique(cleanData$datacqtr))
years = quarters/4

#sortedQuarters = sort(unique(YXfactor$datacqtr))


cleanData$datacqtr = zoo::as.yearqtr(cleanData$datacqtr, format = "%YQ%q")
require(ggplot2)
ggplot(data = cleanData, aes(x = datacqtr)) +
  geom_bar(stat = "count") +
  labs(title = "Compustat Filtered Data",
       subtitle = dataRange,
       x = "Quarter", y = "Observations")

yearlyFrequencyCleanData = table(cleanData$datacqtr)
View(yearlyFrequencyCleanData)


temporalFrequencyTable = merge(yearlyFrequencyOriginalData, yearlyFrequencyCleanData, 
                               by = "Var1", suffixes = c(".Original", ".Filtered"), all = TRUE)

write.csv(temporalFrequencyTable, "Descr/temporalFrequencyTable.csv")


# #Histogram (BeforeMktCap)
# revenueWinsorized = DescTools::Winsorize(cleanData$revtq, probs = c(0.01, 0.99))
# 
# hist(revenueWinsorized, breaks=1000, xlab="Revenue", main = "Histogram of Revenue")
# # hist(log(cleanData$revtq), breaks=50, xlab="Revenue", main = "Histogram of Revenue")

```



```{r priceData}

#Import price data
require(data.table)
prices <- fread("Data/OriginalData/CompustatPrices.csv")
#221 835 959 x 6


cleanData = fread("Data/CleanDataQ.csv")
#data       final    preliminary
#datadate   fdateq   pdateq
# 20100331 20100503 20100503
# 20100630 20100907 20100801
# 20100930 20101105 20101105
# 20101231 20110330 20110306
# 20110630 20110811 20110730
# 20111231 20120331 20120309
#Check update code in fundamentals data

sum(data$updq==3)
sum(data$updq!=3)
View(data[data$updq!=3,][1:100,])
#nrow(cleanData[cleanData$updq==3,])
#almost all data is final (update code 3, instead of 2)
#I will use "fdateq" as a conservative approach


#reduce data size
removeGVKEY = prices[prices$gvkey %in% cleanData$gvkey,]

#Oldest date in cleanData is 2009/11/30
min(cleanData$fdateq)
newPrices = removeGVKEY[removeGVKEY$datadate>=(min(cleanData$fdateq)-300),]

#prirow is the "primary issue tag". Only use primary issue
newPrices = newPrices[newPrices$iid == newPrices$prirow, -c("iid", "prirow")]


fwrite(newPrices,"Data/CleanPrices.csv", row.names = FALSE)

```

```{r CheckingForLookAheadBias}
# require(data.table)
# cleanData <- fread("Data/CleanDataQ.csv")
# 
# 
# nrow(cleanData[cleanData$datadate>cleanData$pdateq,])
# nrow(cleanData[cleanData$pdateq>cleanData$fdateq,])
# View(cleanData[cleanData$pdateq>cleanData$fdateq,])
# 
# cleanData$datacqtr = zoo::as.yearqtr(cleanData$datacqtr, format = "%YQ%q")
# nrow(cleanData[cleanData$datacqtr>cleanData$fdateq,])
# 
# cleanData$fdateqZoo = as.character(cleanData$fdateq)
# cleanData$fdateqZoo = as.Date(cleanData$fdateqZoo, format = "%Y%m%d")
# #head(cleanData$fdateqZoo - cleanData$datacqtr)
# 
# require(ggplot2)
# ggplot(data = cleanData, aes(x = fdateqZoo)) +
#   geom_bar(stat = "count") +
#   labs(x = "Date", y = "Observations")
# 
# 
# #Check what the fdateq distribution looks like; to see if the dates are clustered
# #Put all years into one calendar year
# cleanData$fdateqMD = as.character(cleanData$fdateq)
# cleanData$fdateqMD = substr(cleanData$fdateqMD, 5, 8)
# cleanData$fdateqMD = as.Date(cleanData$fdateqMD, format = "%m%d")
# require(ggplot2)
# ggplot(data = cleanData, aes(x = fdateqMD)) +
#   geom_bar(stat = "count") +
#   labs(x = "Date", y = "Observations")
# #View(table(cleanData$fdateqMD))
# 
# View(cleanData[1:1000,])


```




```{r combineFundamentalsAndPrices}

require(data.table)
newPrices <- fread("Data/CleanPrices.csv")
cleanData <- fread("Data/CleanDataQ.csv")
setkey(newPrices, gvkey, datadate)

#View(newPrices[1:1000,])
#View price data in wide format
#widePrices = dcast(newPrices[,], datadate~gvkey + iid, value.var = "prccd")
#widePrices = dcast(newPrices[,], datadate~gvkey, value.var = "prccd")


fundamentalsData=cleanData
View(cleanData[1:1000,])
#https://stackoverflow.com/questions/35046161/how-to-do-a-data-table-rolling-join
# create an interval in the 'compuStat' data.table


#Rebalancing 1.2, 1.5, 1.8, and 1.11
periodization = function(date){
  YYYY = signif(date, 4)
  MMDD = date %% 10000
  
  if(1100<MMDD) {res = YYYY + 201 + 10000}
  else if(800<MMDD) {res = YYYY + 1101}
  else if(500<MMDD) {res = YYYY + 801}
  else if(200<MMDD) {res = YYYY + 501}
  else {res = YYYY + 201}
  "start" = res
}

start = unlist(lapply(fundamentalsData$fdateq, FUN=periodization))
end = start + 2    #56753 with 1    #65325 with 2     #65 331 with 3     #65 333 with 4    #65 334 with 5   #usually I use 7
fundamentalsData[, `:=` (start=start, end=end)]

#fundamentalsData[, `:=` (start = fdateq + 1, end = fdateq + 300)]
#as.POSIXct(zoo::as.yearqtr(strptime(20221212, format = "%Y%m%d")))


# create a second date in the 'prices' data.table
newPrices[, Date2 := datadate]

# set the keys for the two data tables
setkey(fundamentalsData, gvkey, prirow, start, end)
setkey(newPrices, gvkey, iid, datadate, Date2) #iid is "Issue ID"

# create a vector of columnnames which can be removed afterwards
deletecols <- c("Date2", "start", "end", "prirow", "i.datadate")

# perform the overlap join and remove the helper columns
mergedData <- foverlaps(fundamentalsData, newPrices)[, (deletecols) := NULL]



##alternative: non-equi join
# fundamentalsData[, `:=` (start = fdateq - 100, end = fdateq - 1)]
# mergedData2 = newPrices[fundamentalsData, on = .(gvkey, datadate >= start, datadate <= end)]


mergedData = mergedData[order(mergedData$gvkey, -mergedData$datadate, -mergedData$fdateq),]
finalPriceData = mergedData[!duplicated(cbind(mergedData$fdateq,
                                                mergedData$gvkey)),]

finalPriceData = finalPriceData[order(finalPriceData$gvkey, finalPriceData$fdateq),] 

#Remove non-primary IDs
#finalPriceData = finalPriceData[finalPriceData$iid==finalPriceData$prirow,]

fwrite(finalPriceData,"Data/finalPriceData.csv", row.names = FALSE)


#Write into word doc:
# StartDate >= ReleaseDate(YYYYMMDD)+1
# && 
# StartDate < ReleaseDate(YYYYMMDD)+100
# Minimum Date => First date where this holds

##Should be: datadate == fdateq + 1
# mergedData = merge(cleanData, removeGVKEY, 
#                    by.x = c("gvkey","fdateq"),
#                    by.y = c("gvkey","datadate"))

#https://stackoverflow.com/questions/16095680/merge-dataframes-on-matching-a-b-and-closest-c
# removeGVKEY <- data.table(removeGVKEY, key = c("gvkey","datadate"))
# cleanData <- data.table(cleanData, key = c("gvkey","fdateq"))
# mergedData <- cleanData[removeGVKEY, roll=-1]

```



```{r calculateReturnsAndMktCap}
require(data.table)
finalPriceData <- fread("Data/finalPriceData.csv")

#View price data in wide format
#widePrices = dcast(finalPriceData, datadate~gvkey + iid, value.var = "prccd")
#widePrices = dcast(finalPriceData, datadate~gvkey, value.var = "prccd")


ret = function(price, returnFactor, adjustmentFactor) {
  n = length(price)
  numerator = returnFactor[-1]*price[-1]/adjustmentFactor[-1]
  denominator = (returnFactor[-n]*price[-n]/adjustmentFactor[-n])
  
  (numerator/denominator-1) * 100
}

MktCapFunction = function(price, sharesOutstanding) {
  price * sharesOutstanding
}



# returnData = finalPriceData
# returnData = returnData[, "returns" := c(ret(prccd, trfd, ajexdi), NA), by = gvkey]
# returnData = returnData[, "MktCap" := MktCapFunction(prccd, cshoc)]
# returnData = returnData[!is.na(returnData$returns),]


newPrices = fread("Data/CleanPrices.csv")
returnData = newPrices
returnData[, ":=" ("returns" = c(ret(prccd, trfd, ajexdi), NA), 
                   "daysByFirm" = length(datadate)), 
           by = gvkey]
returnData[, "MktCap" := MktCapFunction(prccd, cshoc)]

returnData[, "year" := floor(datadate/10^4)]
returnData[, "month" := floor((datadate/10^2) %% 10^2)]
returnData[, "quarter" := ceiling((month-1)/3)]
returnData[quarter == 0, ":=" ("quarter" = 4, "year" = year-1)]
View(returnData[1:5000,])

returnData[, ":=" ("monthlyReturns" = c((prod(returns/100+1, na.rm = T) - 1)*100), 
                   "daysInMonth" = length(datadate)), 
           by = list(gvkey, year, month)]
returnData[, ":=" ("quarterlyReturns" = c((prod(returns/100+1, na.rm = T) - 1)*100), 
                   "daysInQuarter" = length(datadate)), 
           by = list(gvkey, year, quarter)]

# returnData[, "yearlyReturns" := c((prod(returns/100+1, na.rm = T) - 1)*100), by = list(gvkey, year)]
View(returnData[1:5000,])

returnData = returnData[!is.na(returnData$returns),]
monthlyReturnData = unique(returnData, by=c("gvkey", "year", "month"))
monthlyReturnData[, "monthlyReturns-1" := c(monthlyReturns[-1], NA), by = gvkey]
monthlyReturnData[, "monthlyReturns-2" := c(monthlyReturns[-c(1:2)], rep(NA, 2)), by = gvkey]
monthlyReturnData[, "monthlyReturns-3" := c(monthlyReturns[-c(1:3)], rep(NA, 3)), by = gvkey]

monthlyReturnData = monthlyReturnData[, "totalMonthlyObservations" := length(datadate), by = gvkey]
#duplicated(monthlyReturnData, by=c("gvkey", "yearMonth"))
View(monthlyReturnData[1:5000,])

quarterlyReturnData = monthlyReturnData[,c("gvkey", "datadate", "year", "quarter", "curcdd", "quarterlyReturns", "MktCap")]
quarterlyReturnData = unique(quarterlyReturnData, by=c("gvkey", "year", "quarter"))
View(quarterlyReturnData[1:5000,])

fwrite(quarterlyReturnData,"Data/returnsData.csv", row.names = FALSE)



#monthlyReturnData = monthlyReturnData[,c("gvkey", "datadate", "year", "month", "monthlyReturns", "MktCap")]
#View(monthlyReturnData[1:5000,])

# duplicates = duplicated(cbind(returnData$gvkey, returnData$iid))
# duplicates[1] = TRUE
# returnData = returnData[duplicates,]

fwrite(monthlyReturnData,"Data/monthlyReturnData.csv", row.names = FALSE)


```

```{r combineFundamentalsAndPricesNewVersion}

require(data.table)
returnData <- fread("Data/returnsData.csv")
cleanData <- fread("Data/CleanDataQ.csv")

# #Remove firms with large fdateq-datadate discrepancy
# cleanData$fdateqDateObject = as.Date(as.character(cleanData$fdateq), format = "%Y%m%d")
# cleanData$datadate = as.Date(as.character(cleanData$datadate), format = "%Y%m%d")
# cleanData[, "TimeDifferenceInDays" := difftime(fdateqDateObject, datadate)]
# View(cleanData[1:1000,])
# 
# #fundamentalsData = cleanData[TimeDifferenceInDays<180,]
# 
# #Create fdateq YearQuarter variable
# 
# #fundamentalsData$fdateqYearQuarter = sapply(fundamentalsData$fdateq, FUN=periodization)

cleanData[, "year" := floor(fdateq/10^4)]
cleanData[, "month" := floor((fdateq/10^2) %% 10^2)]
cleanData[, "quarter" := ceiling((month-1)/3)]
cleanData[quarter == 0, ":=" ("quarter" = 4, "year" = year-1)]

setkey(cleanData, gvkey, year, quarter)

fundamentalsData = unique(cleanData, fromLast=T, by = key(cleanData))
#View(fundamentalsData[1:1000,])

#Combine datasets
setkey(fundamentalsData, gvkey, year, quarter)
setkey(returnData, gvkey, year, quarter)

mergedData = merge(returnData, fundamentalsData, 
                   by = c("gvkey", "year", "quarter"),
                   suffixes = c(".price", ".fund"))
#View(mergedData[1:1000,])

mergedData = mergedData[curcdd==curcdq, !c("curcdd", "curcdq")]

fwrite(mergedData, "Data/mergedData.csv", row.names = FALSE)


```

```{r Descriptives}

require(data.table)
cleanData <- fread("Data/cleanData.csv") #Should this whole  chunk should be updated to finalData.csv?

colnames(cleanData)[122]
numsData = cleanData[21:105]


#Descriptive statistics
library(moments)
Skewness = c(skewness(numsData, na.rm=T))
Kurtosis = c(kurtosis(numsData, na.rm=T))
kS = round(cbind(Skewness, Kurtosis), 0)
write.csv(kS, "Descr/kurtSkew.csv")



library(stargazer)
stargazer(numsData,
          type = "text",
          out = "Descr/numsDesc.html",
          title = "Descriptive statistics",
          style = "default", #"aer", "qje"
          flip = F,
          digits = 0,
          digits.extra = 0,
          median=T,
          iqr=T#,
          #add.lines = list(Skewness, Kurtosis)
          )

groupData = cleanData[,123:134]

countryIndustry = table(cleanData[,c("loc", "gsector")]) #GICS industry code
allIndustries = apply(countryIndustry, MARGIN=2, sum)
countryIndustryWithIndSum = rbind(countryIndustry, "All Industries"=allIndustries)
allCountries = apply(countryIndustryWithIndSum, MARGIN=1, sum)
countryIndustry = cbind(countryIndustryWithIndSum, "All Countries"=allCountries)

write.csv(countryIndustry, file = "Descr/countryIndustry.csv")



countryIndustry = table(finalData[,c("loc", "gsector")]) #GICS industry code
allIndustries = apply(countryIndustry, MARGIN=2, sum)
countryIndustryWithIndSum = rbind(countryIndustry, "All Industries"=allIndustries)
allCountries = apply(countryIndustryWithIndSum, MARGIN=1, sum)
countryIndustry = cbind(countryIndustryWithIndSum, "All Countries"=allCountries)

write.csv(countryIndustry, file = "Descr/countryIndustry2.csv")


countryIndustry = table(data[,c("loc", "gsector")]) #GICS industry code
allIndustries = apply(countryIndustry, MARGIN=2, sum)
countryIndustryWithIndSum = rbind(countryIndustry, "All Industries"=allIndustries)
allCountries = apply(countryIndustryWithIndSum, MARGIN=1, sum)
countryIndustry = cbind(countryIndustryWithIndSum, "All Countries"=allCountries)

write.csv(countryIndustry, file = "Descr/countryIndustryOrgFile.csv")


industryNames = c(
  "10 Energy",
  "15 Materials",
  "20 Industrials",
  "25 Consumer Discretionary (Consumer Cyclical)",
  "30 Consumer Staples (Consumer Defensive)",
  "35 Health Care",
  "40 Financials",
  "45 Information Technology",
  "50 Communication Services",
  "55 Utilities",
  "60 Real Estate",
  "Country"
)
colnames(countryIndustryOrg) = industryNames
countryIndustryFormatted = knitr::kable(countryIndustryOrg)


#How many companies? 8450
dim(table(cleanData[,c("gvkey")]))


```


```{r ImportHereAndFactorize}

require(data.table)
YX <- fread("Data/YX.csv")
#YX <- fread("Data/ScaledWinsRatioData.csv")

#factorizing
YXfactor = YX
# factorCols = c("gvkey", "curcdq", "exchg", "fic", "city", "fyrc",
#         "ggroup", "gind", "gsector", 
#         "gsubind", "idbflag", "loc", 
#         "naics", "sic", "fdateq", "datadate")#, "datacqtr")

factorCols = c("gvkey", "fdateYQ", "Split", "naics", "loc", "city",
              "acctstdq", "compstq")#, "bsprq", "scfq", "staltq")

#YXfactor[, c(factorCols) := lapply(.SD, factor), .SDcols = c(factorCols)]


#YXfactor[, c(factorCols) := lapply(.SD, FUN = function(x) caret::dummyVars(formula= " ~ .", data = x, fullRank=FALSE)), .SDcols = c(factorCols)]

# dmy <- caret::dummyVars(" ~ .", data = YX[, c(factorCols)], fullRank=FALSE)
# YXfactor <- data.frame(predict(dmy, newdata = YX[, c(factorCols)]))

#Create date objects
# YXfactor$fdateq = as.Date(YXfactor$fdateq, format = '%Y%m%d')
# YXfactor$datadate = as.Date(YXfactor$datadate, format = '%Y%m%d')
# YXfactor$datacqtr = zoo::as.yearqtr(YXfactor$datacqtr, format = "%YQ%q")


# #Remove non-numerics
# finalNumericData = Filter(is.numeric, YXfactor[, -c("gvkey", "fdateq", "MktCap")])
# #sum(is.na(finalNumericData))
# finalNumericData = na.omit(finalNumericData)


# # library(dplyr) #Contains "mutate_if" function
# # library(magrittr) #Contains pipes, e.g. %<>%
# # X2 = X
# # X2 %<>% mutate_if(is.factor, as.numeric)
# # X3 <- as.matrix(X2)


```

```{r TrainValidationTestSplit}

#Plot the time series
# require(ggplot2)
# ggplot(data = YXfactor, aes(x = fdateq)) +
#       geom_bar(stat = "count", fill = "green")
# 
# ggplot(data = YXfactor, aes(x = datadate)) +
#       geom_bar(stat = "count", fill = "red")


# #I should change these to fdateq quarters
# startQuarter = min(YXfactor$datacqtr)
# endQuarter = max(YXfactor$datacqtr)
# dataRange = paste(as.character(startQuarter), "-", as.character(endQuarter))
# quarters = length(unique(YXfactor$datacqtr))
# years = quarters/4
# 
# sortedQuarters = sort(unique(YXfactor$datacqtr))



mergedData <- fread("Data/mergedData.csv")

mergedData$fdateYQ = paste(mergedData$year, mergedData$quarter, sep="Q")
mergedData$fdateYQ = zoo::as.yearqtr(mergedData$fdateYQ, format="%YQ%q")


startQuarter = min(mergedData$fdateYQ)
endQuarter = max(mergedData$fdateYQ)
dataRange = paste(as.character(startQuarter), "-", as.character(endQuarter))
quarters = length(unique(mergedData$fdateYQ))
years = quarters/4

sortedQuarters = sort(unique(mergedData$fdateYQ))



trainValidSplitPoint = round(quarters*0.6)
validTestSplitPoint = round(quarters*0.8)

trainQuarters = sortedQuarters[1:trainValidSplitPoint]
validQuarters = sortedQuarters[(trainValidSplitPoint+1):validTestSplitPoint]
testQuarters = sortedQuarters[(validTestSplitPoint+1):quarters]

tTVSplitFunction = function(x){
  if(x %in% trainQuarters) {"Train"}
  else if(x %in% validQuarters) {"Validation"}
  else {"Test"}
}

mergedData[, "Split" := sapply(fdateYQ, tTVSplitFunction)]
#YXfactor$Split = factor(YXfactor$Split)
#View(mergedData[1:1000,])

fwrite(mergedData,"Data/mergedDataWithSplit.csv", row.names = FALSE)

```



```{r TrainValidationTestQuarterlyPlot}

#Plot for document
require(ggplot2)
ggplot(data = mergedData, aes(x = fdateYQ)) +
  geom_bar(stat = "count") +
  labs(title = "Final Data",
       subtitle = dataRange,
       x = "Quarter", y = "Observations") +
  zoo::scale_x_yearqtr(format = '%Y', n = years)


ggplot(data = mergedData, aes(x = fdateYQ, fill = Split)) +
  geom_bar(stat = "count") +
  labs(title = "Train-Validation-Test Quarterly Split",
       subtitle = dataRange, 
       x = "Quarter", y = "Observations") + 
  zoo::scale_x_yearqtr(format = '%Y', n = years)



# hist(YXfactor$aoq, breaks=200)
# hist(YXfactor$cogsq, breaks=200)
# hist(YXfactor$lctq, breaks=200)
# 
# 
# #Histograms for document
# hist(YXfactor$revtq, breaks=200, xlab="Revenue", main = "Histogram of Revenue (Winsorized and Scaled)")
# hist(log(YXfactor$revtq), breaks=200, xlab="Ln(Revenue)", main = "Histogram of LogRevenue")
# 
# hist(YXfactor$returns, breaks=200, xlab="Returns", main = "Histogram of Returns (Winsorized and Scaled)")
# hist(log(YXfactor$returns/100 + 1), breaks=200, xlab="Ln(Returns)", main = "Histogram of LogReturns")


```


```{r removingExtraVariables}
require(data.table)
mergedData <- fread("Data/mergedDataWithSplit.csv")
#View(mergedData[1:1000,])


# #Removing Unnecessary variables 
# variableNames = c("returns", "MktCap", "gvkey", "fdateq", "datadate", "datacqtr", "accdq", "acoq", "acoxq", "actq", "ancq", "aoq", "apq", "atq", "capsq", "ceqq", "cheq", "cogsq", "cstkq", "dfxaq", "dlcq", "dlttq", "dpq", "eqrtq", "eroq", "gpq", "ibmiiq", "ibq", "iditq", "intanq", "invtq", "ivaoq", "lcoq", "lcoxq", "lctq", "lltq", "loq", "lseq", "ltmibq", "ltq", "nopioq", "nopiq", "oiadpq", "oibdpq", "piq", "ppentq", "reccoq", "rectoq", "rectq", "rectrq", "req", "revtq", "saleq", "sctq", "seqq", "teqq", "txtq", "xintq", "xoproq", "xoprq", "xsgaq", "loc", "city", "naics", "acctstdq", "bsprq", "compstq", "scfq", "staltq")
# 
# 
# #YX = lapply(YXfactor, function(x){x[, ..variableNames]})
# #YX = lapply(YX, function(x){na.omit(x)})
# YX = finalData[, ..variableNames]
finalData = mergedData[, c("year", "quarter", "month", "datadate.price", "datadate.fund", "fdateq", "pdateq", "datacqtr"):=NULL]
colnames(finalData)[colnames(finalData) == 'quarterlyReturns'] <- 'returns'
#View(finalData[1:1000,])


# #removeMissingCalendarData. I could probably manually add these three instead. A simple script for adding
# #which(finalData$datacqtr=="")
# #finalData[c(12721, 68453, 71789),]
# YX=YX[!YX$datacqtr=="",]

# #Trash??
# #a = factor(YX[1]$'2009Q4'$sic)
# YX[,"gvkey"] = apply(YX[,"gvkey"], MARGIN = 2, factor)


fwrite(finalData,"Data/YX.csv", row.names = FALSE)


#Histogram
#revenueWinsorized = DescTools::Winsorize(finalData$revtq/finalData$MktCap, probs = c(0.01, 0.99))

# hist(revenueWinsorized, breaks=1000, xlab="Revenue", main = "Histogram of Revenue")
# hist(log(cleanData$revtq), breaks=50, xlab="Revenue", main = "Histogram of Revenue")

```

```{r PLMSpeedTest}

YX = data.table::fread("Data/YX.csv")
YXfactor = YX[, !c("MktCap", "Split", "staltq", "bsprq", "scfq")]
YXfactor = na.omit(YXfactor)

factorCols = c("gvkey", "loc", "city", "naics",
        "acctstdq", "compstq",
        "fdateYQ")

YXfactor[, c(factorCols) := lapply(.SD, factor), .SDcols = c(factorCols)]
YXPanel = plm::pdata.frame(YXfactor, index = c("gvkey", "fdateYQ"))

n = names(YXfactor)
exclude = c("returns", "gvkey", "fdateYQ")
f <- as.formula(paste("returns ~", paste(n[!n %in% exclude], collapse = " + ")))

plmSpeedTest = plm::plm(f, effect="twoways", model= "within", data = YXPanel)


#table(index(YXfactor), useNA = "ifany")


```


```{r stationarityAndUnitRoot}
#Stationarity test
plm::purtest(finalNumericData)
sum(is.na(finalNumericData))


#Check biggest return
min(finalData$returns, na.rm = T) #-99.27552
max(finalData$returns, na.rm = T) #4561.336

#Returns stationarity (Will not anyway winsorize these). This is wrong as we have panel data
#fUnitRoots::adfTest(finalData$returns, lags = 252, type = c("nc"), title = NULL, description = NULL)
#plm::purtest(finalNumericData, index="gvkey")


```



```{r TrainTestSplitChecking}

testSet = YXfactor[YXfactor$Split=="Test",]
testSet$datacqtr = factor(testSet$datacqtr)
testSetQuarters = split(testSet, testSet$datacqtr)



#Checking if the data periodization makes sense
# for(n in testSet){
#   print(c(min(n$fdateq), max(n$fdateq)))
# }
# 
# for(n in trainValidation){
#   print(c(min(n$fdateq), max(n$fdateq)))
# }
# 
# 
# #Minimum dates look approximately fine. Maximum dates on the other hand...
# which(trainValidation$'2009Q4'$fdateq == max(trainValidation$'2009Q4'$fdateq))
# 
# #Check this for the initial dataset. Is this an error?
# finalData[finalData$gvkey==285237,]

```


```{r CVfunction}




```

```{r removeRedVariables}


```


```{r Pooling}

drop = c("datacqtr", "fdateq")

sparseTestSet = lapply(testSet, function(x) Matrix::sparse.model.matrix(returns ~ ., x[,!..drop]))

sparseTrainValidation = lapply(trainValidation, function(x) Matrix::sparse.model.matrix(returns ~ ., x[, !..drop]))


# sparseYX = Matrix::sparse.model.matrix(returns ~ ., YX)
# Y = YX$returns


YX = sparseTrainValidation
CVsize = round(length(YX)/2):(length(YX)-1)
CVsize = (length(YX)-2):(length(YX)-1)



YX = lapply(trainValidation, function(x) x[, !..drop])

trainXPools = list()
validXPools = list()

trainYPools = list()
validYPools = list()
for (n in CVsize) {
  #Create training set
  set = YX[1:n]
  pooled = do.call(rbind, set)
  trainX = Matrix::sparse.model.matrix(returns ~ ., pooled)
  #trainXRegular = Matrix::model.matrix(returns ~ ., pooled)
  trainY = pooled$returns
  
  #Created validation set
  valid = YX[n + 1][[1]]
  validX = Matrix::sparse.model.matrix(returns ~ ., valid)
  #validXRegular = Matrix::model.matrix(returns ~ ., valid)
  validY = valid$returns
  
  #remove columns that arent in both data sets
  trainCols = trainX@Dimnames[[2]]
  validCols = validX@Dimnames[[2]]
  trainX = trainX[,  trainCols %in% validCols]
  validX = validX[,  validCols %in% trainCols]
  
  trainXPools = append(trainXPools, list(trainX))
  trainYPools = append(trainYPools, list(trainY))
  
  validXPools = append(validXPools, list(validX))
  validYPools = append(validYPools, list(valid$returns))
}




# sparseTrainPools = lapply(trainPools, function(x) {
#   Matrix::sparse.model.matrix(returns ~ ., x)
# })




# for (n in CVsize) {
#   set = YX[1:n]
#   pooled = do.call(rbind, set)
#   
#   #Create training set
#   trainX = Matrix::sparse.model.matrix(returns ~ ., pooled)
#   trainY = pooled$returns
#   
#   #Created validation set
#   valid = YX[n + 1][[1]]
#   validX = sparse.model.matrix(returns ~ ., valid)
#   validY = valid$returns
#   
#   #remove columns that arent in both data sets
#   validCols = validX@Dimnames[[2]]
#   trainCols = trainX@Dimnames[[2]]
#   trainX = trainX[,  trainCols %in% validCols]
#   validX = validX[,  validCols %in% trainCols]
# }




```

## Model horse-race

```{r OLS}

# OLSData = do.call(rbind, trainValidation)[, -c("fdateq")]
# 
# #check duplicates
# unique = c("gvkey", "datacqtr")
# #dups = OLSData[duplicated(OLSData[,..unique])]
# OLSDataUnique = OLSData[!duplicated(OLSData[,..unique])]
# 
# #n = names(OLSPanel)
# n = names(Filter(is.numeric, OLSDataUnique))
# exclude = c("returns", "gvkey", "datacqtr")
# f <- as.formula(paste("returns ~", paste(n[!n %in% exclude], collapse = " + ")))
# 
# OLSPanel = plm::pdata.frame(OLSDataUnique, index = c("gvkey", "datacqtr"))
# OLSmodel2 = plm::plm(f, effect="twoways", model= "within", data=OLSPanel)
# #OLSsummary = summary(OLSmodel2) #computationally singular -> Because of multicollinearity. No issue; just ignore the message
# #OLSsummary$r.squared
# 
# 
# 
# 
# 
# OLSError = c()
# for (n in 1:validationSize) {
#   trainX = trainXPools[[n]]
#   trainY = trainYPools[[n]]
#   validX = validXPools[[n]]
#   validY = validYPools[[n]]
#   
#   OLSmodel3 = glmnet(
#     x = trainX,
#     y = trainY,
#     standardize = TRUE,
#     lambda = 0
#   )
#   
#   OLSpred = predict(OLSmodel3, newx = validX)
#   OLSMSE = mean((OLSpred - validY) ^ 2)
#   OLSMAE =  mean(abs(OLSpred - validY))
#   
#   OLSError = rbind(OLSError, c("OLSMSE" = lassoMSE, "OLSMAE" = lassoMAE))
# }
# meanCV = apply(OLSError, 2, mean)
# meanCV



```


```{r LassoRidge}

# library(glmnet)
#Use function glmnet (from library glmnet) to perform linear regression with Lasso regularization
#alpha=1 is lasso.



validationSize = length(validXPools)
CVfunction = function(lambda, alpha) {
  lassoError = c()
  for (n in 1:validationSize) {
    trainX = trainXPools[[n]]
    trainY = trainYPools[[n]]
    validX = validXPools[[n]]
    validY = validYPools[[n]]
    
    lassoModel = glmnet(
      x = trainX,
      y = trainY,
      alpha = alpha,
      standardize = TRUE,
      lambda = lambda
    )

    lassoPred = predict(lassoModel, newx = validX)
    lassoMSE = mean((lassoPred - validY) ^ 2)
    lassoMAE = mean(abs(lassoPred - validY))
    
    lassoError = rbind(lassoError, c("lassoMSE" = lassoMSE, "lassoMAE" = lassoMAE))
  }
  meanCV = apply(lassoError, 2, mean)
  meanCV
}


grid = c(10^seq(3,-10, length=5), 0)


MSEMatrix = c()
for (alpha in 1:2) {
  for (lambda in grid) {
    lambdaCV = CVfunction(lambda)
    MSEMatrix = rbind(MSEMatrix,
                      c("lambda" = lambda, "alpha" = alpha, lambdaCV))
  }
}
MSEMatrix


```






```{r PCR}

# library(pls)
# #PCR Regression using function pcr (part of pls library)
# PCRfit=pcr(returns~., data=USData, scale=TRUE, subset=train, validation ="CV")
# 
# 
# 
# CVfunction = function(lambda, alpha) {
#   errorMatrix = c()
#   for (n in 1:validationSize) {
#     trainX = trainXPools[[n]]
#     trainY = trainYPools[[n]]
#     validX = validXPools[[n]]
#     validY = validYPools[[n]]
#     
#     train = cbind(trainX, trainY)
#     
#     PCRfit=pcr(returns~., data=train, scale=TRUE)
# 
#     lassoPred = predict(lassoModel, newx = validX)
#     lassoMSE = mean((lassoPred - validY) ^ 2)
#     lassoMAE = mean(abs(lassoPred - validY))
#     
#     lassoError = rbind(lassoError, c("lassoMSE" = lassoMSE, "lassoMAE" = lassoMAE))
#   }
#   meanCV = apply(lassoError, 2, mean)
#   meanCV
# }
# 
# pcrCV = CVfunction(lambda)
# 
# 
# 
# 
# 
# #Validation plot
# validationplot(PCRfit, val.type="MSEP")
# 
# #Scree plotting
# PVE=PCRfit$Xvar/PCRfit$Xtotvar
# plot(PVE, xlab="Principal Component", ylab="Proportion of Variance Explained", 
#      ylim=c(0,1), type="b")
# plot(cumsum(PVE), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", 
#      ylim=c(0,1), type="b")


```




```{r Decision Trees}
# library(haven) #For importing Stata data
# library(tree) #For fitting trees
# library(randomForest) #For bagging and randomforests
# library(gbm) #For boosting


# #Fit a tree using function "tree" (in library "tree")
# treeROA = tree(roa_w~. , USData, subset=train)
# summary(treeROA)
# #treeROA
# 
# #Plot
# plot(treeROA); text(treeROA, pretty=0, cex=0.7, srt=25)
# 
# #MSE
# yhat=predict(treeROA, newdata=USData[-train,])
# ROAtest=USData[-train, "roa_w"]
# plot(yhat, ROAtest$roa_w, ylab = "roa_w")
# abline(0,1)
# mean((yhat-ROAtest$roa_w)^2)












#Bagging
# library(randomForest)
CVsize = (length(YX)-2):(length(YX)-1)

treefunction = function(p, nTree, nodes) {
  errorMatrix = c()
  for (n in CVsize) {
    set = YX[1:n]
    pooled = do.call(rbind, set)
    # trainX = stats::model.matrix(returns ~ ., pooled)
    # trainY = pooled$returns
    # 
    # #Created validation set
    valid = YX[n + 1][[1]]
    #validX = stats::model.matrix(returns ~ ., valid)
    validX = valid[,!"returns"]
    validY = valid$returns
    
    
    #remove columns that arent in both data sets
    trainCols = names(pooled)
    validCols = names(valid)
    pooled = pooled[,  (trainCols %in% validCols), with=FALSE]

    validXCols = names(validX)
    validX = validX[,  (validXCols %in% trainCols), with=FALSE]

    #remove columns that arent in both data sets
    # trainCols = trainX@Dimnames[[2]]
    # validCols = validX@Dimnames[[2]]
    # trainX = trainX[,  trainCols %in% validCols]
    # validX = validX[,  validCols %in% trainCols]
    
    # train = cbind(trainX, "returns"=trainY)
    # train = as.data.table(as.matrix(train))
    
    model = randomForest(
      returns ~ .,
      data = pooled,
      mtry = p,
      ntree = nTree,
      maxnodes = nodes,
      importance = TRUE
    )
    
    pred = predict(model, newx = validX) #something is not working here nrows(pred) is the same as nrows(pooled)
    MSE = mean((pred - validY) ^ 2)
    MAE = mean(abs(pred - validY))
    
    errorMatrix = rbind(errorMatrix, c("MSE" = MSE, "MAE" = MAE))
  }
  meanCV = apply(errorMatrix, 2, mean)
  meanCV
}



MSEMatrix = c()
numOfPreds = c(ncol(pooled)/3, ncol(pooled)-1)
nTrees = c(10, 15)
maxNodes = c(2, 3)
for (p in numOfPreds) {
  for (nTree in nTrees) {
    for (nodes in maxNodes) {
      CV = treefunction(p, nTree, nodes)
      MSEMatrix = rbind(MSEMatrix,
                        c(
                          "predictors" = p,
                          "trees" = nTree,
                          "nodes" = nodes,
                          CV
                        ))
    }
  }
}
MSEMatrix





# #Bagging using the function randomForest,
# #inside the randomForest library. When mtry is for all variables, it is bagging. 
# bagROA = randomForest(roa_w~., USData, subset=train, mtry=ncol(USData)-1, importance =TRUE)
# bagROA
# plot(bagROA)
# 
# #MSE
# yhat=predict(bagROA, newdata=USData[-train,])
# ROAtest=USData[-train, "roa_w"]
# plot(yhat, ROAtest$roa_w)
# abline(0,1)
# mean((yhat-ROAtest$roa_w)^2)










# #Random forests
# 
# set.seed(1)
# #Fit a random forest using the function randomForest,
# #inside the randomForest library. When mtry is less than
# #than all variables in the data, it is a random forests model
# forestROA = randomForest(roa_w~., USData, subset=train, importance = TRUE)
# 
# #Should be done on several different mtry
# #sqrt(ncol(USData))
# #ncol(USData)/3
# 
# forestROA
# plot(forestROA)
# 
# importance(forestROA)
# varImpPlot(forestROA)
# 
# #MSE
# yhat=predict(forestROA, newdata=USData[-train,])
# ROAtest=USData[-train, "roa_w"]
# plot(yhat, ROAtest$roa_w)
# abline(0,1)
# mean((yhat-ROAtest$roa_w)^2)







# #Boosting
# 
# set.seed(1)
# #Boosting using the "gbm" function inside the "gbm" library
# boostROA = gbm(roa_w~., USData[train,], distribution="gaussian", n.trees=500, interaction.depth=4, shrinkage = 0.1)
# #all or only training set? cv.folds? check help(gbm)
# #We use cross-validation to select B.
# 
# 
# boostROA
# summary(boostROA)
# plot(boostROA, i="profit_margin_w"); plot(boostROA, i="ebitda_w")
# 
# #MSE
# yhat=predict(boostROA, newdata=USData[-train,])
# ROAtest=USData[-train, "roa_w"]
# plot(yhat, ROAtest$roa_w)
# abline(0,1)
# mean((yhat-ROAtest$roa_w)^2)


```


```{r NeuralNetwork_Torch}

library(torch)
library(luz) #High-level interface for torch
library(magrittr) #Contains pipes, e.g. %<>%

if(cuda_is_available()){
  device = torch_device("cuda")
} else{
  device = torch_device("cpu")
}
  
#torch_manual_seed(13)

NNfunction = function(type, layers, hidUnits, actvFunc, dropout, l1, l2, epo) {
  errorMatrix = c()
  
  if (type == "LSTM" || type == "GRU")
    source("src/LSTM.R")
  else if (type == "Forward")
    source("src/FeedForwardNN.R")
  else
    source("src/Linear.R")
  
  modnn <- modnn %>%
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_rmsprop, #optim_sgd #optim_rmsprop
    metrics = list(luz_metric_mae())
  ) %>%
    set_hparams(
      input_size = 86,
      layers = layers,
      type = type,
      hidUnits = hidUnits,
      dropout=dropout,
      actvFunc=actvFunc
    ) #3130 #inputSize
  
  
  for(n in 1:validationSize) {
    trainX = trainXNN[[n]]
    trainY = trainYNN[[n]]
    validX = validXNN[[n]]
    validY = validYNN[[n]]
    
    
    #Model Fitting
    
    #batch number seems to automatically be 32. 
    #dataloader_options: Options used when creating a dataloader. See torch::dataloader().
    #shuffle=TRUE by default for the training data and batch_size=32 by default.
    #It will error if not NULL and data is already a dataloader.
    fitted <- modnn %>%
      fit(
        data = list(trainX, trainY),
        valid_data = list(validX, validY),
        epochs = epo,
        verbose = TRUE
        #batch_size=100
        #dataloader_options = c(batch_size=100)
      )
    
    #MSE and MAE (The same ones as above)
    pred = predict(fitted, validX)
    MSE = mean((validY - pred) ^ 2)
    MAE = mean(abs(validY - pred))
    
    errorMatrix = rbind(errorMatrix, c("MSE" = MSE$item(), "MAE" = MAE$item()))
  }
  meanCV = apply(errorMatrix, 2, mean)
  meanCV
}


#Hyper-Parameters
modelType = c("Forward", "LSTM", "GRU")
hiddenLayers = c(2, 3)
hiddenUnits = c(2, 3)
activationFunc = c("relu", "sigmoid") #softmax, etc. https://keras.io/api/layers/activations/
dropOut = c(0.5, 0)
#regGrid = c(10^seq(3,-10, length=4), 0)
epochs = 1 #Just choose best model. => Run for 5 epochs, but if 3rd epoch is better, choose it
inputSize = 86 #ncol(trainXPools[[1]])


NNParams = expand.grid(
  stringsAsFactors = F,
  "type" = c("Forward"),
  "layers" = hiddenLayers,
  "hidUnits" = hiddenUnits,
  "actvFunc" = activationFunc,
  "dropout" = dropOut,
  "epo" = epochs
  # "l1" = regGrid,
  # "l2" = regGrid
)

RNNParams = expand.grid(
  stringsAsFactors = F,
  "type" = c("LSTM", "GRU"),
  "layers" = hiddenLayers,
  "hidUnits" = hiddenUnits,
  "actvFunc" = c("NA"),
  "dropout" = dropOut,
  "epo" = epochs
)

LinearParams = list(
  "type" = "Linear",
  "layers" = 0,
  "hidUnits" = 0,
  "actvFunc" = "NA",
  "dropout" = 0,
  "epo" = 1
)
hyperParams = rbind(LinearParams, NNParams, RNNParams)

validationSize = 2 #length(validXPools)
MSEMatrix = c()
HPsize = nrow(hyperParams)

for(n in 1:HPsize) {
  print("\n")
  cat("Model", n, "out of", HPsize)
  print("\n")
  
  HP = as.list(hyperParams[n,])
  CV = do.call(NNfunction, HP)
  MSEMatrix = rbind(MSEMatrix, c(HP, CV))
}
MSEMatrix
finalResults = as.data.frame(MSEMatrix)
finalResults[,-1] = apply(finalResults[,-1], MARGIN=2, FUN=as.numeric)
finalResults = finalResults[order(finalResults[,"MSE"]),]
finalResults <- apply(finalResults, 2, as.character)

write.csv2(finalResults, "Results/results3.csv", row.names = FALSE)



```


```{r Keras}


# library(tensorflow)
# library(keras)

# # #Model Creation
# # library(keras)
# inputSize = ncol(trainXPools[[1]])
# model <- keras_model_sequential()
# model %>%
#   layer_dense(units = hidUnits,
#               activation = 'actvFunc',
#               input_shape = inputSize) %>%
#   layer_activity_regularization(l1 = l1, l2 = l2) %>%
#   layer_dropout(rate = dropout)  %>%
#   layer_dense(units = 1)
#
# #Model Compilation
# model %>% compile(loss = 'mse',
#                   optimizer = 'rmsprop',
#                   metrics = 'mae')
#
# readline()




# #Model Fitting
# mymodel <- model %>%
#   fit(
#     trainX,
#     trainY,
#     epochs = epochs,
#     batch_size = 3, #32 default. Add as an extra hyper-parameter
#     validation_data = list(validX, validY)
#   )


# # #Prediction
# # model %>% evaluate(validX, validY)
# # pred <- model %>% predict(validX)
#
#
# # #Scatter Plot Original vs Predicted
# # plot(testtarget, pred)
# #
# # #Plot model error for each epoch
# # plot(mymodel)


```

## LoadFactors

```{r FamaFrenchData}

library(data.table)
# factorData <- 
#   list.files(path = "FactorData/", pattern = "*.csv") %>% 
#   purrr::map_df(~fread(.))


#Import factors
developedFactors=fread("FactorData/Developed_ex_US_5_Factors.csv", na.strings=c(getOption("datatable.na.strings","NA"), -99.99))
developedMomentum=fread("FactorData/Developed_ex_US_MOM_Factor.csv", na.strings=c(getOption("datatable.na.strings","NA"), -99.99))
developed = dplyr::inner_join(x = developedFactors, y = developedMomentum, by = "V1")

emergingFactors=fread("FactorData/Emerging_5_Factors.csv", na.strings=c(getOption("datatable.na.strings","NA"), -99.99))
emergingMomentum=fread("FactorData/Emerging_MOM_Factor.csv", na.strings=c(getOption("datatable.na.strings","NA"), -99.99))
emerging = dplyr::inner_join(x = emergingFactors, y = emergingMomentum, by = "V1")


allFactors = dplyr::inner_join(x = developed, y = emerging, by = "V1", suffix = c(".DM", ".EM"))

write.csv(allFactors, "FactorData/allMonthlyFactors.csv", row.names = FALSE)


#allFactors[, `Mkt-RF.DM`+`Mkt-RF.EM`]

#allFactors[, (`Mkt-RF.DM`:WML.DM + `Mkt-RF.EM`:WML.EM)/2]


weightedFactors = (allFactors[, `Mkt-RF.DM`:WML.DM] +  allFactors[,`Mkt-RF.EM`:WML.EM])/2
weightedFactors = cbind(allFactors$V1, weightedFactors)
colnames(weightedFactors) = names(developed)


```


```{r QuarterlyFactors}
toQuarters = function(x) {
  q = ceiling((x %% 100 - 1) / 3)
}

toYears = function(x) {
  as.numeric(substr(x, 1, 4))
} 

toYQ = function(x) {
  Q=toQuarters(x)
  Y=signif(x, 4)
  Y+Q
} 

library(data.table)
factorsWithYQ = cbind(weightedFactors, weightedFactors[, .("Y" = toYears(V1),
                                                           "Q" = toQuarters(V1),
                                                           "YQ" = toYQ(V1))])
factorsWithYQ[Q==0, ":="(Q = 4, Y = Y-1)]
factorsWithYQ$YQ = paste(factorsWithYQ$Y, factorsWithYQ$Q, sep="Q")
factorsWithYQ$YQ = zoo::as.yearqtr(factorsWithYQ$YQ, format="%YQ%q")

# geom = function(x) {
#   exp(mean(log(1+x/100))) - 1
# }

factorProd = function(x) {
  decimal = prod((x/100)+1) - 1
  decimal*100
}


# quarterlyFactors = factorsWithYQ[, .(
#   Mkt = factorProd(`Mkt-RF`),
#   SMB = factorProd(SMB),
#   HML = factorProd(HML),
#   RMW = factorProd(RMW),
#   CMA = factorProd(CMA)
# ), by = YQ]
# 
# 
# quarterlyFactors = factorsWithYQ[, .(
#   Mkt = factorProd(`Mkt-RF`),
#   SMB = factorProd(SMB),
#   HML = factorProd(HML),
#   RMW = factorProd(RMW),
#   CMA = factorProd(CMA)
# ), by = .(Y,Q)]


YQ = zoo::as.yearqtr(as.character(weightedFactors$V1), format = "%Y%m")
factorsWithYQ = cbind(weightedFactors, YQ)

quarterlyFactors = factorsWithYQ[, .(
  Mkt = factorProd(`Mkt-RF`),
  SMB = factorProd(SMB),
  HML = factorProd(HML),
  RMW = factorProd(RMW),
  CMA = factorProd(CMA),
  WML = factorProd(WML),
  RF  = factorProd(RF),
  RFpred = ((RF[1]/100+1)^(length(RF))-1)*100
), by = YQ]



write.csv(quarterlyFactors, "FactorData/quarterlyFactors.csv", row.names = FALSE)


```

## Abnormal returns

```{r LoadKerasModelFromPython}

fs::dir_tree("Results/BestKerasModel1")

#library(tensorflow)
library(keras)


model <- keras::load_model_tf('Results/BestKerasModel1')
model2 <- keras::load_model_tf('Results/BestKerasModel2') #load second best model
model3 <- keras::load_model_tf('Results/BestKerasModel3') #load third best model
StandardLinearModel <- keras::load_model_tf('Results/KerasLinearModel') #load linear model

# Check its architecture
summary(model)
summary(model2)
summary(model3)

bestModels = list(model, model2, model3)

# #Model Fitting
# mymodel <- model %>%
#   fit(
#     trainX,
#     trainY,
#     epochs = epochs,
#     batch_size = 32, #32 default. Add as an extra hyper-parameter
#     validation_data = list(validX, validY)
#   )




#   pooled = do.call(rbind, trainXNN)
#   pooled = do.call(rbind, trainYNN)
#   pooled = do.call(rbind, validXNN)
#   pooled = do.call(rbind, validYNN)


#calculate variances
  
  
  


```

```{r ExportModelComparisonTable}

# modelComparisonTable = data.table::fread("Results/ModelComparison.csv")
# 
# 
# write.csv(modelComparisonTable, "Results/ModelComparisonTableInR.csv")


```


```{r Predict}
require(data.table)
#ScaledWinsRatioData <- fread("Data/ScaledWinsRatioData.csv")

YXfactor <- fread("Data/dummiesData.csv")


library(magrittr)
library(keras) #Contains "keras_model_sequential" function

# YXfactor[acctstdq =="", acctstdq :=NA]
# YXfactor[compstq =="", compstq :=NA]
# YXfactor[city =="", city :=NA]

# YXfactor$acctstdq = factor(YXfactor$acctstdq)
# YXfactor$compstq = factor(YXfactor$compstq)
# YXfactor$city = factor(YXfactor$city)

testSet = YXfactor[YXfactor$Split=="Test",-c("MktCap", "Split")]#, "staltq")]#, "staltq", "bsprq", "scfq")]
#testSet$fdateYQ = factor(testSet$fdateYQ)
#View(as.data.frame(colnames(YXfactor)))

#View(testSet[acctstdq =="" | compstq =="" | city ==""])
#testSet = testSet[acctstdq !="" & compstq !="" & city !=""]

# View(as.data.frame(levels(testSet$loc)))
# View(as.data.frame(levels(testSet$city)))
# View(as.data.frame(levels(testSet$naics)))
# View(as.data.frame(levels(testSet$acctstdq)))
# View(as.data.frame(levels(testSet$compstq)))

#testSet$datacqtr = factor(testSet$datacqtr)
#testSet$datacqtr = as.numeric(testSet$datacqtr)
#testSetNumericData = Filter(is.numeric, testSet[, -c("gvkey", "fdateq", "MktCap")])
#sum(is.na(finalNumericData))
#testSetNumericData = na.omit(testSetNumericData)

#testSetQuarters = split(testSetNumericData, testSetNumericData$datacqtr)
#testSetQuarters = split(testSet, testSet$datacqtr)


quarters = levels(as.factor(testSet$fdateYQ))
#Prediction
allPreds = c(); m = 1
allPerformances = list(); m = 1
for(q in quarters){
  
  cat("Quarter: ", q)
  test = testSet[testSet$fdateYQ==q, ]
  testX = as.matrix(test[,!c("returns", "fdateYQ", "gvkey")])
  testY = test$returns
  
  modelPerformance = as.data.frame(lapply(bestModels, function(x) x %>% evaluate(testX, testY))) #verbose=F
  pred = as.data.table(lapply(bestModels, function(x) x %>% predict(testX))) #verbose=F
  
  colnames(modelPerformance) = c("BestModel", "SecondBestModel", "ThirdBestModel")
  colnames(pred) = c("BestModelPred", "SecondModelPred", "ThirdModelPred")

  # modelPerformance = model %>% evaluate(testX, testY)
  # pred <- model %>% predict(testX)
  #colnames(pred)[1] = "predictedReturn"
  
  firmPreds = cbind(test[,c("gvkey", "fdateYQ")], pred)
  allPreds = rbind(allPreds, firmPreds)
  
  qName = paste("Quarter:", q)
  allPerformances = c(allPerformances, list(modelPerformance))
  names(allPerformances)[m] = qName
  m = m + 1
}

#View(allPerformances)


#testSetQuarters = split(testSet, testSet$fdateYQ)
#testSet = testSetQuarters
# #Prediction
# allPreds = list(); m = 1
# for(n in testSet){
#   
#   #cat("Model number", m)
#   cat("Quarter number", m)
#   #pooled = Filter(function(x) {is.numeric(x)}, n) #|is.factor(x)
#   
#   # pooled = n
#   # testX = pooled[, !c("fdateYQ", "gvkey")]  #c("gvkey", "fdateq", 
#   # testX = stats::model.matrix(returns ~ ., testX)[,-1]
#   testX = as.matrix(n[,!c("returns", "fdateYQ", "gvkey")])
#   #testX = apply(testX, MARGIN = 2, scale)
#   testY = n$returns
#   
#   
#   # testX = n$returns
#   # testY = n$returns
#   model %>% evaluate(testX, testY)
#   pred <- model %>% predict(testX)
#   
#   firmPreds = cbind(n[,c("gvkey", "fdateYQ")], pred)
#   allPreds = c(allPreds, list(firmPreds))
#   m = m + 1
# }



#Scatter Plot Original vs Predicted
#plot(pred, testY, log = "x")

#Plot model error for each epoch
#plot(model)


```

```{r Combine predictions and factors}

quarterlyFactors=fread("FactorData/quarterlyFactors.csv")

#Extract correct risk-free return
#cqtr = max(firmPreds$datacqtr)

# cqtr = lapply(allPreds, function(x) max(x$datacqtr))
# cqtr = lapply(cqtr, function(x) zoo::as.yearqtr(x, format = "%YQ%q"))

# cqtr = lapply(allPreds, function(x) max(x$fdateYQ))
# cqtr = lapply(cqtr, function(x) zoo::as.yearqtr(x, format = "%YQ%q"))

# cqtr = lapply(allPreds, function(x) max(x$fdateYQ))
# cqtr = zoo::as.yearqtr(allPreds, format = "%YQ%q")

# dateTime = as.POSIXct(cqtr, format="%Y%m")
# yearMonth = as.numeric(format(dateTime, format="%Y%m"))
# yearMonth = as.numeric(format(dateTime, format="%Y%m"))


#allPreds$fdateYQ = zoo::as.yearqtr(allPreds$YQ, format = "%Y Q%q")
cqtr = unique(zoo::as.yearqtr(allPreds$fdateYQ))
quarterlyFactors$YQ = zoo::as.yearqtr(quarterlyFactors$YQ, format = "%Y Q%q")
#rf0 = quarterlyFactors[YQ %in% cqtr]$RF
#rf0 = quarterlyFactors[YQ %in% cqtr]$RFpred
rf0 = quarterlyFactors[YQ %in% cqtr]$RFpred

predsFactorsMerged = merge(allPreds, quarterlyFactors, by.x = c("fdateYQ"), by.y = c("YQ"))
predsFactorsMerged$ZeroPred = 0


YX = fread("Data/YX.csv")
YX = YX[, c("gvkey", "fdateYQ", "returns")]
setorder(YX, "fdateYQ", "gvkey")
#predsFactorsMerged[, ":=" ("window" = .GRP), by = c("fdateYQ")]
#predsFactorsMerged[, ":=" ("window" = ), by = c("fdateYQ")]
#YX[, ":=" ("meanPred" = mean(returns, na.rm = T)), by = c("fdateYQ"), roll = -Inf]
#predsFactorsMerged[, ":=" ("meanPred" = frollmean(returns, na.rm = T)), by = c("fdateYQ")]
#predsFactorsMerged[, ":=" ("firmMeanPred" = mean(returns, na.rm = T)), by = c("gvkey", "fdateYQ")]
#YX[, ":=" ("firmMeanPred" = mean(returns, na.rm = T)), by = c("gvkey"), roll = -Inf]

YX[, ":=" ("meanPred" = 0)]
for(q in unique(YX$fdateYQ) ){
  window = YX[YX$fdateYQ<q, c("returns")]
  if(nrow(window) == 0) ans = NA
  else ans = mean(window$returns)
  YX[YX$fdateYQ==q, ":=" ("meanPred" = ans)]
}

YX[, ":=" ("firmMeanPred" = c(NA, cumsum(returns) / seq_along(returns))[-length(returns)]), by = c("gvkey")]
predsFactorsMerged = merge(predsFactorsMerged, YX, by.x = c("fdateYQ", "gvkey"), by.y = c("fdateYQ", "gvkey"))

predsFactorsMerged[, ":=" ("NNER"=BestModelPred - RFpred, 
             "SecondER"=SecondModelPred - RFpred, 
             "ThirdER"=ThirdModelPred - RFpred, 
             "ZeroER"=ZeroPred - RFpred, 
             "firmMeanER"=firmMeanPred - RFpred,
             "MeanER"=meanPred - RFpred)]

# for(n in 1:nrow(YX) ){
#   window = YX[YX$fdateYQ < YX[n,]$fdateYQ & YX$fdateYQ==YX[n,]$gvkey, c("returns")]
#   if(nrow(window) == 0) ans = NA
#   else ans = mean(window$returns)
#   YX[YX$fdateYQ == YX[n,]$fdateYQ & YX$gvkey == YX[n,]$gvkey, ":=" ("firmMeanPred" = ans)]
#   if(n%%1000 == 0) print(n)
# }

```


```{r DieboldMariano}


#MSE-F

#OOS R-squared  = MSE / MSE0 = (compare best model to NaiveZeroPrediction)

#predsFactorsMerged

#Something like this (pseudocode):
#allPerformances["BestModelMSE"] / allPerformances["NaiveZeroMSE"]


```



```{r TangencyPortfolio}




#sharpeMaximizingWeights = function(inputPredReturn){#################################################################

#trainValidation = YXfactor[YXfactor$Split!="Test", !c("MktCap")]
pooledForVariance = YXfactor[YXfactor$Split!="Test", c("gvkey", "fdateYQ", "returns", "Split")]

# #For creating a covariance matrix
# pooledForVariance = do.call(rbind, trainValidation)
# pooledForVariance = as.data.table(pooledForVariance[, c("gvkey", "datacqtr", "returns")])
# # pooledForVariance = pooledForVariance[gvkey %in% firmPreds$gvkey,]
# #
# # returnMatrix<-dcast.data.table(data=pooledForVariance, datacqtr~gvkey)
# # dim(returnMatrix[,!"datacqtr"])
# # length(as.matrix(returnMatrix))
# # sum(is.na(returnMatrix))
# #
# # V = cov(returnMatrix[, !"datacqtr"])


# a = dcast.data.table(data=pooledForVariance, datacqtr~gvkey, value.var="returns")
# b = duplicated(pooledForVariance[,-c(returns)])
# d = pooledForVariance[b,]
# sum(duplicated(d[,-c(returns)]))


#For removing the possibility of duplicates
pooledForVariance = pooledForVariance[, "returns" := mean(returns, na.rm = T), by = c("gvkey", "fdateYQ")]
pooledForVariance = unique(pooledForVariance)

allPredsList = split(allPreds, allPreds$fdateYQ)
pooledForVarianceList = lapply(allPredsList, function(x) pooledForVariance[gvkey %in% x$gvkey,])
returnMatrixList = lapply(pooledForVarianceList, function(x) dcast.data.table(data=x, fdateYQ~gvkey, value.var="returns"))
VList = lapply(returnMatrixList, function(x) cov(x[, !"fdateYQ"], use="pairwise.complete.obs"))
# #VList = lapply(returnMatrixList, function(x) cov(x[, !"fdateYQ"], use="complete.obs"))

#returnMatrix = dcast.data.table(data=pooledForVariance, gvkey~fdateYQ+Split, value.var="returns")
pooledForVariance = YXfactor[, c("gvkey", "fdateYQ", "returns", "Split")]
returnMatrix = dcast.data.table(data=pooledForVariance, fdateYQ+Split~gvkey, value.var="returns")




#sharpeMaximizingWeights = function(quarter){#################################################################
# VList = list(); for(q in returnMatrix[Split!="Train"]$fdateYQ){
#   relevantReturns = returnMatrix[fdateYQ < q, !c("fdateYQ", "Split")]
#   relevantReturns = Filter(function(x) !all(is.na(x)), relevantReturns)
#   covMat = cov(relevantReturns, use="pairwise.complete.obs")
#   #covMat = cov(returnMatrix[fdateYQ < q, !c("fdateYQ", "Split")], use="pairwise.complete.obs")
#   covMat = covMat[rowSums(is.na(covMat)) < ncol(covMat), colSums(is.na(covMat)) < nrow(covMat)]
#   VList = c(VList, list(covMat))
#   print(q)
# }; names(VList) = returnMatrix[Split!="Train"]$fdateYQ;
# #names(VList) = as.character(returnMatrix[Split!="Train"]$fdateYQ);
# View(VList$`2017.5`[10:30,10:30]); View(VList$`2022.5`[10:30,10:30]);


CovMatFunction = function(q){
  relevantReturns = returnMatrix[fdateYQ < q, !c("fdateYQ", "Split")]
  relevantReturns = Filter(function(x) !all(is.na(x)), relevantReturns)
  covMat = cov(relevantReturns, use="pairwise.complete.obs")
  #covMat = cov(returnMatrix[fdateYQ < q, !c("fdateYQ", "Split")], use="pairwise.complete.obs")
  covMat = covMat[rowSums(is.na(covMat)) < ncol(covMat), colSums(is.na(covMat)) < nrow(covMat)]
  covMat = covMat[rowSums(is.na(covMat)) < ncol(covMat)/5+1, colSums(is.na(covMat)) < nrow(covMat)/5+1]
  covMat = covMat[rowSums(is.na(covMat)) == 0, colSums(is.na(covMat)) == 0]
  covMat
}

# V=VList[[10]]
# View(V)
# colSums(is.na(V))
# rowSums(is.na(V))

#VListLessNA = lapply(VList, function(x) x[rowSums(is.na(x)) != ncol(x), colSums(is.na(x)) != nrow(x)])

# VListLessNA = VList[1:2]
# VListLessNA = VList$`2017.5`[rowSums(is.na(VList$`2017.5`)) < ncol(VList$`2017.5`), 
#                              colSums(is.na(VList$`2017.5`)) < nrow(VList$`2017.5`)]
# VListLessNA = VList$`2022.5`[rowSums(is.na(VList$`2022.5`)) < ncol(VList$`2022.5`), 
#                              colSums(is.na(VList$`2022.5`)) < nrow(VList$`2022.5`)]
# 
# lapply(VListLessNA, function(DT)
#   DT[,
#      which(unlist(lapply(DT,
#                          function(x)
#                            ! all(is.na(
#                              x
#                            ))))),
#      with = F])

# # apply(VList$`2017.5`, FUN = function(x) all(is.na(x)), MARGIN = 1)
# # apply(VList$`2017.5`, FUN = function(x) all(!is.na(x)), MARGIN = 1)
# 
# VListLessNA = lapply(VList, function(x) x[rowSums(is.na(x)) < ncol(x), colSums(is.na(x)) < nrow(x)])
# # VListLessNA = lapply(VList, function(x) x[apply(x, FUN = function(x) all(is.na(x)), MARGIN = 1), 
# #                                           apply(x, FUN = function(x) all(is.na(x)), MARGIN = 2)])
# VListLessNA = lapply(VList, function(x) x[rowSums(is.na(x)) < ncol(x)/5+1, colSums(is.na(x)) < nrow(x)/5+1])
# VListLessNA = lapply(VListLessNA, function(x) x[rowSums(is.na(x)) == 0, colSums(is.na(x)) == 0])
# # V=VListLessNA[[10]]
# # View(V)




# RwithTickers = firmPreds[gvkey %in% pooledForVariance$gvkey,]
# R = RwithTickers$V1

# RwithTickers = lapply(allPreds, function(x) x[gvkey %in% pooledForVariance$gvkey,])
# R = lapply(RwithTickers, function(x) x$V1)

R = list()
RwithTickers = list()
for(n in 1:length(allPredsList)){
  RwithTicker = allPredsList[[n]][allPredsList[[n]]$gvkey %in% colnames(VListLessNA[[n]]), ]
  R = c(R, list(RwithTicker$predictedReturn))
  RwithTickers = c(RwithTickers, list(RwithTicker))
}



# #RAndRF = list(R=R, rf0=rf0)
# 
# rfMatrix=list()
# for(n in 1:length(R)){
#   rfRep = rep(rf0[n], length(R[[n]]))
#   rfMatrix = c(rfMatrix, list(rfRep))
# }
# 
# rfMatr = do.call(cbind, rfMatrix)
# RMatr = do.call(cbind, R)
# 
# excessReturn = RMatr - rfMatr
# 
# 


excessReturn=list()
for(n in 1:length(rf0)){
  eR = R[[n]] - rf0[n]
  excessReturn = c(excessReturn, list(eR))
}



# length(V)
# sum(is.na(V))
# dim(V)
# isSymmetric(V)
# 
# #doubleChecking if PSD with a ready made package
# #matrixcalc::is.positive.semi.definite(V)
# 
# 
# V=VList[[10]]
# #packages for taking inverses of matrices
# inverse = Rfast::spdinv(V)
# inverse = mnormt::pd.solve(V)
# inverse = chol2inv(chol(V))
# inverse = solve(V)
# inverse = MASS::ginv(V)
# inverse = matlib::inv(V)
# View(V[1:10,1:10])
# 
# 
# length(inverse)
# sum(is.na(inverse))
# dim(inverse)
# isSymmetric(inverse)
# #View(inverse[1:10,1:10])
# 
# #Tangency portfolio
# numerator = inverse %*% (R-rf)
# denominator = rep(1, nrow(inverse)) %*% inverse %*% (R-rf)
# weights <- numerator / denominator[1]   #dividing the numerator by denominator gives us the weights
# sum(weights)
# 
# 
# portfolio = data.table("gvkey" = RwithTickers$gvkey, "date" = yearMonth, "weight" = c(weights))
# 
# 
# testYwithKey = pooled[,c("gvkey", "returns")]
# portfolioWithRealized = merge(portfolio, testYwithKey, by = "gvkey") 
# sum(portfolioWithRealized$weight)



#Invert
invList = lapply(VListLessNA, MASS::ginv)
for(n in 1:length(invList)){
  colnames(invList[[n]]) = colnames(VListLessNA[[n]])
  rownames(invList[[n]]) = rownames(VListLessNA[[n]])
}

#invList2 = lapply(VListLessNA, Rfast::spdinv)
VList=VListLessNA

#Equivalent solution with lists
weights=list()
for(n in 1:length(excessReturn)) {
  V = invList[[n]]
  numerator = V %*% excessReturn[[n]]
  denominator = rep(1, nrow(V)) %*% V %*% excessReturn[[n]]
  weight = numerator / denominator[1]
  weights = c(weights, list(weight))
}


optimalPortfolios = c()
for(n in 1:length(returnMatrix[Split=="Test"]$fdateYQ)) {
  q = returnMatrix[Split=="Test"]$fdateYQ[n]
  
  #V = invList[[n]]
  V = CovMatFunction(q); cat(q, "CovMat", "\n");
  VColNames = colnames(V); VRowNames = rownames(V);
  V = MASS::ginv(V); cat(q, "CovMat inverted", "\n");
  colnames(V) = VColNames; rownames(V) = VRowNames
  
  ER = predsFactorsMerged[fdateYQ == q & gvkey %in% colnames(V), c("NNER", "SecondER", "ThirdER", 
                                                                   "ZeroER", "firmMeanER", "MeanER", 
                                                                   "gvkey", "fdateYQ")]
  V = V[rownames(V) %in% ER$gvkey, colnames(V) %in% ER$gvkey]
  
  #check correct order
  #as.character(eR$gvkey) == colnames(V)
  
  er = as.matrix(ER[,c("NNER", "SecondER", "ThirdER", 
                       "ZeroER", "firmMeanER", "MeanER")])
  #Optimal tangency formula
  numerator = V %*% er
  denominator = rep(1, nrow(V)) %*% V %*% er
  weight = sweep(numerator, 2, denominator, `/`)
  colnames(weight) = c("NNWeights", "ThirdER", "ZeroER", 
                       "ZeroWeights", "firmMeanWeights", "MeanWeights")
  
  ER = cbind(ER[, c("gvkey", "fdateYQ")], weight)
  optimalPortfolios = rbind(optimalPortfolios, ER)
  cat(q, "Optimal portfolio calculated", "\n")
  
  print(colSums(optimalPortfolios[,c("NNWeights", "ThirdER", "ZeroER", 
                             "ZeroWeights", "firmMeanWeights", "MeanWeights")], na.rm = T))
  cat("\n", "\n")
}



#Check
unlist(lapply(weights, sum))

#}################################################################
#sharpeMaximizingWeights()



# portfolios=list()
# for(n in 1:length(weights)) {
#   port = data.table("gvkey" = RwithTickers[[n]]$gvkey, "date" = cqtr[[n]], "weight" = c(weights[[n]]))
#   portfolios = c(portfolios, list(port))
# }


monthlyReturns = fread("Data/monthlyReturnData.csv")
#View(monthlyReturns[1:100,])
monthlyReturns$fdateYQ = paste(monthlyReturns$year, monthlyReturns$quarter, sep="Q")
monthlyReturns$fdateYQ = zoo::as.yearqtr(monthlyReturns$fdateYQ, format="%YQ%q")
monthlyReturns$YM = paste(monthlyReturns$year, monthlyReturns$month, sep="M")
monthlyReturns$YM = zoo::as.yearmon(monthlyReturns$YM, format="%YM%m")

monthlyReturns = monthlyReturns[ , c("gvkey", "monthlyReturns", "fdateYQ", "year", "month", "YM")]


optimalPortfoliosWithRealized = merge(optimalPortfolios, monthlyReturns, by = c("gvkey", "fdateYQ"))


#No duplicates! Great!
#anyDuplicated(monthlyReturns, incomparables=FALSE, fromLast=FALSE, by=c("gvkey", "fdateYQ", "month"))

# portfolioWithRealizedList = list()
# for(n in 1:length(weights)) {
#   portfolioWithRealized = merge(portfolios[[n]], monthlyReturns, by.x = c("gvkey", "date"), by.y = c("gvkey", "fdateYQ"))
#   portfolioWithRealizedList = c(portfolioWithRealizedList, list(portfolioWithRealized))
#   
#   sumN = sum(portfolioWithRealized$weight) #Should be approx. = 3
#   print(sumN)
# }




# #Pooled version. All inside one data.table
# pooledPortfolios=c()
# for(n in 1:length(weights)) {
#   port = data.table("gvkey" = RwithTickers[[n]]$gvkey, "fdateYQ" = cqtr[[n]], "weight" = c(weights[[n]]))
#   pooledPortfolios = rbind(pooledPortfolios, port)
# }
# 
# monthlyReturnsWithPortfolios = merge(monthlyReturns, pooledPortfolios, by = c("gvkey", "fdateYQ"))






# #testSet = testSetQuarters #From earlier in the script: TrainTestSplit_QuarterlySplit
# portfolioWithRealizedList = list()
# for(n in 1:length(weights)) {
#   
#   #cat("Model number", m)
#   set = testSet[[n]]
#   #set = Filter(is.numeric, set)
#   set = na.omit(set)
#   testYwithKey = set[, c("gvkey", "fdateYQ", "returns")]
#   portfolioWithRealized = merge(portfolios[[n]], testYwithKey, by = "gvkey")
#   portfolioWithRealizedList = c(portfolioWithRealizedList, list(portfolioWithRealized))
#   
#   sumN = sum(portfolioWithRealized$weight)
#   print(sumN)
# }


```



```{r NaivePortfolios}

# #Equal-weighted portfolio: 1/n
# portfolioWithRealizedList = lapply(portfolioWithRealizedList, 
#                                    FUN= function(x) x[,":=" ("equalWeight"=1/length(gvkey)), by = fdateYQ])
# 
# monthlyReturnsWithPortfolios = monthlyReturnsWithPortfolios[, ":=" ("equalWeights"= 1/length(gvkey)), by = fdateYQ]

optimalPortfoliosWithRealized = optimalPortfoliosWithRealized[, ":=" ("equalWeights"= 1/length(gvkey)), by = YM]



#Value-weighted (Global) portfolio: MktCap / sum(AllMktCaps)

#OLS model: Above script

#Best model 1, 2, and 3: Above script

#NaiveZeroModel: Above script with R = 0

#NaiveMeanModel: Above script with R = mean

#NaiveFirmMeanModel: Above script with R = mean_n



```



```{r AbnormalReturns}

optimalPortfoliosWithRealized[, ":=" ("NNReturn" = NNWeights * monthlyReturns, 
                           "ZeroReturn" = ZeroWeights * monthlyReturns,
                           "MeanReturn" = MeanWeights * monthlyReturns,
                           "FirmMeanReturn" = firmMeanWeights * monthlyReturns,
                           "EWReturn" = equalWeights * monthlyReturns)]
optimalPortfoliosWithRealized[, ":=" ("NNPortfolioReturn" = sum(NNReturn),
                           "ZeroPortfolioReturn" = sum(ZeroReturn),
                           "MeanPortfolioReturn" = sum(MeanReturn),
                           "FirmMeanPortfolioReturn" = sum(FirmMeanReturn),
                           "EWPortfolioReturn" = sum(EWReturn)), by = c("YM")]

realizedPortfolios = unique(optimalPortfoliosWithRealized[, c("YM", 
                                                   "NNPortfolioReturn", 
                                                   "ZeroPortfolioReturn", 
                                                   "MeanPortfolioReturn", 
                                                   "FirmMeanPortfolioReturn", 
                                                   "EWPortfolioReturn")], by=c("YM"))

optimalPortfoliosWithRealized[, list(sum(NNWeights), 
                                     sum(MeanWeights), 
                                     sum(firmMeanWeights), 
                                     sum(equalWeights)), by = c("YM")]


monthlyFactors=fread("FactorData/allMonthlyFactors.csv")
monthlyFactors$YM = zoo::as.yearmon(as.character(monthlyFactors$V1), format = "%Y%m")


factorsWithReturns = merge(realizedPortfolios, monthlyFactors, by.x = "YM", by.y="YM", all = TRUE)
factorsWithReturns[, ":=" ("NNPortfolioER" = NNPortfolioReturn - RF.DM,
                           "ZeroPortfolioER" = ZeroPortfolioReturn- RF.DM,
                           "MeanPortfolioER" = MeanPortfolioReturn- RF.DM,
                           "FirmMeanPortfolioER" = FirmMeanPortfolioReturn - RF.DM,
                           "EWPortfolioER" = EWPortfolioReturn - RF.DM), by = c("YM")]

NNF = `NNPortfolioER` ~ `Mkt-RF.DM` + `SMB.DM` + `HML.DM` + `RMW.DM` + `CMA.DM` + `WML.DM` + `Mkt-RF.EM` + `SMB.EM` + `HML.EM` + `RMW.EM` + `CMA.EM` + `WML.EM`

ZeroF = `ZeroPortfolioER` ~ `Mkt-RF.DM` + `SMB.DM` + `HML.DM` + `RMW.DM` + `CMA.DM` + `WML.DM` + `Mkt-RF.EM` + `SMB.EM` + `HML.EM` + `RMW.EM` + `CMA.EM` + `WML.EM`

MeanF = `MeanPortfolioER` ~ `Mkt-RF.DM` + `SMB.DM` + `HML.DM` + `RMW.DM` + `CMA.DM` + `WML.DM` + `Mkt-RF.EM` + `SMB.EM` + `HML.EM` + `RMW.EM` + `CMA.EM` + `WML.EM`

FirmMeanF = `FirmMeanPortfolioER` ~ `Mkt-RF.DM` + `SMB.DM` + `HML.DM` + `RMW.DM` + `CMA.DM` + `WML.DM` + `Mkt-RF.EM` + `SMB.EM` + `HML.EM` + `RMW.EM` + `CMA.EM` + `WML.EM`

EWF = `EWPortfolioER` ~ `Mkt-RF.DM` + `SMB.DM` + `HML.DM` + `RMW.DM` + `CMA.DM` + `WML.DM` + `Mkt-RF.EM` + `SMB.EM` + `HML.EM` + `RMW.EM` + `CMA.EM` + `WML.EM`

NNFamaFrench = lm(NNF, data=factorsWithReturns)
ZeroFamaFrench = lm(ZeroF, data=factorsWithReturns)
MeanFamaFrench = lm(MeanF, data=factorsWithReturns)
FirmMeanFamaFrench = lm(FirmMeanF, data=factorsWithReturns)
EWFamaFrench = lm(EWF, data=factorsWithReturns)
#summary(FamaFrenchSix)

#models = list(NNFamaFrench, ZeroFamaFrench, MeanFamaFrench, FirmMeanFamaFrench, EWFamaFrench)
models = list(NNFamaFrench, MeanFamaFrench, FirmMeanFamaFrench, EWFamaFrench)

#There are weight issues!!!
#And where is risk-free return???

#Look at the portfolio returns, Are there huge outliers?





# # realizedPortfolios = lapply(portfolioWithRealizedList, function(x) x[, c("weight", "monthlyReturns", "YM")])
# # realizedPortfolios = lapply(realizedPortfolios, function(x) x[, "portfolioReturn" := x$weight * x$monthlyReturns])
# 
# #Using lists
# #realizedPortfolios = lapply(portfolioWithRealizedList, function(x) x$weight * x$returns)
# realizedPortfolios = lapply(portfolioWithRealizedList, function(x) x$weight * x$monthlyReturns)
# realizedPortfolios = lapply(portfolioWithRealizedList, function(x) x$weight * x$monthlyReturns)
# portfolioReturn = lapply(realizedPortfolios, function(x) sum(x))
# 
# 
# dateList = lapply(portfolioWithRealizedList, function(x) x$YM)
# portfolioPerformance = cbind("Excess Return"=unlist(portfolioReturn), "date"=unlist(dateList))
# #factorsWithReturns = merge(portfolioPerformance, quarterlyFactors, by.x = "date", by.y="YQ", all = TRUE)
# factorsWithReturns = merge(portfolioPerformance, monthlyFactors, by.x = "YM", by.y="YM", all = TRUE)
# 
# # dateList = lapply(portfolioWithRealizedList, function(x) max(x$date))
# # portfolioPerformance = cbind("Excess Return"=unlist(portfolioReturn), "date"=unlist(dateList))
# # #factorsWithReturns = merge(portfolioPerformance, quarterlyFactors, by.x = "date", by.y="YQ", all = TRUE)
# # factorsWithReturns = merge(portfolioPerformance, monthlyFactors, by.x = "date", by.y="V1", all = TRUE)
# 
# 
# f = `Excess Return` ~ Mkt+SMB+HML+RMW+CMA+WML
# FamaFrenchSix = lm(f, data=factorsWithReturns)
# summary(FamaFrenchSix)

```


```{r FamaFrenchReporting}


#winsorize
#winData = Winsorize(data, probs = c(0.01, 0.99), na.rm=T)



#maybe add diagnostic tests
residuals = lapply(models, residuals)
JB = lapply(residuals, moments::jarque.test)
BP = lapply(models, lmtest::bptest, data=factorsWithReturns)
BG = lapply(models, lmtest::bgtest, order=7, data=factorsWithReturns)

VIF = as.data.frame(lapply(models, function(x) data.frame(round(car::vif(x), 2))))
colnames(VIF)=c("NNVIF", "MeanVIF", "FirmMeanVIF", "EWVIF")
VIF1 = VIF[1]
colnames(VIF1) = "VIF"
write.csv(VIF1, "Results/VIF3.csv")



# #Unit root for panel data
# finalNumericData = Filter(is.numeric, YXfactor[, -c("gvkey", "fdateq")])
# #sum(is.na(finalNumericData))
# finalNumericData = na.omit(finalNumericData)
# compustatStationarity = plm::purtest(finalNumericData)




# #Unit root
# fUnitRoots::adfTest(factorsWithReturns$excessReturn, lags = 1, type = c("nc"), title = NULL, description = NULL)


# #Slumpmässig eller fasteffekt (Hausman)
# phtest(firmRandom,firmFixed)
# summary(firmFixedTwo) #bättre
# 
# #Slump eller pooled
# plmtest(factorsWithReturns, type="bp")
# 
# #Fast eller pooled
# plm::pFtest(firmFixedTwo,firmPooledTwo)
# lmtest::waldtest(firmFixedTwo,firmPooledTwo)
# plm::pwaldtest(firmFixed,firmPooled)


diagnostics = list(c("Jarque-Bera", unlist(lapply(JB, function(x) round(x$p.value, 3)))),
                   c("Breusch-Pagan", unlist(lapply(BP, function(x) round(x$p.value, 3)))),
                   c("Breusch-Godfrey", unlist(lapply(BG, function(x) round(x$p.value, 3)))))

#Export
stargazer::stargazer(models,
                     type="text",
                     report=("vc*p"),
                     add.lines = diagnostics,
                     out = "Results/FamaFrenchSixFactorsWithNaiveModels.html"
                     )



#robust errors: HAC?
#robustErrors = sandwich::vcovHC(models[[1]], method="arellano")
robustErrors = lapply(models, sandwich::vcovHAC)


#Export wirh Robust errors
stargazer::stargazer(models,
                     type="text",
                     report=("vc*p"),
                     se  = robustErrors,
                     out = "Results/FamaFrenchSixFactorsWithRobustErrors.html"
                     )



```



```{r RNN Keras}


model <- keras_model_sequential() 
model %>% 
         layer_embedding(input_dim = 500, output_dim = 32) %>%
         layer_simple_rnn(units = 32) %>%  
         layer_dense(units = 1, activation = "sigmoid")

```


```{r LSTM Keras}

model <- keras_model_sequential() 
model %>% 
         layer_embedding(input_dim = 500, output_dim = 32) %>%
         layer_lstm(units = 32) %>%  
         layer_dense(units = 1, activation = "sigmoid")

```


```{r ForecastPlot}



#dccForecast = dccforecast(triGarch, n.roll = triForecastLength)

```




```{r OutputSummary}

#For nicer output (printing)
comparisonMatrix = matrix(c(NN1MSE, NN1MAE, 
                            lassoMSE, lassoMAE), 
                          ncol=2,
                          byrow=TRUE)
comparisonMatrix = round(comparisonMatrix, 4)

colnames(comparisonMatrix) = c("MSE", "MAE")
rownames(comparisonMatrix) = c("Neural network with one hidden layer", 
                               "Linear regression with Lasso regularisation")
print(comparisonMatrix)


```




