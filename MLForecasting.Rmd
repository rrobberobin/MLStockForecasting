---
title: "Master's Thesis"
author: "Robin Perala"
date: "2022-09-10"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown


```{r libs}


#Check files: RnD, Lab8-Garch, OptimalHedgeRatio, Ex4Part2, nnboston, HW7, Gridsearch, LSTM



# library(torch)
# library(luz) # high-level interface for torch
# library(torchvision) # for datasets and image transformation
# library(torchdatasets) # for datasets we are going to use
# library(zeallot)


# library(dplyr) #Contains "mutate_if" function
# library(magrittr) #Contains pipes, e.g. %<>%
# # library(neuralnet) #For fitting neural networks
# # library(keras) #Contains "keras_model_sequential" function
# library(glmnet) #For regularized linear models (e.g. lasso and ridge)



```




```{r Compustat}

#I should clean data
#Check all variables what they are. Remove unnecessary
#2 414 247 obs. of  408 =>

#Import data
require(data.table)
data <- fread("Data/OriginalData/Compustat.csv")
#orderedData = data[order(names(data))]
#setcolorder(data, order(names(data)))
#orderedData = setorderv(data, order(names(data)))[]
#orderedData = data[order(data$capr1q), ]


#Which firms do not have quarters?
quarterIndex = !data$datacqtr==""
dataNoQuarter = data[quarterIndex, ]
#dataNoNA = Filter(!data$datacqtr=="", data)


#We have both banks and industrials

#Clean data
sumOfNotNA = apply(dataNoQuarter, 
                  MARGIN=2,
                  FUN = function(x){
                    sum(!is.na(x))
                    }
                  )
#View(sumOfNotNA)

lessThanHalfNA = apply(dataNoNA, 
                  MARGIN=2,
                  FUN = function(x){
                    sum(!is.na(x))>nrow(data)/2
                    }
                  )


#First: Remove columns with over half NA
dataFullCol = dataNoQuarter[,lessThanHalfNA]

#Second: Remove all rows with NA
dataNoNA = na.omit(dataFullCol)
#78 166 x 137

fwrite(dataNoNA,"Data/CleanData.csv", row.names = FALSE)



```



```{r GGPlot}


startQuarter = min(data[!data$datacqtr=="",datacqtr])
endQuarter = max(data[!data$datacqtr=="",datacqtr])
dataRange = paste(as.character(startQuarter), "-", as.character(endQuarter))
quarters = length(unique(data$datacqtr))
years = quarters/4

#sortedQuarters = sort(unique(YXfactor$datacqtr))


data$datacqtr = zoo::as.yearqtr(data$datacqtr, format = "%YQ%q")


#Plot for document
require(ggplot2)
ggplot(data = data[datacqtr>"2009 Q3",], aes(x = datacqtr)) +
  geom_bar(stat = "count") +
  labs(title = "Compustat Full Data",
       subtitle = dataRange,
       x = "Quarter", y = "Observations")

yearlyFrequencyOriginalData = table(data$datacqtr)
View(yearlyFrequencyOriginalData)






#Same for cleaned data
cleanData = fread("Data/CleanData.csv")

startQuarter = min(cleanData[!cleanData$datacqtr=="",datacqtr])
endQuarter = max(cleanData[!cleanData$datacqtr=="",datacqtr])
dataRange = paste(as.character(startQuarter), "-", as.character(endQuarter))
quarters = length(unique(cleanData$datacqtr))
years = quarters/4

#sortedQuarters = sort(unique(YXfactor$datacqtr))


cleanData$datacqtr = zoo::as.yearqtr(cleanData$datacqtr, format = "%YQ%q")

ggplot(data = cleanData, aes(x = datacqtr)) +
  geom_bar(stat = "count") +
  labs(title = "Compustat Filtered Data",
       subtitle = dataRange,
       x = "Quarter", y = "Observations")

yearlyFrequencyCleanData = table(cleanData$datacqtr)
View(yearlyFrequencyCleanData)


temporalFrequencyTable = merge(yearlyFrequencyOriginalData, yearlyFrequencyCleanData, 
                               by = "Var1", suffixes = c(".Original", ".Filtered"), all = TRUE)

write.csv(temporalFrequencyTable, "Descr/temporalFrequencyTable.csv")



```



```{r WithoutMissingValues}
#Second clean

require(data.table)
cleanData = fread("Data/CleanData.csv")
setcolorder(cleanData, order(names(cleanData)))


# allNA = apply(cleanData, 
#               MARGIN=2,
#               FUN = function(x){ 
#                 all(is.na(x))
#                 }
#               )
# sum(allNA)
# 
# 
# orderedCleanData = cleanData[ , order(names(cleanData))]
# 
# #First: Remove columns with over half NA
# dataFullCol = orderedCleanData[,!allNA]
# 
# #Second: Remove all rows with NA
# dataNoNA = na.omit(dataFullCol)
```


```{r priceData}

#Import price data
require(data.table)
prices <- fread("Data/OriginalData/CompustatPrices.csv")
#221 835 959 x 6

#data       final    preliminary
#datadate   fdateq   pdateq
# 20100331 20100503 20100503
# 20100630 20100907 20100801
# 20100930 20101105 20101105
# 20101231 20110330 20110306
# 20110630 20110811 20110730
# 20111231 20120331 20120309
#Check update code in fundamentals data

nrow(cleanData[cleanData$updq==3,]) 
#almost all data is final (update code 3, instead of 2)
#I will use "fdateq" as a conservative approach


#reduce data size
removeGVKEY = prices[prices$gvkey %in% cleanData$gvkey,]

#Oldest date in cleanData is 20091130S
newPrices = removeGVKEY[removeGVKEY$datadate>=(min(cleanData$fdateq)-300),]

fwrite(newPrices,"Data/CleanPrices.csv", row.names = FALSE)

```


```{r combineCompustatAndCRSP}

require(data.table)
newPrices <- fread("Data/CleanPrices.csv")
cleanData <- fread("Data/CleanData.csv")


#View(newPrices[1:1000,])
#View price data in wide format
#widePrices = dcast(newPrices[,], datadate~gvkey + iid, value.var = "prccd")


compuStat=cleanData
#https://stackoverflow.com/questions/35046161/how-to-do-a-data-table-rolling-join
# create an interval in the 'compuStat' data.table


#Rebalancing 1.2, 1.5, 1.8, and 1.11
periodization = function(date){
  YYYY = signif(date, 4)
  MMDD = date %% 10000
  
  if(1100<MMDD) {res = YYYY + 201 + 10000}
  else if(800<MMDD) {res = YYYY + 1101}
  else if(500<MMDD) {res = YYYY + 801}
  else if(200<MMDD) {res = YYYY + 501}
  else {res = YYYY + 201}
  "start" = res
}

start = unlist(lapply(compuStat$fdateq, FUN=periodization))
end = start + 2    #56753 with 1    #65325 with 2     #65 331 with 3     #65 333 with 4    #65 334 with 5   #usually I use +7
compuStat[, `:=` (start=start, end=end)]

#compuStat[, `:=` (start = fdateq + 1, end = fdateq + 300)]
#as.POSIXct(zoo::as.yearqtr(strptime(20221212, format = "%Y%m%d")))


# create a second date in the 'prices' data.table
newPrices[, Date2 := datadate]

# set the keys for the two data tables
setkey(compuStat, gvkey, prirow, start, end)
setkey(newPrices, gvkey, iid, datadate, Date2) #iid is "Issue ID"

# create a vector of columnnames which can be removed afterwards
deletecols <- c("Date2", "start", "end", "prirow", "i.datadate")

# perform the overlap join and remove the helper columns
mergedData <- foverlaps(compuStat, newPrices)[, (deletecols) := NULL]



##alternative: non-equi join
# compuStat[, `:=` (start = fdateq - 100, end = fdateq - 1)]
# mergedData2 = newPrices[compuStat, on = .(gvkey, datadate >= start, datadate <= end)]


mergedData = mergedData[order(mergedData$gvkey, -mergedData$datadate, -mergedData$fdateq),]
finalPriceData = mergedData[!duplicated(cbind(mergedData$fdateq,
                                                mergedData$gvkey)),]

finalPriceData = finalPriceData[order(finalPriceData$gvkey, finalPriceData$fdateq),] 

#Remove non-primary IDs
#finalPriceData = finalPriceData[finalPriceData$iid==finalPriceData$prirow,]

fwrite(finalPriceData,"Data/finalPriceData.csv", row.names = FALSE)


#Write into word doc:
# StartDate >= ReleaseDate(YYYYMMDD)+1
# && 
# StartDate < ReleaseDate(YYYYMMDD)+100
# Minimum Date => First date where this holds

##Should be: datadate == fdateq + 1
# mergedData = merge(cleanData, removeGVKEY, 
#                    by.x = c("gvkey","fdateq"),
#                    by.y = c("gvkey","datadate"))

#https://stackoverflow.com/questions/16095680/merge-dataframes-on-matching-a-b-and-closest-c
# removeGVKEY <- data.table(removeGVKEY, key = c("gvkey","datadate"))
# cleanData <- data.table(cleanData, key = c("gvkey","fdateq"))
# mergedData <- cleanData[removeGVKEY, roll=-1]

```


```{r calculateReturnsAndMktCaps}
require(data.table)
finalPriceData <- fread("Data/finalPriceData.csv")

#View price data in wide format
#widePrices = dcast(finalPriceData, datadate~gvkey + iid, value.var = "prccd")


ret = function(price, returnFactor, adjustmentFactor) {
  n = length(price)
  numerator = returnFactor[-1]*price[-1]/adjustmentFactor[-1]
  denominator = (returnFactor[-n]*price[-n]/adjustmentFactor[-n])
  
  (numerator/denominator-1) * 100
}

MktCapFunction = function(price, sharesOutstanding) {
  price * sharesOutstanding
}


returns = ret(price=finalPriceData$prccd, 
                 returnFactor=finalPriceData$trfd, 
                 adjustmentFactor=finalPriceData$ajexdi)

MktCap = MktCapFunction(price = finalPriceData$prccd,
                         sharesOutstanding = finalPriceData$cshoc)



#I'm dropping the last observation here, which might not be necessary. 
#I could implement an algorithm which incldes the observation
returnData = cbind("returns" = returns, "MktCap" = MktCap[-length(MktCap)], finalPriceData[-nrow(finalPriceData), ])



# #1 is there because I already dropped the first row when calculating returns
# duplicates = duplicated(cbind(returnData$gvkey, returnData$iid))
# duplicates[1] = TRUE
#returnData = returnData[duplicates,]

fwrite(returnData,"Data/finalData.csv", row.names = FALSE)




#I think everything above is wrong; while this is correct
returnData2 = finalPriceData
returnData2 = returnData2[, "returns" := c(ret(prccd, trfd, ajexdi), NA), by = gvkey]
returnData2 = returnData2[, "MktCap" := MktCapFunction(prccd, cshoc)]
returnData2 = returnData2[!is.na(returnData2$returns),]


# duplicates = duplicated(cbind(returnData$gvkey, returnData$iid))
# duplicates[1] = TRUE
# returnData = returnData[duplicates,]


fwrite(returnData2,"Data/finalData.csv", row.names = FALSE)


```


```{r Descriptives}

require(data.table)
cleanData <- fread("Data/cleanData.csv") #this whole  chunk should be updated to finalData.csv

colnames(cleanData)[122]
numsData = cleanData[21:105]


#Descriptive statistics
library(moments)
Skewness = c(skewness(numsData, na.rm=T))
Kurtosis = c(kurtosis(numsData, na.rm=T))
kS = round(cbind(Skewness, Kurtosis), 0)
write.csv(kS, "Descr/kurtSkew.csv")




library(stargazer)
stargazer(numsData,
          type = "text",
          out = "Descr/numsDesc.html",
          title = "Descriptive statistics",
          style = "default", #"aer", "qje"
          flip = F,
          digits = 0,
          digits.extra = 0,
          median=T,
          iqr=T#,
          #add.lines = list(Skewness, Kurtosis)
          )

groupData = cleanData[,123:134]

countryIndustry = table(cleanData[,c("loc", "gsector")]) #GICS industry code
allIndustries = apply(countryIndustry, MARGIN=2, sum)
countryIndustryWithIndSum = rbind(countryIndustry, "All Industries"=allIndustries)
allCountries = apply(countryIndustryWithIndSum, MARGIN=1, sum)
countryIndustry = cbind(countryIndustryWithIndSum, "All Countries"=allCountries)

write.csv(countryIndustry, file = "Descr/countryIndustry.csv")



countryIndustry = table(finalData[,c("loc", "gsector")]) #GICS industry code
allIndustries = apply(countryIndustry, MARGIN=2, sum)
countryIndustryWithIndSum = rbind(countryIndustry, "All Industries"=allIndustries)
allCountries = apply(countryIndustryWithIndSum, MARGIN=1, sum)
countryIndustry = cbind(countryIndustryWithIndSum, "All Countries"=allCountries)

write.csv(countryIndustry, file = "Descr/countryIndustry2.csv")


countryIndustry = table(data[,c("loc", "gsector")]) #GICS industry code
allIndustries = apply(countryIndustry, MARGIN=2, sum)
countryIndustryWithIndSum = rbind(countryIndustry, "All Industries"=allIndustries)
allCountries = apply(countryIndustryWithIndSum, MARGIN=1, sum)
countryIndustry = cbind(countryIndustryWithIndSum, "All Countries"=allCountries)

write.csv(countryIndustry, file = "Descr/countryIndustryOrgFile.csv")


industryNames = c(
  "10 Energy",
  "15 Materials",
  "20 Industrials",
  "25 Consumer Discretionary (Consumer Cyclical)",
  "30 Consumer Staples (Consumer Defensive)",
  "35 Health Care",
  "40 Financials",
  "45 Information Technology",
  "50 Communication Services",
  "55 Utilities",
  "60 Real Estate",
  "Country"
)
colnames(countryIndustryOrg) = industryNames
countryIndustryFormatted = knitr::kable(countryIndustryOrg)


#How many companies? 8450
dim(table(cleanData[,c("gvkey")]))


```




# ```{r TrainTestSplit}
#
# # library(dplyr) #Contains "mutate_if" function
# # library(magrittr) #Contains pipes, e.g. %<>%
# # X2 = X
# # X2 %<>% mutate_if(is.factor, as.numeric)
# # X3 <- as.matrix(X2)
#
# require(data.table)
# finalData <- fread("Data/finalData.csv")
# #finalData2 <- fread("Data/finalData2.csv")
#
#
# orderedData = finalData[order(-finalData$fdateq, finalData$gvkey, finalData$iid, ),]
# testSize = ceiling(nrow(finalData)*0.2)
# testStartDate = orderedData$fdateq[testSize]
# test = orderedData[orderedData$fdateq>=testStartDate,]
#
# #Training-Validation split
# trainValidation = orderedData[orderedData$fdateq<testStartDate,]
#
# # n <- nrow(trainValidation)
# # nr <- 10
# # tenFold = split(trainValidation, rep(1:ceiling(nr/n), each=n, length.out=nr))
#
# #Validation set (50-50 split)
# # validation=sample(c(TRUE, FALSE), nrow(X), rep=TRUE)
# # trainTest = !validation
#
# foldSize <- nrow(trainValidation)/10
# a <- seq_along(trainValidation$fdateq)
# tenFoldDates <- split(trainValidation$fdateq, ceiling(a/foldSize))
#
# tenFoldDates = unlist(tenFoldDates)
# tenFoldEndDates = lapply(tenFoldDates, function(l) l[[1]])
# tenFoldEndDates = rev(unlist(tenFoldEndDates))
#
#
#
# dataLeft = trainValidation
# tenFold = list()
# for(n in 1:10){
#   fold = dataLeft[dataLeft$fdateq <= tenFoldEndDates[n]]
#   tenFold[[n]] = fold
#   dataLeft = dataLeft[dataLeft$fdateq > tenFoldEndDates[n]]
# }
#
#
#
# ```




# ```{r TrainTestSplitSecondTry_QuarterlySplit}
#
#
#
# require(data.table)
# finalData <- fread("Data/finalData.csv")
# #finalData2 <- fread("Data/finalData2.csv")
#
#
# orderedData = finalData[order(-finalData$fdateq, finalData$gvkey, finalData$iid, ),]
# testSize = ceiling(nrow(finalData)*0.2)
# testStartDate = orderedData$fdateq[testSize]
# test = orderedData[orderedData$fdateq>=testStartDate,]
#
# #Training-Validation split
# trainValidation = orderedData[orderedData$fdateq<testStartDate,]
#
#
# dataLeft = trainValidation
# quarters = list()
#
# startDate = min(trainValidation$fdateq)
# startYear = startDate[1:4]
# floor(date/100) * 100
#
# quarter = startQuarter
# endDate = max(trainValidation$fdateq)
#
# for(n in 1:10){
#   fold = dataLeft[dataLeft$fdateq <= tenFoldEndDates[n]]
#   tenFold[[n]] = fold
#   dataLeft = dataLeft[dataLeft$fdateq > tenFoldEndDates[n]]
#   quarter = quarter + 300
# }
#
#
#
#
#
#
# ```

```{r removingExtraVariables}

require(data.table)
finalData <- fread("Data/finalData.csv")

#prirow is the "primary issue tag". Only use primary issue
#finalData=finalData[finalData$iid==finalData$prirow,]

#Removing Unncecessary variables 
variableNames = c("returns", "MktCap", "gvkey", "fdateq", "datadate", "datacqtr", "curcdq","accdq", "acoq", "acoxq", "actq", "ancq", "aoq", "apq", "atq", "capsq", "ceqq", "cheq", "cogsq", "cstkq", "dfxaq", "dlcq", "dlttq", "dpq", "eqrtq", "eroq", "gpq", "ibmiiq", "ibq", "iditq", "intanq", "invtq", "ivaoq", "lcoq", "lcoxq", "lctq", "lltq", "loq", "lseq", "ltmibq", "ltq", "nopioq", "nopiq", "oiadpq", "oibdpq", "piq", "ppentq", "reccoq", "rectoq", "rectq", "rectrq", "req", "revtq", "saleq", "sctq", "seqq", "teqq", "txtq", "xintq", "xoproq", "xoprq", "xsgaq", "capxy", "chechy", "cogsy", "dfxay", "dpcy", "dpy", "fincfy", "fopoy", "gpy", "ibcy", "ibmiiy", "iby", "idity", "ivncfy", "ltdchy", "nopioy", "nopiy", "oancfy", "oiadpy", "oibdpy", "piy", "recchy", "revty", "saley", "txty", "wcapopcy", "xinty", "xoproy", "xopry", "xsgay", "exchg", "fic", "city", "fyrc", "ggroup", "gind", "gsector", "gsubind", "idbflag", "loc", "naics", "sic")

#What is i.datadate???
#a = finalData[,c("datadate","i.datadate")]


#YX = lapply(YXfactor, function(x){x[, ..variableNames]})
#YX = lapply(YX, function(x){na.omit(x)})
YX = finalData[, ..variableNames]


#removeMissingCalendarData. I could probably manually add these three instead. A simple script for adding
#which(finalData$datacqtr=="")
#finalData[c(12721, 68453, 71789),]
YX=YX[!YX$datacqtr=="",]

#Trash??
#a = factor(YX[1]$'2009Q4'$sic)
YX[,"gvkey"] = apply(YX[,"gvkey"], MARGIN = 2, factor)



fwrite(YX,"Data/YX.csv", row.names = FALSE)

```

```{r ImportHereAndFactorizing}

require(data.table)
#YX <- fread("Data/YX.csv")
YX <- fread("Data/ScaledWinsRatioData.csv")


#factorizing
YXfactor = YX
factorCols = c("gvkey", "curcdq", "exchg", "fic", "city", "fyrc",
        "ggroup", "gind", "gsector", 
        "gsubind", "idbflag", "loc", 
        "naics", "sic", "fdateq", "datadate")#, "datacqtr")

YXfactor[, c(factorCols) := lapply(.SD, factor), .SDcols = c(factorCols)]

#Create date objects
# YXfactor$fdateq = as.Date(YXfactor$fdateq, format = '%Y%m%d')
# YXfactor$datadate = as.Date(YXfactor$datadate, format = '%Y%m%d')
# YXfactor$datacqtr = zoo::as.yearqtr(YXfactor$datacqtr, format = "%YQ%q")


#Remove non-numerics
finalNumericData = Filter(is.numeric, YXfactor[, -c("gvkey", "fdateq", "MktCap")])
#sum(is.na(finalNumericData))
finalNumericData = na.omit(finalNumericData)



```

```{r TrainValidationTestQuarterlyPlot}

#Plot the time series
# require(ggplot2)
# ggplot(data = YXfactor, aes(x = fdateq)) +
#       geom_bar(stat = "count", fill = "green")
# 
# ggplot(data = YXfactor, aes(x = datadate)) +
#       geom_bar(stat = "count", fill = "red")

startQuarter = min(YXfactor$datacqtr)
endQuarter = max(YXfactor$datacqtr)
dataRange = paste(as.character(startQuarter), "-", as.character(endQuarter))
quarters = length(unique(YXfactor$datacqtr))
years = quarters/4

sortedQuarters = sort(unique(YXfactor$datacqtr))

trainValidSplitPoint = round(quarters*0.4)
validTestSplitPoint = round(quarters*0.7)

trainQuarters = sortedQuarters[1:trainValidSplitPoint]
validQuarters = sortedQuarters[trainValidSplitPoint:validTestSplitPoint]
testQuarters = sortedQuarters[validTestSplitPoint:quarters]

tTVSplitFunction = function(x){
  if(x %in% trainQuarters) {"Train"}
  else if(x %in% validQuarters) {"Validation"}
  else {"Test"}
}

YXfactor = YXfactor[, "Split" := unlist(lapply(datacqtr, tTVSplitFunction))]
YXfactor$Split = factor(YXfactor$Split)


#Plot for document
require(ggplot2)
ggplot(data = YXfactor, aes(x = datacqtr)) +
  geom_bar(stat = "count") +
  labs(title = "Final Data",
       subtitle = dataRange,
       x = "Quarter", y = "Observations") +
  zoo::scale_x_yearqtr(format = '%Y', n = years)


ggplot(data = YXfactor, aes(x = datacqtr, fill = Split)) +
  geom_bar(stat = "count") +
  labs(title = "Train-Validation-Test Quarterly Split",
       subtitle = dataRange, 
       x = "Quarter", y = "Observations") + 
  zoo::scale_x_yearqtr(format = '%Y', n = years)



#fwrite(YXfactor,"Data/YXWithSplit.csv", row.names = FALSE)



```



```{r stationarityAndUnitRoot}
#Stationarity test
plm::purtest(finalNumericData)
sum(is.na(finalNumericData))


#Check biggest return
min(finalData$returns, na.rm = T) #-99.27552
max(finalData$returns, na.rm = T) #4561.336

#Returns stationarity (Will not anyway winsorize these). This is wrong as we have panel data
#fUnitRoots::adfTest(finalData$returns, lags = 252, type = c("nc"), title = NULL, description = NULL)
#plm::purtest(finalNumericData, index="gvkey")


```



```{r TrainTestSplit_QuarterlySplit}

#YXfactor <- fread("Data/YXWithSplit.csv")


finalData = YXfactor

#sorting
orderedData = finalData[order(-finalData$fdateq, finalData$gvkey),]

quarterGroups = split(orderedData, orderedData$datacqtr)

splitPoint = round(length(quarterGroups)*0.7)
testSize = splitPoint:length(quarterGroups)
trainValidationSize = 1:(splitPoint-1)

testSet = quarterGroups[testSize]
trainValidation =  quarterGroups[trainValidationSize]





testSet = YXfactor[YXfactor$Split=="Test",]
testSet$datacqtr = factor(testSet$datacqtr)
testSetQuarters = split(testSet, testSet$datacqtr)



#Checking if the data periodization makes sense
# for(n in testSet){
#   print(c(min(n$fdateq), max(n$fdateq)))
# }
# 
# for(n in trainValidation){
#   print(c(min(n$fdateq), max(n$fdateq)))
# }
# 
# 
# #Minimum dates look approximately fine. Maximum dates on the other hand...
# which(trainValidation$'2009Q4'$fdateq == max(trainValidation$'2009Q4'$fdateq))
# 
# #Check this for the initial dataset. Is this an error?
# finalData[finalData$gvkey==285237,]

```


```{r CVfunction}




```

```{r removeRedVariables}


```


```{r Pooling}

drop = c("datacqtr", "fdateq")

sparseTestSet = lapply(testSet, function(x) {
  Matrix::sparse.model.matrix(returns ~ ., x[,!..drop])
})

sparseTrainValidation = lapply(trainValidation, function(x) {
  Matrix::sparse.model.matrix(returns ~ ., x[, !..drop])
})



# sparseYX = Matrix::sparse.model.matrix(returns ~ ., YX)
# Y = YX$returns


YX = sparseTrainValidation
CVsize = round(length(YX)/2):(length(YX)-1)
CVsize = (length(YX)-2):(length(YX)-1)



YX = lapply(trainValidation, function(x) x[, !..drop])

trainXPools = list()
validXPools = list()

trainYPools = list()
validYPools = list()
for (n in CVsize) {
  #Create training set
  set = YX[1:n]
  pooled = do.call(rbind, set)
  trainX = Matrix::sparse.model.matrix(returns ~ ., pooled)
  #trainXRegular = Matrix::model.matrix(returns ~ ., pooled)
  trainY = pooled$returns
  
  #Created validation set
  valid = YX[n + 1][[1]]
  validX = Matrix::sparse.model.matrix(returns ~ ., valid)
  #validXRegular = Matrix::model.matrix(returns ~ ., valid)
  validY = valid$returns
  
  #remove columns that arent in both data sets
  trainCols = trainX@Dimnames[[2]]
  validCols = validX@Dimnames[[2]]
  trainX = trainX[,  trainCols %in% validCols]
  validX = validX[,  validCols %in% trainCols]
  
  trainXPools = append(trainXPools, list(trainX))
  trainYPools = append(trainYPools, list(trainY))
  
  validXPools = append(validXPools, list(validX))
  validYPools = append(validYPools, list(valid$returns))
}




# sparseTrainPools = lapply(trainPools, function(x) {
#   Matrix::sparse.model.matrix(returns ~ ., x)
# })




# for (n in CVsize) {
#   set = YX[1:n]
#   pooled = do.call(rbind, set)
#   
#   #Create training set
#   trainX = Matrix::sparse.model.matrix(returns ~ ., pooled)
#   trainY = pooled$returns
#   
#   #Created validation set
#   valid = YX[n + 1][[1]]
#   validX = sparse.model.matrix(returns ~ ., valid)
#   validY = valid$returns
#   
#   #remove columns that arent in both data sets
#   validCols = validX@Dimnames[[2]]
#   trainCols = trainX@Dimnames[[2]]
#   trainX = trainX[,  trainCols %in% validCols]
#   validX = validX[,  validCols %in% trainCols]
# }




```

```{r OLS}

# OLSData = do.call(rbind, trainValidation)[, -c("fdateq")]
# 
# #check duplicates
# unique = c("gvkey", "datacqtr")
# #dups = OLSData[duplicated(OLSData[,..unique])]
# OLSDataUnique = OLSData[!duplicated(OLSData[,..unique])]
# 
# #n = names(OLSPanel)
# n = names(Filter(is.numeric, OLSDataUnique))
# exclude = c("returns", "gvkey", "datacqtr")
# f <- as.formula(paste("returns ~", paste(n[!n %in% exclude], collapse = " + ")))
# 
# OLSPanel = plm::pdata.frame(OLSDataUnique, index = c("gvkey", "datacqtr"))
# OLSmodel2 = plm::plm(f, effect="twoways", model= "within", data=OLSPanel)
# #OLSsummary = summary(OLSmodel2) #computationally singular -> Because of multicollinearity. No issue; just ignore the message
# #OLSsummary$r.squared
# 
# 
# 
# 
# 
# OLSError = c()
# for (n in 1:validationSize) {
#   trainX = trainXPools[[n]]
#   trainY = trainYPools[[n]]
#   validX = validXPools[[n]]
#   validY = validYPools[[n]]
#   
#   OLSmodel3 = glmnet(
#     x = trainX,
#     y = trainY,
#     standardize = TRUE,
#     lambda = 0
#   )
#   
#   OLSpred = predict(OLSmodel3, newx = validX)
#   OLSMSE = mean((OLSpred - validY) ^ 2)
#   OLSMAE =  mean(abs(OLSpred - validY))
#   
#   OLSError = rbind(OLSError, c("OLSMSE" = lassoMSE, "OLSMAE" = lassoMAE))
# }
# meanCV = apply(OLSError, 2, mean)
# meanCV



```


```{r LassoRidge}

# library(glmnet)
#Use function glmnet (from library glmnet) to perform linear regression with Lasso regularization
#alpha=1 is lasso.



validationSize = length(validXPools)
CVfunction = function(lambda, alpha) {
  lassoError = c()
  for (n in 1:validationSize) {
    trainX = trainXPools[[n]]
    trainY = trainYPools[[n]]
    validX = validXPools[[n]]
    validY = validYPools[[n]]
    
    lassoModel = glmnet(
      x = trainX,
      y = trainY,
      alpha = alpha,
      standardize = TRUE,
      lambda = lambda
    )

    lassoPred = predict(lassoModel, newx = validX)
    lassoMSE = mean((lassoPred - validY) ^ 2)
    lassoMAE = mean(abs(lassoPred - validY))
    
    lassoError = rbind(lassoError, c("lassoMSE" = lassoMSE, "lassoMAE" = lassoMAE))
  }
  meanCV = apply(lassoError, 2, mean)
  meanCV
}


grid = c(10^seq(3,-10, length=5), 0)


MSEMatrix = c()
for (alpha in 1:2) {
  for (lambda in grid) {
    lambdaCV = CVfunction(lambda)
    MSEMatrix = rbind(MSEMatrix,
                      c("lambda" = lambda, "alpha" = alpha, lambdaCV))
  }
}
MSEMatrix


```






```{r PCR}

# library(pls)
# #PCR Regression using function pcr (part of pls library)
# PCRfit=pcr(returns~., data=USData, scale=TRUE, subset=train, validation ="CV")
# 
# 
# 
# CVfunction = function(lambda, alpha) {
#   errorMatrix = c()
#   for (n in 1:validationSize) {
#     trainX = trainXPools[[n]]
#     trainY = trainYPools[[n]]
#     validX = validXPools[[n]]
#     validY = validYPools[[n]]
#     
#     train = cbind(trainX, trainY)
#     
#     PCRfit=pcr(returns~., data=train, scale=TRUE)
# 
#     lassoPred = predict(lassoModel, newx = validX)
#     lassoMSE = mean((lassoPred - validY) ^ 2)
#     lassoMAE = mean(abs(lassoPred - validY))
#     
#     lassoError = rbind(lassoError, c("lassoMSE" = lassoMSE, "lassoMAE" = lassoMAE))
#   }
#   meanCV = apply(lassoError, 2, mean)
#   meanCV
# }
# 
# pcrCV = CVfunction(lambda)
# 
# 
# 
# 
# 
# #Validation plot
# validationplot(PCRfit, val.type="MSEP")
# 
# #Scree plotting
# PVE=PCRfit$Xvar/PCRfit$Xtotvar
# plot(PVE, xlab="Principal Component", ylab="Proportion of Variance Explained", 
#      ylim=c(0,1), type="b")
# plot(cumsum(PVE), xlab="Principal Component", ylab="Cumulative Proportion of Variance Explained", 
#      ylim=c(0,1), type="b")


```




```{r Decision Trees}
# library(haven) #For importing Stata data
# library(tree) #For fitting trees
# library(randomForest) #For bagging and randomforests
# library(gbm) #For boosting


# #Fit a tree using function "tree" (in library "tree")
# treeROA = tree(roa_w~. , USData, subset=train)
# summary(treeROA)
# #treeROA
# 
# #Plot
# plot(treeROA); text(treeROA, pretty=0, cex=0.7, srt=25)
# 
# #MSE
# yhat=predict(treeROA, newdata=USData[-train,])
# ROAtest=USData[-train, "roa_w"]
# plot(yhat, ROAtest$roa_w, ylab = "roa_w")
# abline(0,1)
# mean((yhat-ROAtest$roa_w)^2)












#Bagging
# library(randomForest)
CVsize = (length(YX)-2):(length(YX)-1)

treefunction = function(p, nTree, nodes) {
  errorMatrix = c()
  for (n in CVsize) {
    set = YX[1:n]
    pooled = do.call(rbind, set)
    # trainX = stats::model.matrix(returns ~ ., pooled)
    # trainY = pooled$returns
    # 
    # #Created validation set
    valid = YX[n + 1][[1]]
    #validX = stats::model.matrix(returns ~ ., valid)
    validX = valid[,!"returns"]
    validY = valid$returns
    
    
    #remove columns that arent in both data sets
    trainCols = names(pooled)
    validCols = names(valid)
    pooled = pooled[,  (trainCols %in% validCols), with=FALSE]

    validXCols = names(validX)
    validX = validX[,  (validXCols %in% trainCols), with=FALSE]

    #remove columns that arent in both data sets
    # trainCols = trainX@Dimnames[[2]]
    # validCols = validX@Dimnames[[2]]
    # trainX = trainX[,  trainCols %in% validCols]
    # validX = validX[,  validCols %in% trainCols]
    
    # train = cbind(trainX, "returns"=trainY)
    # train = as.data.table(as.matrix(train))
    
    model = randomForest(
      returns ~ .,
      data = pooled,
      mtry = p,
      ntree = nTree,
      maxnodes = nodes,
      importance = TRUE
    )
    
    pred = predict(model, newx = validX) #something is not working here nrows(pred) is the same as nrows(pooled)
    MSE = mean((pred - validY) ^ 2)
    MAE = mean(abs(pred - validY))
    
    errorMatrix = rbind(errorMatrix, c("MSE" = MSE, "MAE" = MAE))
  }
  meanCV = apply(errorMatrix, 2, mean)
  meanCV
}



MSEMatrix = c()
numOfPreds = c(ncol(pooled)/3, ncol(pooled)-1)
nTrees = c(10, 15)
maxNodes = c(2, 3)
for (p in numOfPreds) {
  for (nTree in nTrees) {
    for (nodes in maxNodes) {
      CV = treefunction(p, nTree, nodes)
      MSEMatrix = rbind(MSEMatrix,
                        c(
                          "predictors" = p,
                          "trees" = nTree,
                          "nodes" = nodes,
                          CV
                        ))
    }
  }
}
MSEMatrix





# #Bagging using the function randomForest,
# #inside the randomForest library. When mtry is for all variables, it is bagging. 
# bagROA = randomForest(roa_w~., USData, subset=train, mtry=ncol(USData)-1, importance =TRUE)
# bagROA
# plot(bagROA)
# 
# #MSE
# yhat=predict(bagROA, newdata=USData[-train,])
# ROAtest=USData[-train, "roa_w"]
# plot(yhat, ROAtest$roa_w)
# abline(0,1)
# mean((yhat-ROAtest$roa_w)^2)










# #Random forests
# 
# set.seed(1)
# #Fit a random forest using the function randomForest,
# #inside the randomForest library. When mtry is less than
# #than all variables in the data, it is a random forests model
# forestROA = randomForest(roa_w~., USData, subset=train, importance = TRUE)
# 
# #Should be done on several different mtry
# #sqrt(ncol(USData))
# #ncol(USData)/3
# 
# forestROA
# plot(forestROA)
# 
# importance(forestROA)
# varImpPlot(forestROA)
# 
# #MSE
# yhat=predict(forestROA, newdata=USData[-train,])
# ROAtest=USData[-train, "roa_w"]
# plot(yhat, ROAtest$roa_w)
# abline(0,1)
# mean((yhat-ROAtest$roa_w)^2)







# #Boosting
# 
# set.seed(1)
# #Boosting using the "gbm" function inside the "gbm" library
# boostROA = gbm(roa_w~., USData[train,], distribution="gaussian", n.trees=500, interaction.depth=4, shrinkage = 0.1)
# #all or only training set? cv.folds? check help(gbm)
# #We use cross-validation to select B.
# 
# 
# boostROA
# summary(boostROA)
# plot(boostROA, i="profit_margin_w"); plot(boostROA, i="ebitda_w")
# 
# #MSE
# yhat=predict(boostROA, newdata=USData[-train,])
# ROAtest=USData[-train, "roa_w"]
# plot(yhat, ROAtest$roa_w)
# abline(0,1)
# mean((yhat-ROAtest$roa_w)^2)


```




```{r NNdata}

require(data.table)
drop = c("datacqtr", "fdateq")
YXNN = lapply(trainValidation, function(x) x[, !..drop])

trainXNN = list()
trainYNN = list()
validXNN = list()
validYNN = list()

CVsize = 5:6
for (n in CVsize) {
  #Create training set
  set = YXNN[1:n]
  pooled = do.call(rbind, set)
  pooled = Filter(is.numeric, pooled)
  trainX = stats::model.matrix(returns ~ ., pooled)
  trainX[,-1] = apply(trainX[,-1], MARGIN = 2, scale)
  trainY = pooled$returns
  #trainY = as.matrix(pooled[, "returns"])

  
  #Created validation set
  valid = YXNN[n + 1][[1]]
  valid = Filter(is.numeric, valid)
  validX = stats::model.matrix(returns ~ ., valid)
  validX[,-1] = apply(validX[,-1], MARGIN = 2, scale)
  validY = valid$returns
  #validY = as.matrix(valid[, "returns"])
  
  #remove columns that arent in both data sets
  trainCols = colnames(trainX)
  validCols = colnames(validX)
  trainX = trainX[,  trainCols %in% validCols]
  validX = validX[,  validCols %in% trainCols]
  
  trainXNN = append(trainXNN, list(trainX))
  trainYNN = append(trainYNN, list(trainY))
  validXNN = append(validXNN, list(validX))
  validYNN = append(validYNN, list(validY))
}

```




```{r NeuralNetwork}

library(torch)
library(luz) #High-level interface for torch
library(magrittr) #Contains pipes, e.g. %<>%

if(cuda_is_available()){
  device = torch_device("cuda")
} else{
  device = torch_device("cpu")
}
  
#torch_manual_seed(13)

NNfunction = function(type, layers, hidUnits, actvFunc, dropout, l1, l2, epo) {
  errorMatrix = c()
  
  if (type == "LSTM" || type == "GRU")
    source("src/LSTM.R")
  else if (type == "Forward")
    source("src/FeedForwardNN.R")
  else
    source("src/Linear.R")
  
  modnn <- modnn %>%
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_rmsprop, #optim_sgd #optim_rmsprop
    metrics = list(luz_metric_mae())
  ) %>%
    set_hparams(
      input_size = 86,
      layers = layers,
      type = type,
      hidUnits = hidUnits,
      dropout=dropout,
      actvFunc=actvFunc
    ) #3130 #inputSize
  
  
  for(n in 1:validationSize) {
    trainX = trainXNN[[n]]
    trainY = trainYNN[[n]]
    validX = validXNN[[n]]
    validY = validYNN[[n]]
    
    
    #Model Fitting
    
    #batch number seems to automatically be 32. 
    #dataloader_options: Options used when creating a dataloader. See torch::dataloader().
    #shuffle=TRUE by default for the training data and batch_size=32 by default.
    #It will error if not NULL and data is already a dataloader.
    fitted <- modnn %>%
      fit(
        data = list(trainX, trainY),
        valid_data = list(validX, validY),
        epochs = epo,
        verbose = TRUE
        #batch_size=100
        #dataloader_options = c(batch_size=100)
      )
    
    #MSE and MAE (The same ones as above)
    pred = predict(fitted, validX)
    MSE = mean((validY - pred) ^ 2)
    MAE = mean(abs(validY - pred))
    
    errorMatrix = rbind(errorMatrix, c("MSE" = MSE$item(), "MAE" = MAE$item()))
  }
  meanCV = apply(errorMatrix, 2, mean)
  meanCV
}


#Hyper-Parameters
modelType = c("Forward", "LSTM", "GRU")
hiddenLayers = c(2, 3)
hiddenUnits = c(2, 3)
activationFunc = c("relu", "sigmoid") #softmax, etc. https://keras.io/api/layers/activations/
dropOut = c(0.5, 0)
#regGrid = c(10^seq(3,-10, length=4), 0)
epochs = 1 #Just choose best model. => Run for 5 epochs, but if 3rd epoch is better, choose it
inputSize = 86 #ncol(trainXPools[[1]])


NNParams = expand.grid(
  stringsAsFactors = F,
  "type" = c("Forward"),
  "layers" = hiddenLayers,
  "hidUnits" = hiddenUnits,
  "actvFunc" = activationFunc,
  "dropout" = dropOut,
  "epo" = epochs
  # "l1" = regGrid,
  # "l2" = regGrid
)

RNNParams = expand.grid(
  stringsAsFactors = F,
  "type" = c("LSTM", "GRU"),
  "layers" = hiddenLayers,
  "hidUnits" = hiddenUnits,
  "actvFunc" = c("NA"),
  "dropout" = dropOut,
  "epo" = epochs
)

LinearParams = list(
  "type" = "Linear",
  "layers" = 0,
  "hidUnits" = 0,
  "actvFunc" = "NA",
  "dropout" = 0,
  "epo" = 1
)
hyperParams = rbind(LinearParams, NNParams, RNNParams)

validationSize = 2 #length(validXPools)
MSEMatrix = c()
HPsize = nrow(hyperParams)

for(n in 1:HPsize) {
  print("\n")
  cat("Model", n, "out of", HPsize)
  print("\n")
  
  HP = as.list(hyperParams[n,])
  CV = do.call(NNfunction, HP)
  MSEMatrix = rbind(MSEMatrix, c(HP, CV))
}
MSEMatrix
finalResults = as.data.frame(MSEMatrix)
finalResults[,-1] = apply(finalResults[,-1], MARGIN=2, FUN=as.numeric)
finalResults = finalResults[order(finalResults[,"MSE"]),]
finalResults <- apply(finalResults, 2, as.character)

write.csv2(finalResults, "Results/results3.csv", row.names = FALSE)



```


```{r Keras}


# library(tensorflow)
# library(keras)

# # #Model Creation
# # library(keras)
# inputSize = ncol(trainXPools[[1]])
# model <- keras_model_sequential()
# model %>%
#   layer_dense(units = hidUnits,
#               activation = 'actvFunc',
#               input_shape = inputSize) %>%
#   layer_activity_regularization(l1 = l1, l2 = l2) %>%
#   layer_dropout(rate = dropout)  %>%
#   layer_dense(units = 1)
#
# #Model Compilation
# model %>% compile(loss = 'mse',
#                   optimizer = 'rmsprop',
#                   metrics = 'mae')
#
# readline()




# #Model Fitting
# mymodel <- model %>%
#   fit(
#     trainX,
#     trainY,
#     epochs = epochs,
#     batch_size = 3, #32 default. Add as an extra hyper-parameter
#     validation_data = list(validX, validY)
#   )


# # #Prediction
# # model %>% evaluate(validX, validY)
# # pred <- model %>% predict(validX)
#
#
# # #Scatter Plot Original vs Predicted
# # plot(testtarget, pred)
# #
# # #Plot model error for each epoch
# # plot(mymodel)


```


```{r FamaFrenchData}

library(data.table)
# factorData <- 
#   list.files(path = "FactorData/", pattern = "*.csv") %>% 
#   purrr::map_df(~fread(.))


#Import factors
developedFactors=fread("FactorData/Developed_ex_US_5_Factors.csv")
developedMomentum=fread("FactorData/Developed_ex_US_MOM_Factor.csv")
developed = dplyr::inner_join(x = developedFactors, y = developedMomentum, by = "V1")

emergingFactors=fread("FactorData/Emerging_5_Factors.csv")
emergingMomentum=fread("FactorData/Emerging_MOM_Factor.csv")
emerging = dplyr::inner_join(x = emergingFactors, y = emergingMomentum, by = "V1")


allFactors = dplyr::inner_join(x = developed, y = emerging, by = "V1", suffix = c(".DM", ".EM"))


#allFactors[, `Mkt-RF.DM`+`Mkt-RF.EM`]

#allFactors[, (`Mkt-RF.DM`:WML.DM + `Mkt-RF.EM`:WML.EM)/2]


weightedFactors = (allFactors[, `Mkt-RF.DM`:WML.DM] +  allFactors[,`Mkt-RF.EM`:WML.EM])/2
weightedFactors = cbind(allFactors$V1, weightedFactors)
colnames(weightedFactors) = names(developed)


```


```{r QuarterlyFactors}
toQuarters = function(x) {
  ceiling(x %% 100 / 3)
}

toYears = function(x) {
  as.numeric(substr(x, 1, 4))
} 

toYQ = function(x) {
  Q=toQuarters(x)
  Y=signif(x, 4)
  Y+Q
} 

library(data.table)
factorsWithYQ = cbind(weightedFactors, weightedFactors[, .("Y" = toYears(V1),
                                                           "Q" = toQuarters(V1),
                                                           "YQ" = toYQ(V1))])



# geom = function(x) {
#   exp(mean(log(1+x/100))) - 1
# }

factorProd = function(x) {
  decimal = prod((x/100)+1) - 1
  decimal*100
}


# quarterlyFactors = factorsWithYQ[, .(
#   Mkt = factorProd(`Mkt-RF`),
#   SMB = factorProd(SMB),
#   HML = factorProd(HML),
#   RMW = factorProd(RMW),
#   CMA = factorProd(CMA)
# ), by = YQ]
# 
# 
# quarterlyFactors = factorsWithYQ[, .(
#   Mkt = factorProd(`Mkt-RF`),
#   SMB = factorProd(SMB),
#   HML = factorProd(HML),
#   RMW = factorProd(RMW),
#   CMA = factorProd(CMA)
# ), by = .(Y,Q)]


YQ = zoo::as.yearqtr(as.character(weightedFactors$V1), format = "%Y%m")
factorsWithYQ = cbind(weightedFactors, YQ)

quarterlyFactors = factorsWithYQ[, .(
  Mkt = factorProd(`Mkt-RF`),
  SMB = factorProd(SMB),
  HML = factorProd(HML),
  RMW = factorProd(RMW),
  CMA = factorProd(CMA),
  WML = factorProd(WML),
  RF  = factorProd(RF)
), by = YQ]



write.csv(quarterlyFactors, "FactorData/quarterlyFactors.csv", row.names = FALSE)


```



```{r LoadKerasModelFromPython}

fs::dir_tree("Results/BestKerasModel5")

#library(tensorflow)
library(keras)


model <- keras::load_model_tf('Results/BestKerasModel5')

# Check its architecture
summary(model)




# #Model Fitting
# mymodel <- model %>%
#   fit(
#     trainX,
#     trainY,
#     epochs = epochs,
#     batch_size = 32, #32 default. Add as an extra hyper-parameter
#     validation_data = list(validX, validY)
#   )




#   pooled = do.call(rbind, trainXNN)
#   pooled = do.call(rbind, trainYNN)
#   pooled = do.call(rbind, validXNN)
#   pooled = do.call(rbind, validYNN)


#calculate variances
  
  
  


```

```{r ExportModelComparisonTable}

# modelComparisonTable = data.table::fread("Results/ModelComparison.csv")
# 
# 
# write.csv(modelComparisonTable, "Results/ModelComparisonTableInR.csv")


```


```{r Predict}
require(data.table)
#ScaledWinsRatioData <- fread("Data/ScaledWinsRatioData.csv")


library(magrittr)
library(keras) #Contains "keras_model_sequential" function




testSet = YXfactor[YXfactor$Split=="Test",-c("MktCap")]
#testSet$datacqtr = factor(testSet$datacqtr)
#testSet$datacqtr = as.numeric(testSet$datacqtr)
#testSetNumericData = Filter(is.numeric, testSet[, -c("gvkey", "fdateq", "MktCap")])
#sum(is.na(finalNumericData))
#testSetNumericData = na.omit(testSetNumericData)

#testSetQuarters = split(testSetNumericData, testSetNumericData$datacqtr)
testSetQuarters = split(testSet, testSet$datacqtr)
testSet = testSetQuarters

#Prediction
allPreds = list(); m = 1
for(n in testSet){
  
  cat("Model number", m)
  pooled = Filter(function(x) {is.numeric(x)}, n) #|is.factor(x)
  
  pooled = na.omit(pooled)
  testX = pooled[, c("datacqtr"):=NULL]  #c("gvkey", "fdateq", 
  testX = stats::model.matrix(returns ~ ., testX)[,-1]
  #testX = apply(testX, MARGIN = 2, scale)
  testY = pooled$returns
  
  
  # testX = n$returns
  # testY = n$returns
  model %>% evaluate(testX, testY)
  pred <- model %>% predict(testX)
  
  firmPreds = cbind(n[,c("gvkey", "fdateq", "datacqtr")], pred)
  allPreds = c(allPreds, list(firmPreds))
  m = m + 1
}



#Scatter Plot Original vs Predicted
plot(testY, pred, log = "x")

#Plot model error for each epoch
#plot(model)


```

```{r TangencyPortfolio}

quarterlyFactors=fread("FactorData/quarterlyFactors.csv")



#Extract correct risk-free return
#cqtr = max(firmPreds$datacqtr)

cqtr = lapply(allPreds, function(x) max(x$datacqtr))
cqtr = lapply(cqtr, function(x) zoo::as.yearqtr(x, format = "%YQ%q"))

# dateTime = as.POSIXct(cqtr, format="%Y%m")
# yearMonth = as.numeric(format(dateTime, format="%Y%m"))
# yearMonth = as.numeric(format(dateTime, format="%Y%m"))


quarterlyFactors$YQ = zoo::as.yearqtr(quarterlyFactors$YQ, format = "%Y Q%q")
rf0 = quarterlyFactors[YQ %in% cqtr]$RF



trainValidation = YXfactor[YXfactor$Split!="Test",-c("MktCap")]
pooledForVariance = trainValidation[, c("gvkey", "datacqtr", "returns")]

# #For creating a covariance matrix
# pooledForVariance = do.call(rbind, trainValidation)
# pooledForVariance = as.data.table(pooledForVariance[, c("gvkey", "datacqtr", "returns")])
# # pooledForVariance = pooledForVariance[gvkey %in% firmPreds$gvkey,]
# #
# # returnMatrix<-dcast.data.table(data=pooledForVariance, datacqtr~gvkey)
# # dim(returnMatrix[,!"datacqtr"])
# # length(as.matrix(returnMatrix))
# # sum(is.na(returnMatrix))
# #
# # V = cov(returnMatrix[, !"datacqtr"])


# a = dcast.data.table(data=pooledForVariance, datacqtr~gvkey, value.var="returns")
# b = duplicated(pooledForVariance[,-c(returns)])
# d = pooledForVariance[b,]
# sum(duplicated(d[,-c(returns)]))


pooledForVariance = pooledForVariance[, "returns" := mean(returns, na.rm = T), by = c("gvkey", "datacqtr")]
pooledForVariance = unique(pooledForVariance)

pooledForVarianceList = lapply(allPreds, function(x) pooledForVariance[gvkey %in% x$gvkey,])
returnMatrixList = lapply(pooledForVarianceList, function(x) dcast.data.table(data=x, datacqtr~gvkey, value.var="returns"))
VList = lapply(returnMatrixList, function(x) cov(x[, !"datacqtr"], use="pairwise.complete.obs"))
#VList = lapply(returnMatrixList, function(x) cov(x[, !"datacqtr"], use="complete.obs"))



# V=VList[[10]]
# View(V)
# colSums(is.na(V))
# rowSums(is.na(V))

VListLessNA = lapply(VList, function(x) x[rowSums(is.na(x)) != ncol(x), colSums(is.na(x)) != nrow(x)])
VListLessNA = lapply(VListLessNA, function(x) x[rowSums(is.na(x)) < ncol(x)/5+1, colSums(is.na(x)) < nrow(x)/5+1])
VListLessNA = lapply(VListLessNA, function(x) x[rowSums(is.na(x)) == 0, colSums(is.na(x)) == 0])
# V=VListLessNA[[10]]
# View(V)




# RwithTickers = firmPreds[gvkey %in% pooledForVariance$gvkey,]
# R = RwithTickers$V1

# RwithTickers = lapply(allPreds, function(x) x[gvkey %in% pooledForVariance$gvkey,])
# R = lapply(RwithTickers, function(x) x$V1)

R = list()
RwithTickers = list()
for(n in 1:length(allPreds)){
  RwithTicker = allPreds[[n]][allPreds[[n]]$gvkey %in% colnames(VListLessNA[[n]]), ]
  R = c(R, list(RwithTicker$V1))
  RwithTickers = c(RwithTickers, list(RwithTicker))
}


# #RAndRF = list(R=R, rf0=rf0)
# 
# rfMatrix=list()
# for(n in 1:length(R)){
#   rfRep = rep(rf0[n], length(R[[n]]))
#   rfMatrix = c(rfMatrix, list(rfRep))
# }
# 
# rfMatr = do.call(cbind, rfMatrix)
# RMatr = do.call(cbind, R)
# 
# excessReturn = RMatr - rfMatr
# 
# 


excessReturn=list()
for(n in 1:length(rf0)){
  eR = R[[n]] - rf0[n]
  excessReturn = c(excessReturn, list(eR))
}



# length(V)
# sum(is.na(V))
# dim(V)
# isSymmetric(V)
# 
# #doubleChecking if PSD with a ready made package
# #matrixcalc::is.positive.semi.definite(V)
# 
# 
# V=VList[[10]]
# #packages for taking inverses of matrices
# inverse = Rfast::spdinv(V)
# inverse = mnormt::pd.solve(V)
# inverse = chol2inv(chol(V))
# inverse = solve(V)
# inverse = MASS::ginv(V)
# inverse = matlib::inv(V)
# View(V[1:10,1:10])
# 
# 
# length(inverse)
# sum(is.na(inverse))
# dim(inverse)
# isSymmetric(inverse)
# #View(inverse[1:10,1:10])
# 
# #Tangency portfolio
# numerator = inverse %*% (R-rf)
# denominator = rep(1, nrow(inverse)) %*% inverse %*% (R-rf)
# weights <- numerator / denominator[1]   #dividing the numerator by denominator gives us the weights
# sum(weights)
# 
# 
# portfolio = data.table("gvkey" = RwithTickers$gvkey, "date" = yearMonth, "weight" = c(weights))
# 
# 
# testYwithKey = pooled[,c("gvkey", "returns")]
# portfolioWithRealized = merge(portfolio, testYwithKey, by = "gvkey") 
# sum(portfolioWithRealized$weight)



#Invert
invList = lapply(VListLessNA, MASS::ginv)
#invList = lapply(VList, Rfast::spdinv)

VList=VListLessNA

#Equivalent solution with lists
weights=list()
for(n in 1:length(excessReturn)) {
  numerator = VList[[n]] %*% excessReturn[[n]]
  denominator = rep(1, nrow(VList[[n]])) %*% VList[[n]] %*% excessReturn[[n]]
  weight = numerator / denominator[1]
  weights = c(weights, list(weight))
}

#Check
unlist(lapply(weights, sum))



portfolios=list()
for(n in 1:length(weights)) {
  port = data.table("gvkey" = RwithTickers[[n]]$gvkey, "date" = cqtr[[n]], "weight" = c(weights[[n]]))
  portfolios = c(portfolios, list(port))
}


#testSet = testSetQuarters #From earlier in the script: TrainTestSplit_QuarterlySplit
portfolioWithRealizedList = list()
for(n in 1:length(weights)) {
  
  #cat("Model number", m)
  set = testSet[[n]]
  #set = Filter(is.numeric, set)
  set = na.omit(set)
  testYwithKey = set[, c("gvkey", "returns")]
  portfolioWithRealized = merge(portfolios[[n]], testYwithKey, by = "gvkey")
  portfolioWithRealizedList = c(portfolioWithRealizedList, list(portfolioWithRealized))
  
  sumN = sum(portfolioWithRealized$weight)
  print(sumN)
}


```




```{r AbnormalReturns}



# portfolioWithRealized$portfolioReturn = portfolioWithRealized$weight*portfolioWithRealized$returns
# portfolioReturn = sum(portfolioWithRealized$portfolioReturn)
# date = max(portfolioWithRealized$date)
# 
# portfolioPerformance = cbind("r"=portfolioReturn, "date"=cqtr)
# 
# 
# #factorsWithReturns = cbind(factorsWithYQ, portfolioWithRealized)
# factorsWithReturns = merge(portfolioPerformance, quarterlyFactors, by.x = "date", by.y="YQ", all = TRUE)
# 
# 
# factorsWithReturns$excessReturn = factorsWithReturns$r - factorsWithReturns$RF
# f = excessReturn ~ Mkt+SMB+HML+RMW+CMA+WML
# 
# lm(f, data=factorsWithReturns)






#Using lists
realizedPortfolios = lapply(portfolioWithRealizedList, function(x) x$weight * x$returns)
portfolioReturn = lapply(realizedPortfolios, function(x) sum(x))


dateList = lapply(portfolioWithRealizedList, function(x) max(x$date))
portfolioPerformance = cbind("Excess Return"=unlist(portfolioReturn), "date"=unlist(dateList))
factorsWithReturns = merge(portfolioPerformance, quarterlyFactors, by.x = "date", by.y="YQ", all = TRUE)


f = `Excess Return` ~ Mkt+SMB+HML+RMW+CMA+WML
FamaFrenchSix = lm(f, data=factorsWithReturns)
summary(FamaFrenchSix)

```


```{r FamaFrenchReporting}


#winsorize
#winData = Winsorize(data, probs = c(0.01, 0.99), na.rm=T)



#maybe add diagnostic tests
residuals = c(residuals(FamaFrenchSix))
JB = moments::jarque.test(residuals)
BP = lmtest::bptest(FamaFrenchSix, data=factorsWithReturns)
BG = lmtest::bgtest(FamaFrenchSix, order=7, data=factorsWithReturns)

VIF = data.frame(round(car::vif(FamaFrenchSix), 2))
colnames(VIF)=c("VIF")
write.csv(VIF, "Results/VIF2.csv")



#Unit root for panel data
finalNumericData = Filter(is.numeric, YXfactor[, -c("gvkey", "fdateq")])
#sum(is.na(finalNumericData))
finalNumericData = na.omit(finalNumericData)
compustatStationarity = plm::purtest(finalNumericData)




# #Unit root
# fUnitRoots::adfTest(factorsWithReturns$excessReturn, lags = 1, type = c("nc"), title = NULL, description = NULL)


# #Slumpmässig eller fasteffekt (Hausman)
# phtest(firmRandom,firmFixed)
# summary(firmFixedTwo) #bättre
# 
# #Slump eller pooled
# plmtest(factorsWithReturns, type="bp")
# 
# #Fast eller pooled
# plm::pFtest(firmFixedTwo,firmPooledTwo)
# lmtest::waldtest(firmFixedTwo,firmPooledTwo)
# plm::pwaldtest(firmFixed,firmPooled)



#Compile the tests
diagnostics = rbind(c(JB$p.value, JB$statistic),
                    c(BP$p.value, BP$statistic),
                    c(BG$p.value, BG$statistic))
diagnostics = round(diagnostics,3)

diagnostics = list(c("Jarque-Bera", round(JB$p.value,3),JB$statistic),
                   c("Breusch-Pagan", round(BP$p.value,3), BP$statistic),
                   c("Breusch-Godfrey", round(BG$p.value,3), BG$statistic))



#robust errors: HAC?
robustErrors = vcovHC(winsMargin,method="arellano")


#Export
stargazer::stargazer(FamaFrenchSix,
                     type="text",
                     report=("vc*p"),
                     add.lines = diagnostics,
                     out = "Results/FamaFrenchSixFactors2.html"
                     )




```



```{r RNN Keras}


model <- keras_model_sequential() 
model %>% 
         layer_embedding(input_dim = 500, output_dim = 32) %>%
         layer_simple_rnn(units = 32) %>%  
         layer_dense(units = 1, activation = "sigmoid")

```


```{r LSTM Keras}

model <- keras_model_sequential() 
model %>% 
         layer_embedding(input_dim = 500, output_dim = 32) %>%
         layer_lstm(units = 32) %>%  
         layer_dense(units = 1, activation = "sigmoid")

```


```{r ForecastPlot}



#dccForecast = dccforecast(triGarch, n.roll = triForecastLength)

```




```{r OutputSummary}

#For nicer output (printing)
comparisonMatrix = matrix(c(NN1MSE, NN1MAE, 
                            lassoMSE, lassoMAE), 
                          ncol=2,
                          byrow=TRUE)
comparisonMatrix = round(comparisonMatrix, 4)

colnames(comparisonMatrix) = c("MSE", "MAE")
rownames(comparisonMatrix) = c("Neural network with one hidden layer", 
                               "Linear regression with Lasso regularisation")
print(comparisonMatrix)


```




